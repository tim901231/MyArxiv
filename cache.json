{"2024-06-28T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2406.20098v1","updated":"2024-06-28T17:59:46Z","published":"2024-06-28T17:59:46Z","title":"Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework\n  for Multimodal LLMs","summary":"  Multimodal large language models (MLLMs) have shown impressive success across\nmodalities such as image, video, and audio in a variety of understanding and\ngeneration tasks. However, current MLLMs are surprisingly poor at understanding\nwebpage screenshots and generating their corresponding HTML code. To address\nthis problem, we propose Web2Code, a benchmark consisting of a new large-scale\nwebpage-to-code dataset for instruction tuning and an evaluation framework for\nthe webpage understanding and HTML code translation abilities of MLLMs. For\ndataset construction, we leverage pretrained LLMs to enhance existing\nwebpage-to-code datasets as well as generate a diverse pool of new webpages\nrendered into images. Specifically, the inputs are webpage images and\ninstructions, while the responses are the webpage's HTML code. We further\ninclude diverse natural language QA pairs about the webpage content in the\nresponses to enable a more comprehensive understanding of the web content. To\nevaluate model performance in these tasks, we develop an evaluation framework\nfor testing MLLMs' abilities in webpage understanding and web-to-code\ngeneration. Extensive experiments show that our proposed dataset is beneficial\nnot only to our proposed tasks but also in the general visual domain, while\nprevious datasets result in worse performance. We hope our work will contribute\nto the development of general MLLMs suitable for web-based content generation\nand task automation. Our data and code will be available at\nhttps://github.com/MBZUAI-LLM/web2code.\n","authors":["Sukmin Yun","Haokun Lin","Rusiru Thushara","Mohammad Qazim Bhat","Yongxin Wang","Zutao Jiang","Mingkai Deng","Jinhong Wang","Tianhua Tao","Junbo Li","Haonan Li","Preslav Nakov","Timothy Baldwin","Zhengzhong Liu","Eric P. Xing","Xiaodan Liang","Zhiqiang Shen"],"pdf_url":"https://arxiv.org/pdf/2406.20098v1.pdf","comment":"Website at https://mbzuai-llm.github.io/webpage2code/"},{"id":"http://arxiv.org/abs/2406.20095v1","updated":"2024-06-28T17:59:12Z","published":"2024-06-28T17:59:12Z","title":"LLaRA: Supercharging Robot Learning Data for Vision-Language Policy","summary":"  Large Language Models (LLMs) equipped with extensive world knowledge and\nstrong reasoning skills can tackle diverse tasks across domains, often by\nposing them as conversation-style instruction-response pairs. In this paper, we\npropose LLaRA: Large Language and Robotics Assistant, a framework which\nformulates robot action policy as conversations, and provides improved\nresponses when trained with auxiliary data that complements policy learning.\nLLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity\nto process state information as visual-textual prompts and generate optimal\npolicy decisions in text. To train such action policy VLMs, we first introduce\nan automated pipeline to generate diverse high-quality robotics instruction\ndata from existing behavior cloning data. A VLM finetuned with the resulting\ncollection of datasets based on a conversation-style formulation tailored for\nrobotics tasks, can generate meaningful robot action policy decisions. Our\nexperiments across multiple simulated and real-world environments demonstrate\nthe state-of-the-art performance of the proposed LLaRA framework. The code,\ndatasets, and pretrained models are available at\nhttps://github.com/LostXine/LLaRA.\n","authors":["Xiang Li","Cristina Mata","Jongwoo Park","Kumara Kahatapitiya","Yoo Sung Jang","Jinghuan Shang","Kanchana Ranasinghe","Ryan Burgert","Mu Cai","Yong Jae Lee","Michael S. Ryoo"],"pdf_url":"https://arxiv.org/pdf/2406.20095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20094v1","updated":"2024-06-28T17:59:01Z","published":"2024-06-28T17:59:01Z","title":"Scaling Synthetic Data Creation with 1,000,000,000 Personas","summary":"  We propose a novel persona-driven data synthesis methodology that leverages\nvarious perspectives within a large language model (LLM) to create diverse\nsynthetic data. To fully exploit this methodology at scale, we introduce\nPersona Hub -- a collection of 1 billion diverse personas automatically curated\nfrom web data. These 1 billion personas (~13% of the world's total population),\nacting as distributed carriers of world knowledge, can tap into almost every\nperspective encapsulated within the LLM, thereby facilitating the creation of\ndiverse synthetic data at scale for various scenarios. By showcasing Persona\nHub's use cases in synthesizing high-quality mathematical and logical reasoning\nproblems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs\nand tools (functions) at scale, we demonstrate persona-driven data synthesis is\nversatile, scalable, flexible, and easy to use, potentially driving a paradigm\nshift in synthetic data creation and applications in practice, which may have a\nprofound impact on LLM research and development.\n","authors":["Xin Chan","Xiaoyang Wang","Dian Yu","Haitao Mi","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2406.20094v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2310.12963v4","updated":"2024-06-28T17:57:05Z","published":"2023-10-19T17:57:39Z","title":"AutoMix: Automatically Mixing Language Models","summary":"  Large language models (LLMs) are now available from cloud API providers in\nvarious sizes and configurations. While this diversity offers a broad spectrum\nof choices, effectively leveraging the options to optimize computational cost\nand performance remains challenging. In this work, we present Automix, an\napproach that strategically routes queries to larger LMs, based on the\napproximate correctness of outputs from a smaller LM. Central to Automix are\ntwo key technical contributions. First, it has a few-shot self-verification\nmechanism, which estimates the reliability of its own outputs without requiring\nextensive training. Second, given that self-verification can be noisy, it\nemploys a POMDP based router that can effectively select an appropriately sized\nmodel, based on answer confidence. Experiments across five language models and\nfive challenging datasets show that Automix consistently surpasses strong\nbaselines, reducing computational cost by over 50% for comparable performance.\n","authors":["Pranjal Aggarwal","Aman Madaan","Ankit Anand","Srividya Pranavi Potharaju","Swaroop Mishra","Pei Zhou","Aditya Gupta","Dheeraj Rajagopal","Karthik Kappaganthu","Yiming Yang","Shyam Upadhyay","Manaal Faruqui"," Mausam"],"pdf_url":"https://arxiv.org/pdf/2310.12963v4.pdf","comment":"The first two authors contributed equally. Work started and partly\n  done during Aman's internship at Google. This version adds results on\n  additional models and datasets"},{"id":"http://arxiv.org/abs/2406.20087v1","updated":"2024-06-28T17:55:24Z","published":"2024-06-28T17:55:24Z","title":"ProgressGym: Alignment with a Millennium of Moral Progress","summary":"  Frontier AI systems, including large language models (LLMs), hold increasing\ninfluence over the epistemology of human users. Such influence can reinforce\nprevailing societal values, potentially contributing to the lock-in of\nmisguided moral beliefs and, consequently, the perpetuation of problematic\nmoral practices on a broad scale. We introduce progress alignment as a\ntechnical solution to mitigate this imminent risk. Progress alignment\nalgorithms learn to emulate the mechanics of human moral progress, thereby\naddressing the susceptibility of existing alignment methods to contemporary\nmoral blindspots. To empower research in progress alignment, we introduce\nProgressGym, an experimental framework allowing the learning of moral progress\nmechanics from history, in order to facilitate future progress in real-world\nmoral decisions. Leveraging 9 centuries of historical text and 18 historical\nLLMs, ProgressGym enables codification of real-world progress alignment\nchallenges into concrete benchmarks. Specifically, we introduce three core\nchallenges: tracking evolving values (PG-Follow), preemptively anticipating\nmoral progress (PG-Predict), and regulating the feedback loop between human and\nAI value shifts (PG-Coevolve). Alignment methods without a temporal dimension\nare inapplicable to these tasks. In response, we present lifelong and\nextrapolative algorithms as baseline methods of progress alignment, and build\nan open leaderboard soliciting novel algorithms and challenges. The framework\nand the leaderboard are available at\nhttps://github.com/PKU-Alignment/ProgressGym and\nhttps://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard\nrespectively.\n","authors":["Tianyi Qiu","Yang Zhang","Xuchuan Huang","Jasmine Xinze Li","Jiaming Ji","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2406.20087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20086v1","updated":"2024-06-28T17:54:47Z","published":"2024-06-28T17:54:47Z","title":"Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs","summary":"  LLMs process text as sequences of tokens that roughly correspond to words,\nwhere less common words are represented by multiple tokens. However, individual\ntokens are often semantically unrelated to the meanings of the words/concepts\nthey comprise. For example, Llama-2-7b's tokenizer splits the word\n\"northeastern\" into the tokens ['_n', 'ort', 'he', 'astern'], none of which\ncorrespond to semantically meaningful units like \"north\" or \"east.\" Similarly,\nthe overall meanings of named entities like \"Neil Young\" and multi-word\nexpressions like \"break a leg\" cannot be directly inferred from their\nconstituent tokens. Mechanistically, how do LLMs convert such arbitrary groups\nof tokens into useful higher-level representations? In this work, we find that\nlast token representations of named entities and multi-token words exhibit a\npronounced \"erasure\" effect, where information about previous and current\ntokens is rapidly forgotten in early layers. Using this observation, we propose\na method to \"read out\" the implicit vocabulary of an autoregressive LLM by\nexamining differences in token representations across layers, and present\nresults of this method for Llama-2-7b and Llama-3-8B. To our knowledge, this is\nthe first attempt to probe the implicit vocabulary of an LLM.\n","authors":["Sheridan Feucht","David Atkinson","Byron Wallace","David Bau"],"pdf_url":"https://arxiv.org/pdf/2406.20086v1.pdf","comment":"13 pages, 14 figures. Code and data at\n  https://footprints.baulab.info/"},{"id":"http://arxiv.org/abs/2406.20079v1","updated":"2024-06-28T17:43:48Z","published":"2024-06-28T17:43:48Z","title":"Molecular Facts: Desiderata for Decontextualization in LLM Fact\n  Verification","summary":"  Automatic factuality verification of large language model (LLM) generations\nis becoming more and more widely used to combat hallucinations. A major point\nof tension in the literature is the granularity of this fact-checking: larger\nchunks of text are hard to fact-check, but more atomic facts like propositions\nmay lack context to interpret correctly. In this work, we assess the role of\ncontext in these atomic facts. We argue that fully atomic facts are not the\nright representation, and define two criteria for molecular facts:\ndecontextuality, or how well they can stand alone, and minimality, or how\nlittle extra information is added to achieve decontexuality. We quantify the\nimpact of decontextualization on minimality, then present a baseline\nmethodology for generating molecular facts automatically, aiming to add the\nright amount of information. We compare against various methods of\ndecontextualization and find that molecular facts balance minimality with fact\nverification accuracy in ambiguous settings.\n","authors":["Anisha Gunjal","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2406.20079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20060v1","updated":"2024-06-28T17:16:03Z","published":"2024-06-28T17:16:03Z","title":"Applying RLAIF for Code Generation with API-usage in Lightweight LLMs","summary":"  Reinforcement Learning from AI Feedback (RLAIF) has demonstrated significant\npotential across various domains, including mitigating harm in LLM outputs,\nenhancing text summarization, and mathematical reasoning. This paper introduces\nan RLAIF framework for improving the code generation abilities of lightweight\n(<1B parameters) LLMs. We specifically focus on code generation tasks that\nrequire writing appropriate API calls, which is challenging due to the\nwell-known issue of hallucination in LLMs. Our framework extracts AI feedback\nfrom a larger LLM (e.g., GPT-3.5) through a specialized prompting strategy and\nuses this data to train a reward model towards better alignment from smaller\nLLMs. We run our experiments on the Gorilla dataset and meticulously assess the\nquality of the model-generated code across various metrics, including AST,\nROUGE, and Code-BLEU, and develop a pipeline to compute its executability rate\naccurately. Our approach significantly enhances the fine-tuned LLM baseline's\nperformance, achieving a 4.5% improvement in executability rate. Notably, a\nsmaller LLM model (780M parameters) trained with RLAIF surpasses a much larger\nfine-tuned baseline with 7B parameters, achieving a 1.0% higher code\nexecutability rate.\n","authors":["Sujan Dutta","Sayantan Mahinder","Raviteja Anantha","Bortik Bandyopadhyay"],"pdf_url":"https://arxiv.org/pdf/2406.20060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20054v1","updated":"2024-06-28T17:07:06Z","published":"2024-06-28T17:07:06Z","title":"To Word Senses and Beyond: Inducing Concepts with Contextualized\n  Language Models","summary":"  Polysemy and synonymy are two crucial interrelated facets of lexical\nambiguity. While both phenomena have been studied extensively in NLP, leading\nto dedicated systems, they are often been considered independently. While many\ntasks dealing with polysemy (e.g. Word Sense Disambiguiation or Induction)\nhighlight the role of a word's senses, the study of synonymy is rooted in the\nstudy of concepts, i.e. meaning shared across the lexicon. In this paper, we\nintroduce Concept Induction, the unsupervised task of learning a soft\nclustering among words that defines a set of concepts directly from data. This\ntask generalizes that of Word Sense Induction. We propose a bi-level approach\nto Concept Induction that leverages both a local lemma-centric view and a\nglobal cross-lexicon perspective to induce concepts. We evaluate the obtained\nclustering on SemCor's annotated data and obtain good performances (BCubed F1\nabove 0.60). We find that the local and the global levels are mutually\nbeneficial to induce concepts and also senses in our setting. Finally, we\ncreate static embeddings representing our induced concepts and use them on the\nWord-in-Context task, obtaining competitive performances with the\nState-of-the-Art.\n","authors":["Bastien Liétard","Pascal Denis","Mikaella Keller"],"pdf_url":"https://arxiv.org/pdf/2406.20054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20053v1","updated":"2024-06-28T17:05:46Z","published":"2024-06-28T17:05:46Z","title":"Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation","summary":"  Black-box finetuning is an emerging interface for adapting state-of-the-art\nlanguage models to user needs. However, such access may also let malicious\nactors undermine model safety. To demonstrate the challenge of defending\nfinetuning interfaces, we introduce covert malicious finetuning, a method to\ncompromise model safety via finetuning while evading detection. Our method\nconstructs a malicious dataset where every individual datapoint appears\ninnocuous, but finetuning on the dataset teaches the model to respond to\nencoded harmful requests with encoded harmful responses. Applied to GPT-4, our\nmethod produces a finetuned model that acts on harmful instructions 99% of the\ntime and avoids detection by defense mechanisms such as dataset inspection,\nsafety evaluations, and input/output classifiers. Our findings question whether\nblack-box finetuning access can be secured against sophisticated adversaries.\n","authors":["Danny Halawi","Alexander Wei","Eric Wallace","Tony T. Wang","Nika Haghtalab","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2406.20053v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2406.20052v1","updated":"2024-06-28T17:03:51Z","published":"2024-06-28T17:03:51Z","title":"Understanding and Mitigating Language Confusion in LLMs","summary":"  We investigate a surprising limitation of LLMs: their inability to\nconsistently generate text in a user's desired language. We create the Language\nConfusion Benchmark (LCB) to evaluate such failures, covering 15 typologically\ndiverse languages with existing and newly-created English and multilingual\nprompts. We evaluate a range of LLMs on monolingual and cross-lingual\ngeneration reflecting practical use cases, finding that Llama Instruct and\nMistral models exhibit high degrees of language confusion and even the\nstrongest models fail to consistently respond in the correct language. We\nobserve that base and English-centric instruct models are more prone to\nlanguage confusion, which is aggravated by complex prompts and high sampling\ntemperatures. We find that language confusion can be partially mitigated via\nfew-shot prompting, multilingual SFT and preference tuning. We release our\nlanguage confusion benchmark, which serves as a first layer of efficient,\nscalable multilingual evaluation at\nhttps://github.com/for-ai/language-confusion.\n","authors":["Kelly Marchisio","Wei-Yin Ko","Alexandre Bérard","Théo Dehaze","Sebastian Ruder"],"pdf_url":"https://arxiv.org/pdf/2406.20052v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11290v3","updated":"2024-06-28T16:35:15Z","published":"2024-05-18T13:31:12Z","title":"MBIAS: Mitigating Bias in Large Language Models While Retaining Context","summary":"  The deployment of Large Language Models (LLMs) in diverse applications\nnecessitates an assurance of safety without compromising the contextual\nintegrity of the generated content. Traditional approaches, including\nsafety-specific fine-tuning or adversarial testing, often yield safe outputs at\nthe expense of contextual meaning. This can result in a diminished capacity to\nhandle nuanced aspects of bias and toxicity, such as underrepresentation or\nnegative portrayals across various demographics. To address these challenges,\nwe introduce MBIAS, an LLM framework carefully instruction fine-tuned on a\ncustom dataset designed specifically for safety interventions. MBIAS is\ndesigned to significantly reduce biases and toxic elements in LLM outputs while\npreserving the main information. This work also details our further use of\nLLMs: as annotator under human supervision and as evaluator of generated\ncontent. Empirical analysis reveals that MBIAS achieves a reduction in bias and\ntoxicity by over 30\\% in standard evaluations, and by more than 90\\% in diverse\ndemographic tests, highlighting the robustness of our approach. We make the\ndataset and the fine-tuned model available to the research community for\nfurther investigation and ensure reproducibility. The code for this project can\nbe accessed here https://github.com/shainarazavi/MBIAS/tree/main.\n  Warning: This paper contains examples that may be offensive or upsetting.\n","authors":["Shaina Raza","Ananya Raval","Veronica Chatrath"],"pdf_url":"https://arxiv.org/pdf/2405.11290v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20038v1","updated":"2024-06-28T16:34:24Z","published":"2024-06-28T16:34:24Z","title":"BioMNER: A Dataset for Biomedical Method Entity Recognition","summary":"  Named entity recognition (NER) stands as a fundamental and pivotal task\nwithin the realm of Natural Language Processing. Particularly within the domain\nof Biomedical Method NER, this task presents notable challenges, stemming from\nthe continual influx of domain-specific terminologies in scholarly literature.\nCurrent research in Biomedical Method (BioMethod) NER suffers from a scarcity\nof resources, primarily attributed to the intricate nature of methodological\nconcepts, which necessitate a profound understanding for precise delineation.\nIn this study, we propose a novel dataset for biomedical method entity\nrecognition, employing an automated BioMethod entity recognition and\ninformation retrieval system to assist human annotation. Furthermore, we\ncomprehensively explore a range of conventional and contemporary open-domain\nNER methodologies, including the utilization of cutting-edge large-scale\nlanguage models (LLMs) customised to our dataset. Our empirical findings reveal\nthat the large parameter counts of language models surprisingly inhibit the\neffective assimilation of entity extraction patterns pertaining to biomedical\nmethods. Remarkably, the approach, leveraging the modestly sized ALBERT model\n(only 11MB), in conjunction with conditional random fields (CRF), achieves\nstate-of-the-art (SOTA) performance.\n","authors":["Chen Tang","Bohao Yang","Kun Zhao","Bo Lv","Chenghao Xiao","Frank Guerin","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2406.20038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16035v2","updated":"2024-06-28T16:21:45Z","published":"2023-09-27T21:26:03Z","title":"MKRAG: Medical Knowledge Retrieval Augmented Generation for Medical\n  Question Answering","summary":"  Large Language Models (LLMs), although powerful in general domains, often\nperform poorly on domain-specific tasks like medical question answering (QA).\nMoreover, they tend to function as \"black-boxes,\" making it challenging to\nmodify their behavior. To address the problem, our study delves into retrieval\naugmented generation (RAG), aiming to improve LLM responses without the need\nfor fine-tuning or retraining. Specifically, we propose a comprehensive\nretrieval strategy to extract medical facts from an external knowledge base,\nand then inject them into the query prompt for LLMs. Focusing on medical QA\nusing the MedQA-SMILE dataset, we evaluate the impact of different retrieval\nmodels and the number of facts provided to the LLM. Notably, our\nretrieval-augmented Vicuna-7B model exhibited an accuracy improvement from\n44.46% to 48.54%. This work underscores the potential of RAG to enhance LLM\nperformance, offering a practical approach to mitigate the challenges of\nblack-box LLMs.\n","authors":["Yucheng Shi","Shaochen Xu","Tianze Yang","Zhengliang Liu","Tianming Liu","Xiang Li","Ninghao Liu"],"pdf_url":"https://arxiv.org/pdf/2309.16035v2.pdf","comment":"Accepted by AMIA 2024 Annual Symposium"},{"id":"http://arxiv.org/abs/2406.20030v1","updated":"2024-06-28T16:17:41Z","published":"2024-06-28T16:17:41Z","title":"LEMoE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of\n  Large Language Models","summary":"  Large language models (LLMs) require continual knowledge updates to stay\nabreast of the ever-changing world facts, prompting the formulation of lifelong\nmodel editing task. While recent years have witnessed the development of\nvarious techniques for single and batch editing, these methods either fail to\napply or perform sub-optimally when faced with lifelong editing. In this paper,\nwe introduce LEMoE, an advanced Mixture of Experts (MoE) adaptor for lifelong\nmodel editing. We first analyze the factors influencing the effectiveness of\nconventional MoE adaptor in lifelong editing, including catastrophic\nforgetting, inconsistent routing and order sensitivity. Based on these\ninsights, we propose a tailored module insertion method to achieve lifelong\nediting, incorporating a novel KV anchor routing to enhance routing consistency\nbetween training and inference stage, along with a concise yet effective\nclustering-based editing order planning. Experimental results demonstrate the\neffectiveness of our method in lifelong editing, surpassing previous model\nediting techniques while maintaining outstanding performance in batch editing\ntask. Our code will be available.\n","authors":["Renzhi Wang","Piji Li"],"pdf_url":"https://arxiv.org/pdf/2406.20030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18492v2","updated":"2024-06-28T16:12:39Z","published":"2024-05-28T18:01:52Z","title":"LLMs and Memorization: On Quality and Specificity of Copyright\n  Compliance","summary":"  Memorization in large language models (LLMs) is a growing concern. LLMs have\nbeen shown to easily reproduce parts of their training data, including\ncopyrighted work. This is an important problem to solve, as it may violate\nexisting copyright laws as well as the European AI Act. In this work, we\npropose a systematic analysis to quantify the extent of potential copyright\ninfringements in LLMs using European law as an example. Unlike previous work,\nwe evaluate instruction-finetuned models in a realistic end-user scenario. Our\nanalysis builds on a proposed threshold of 160 characters, which we borrow from\nthe German Copyright Service Provider Act and a fuzzy text matching algorithm\nto identify potentially copyright-infringing textual reproductions. The\nspecificity of countermeasures against copyright infringement is analyzed by\ncomparing model behavior on copyrighted and public domain data. We investigate\nwhat behaviors models show instead of producing protected text (such as refusal\nor hallucination) and provide a first legal assessment of these behaviors. We\nfind that there are huge differences in copyright compliance, specificity, and\nappropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous\nperform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing\na particularly low absolute number of potential copyright violations. Code will\nbe published soon.\n","authors":["Felix B Mueller","Rebekka Görge","Anna K Bernzen","Janna C Pirk","Maximilian Poretschkin"],"pdf_url":"https://arxiv.org/pdf/2405.18492v2.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.20015v1","updated":"2024-06-28T16:03:30Z","published":"2024-06-28T16:03:30Z","title":"ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for\n  Tool-Augmented Large Language Models","summary":"  Tool-augmented large language models (LLMs) are rapidly being integrated into\nreal-world applications. Due to the lack of benchmarks, the community still\nneeds to fully understand the hallucination issues within these models. To\naddress this challenge, we introduce a comprehensive diagnostic benchmark,\nToolBH. Specifically, we assess the LLM's hallucinations through two\nperspectives: depth and breadth. In terms of depth, we propose a multi-level\ndiagnostic process, including (1) solvability detection, (2) solution planning,\nand (3) missing-tool analysis. For breadth, we consider three scenarios based\non the characteristics of the toolset: missing necessary tools, potential\ntools, and limited functionality tools. Furthermore, we developed seven tasks\nand collected 700 evaluation samples through multiple rounds of manual\nannotation. The results show the significant challenges presented by the ToolBH\nbenchmark. The current advanced models Gemini-1.5-Pro and GPT-4o only achieve a\ntotal score of 45.3 and 37.0, respectively, on a scale of 100. In this\nbenchmark, larger model parameters do not guarantee better performance; the\ntraining data and response strategies also play a crucial role in tool-enhanced\nLLM scenarios. Our diagnostic analysis indicates that the primary reason for\nmodel errors lies in assessing task solvability. Additionally, open-weight\nmodels suffer from performance drops with verbose replies, whereas proprietary\nmodels excel with longer reasoning.\n","authors":["Yuxiang Zhang","Jing Chen","Junjie Wang","Yaxin Liu","Cheng Yang","Chufan Shi","Xinyu Zhu","Zihao Lin","Hanwen Wan","Yujiu Yang","Tetsuya Sakai","Tian Feng","Hayato Yamana"],"pdf_url":"https://arxiv.org/pdf/2406.20015v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12568v4","updated":"2024-06-28T15:44:36Z","published":"2023-08-24T05:15:43Z","title":"A Small and Fast BERT for Chinese Medical Punctuation Restoration","summary":"  In clinical dictation, utterances after automatic speech recognition (ASR)\nwithout explicit punctuation marks may lead to the misunderstanding of dictated\nreports. To give a precise and understandable clinical report with ASR,\nautomatic punctuation restoration is required. Considering a practical\nscenario, we propose a fast and light pre-trained model for Chinese medical\npunctuation restoration based on 'pretraining and fine-tuning' paradigm. In\nthis work, we distill pre-trained models by incorporating supervised\ncontrastive learning and a novel auxiliary pre-training task (Punctuation Mark\nPrediction) to make it well-suited for punctuation restoration. Our experiments\non various distilled models reveal that our model can achieve 95% performance\nwhile 10% model size relative to state-of-the-art Chinese RoBERTa.\n","authors":["Tongtao Ling","Yutao Lai","Lei Chen","Shilei Huang","Yi Liu"],"pdf_url":"https://arxiv.org/pdf/2308.12568v4.pdf","comment":"5 pages, 2 figures, Accepted by INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2405.14105v2","updated":"2024-06-28T15:34:26Z","published":"2024-05-23T02:14:17Z","title":"Distributed Speculative Inference of Large Language Models","summary":"  Accelerating the inference of large language models (LLMs) is an important\nchallenge in artificial intelligence. This paper introduces distributed\nspeculative inference (DSI), a novel distributed inference algorithm that is\nprovably faster than speculative inference (SI) [leviathan2023fast,\nchen2023accelerating, miao2023specinfer] and traditional autoregressive\ninference (non-SI). Like other SI algorithms, DSI works on frozen LLMs,\nrequiring no training or architectural modifications, and it preserves the\ntarget distribution.\n  Prior studies on SI have demonstrated empirical speedups (compared to non-SI)\nbut require a fast and accurate drafter LLM. In practice, off-the-shelf LLMs\noften do not have matching drafters that are sufficiently fast and accurate. We\nshow a gap: SI gets slower than non-SI when using slower or less accurate\ndrafters. We close this gap by proving that DSI is faster than both SI and\nnon-SI given any drafters. By orchestrating multiple instances of the target\nand drafters, DSI is not only faster than SI but also supports LLMs that cannot\nbe accelerated with SI.\n  Our simulations show speedups of off-the-shelf LLMs in realistic settings:\nDSI is 1.29-1.92x faster than SI.\n","authors":["Nadav Timor","Jonathan Mamou","Daniel Korat","Moshe Berchansky","Oren Pereg","Moshe Wasserblat","Tomer Galanti","Michal Gordon","David Harel"],"pdf_url":"https://arxiv.org/pdf/2405.14105v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19999v1","updated":"2024-06-28T15:34:26Z","published":"2024-06-28T15:34:26Z","title":"The SIFo Benchmark: Investigating the Sequential Instruction Following\n  Ability of Large Language Models","summary":"  Following multiple instructions is a crucial ability for large language\nmodels (LLMs). Evaluating this ability comes with significant challenges: (i)\nlimited coherence between multiple instructions, (ii) positional bias where the\norder of instructions affects model performance, and (iii) a lack of\nobjectively verifiable tasks. To address these issues, we introduce a benchmark\ndesigned to evaluate models' abilities to follow multiple instructions through\nsequential instruction following (SIFo) tasks. In SIFo, the successful\ncompletion of multiple instructions is verifiable by examining only the final\ninstruction. Our benchmark evaluates instruction following using four tasks\n(text modification, question answering, mathematics, and security rule\nfollowing), each assessing different aspects of sequential instruction\nfollowing. Our evaluation of popular LLMs, both closed-source and open-source,\nshows that more recent and larger models significantly outperform their older\nand smaller counterparts on the SIFo tasks, validating the benchmark's\neffectiveness. All models struggle with following sequences of instructions,\nhinting at an important lack of robustness of today's language models.\n","authors":["Xinyi Chen","Baohao Liao","Jirui Qi","Panagiotis Eustratiadis","Christof Monz","Arianna Bisazza","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2406.19999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19995v1","updated":"2024-06-28T15:27:57Z","published":"2024-06-28T15:27:57Z","title":"Single Parent Family: A Spectrum of Family Members from a Single\n  Pre-Trained Foundation Model","summary":"  This paper introduces a novel method of Progressive Low Rank Decomposition\n(PLRD) tailored for the compression of large language models. Our approach\nleverages a pre-trained model, which is then incrementally decompressed to\nsmaller sizes using progressively lower ranks. This method allows for\nsignificant reductions in computational overhead and energy consumption, as\nsubsequent models are derived from the original without the need for retraining\nfrom scratch. We detail the implementation of PLRD, which strategically\ndecreases the tensor ranks, thus optimizing the trade-off between model\nperformance and resource usage. The efficacy of PLRD is demonstrated through\nextensive experiments showing that models trained with PLRD method on only 1B\ntokens maintain comparable performance with traditionally trained models while\nusing 0.1% of the tokens. The versatility of PLRD is highlighted by its ability\nto generate multiple model sizes from a single foundational model, adapting\nfluidly to varying computational and memory budgets. Our findings suggest that\nPLRD could set a new standard for the efficient scaling of LLMs, making\nadvanced AI more feasible on diverse platforms.\n","authors":["Habib Hajimolahoseini","Mohammad Hassanpour","Foozhan Ataiefard","Boxing Chen","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2406.19995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11583v2","updated":"2024-06-28T15:01:07Z","published":"2023-11-20T07:41:30Z","title":"How well ChatGPT understand Malaysian English? An Evaluation on Named\n  Entity Recognition and Relation Extraction","summary":"  Recently, ChatGPT has attracted a lot of interest from both researchers and\nthe general public. While the performance of ChatGPT in named entity\nrecognition and relation extraction from Standard English texts is\nsatisfactory, it remains to be seen if it can perform similarly for Malaysian\nEnglish. Malaysian English is unique as it exhibits morphosyntactic and\nsemantical adaptation from local contexts. In this study, we assess ChatGPT's\ncapability in extracting entities and relations from the Malaysian English News\n(MEN) dataset. We propose a three-step methodology referred to as\n\\textbf{\\textit{educate-predict-evaluate}}. The performance of ChatGPT is\nassessed using F1-Score across 18 unique prompt settings, which were carefully\nengineered for a comprehensive review. From our evaluation, we found that\nChatGPT does not perform well in extracting entities from Malaysian English\nnews articles, with the highest F1-Score of 0.497. Further analysis shows that\nthe morphosyntactic adaptation in Malaysian English caused the limitation.\nHowever, interestingly, this morphosyntactic adaptation does not impact the\nperformance of ChatGPT for relation extraction.\n","authors":["Mohan Raj Chanthran","Lay-Ki Soon","Huey Fang Ong","Bhawani Selvaretnam"],"pdf_url":"https://arxiv.org/pdf/2311.11583v2.pdf","comment":"Accepted in Generation, Evaluation & Metrics (GEM) Workshop at EMNLP\n  2023"},{"id":"http://arxiv.org/abs/2406.19967v1","updated":"2024-06-28T14:56:21Z","published":"2024-06-28T14:56:21Z","title":"Into the Unknown: Generating Geospatial Descriptions for New\n  Environments","summary":"  Similar to vision-and-language navigation (VLN) tasks that focus on bridging\nthe gap between vision and language for embodied navigation, the new Rendezvous\n(RVS) task requires reasoning over allocentric spatial relationships\n(independent of the observer's viewpoint) using non-sequential navigation\ninstructions and maps. However, performance substantially drops in new\nenvironments with no training data. Using opensource descriptions paired with\ncoordinates (e.g., Wikipedia) provides training data but suffers from limited\nspatially-oriented text resulting in low geolocation resolution. We propose a\nlarge-scale augmentation method for generating high-quality synthetic data for\nnew environments using readily available geospatial data. Our method constructs\na grounded knowledge-graph, capturing entity relationships. Sampled entities\nand relations (`shop north of school') generate navigation instructions via (i)\ngenerating numerous templates using context-free grammar (CFG) to embed\nspecific entities and relations; (ii) feeding the entities and relation into a\nlarge language model (LLM) for instruction generation. A comprehensive\nevaluation on RVS, showed that our approach improves the 100-meter accuracy by\n45.83% on unseen environments. Furthermore, we demonstrate that models trained\nwith CFG-based augmentation achieve superior performance compared with those\ntrained with LLM-based augmentation, both in unseen and seen environments.\nThese findings suggest that the potential advantages of explicitly structuring\nspatial information for text-based geospatial reasoning in previously unknown,\ncan unlock data-scarce scenarios.\n","authors":["Tzuf Paz-Argaman","John Palowitch","Sayali Kulkarni","Reut Tsarfaty","Jason Baldridge"],"pdf_url":"https://arxiv.org/pdf/2406.19967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19966v1","updated":"2024-06-28T14:54:12Z","published":"2024-06-28T14:54:12Z","title":"Simulating Financial Market via Large Language Model based Agents","summary":"  Most economic theories typically assume that financial market participants\nare fully rational individuals and use mathematical models to simulate human\nbehavior in financial markets. However, human behavior is often not entirely\nrational and is challenging to predict accurately with mathematical models. In\nthis paper, we propose \\textbf{A}gent-based \\textbf{S}imulated\n\\textbf{F}inancial \\textbf{M}arket (ASFM), which first constructs a simulated\nstock market with a real order matching system. Then, we propose a large\nlanguage model based agent as the stock trader, which contains the profile,\nobservation, and tool-learning based action module. The trading agent can\ncomprehensively understand current market dynamics and financial policy\ninformation, and make decisions that align with their trading strategy. In the\nexperiments, we first verify that the reactions of our ASFM are consistent with\nthe real stock market in two controllable scenarios. In addition, we also\nconduct experiments in two popular economics research directions, and we find\nthat conclusions drawn in our \\model align with the preliminary findings in\neconomics research. Based on these observations, we believe our proposed ASFM\nprovides a new paradigm for economic research.\n","authors":["Shen Gao","Yuntao Wen","Minghang Zhu","Jianing Wei","Yuhan Cheng","Qunzi Zhang","Shuo Shang"],"pdf_url":"https://arxiv.org/pdf/2406.19966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12055v2","updated":"2024-06-28T14:53:35Z","published":"2024-02-19T11:19:02Z","title":"Are LLM-based Evaluators Confusing NLG Quality Criteria?","summary":"  Some prior work has shown that LLMs perform well in NLG evaluation for\ndifferent tasks. However, we discover that LLMs seem to confuse different\nevaluation criteria, which reduces their reliability. For further verification,\nwe first consider avoiding issues of inconsistent conceptualization and vague\nexpression in existing NLG quality criteria themselves. So we summarize a clear\nhierarchical classification system for 11 common aspects with corresponding\ndifferent criteria from previous studies involved. Inspired by behavioral\ntesting, we elaborately design 18 types of aspect-targeted perturbation attacks\nfor fine-grained analysis of the evaluation behaviors of different LLMs. We\nalso conduct human annotations beyond the guidance of the classification system\nto validate the impact of the perturbations. Our experimental results reveal\nconfusion issues inherent in LLMs, as well as other noteworthy phenomena, and\nnecessitate further research and improvements for LLM-based evaluation.\n","authors":["Xinyu Hu","Mingqi Gao","Sen Hu","Yang Zhang","Yicheng Chen","Teng Xu","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2402.12055v2.pdf","comment":"Accepted by ACL 2024"},{"id":"http://arxiv.org/abs/2406.19954v1","updated":"2024-06-28T14:40:03Z","published":"2024-06-28T14:40:03Z","title":"BESTOW: Efficient and Streamable Speech Language Model with the Best of\n  Two Worlds in GPT and T5","summary":"  Incorporating speech understanding capabilities into pretrained\nlarge-language models has become a vital research direction (SpeechLLM). The\nprevious architectures can be categorized as: i) GPT-style, prepend speech\nprompts to the text prompts as a sequence of LLM inputs like a decoder-only\nmodel; ii) T5-style, introduce speech cross-attention to each layer of the\npretrained LLMs. We propose BESTOW architecture to bring the BESt features from\nTwO Worlds into a single model that is highly efficient and has strong\nmultitask capabilities. Moreover, there is no clear streaming solution for\neither style, especially considering the solution should generalize to speech\nmultitask. We reformulate streamable SpeechLLM as a read-write policy problem\nand unifies the offline and streaming research with BESTOW architecture. Hence\nwe demonstrate the first open-source SpeechLLM solution that enables Streaming\nand Multitask at scale (beyond ASR) at the same time. This streamable solution\nachieves very strong performance on a wide range of speech tasks (ASR, AST,\nSQA, unseen DynamicSuperb). It is end-to-end optimizable, with lower\ntraining/inference cost, and demonstrates LLM knowledge transferability to\nspeech.\n","authors":["Zhehuai Chen","He Huang","Oleksii Hrinchuk","Krishna C. Puvvada","Nithin Rao Koluguri","Piotr Żelasko","Jagadeesh Balam","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2406.19954v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19951v1","updated":"2024-06-28T14:36:31Z","published":"2024-06-28T14:36:31Z","title":"Mining Reasons For And Against Vaccination From Unstructured Data Using\n  Nichesourcing and AI Data Augmentation","summary":"  We present Reasons For and Against Vaccination (RFAV), a dataset for\npredicting reasons for and against vaccination, and scientific authorities used\nto justify them, annotated through nichesourcing and augmented using GPT4 and\nGPT3.5-Turbo. We show how it is possible to mine these reasons in\nnon-structured text, under different task definitions, despite the high level\nof subjectivity involved and explore the impact of artificially augmented data\nusing in-context learning with GPT4 and GPT3.5-Turbo. We publish the dataset\nand the trained models along with the annotation manual used to train\nannotators and define the task.\n","authors":["Damián Ariel Furman","Juan Junqueras","Z. Burçe Gümüslü","Edgar Altszyler","Joaquin Navajas","Ophelia Deroy","Justin Sulik"],"pdf_url":"https://arxiv.org/pdf/2406.19951v1.pdf","comment":"8 pages + references and appendix"},{"id":"http://arxiv.org/abs/2406.19949v1","updated":"2024-06-28T14:33:05Z","published":"2024-06-28T14:33:05Z","title":"Calibrating LLMs with Preference Optimization on Thought Trees for\n  Generating Rationale in Science Question Scoring","summary":"  Generating rationales that justify scoring decisions has been a promising way\nto facilitate explainability in automated scoring systems. However, existing\nmethods do not match the accuracy of classifier-based methods. Plus, the\ngenerated rationales often contain hallucinated information. To address these\nissues, we propose a novel framework capable of generating more faithful\nrationales and, more importantly, matching performance with classifier-based\nblack-box scoring systems. We first mimic the human assessment process by\nquerying Large Language Models (LLMs) to generate a thought tree. We then\nsummarise intermediate assessment decisions from each thought tree path for\ncreating synthetic rationale data and rationale preference data. Finally, we\nutilise the generated synthetic data to calibrate LLMs through a two-step\ntraining process: supervised fine-tuning and preference optimization. Extensive\nexperimental results demonstrate that our framework achieves a 38% assessment\nperformance improvement in the QWK score compared to prior work while producing\nhigher-quality rationales, as recognised by human evaluators and LLMs. Our work\nsheds light on the effectiveness of performing preference optimization using\nsynthetic preference data obtained from thought tree paths.\n","authors":["Jiazheng Li","Hainiu Xu","Zhaoyue Sun","Yuxiang Zhou","David West","Cesare Aloisi","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2406.19949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19934v1","updated":"2024-06-28T14:04:10Z","published":"2024-06-28T14:04:10Z","title":"From the Least to the Most: Building a Plug-and-Play Visual Reasoner via\n  Data Synthesis","summary":"  We explore multi-step reasoning in vision-language models (VLMs). The problem\nis challenging, as reasoning data consisting of multiple steps of visual and\nlanguage processing are barely available. To overcome the challenge, we first\nintroduce a least-to-most visual reasoning paradigm, which interleaves steps of\ndecomposing a question into sub-questions and invoking external tools for\nresolving sub-questions. Based on the paradigm, we further propose a novel data\nsynthesis approach that can automatically create questions and multi-step\nreasoning paths for an image in a bottom-up manner. Our approach divides the\ncomplex synthesis task into a few simple sub-tasks, and (almost entirely)\nrelies on open-sourced models to accomplish the sub-tasks. Therefore, the\nentire synthesis process is reproducible and cost-efficient, and the\nsynthesized data is quality guaranteed. With the approach, we construct $50$k\nvisual reasoning examples. Then, we develop a visual reasoner through\nsupervised fine-tuning, which is capable of generally enhancing the reasoning\nabilities of a wide range of existing VLMs in a plug-and-play fashion.\nExtensive experiments indicate that the visual reasoner can consistently and\nsignificantly improve four VLMs on four VQA benchmarks. Our code and dataset\nare available at https://github.com/steven-ccq/VisualReasoner.\n","authors":["Chuanqi Cheng","Jian Guan","Wei Wu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2406.19934v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19928v1","updated":"2024-06-28T13:57:27Z","published":"2024-06-28T13:57:27Z","title":"Interactive Topic Models with Optimal Transport","summary":"  Topic models are widely used to analyze document collections. While they are\nvaluable for discovering latent topics in a corpus when analysts are unfamiliar\nwith the corpus, analysts also commonly start with an understanding of the\ncontent present in a corpus. This may be through categories obtained from an\ninitial pass over the corpus or a desire to analyze the corpus through a\npredefined set of categories derived from a high level theoretical framework\n(e.g. political ideology). In these scenarios analysts desire a topic modeling\napproach which incorporates their understanding of the corpus while supporting\nvarious forms of interaction with the model. In this work, we present EdTM, as\nan approach for label name supervised topic modeling. EdTM models topic\nmodeling as an assignment problem while leveraging LM/LLM based document-topic\naffinities and using optimal transport for making globally coherent\ntopic-assignments. In experiments, we show the efficacy of our framework\ncompared to few-shot LLM classifiers, and topic models based on clustering and\nLDA. Further, we show EdTM's ability to incorporate various forms of analyst\nfeedback and while remaining robust to noisy analyst inputs.\n","authors":["Garima Dhanania","Sheshera Mysore","Chau Minh Pham","Mohit Iyyer","Hamed Zamani","Andrew McCallum"],"pdf_url":"https://arxiv.org/pdf/2406.19928v1.pdf","comment":"Pre-print; Work in progress"},{"id":"http://arxiv.org/abs/2310.15959v3","updated":"2024-06-28T13:28:08Z","published":"2023-10-24T15:59:43Z","title":"NoteChat: A Dataset of Synthetic Doctor-Patient Conversations\n  Conditioned on Clinical Notes","summary":"  We introduce NoteChat, a novel cooperative multi-agent framework leveraging\nLarge Language Models (LLMs) to generate patient-physician dialogues. NoteChat\nembodies the principle that an ensemble of role-specific LLMs, through\nstructured role-play and strategic prompting, can perform their assigned roles\nmore effectively. The synergy among these role-playing LLMs results in a\ncohesive and efficient dialogue generation. Evaluation on MTS-dialogue, a\nbenchmark dataset for patient-physician dialogues-note pairs, shows that models\ntrained with the augmented synthetic patient-physician dialogues by NoteChat\noutperforms other state-of-the-art models for generating clinical notes. Our\ncomprehensive automatic and human evaluation demonstrates that NoteChat\nsubstantially surpasses state-of-the-art models like ChatGPT and GPT-4 up to\n22.78% by domain experts in generating superior synthetic patient-physician\ndialogues based on clinical notes. NoteChat has the potential to engage\npatients directly and help clinical documentation, a leading cause of physician\nburnout.\n","authors":["Junda Wang","Zonghai Yao","Zhichao Yang","Huixue Zhou","Rumeng Li","Xun Wang","Yucheng Xu","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2310.15959v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17887v4","updated":"2024-06-28T13:23:31Z","published":"2024-02-27T21:01:41Z","title":"JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning\n  and Professional Question Answering Capability","summary":"  Large Language Models (LLMs) have demonstrated a remarkable potential in\nmedical knowledge acquisition and question-answering. However, LLMs can\npotentially hallucinate and yield factually incorrect outcomes, even with\ndomain-specific pretraining. Previously, retrieval augmented generation (RAG)\nhas limited success in addressing hallucinations. Unlike previous methods in\nRAG where the retrieval model was trained separately from the LLM, we introduce\nJMLR (for Jointly trains LLM and information Retrieval) during the fine-tuning\nphase. The synchronized training mechanism enhances JMLR's ability to retrieve\nclinical guidelines and leverage medical knowledge to reason and answer\nquestions and reduces the demand for computational resources. We evaluated JMLR\non the important medical question-answering application. Our experimental\nresults demonstrate that JMLR-13B (70.5%) outperforms a previous\nstate-of-the-art open-source model using conventional pre-training and\nfine-tuning Meditron-70B (68.9%) and Llama2-13B with RAG (67.7%) on a medical\nquestion-answering dataset. Comprehensive evaluations reveal JMLR-13B enhances\nreasoning quality and reduces hallucinations better than Claude3-Opus.\nAdditionally, JMLR-13B (148 GPU hours) also trains much faster than\nMeditron-70B (42630 GPU hours). Through this work, we provide a new and\nefficient knowledge enhancement method for healthcare, demonstrating the\npotential of integrating retrieval and LLM training for medical\nquestion-answering systems.\n","authors":["Junda Wang","Zhichao Yang","Zonghai Yao","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2402.17887v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14862v3","updated":"2024-06-28T13:19:37Z","published":"2024-06-21T04:39:03Z","title":"LatentExplainer: Explaining Latent Representations in Deep Generative\n  Models with Multi-modal Foundation Models","summary":"  Deep generative models like VAEs and diffusion models have advanced various\ngeneration tasks by leveraging latent variables to learn data distributions and\ngenerate high-quality samples. Despite the field of explainable AI making\nstrides in interpreting machine learning models, understanding latent variables\nin generative models remains challenging. This paper introduces\nLatentExplainer, a framework for automatically generating semantically\nmeaningful explanations of latent variables in deep generative models.\nLatentExplainer tackles three main challenges: inferring the meaning of latent\nvariables, aligning explanations with inductive biases, and handling varying\ndegrees of explainability. By perturbing latent variables and interpreting\nchanges in generated data, the framework provides a systematic approach to\nunderstanding and controlling the data generation process, enhancing the\ntransparency and interpretability of deep generative models. We evaluate our\nproposed method on several real-world and synthetic datasets, and the results\ndemonstrate superior performance in generating high-quality explanations of\nlatent variables.\n","authors":["Mengdan Zhu","Raasikh Kanjiani","Jiahui Lu","Andrew Choi","Qirui Ye","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.14862v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19898v1","updated":"2024-06-28T13:06:31Z","published":"2024-06-28T13:06:31Z","title":"Paraphrase Types Elicit Prompt Engineering Capabilities","summary":"  Much of the success of modern language models depends on finding a suitable\nprompt to instruct the model. Until now, it has been largely unknown how\nvariations in the linguistic expression of prompts affect these models. This\nstudy systematically and empirically evaluates which linguistic features\ninfluence models through paraphrase types, i.e., different linguistic changes\nat particular positions. We measure behavioral changes for five models across\n120 tasks and six families of paraphrases (i.e., morphology, syntax, lexicon,\nlexico-syntax, discourse, and others). We also control for other prompt\nengineering factors (e.g., prompt length, lexical diversity, and proximity to\ntraining data). Our results show a potential for language models to improve\ntasks when their prompts are adapted in specific paraphrase types (e.g., 6.7%\nmedian gain in Mixtral 8x7B; 5.5% in LLaMA 3 8B). In particular, changes in\nmorphology and lexicon, i.e., the vocabulary used, showed promise in improving\nprompts. These findings contribute to developing more robust language models\ncapable of handling variability in linguistic expression.\n","authors":["Jan Philip Wahle","Terry Ruas","Yang Xu","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2406.19898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19892v1","updated":"2024-06-28T13:00:30Z","published":"2024-06-28T13:00:30Z","title":"Untangling the Unrestricted Web: Automatic Identification of\n  Multilingual Registers","summary":"  This article explores deep learning models for the automatic identification\nof registers - text varieties such as news reports and discussion forums - in\nweb-based datasets across 16 languages. Web register (or genre) identification\nwould provide a robust solution for understanding the content of web-scale\ndatasets, which have become crucial in computational linguistics. Despite\nrecent advances, the potential of register classifiers on the noisy web remains\nlargely unexplored, particularly in multilingual settings and when targeting\nthe entire unrestricted web. We experiment with a range of deep learning models\nusing the new Multilingual CORE corpora, which includes 16 languages annotated\nusing a detailed, hierarchical taxonomy of 25 registers designed to cover the\nentire unrestricted web. Our models achieve state-of-the-art results, showing\nthat a detailed taxonomy in a hierarchical multi-label setting can yield\ncompetitive classification performance. However, all models hit a glass ceiling\nat approximately 80% F1 score, which we attribute to the non-discrete nature of\nweb registers and the inherent uncertainty in labeling some documents. By\npruning ambiguous examples, we improve model performance to over 90%. Finally,\nmultilingual models outperform monolingual ones, particularly benefiting\nlanguages with fewer training examples and smaller registers. Although a\nzero-shot setting decreases performance by an average of 7%, these drops are\nnot linked to specific registers or languages. Instead, registers show\nsurprising similarity across languages.\n","authors":["Erik Henriksson","Amanda Myntti","Anni Eskelinen","Selcen Erten-Johansson","Saara Hellström","Veronika Laippala"],"pdf_url":"https://arxiv.org/pdf/2406.19892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09881v2","updated":"2024-06-28T12:58:11Z","published":"2024-06-14T09:52:27Z","title":"A Unified Data Augmentation Framework for Low-Resource Multi-Domain\n  Dialogue Generation","summary":"  Current state-of-the-art dialogue systems heavily rely on extensive training\ndatasets. However, challenges arise in domains where domain-specific training\ndatasets are insufficient or entirely absent. To tackle this challenge, we\npropose a novel data \\textbf{A}ugmentation framework for\n\\textbf{M}ulti-\\textbf{D}omain \\textbf{D}ialogue \\textbf{G}eneration, referred\nto as \\textbf{AMD$^2$G}. The AMD$^2$G framework consists of a data augmentation\nprocess and a two-stage training approach: domain-agnostic training and domain\nadaptation training. We posit that domain corpora are a blend of\ndomain-agnostic and domain-specific features, with certain representation\npatterns shared among diverse domains. Domain-agnostic training aims to enable\nmodels to learn these common expressive patterns. To construct domain-agnostic\ndialogue corpora, we employ a \\textit{\\textbf{de-domaining}} data processing\ntechnique used to remove domain-specific features. By mitigating the effects of\ndomain-specific features, the model trained on the de-domained corpora can\neffectively learn common expression patterns in different domains.\nSubsequently, we adapt the learned domain-agnostic features to the target\ndomain through domain adaptation training. We conduct experiments on Chinese\ndialogue datasets from five different domains and show that AMD$^2$G achieves\nsuperior performance compared to both direct training on the target domain\ncorpus and collective training on all five domain corpora. Our work underscores\nAMD$^2$G as a viable alternative solution for low-resource multi-domain\ndialogue generation. Code and data associated with our work are available on\nGitHub repository$^{\\text 1}$.\n","authors":["Yongkang Liu","Ercong Nie","Shi Feng","Zheng Hua","Zifeng Ding","Daling Wang","Yifei Zhang","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2406.09881v2.pdf","comment":"17pages,ECML-PKDD"},{"id":"http://arxiv.org/abs/2406.19884v1","updated":"2024-06-28T12:49:27Z","published":"2024-06-28T12:49:27Z","title":"Investigating the Timescales of Language Processing with EEG and\n  Language Models","summary":"  This study explores the temporal dynamics of language processing by examining\nthe alignment between word representations from a pre-trained transformer-based\nlanguage model, and EEG data. Using a Temporal Response Function (TRF) model,\nwe investigate how neural activity corresponds to model representations across\ndifferent layers, revealing insights into the interaction between artificial\nlanguage models and brain responses during language comprehension. Our analysis\nreveals patterns in TRFs from distinct layers, highlighting varying\ncontributions to lexical and compositional processing. Additionally, we used\nlinear discriminant analysis (LDA) to isolate part-of-speech (POS)\nrepresentations, offering insights into their influence on neural responses and\nthe underlying mechanisms of syntactic processing. These findings underscore\nEEG's utility for probing language processing dynamics with high temporal\nresolution. By bridging artificial language models and neural activity, this\nstudy advances our understanding of their interaction at fine timescales.\n","authors":["Davide Turco","Conor Houghton"],"pdf_url":"https://arxiv.org/pdf/2406.19884v1.pdf","comment":"Accepted at the 2024 Conference on Cognitive Computational\n  Neuroscience (CCN 2024)"},{"id":"http://arxiv.org/abs/2406.19874v1","updated":"2024-06-28T12:28:52Z","published":"2024-06-28T12:28:52Z","title":"Detecting Subtle Differences between Human and Model Languages Using\n  Spectrum of Relative Likelihood","summary":"  Human and model-generated texts can be distinguished by examining the\nmagnitude of likelihood in language. However, it is becoming increasingly\ndifficult as language model's capabilities of generating human-like texts keep\nevolving. This study provides a new perspective by using the relative\nlikelihood values instead of absolute ones, and extracting useful features from\nthe spectrum-view of likelihood for the human-model text detection task. We\npropose a detection procedure with two classification methods, supervised and\nheuristic-based, respectively, which results in competitive performances with\nprevious zero-shot detection methods and a new state-of-the-art on short-text\ndetection. Our method can also reveal subtle differences between human and\nmodel languages, which find theoretical roots in psycholinguistics studies. Our\ncode is available at https://github.com/CLCS-SUSTech/FourierGPT\n","authors":["Yang Xu","Yu Wang","Hao An","Zhichen Liu","Yongyuan Li"],"pdf_url":"https://arxiv.org/pdf/2406.19874v1.pdf","comment":"13 pages, 12 figures"},{"id":"http://arxiv.org/abs/2305.14493v4","updated":"2024-06-28T12:04:53Z","published":"2023-05-23T19:45:45Z","title":"Do prompt positions really matter?","summary":"  Prompt-based models have gathered a lot of attention from researchers due to\ntheir remarkable advancements in the fields of zero-shot and few-shot learning.\nDeveloping an effective prompt template plays a critical role. However, prior\nstudies have mainly focused on prompt vocabulary searching or embedding\ninitialization within a predefined template with the prompt position fixed. In\nthis empirical study, we conduct the most comprehensive analysis to date of\nprompt position for diverse Natural Language Processing (NLP) tasks. Our\nfindings quantify the substantial impact prompt position has on model\nperformance. We observe that the prompt positions used in prior studies are\noften sub-optimal, and this observation is consistent even in widely used\ninstruction-tuned models. These findings suggest prompt position optimisation\nas a valuable research direction to augment prompt engineering methodologies\nand prompt position-aware instruction tuning as a potential way to build more\nrobust models in the future.\n","authors":["Junyu Mao","Stuart E. Middleton","Mahesan Niranjan"],"pdf_url":"https://arxiv.org/pdf/2305.14493v4.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.19853v1","updated":"2024-06-28T11:52:53Z","published":"2024-06-28T11:52:53Z","title":"YuLan: An Open-source Large Language Model","summary":"  Large language models (LLMs) have become the foundation of many applications,\nleveraging their extensive capabilities in processing and understanding natural\nlanguage. While many open-source LLMs have been released with technical\nreports, the lack of training details hinders further research and development.\nThis paper presents the development of YuLan, a series of open-source LLMs with\n$12$ billion parameters. The base model of YuLan is pre-trained on\napproximately $1.7$T tokens derived from a diverse corpus, including massive\nEnglish, Chinese, and multilingual texts. We design a three-stage pre-training\nmethod to enhance YuLan's overall capabilities. Subsequent phases of training\nincorporate instruction-tuning and human alignment, employing a substantial\nvolume of high-quality synthesized data. To facilitate the learning of complex\nand long-tail knowledge, we devise a curriculum-learning framework throughout\nacross these stages, which helps LLMs learn knowledge in an easy-to-hard\nmanner. YuLan's training is finished on Jan, 2024 and has achieved performance\non par with state-of-the-art LLMs across various English and Chinese\nbenchmarks. This paper outlines a comprehensive technical roadmap for\ndeveloping LLMs from scratch. Our model and codes are available at\nhttps://github.com/RUC-GSAI/YuLan-Chat.\n","authors":["Yutao Zhu","Kun Zhou","Kelong Mao","Wentong Chen","Yiding Sun","Zhipeng Chen","Qian Cao","Yihan Wu","Yushuo Chen","Feng Wang","Lei Zhang","Junyi Li","Xiaolei Wang","Lei Wang","Beichen Zhang","Zican Dong","Xiaoxue Cheng","Yuhan Chen","Xinyu Tang","Yupeng Hou","Qiangqiang Ren","Xincheng Pang","Shufang Xie","Wayne Xin Zhao","Zhicheng Dou","Jiaxin Mao","Yankai Lin","Ruihua Song","Jun Xu","Xu Chen","Rui Yan","Zhewei Wei","Di Hu","Wenbing Huang","Ze-Feng Gao","Yueguo Chen","Weizheng Lu","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2406.19853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19840v1","updated":"2024-06-28T11:28:44Z","published":"2024-06-28T11:28:44Z","title":"AnomaLLMy -- Detecting anomalous tokens in black-box LLMs through\n  low-confidence single-token predictions","summary":"  This paper introduces AnomaLLMy, a novel technique for the automatic\ndetection of anomalous tokens in black-box Large Language Models (LLMs) with\nAPI-only access. Utilizing low-confidence single-token predictions as a\ncost-effective indicator, AnomaLLMy identifies irregularities in model\nbehavior, addressing the issue of anomalous tokens degrading the quality and\nreliability of models. Validated on the cl100k_base dataset, the token set of\nGPT-4, AnomaLLMy detected 413 major and 65 minor anomalies, demonstrating the\nmethod's efficiency with just \\$24.39 spent in API credits. The insights from\nthis research are expected to be beneficial for enhancing the robustness of and\naccuracy of LLMs, particularly in the development and assessment of tokenizers.\n","authors":["Waligóra Witold"],"pdf_url":"https://arxiv.org/pdf/2406.19840v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2406.18313v2","updated":"2024-06-28T11:04:19Z","published":"2024-06-26T12:54:19Z","title":"Advancing Airport Tower Command Recognition: Integrating\n  Squeeze-and-Excitation and Broadcasted Residual Learning","summary":"  Accurate recognition of aviation commands is vital for flight safety and\nefficiency, as pilots must follow air traffic control instructions precisely.\nThis paper addresses challenges in speech command recognition, such as noisy\nenvironments and limited computational resources, by advancing keyword spotting\ntechnology. We create a dataset of standardized airport tower commands,\nincluding routine and emergency instructions. We enhance broadcasted residual\nlearning with squeeze-and-excitation and time-frame frequency-wise\nsqueeze-and-excitation techniques, resulting in our BC-SENet model. This model\nfocuses on crucial information with fewer parameters. Our tests on five keyword\nspotting models, including BC-SENet, demonstrate superior accuracy and\nefficiency. These findings highlight the effectiveness of our model\nadvancements in improving speech command recognition for aviation safety and\nefficiency in noisy, high-stakes environments. Additionally, BC-SENet shows\ncomparable performance on the common Google Speech Command dataset.\n","authors":["Yuanxi Lin","Tonglin Zhou","Yang Xiao"],"pdf_url":"https://arxiv.org/pdf/2406.18313v2.pdf","comment":"Accepted by IALP 2024"},{"id":"http://arxiv.org/abs/2406.19820v1","updated":"2024-06-28T10:53:48Z","published":"2024-06-28T10:53:48Z","title":"BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for\n  Multi-hop Question Answering","summary":"  Large language models (LLMs) have demonstrated strong reasoning capabilities.\nNevertheless, they still suffer from factual errors when tackling\nknowledge-intensive tasks. Retrieval-augmented reasoning represents a promising\napproach. However, significant challenges still persist, including inaccurate\nand insufficient retrieval for complex questions, as well as difficulty in\nintegrating multi-source knowledge. To address this, we propose Beam\nAggregation Reasoning, BeamAggR, a reasoning framework for knowledge-intensive\nmulti-hop QA. BeamAggR explores and prioritizes promising answers at each hop\nof question. Concretely, we parse the complex questions into trees, which\ninclude atom and composite questions, followed by bottom-up reasoning. For\natomic questions, the LLM conducts reasoning on multi-source knowledge to get\nanswer candidates. For composite questions, the LLM combines beam candidates,\nexplores multiple reasoning paths through probabilistic aggregation, and\nprioritizes the most promising trajectory. Extensive experiments on four\nopen-domain multi-hop reasoning datasets show that our method significantly\noutperforms SOTA methods by 8.5%. Furthermore, our analysis reveals that\nBeamAggR elicits better knowledge collaboration and answer aggregation.\n","authors":["Zheng Chu","Jingchang Chen","Qianglong Chen","Haotian Wang","Kun Zhu","Xiyuan Du","Weijiang Yu","Ming Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2406.19820v1.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2406.19232v2","updated":"2024-06-28T10:43:25Z","published":"2024-06-27T14:55:19Z","title":"RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs","summary":"  Minimal pairs are a well-established approach to evaluating the grammatical\nknowledge of language models. However, existing resources for minimal pairs\naddress a limited number of languages and lack diversity of language-specific\ngrammatical phenomena. This paper introduces the Russian Benchmark of\nLinguistic Minimal Pairs (RuBLiMP), which includes 45k pairs of sentences that\ndiffer in grammaticality and isolate a morphological, syntactic, or semantic\nphenomenon. In contrast to existing benchmarks of linguistic minimal pairs,\nRuBLiMP is created by applying linguistic perturbations to automatically\nannotated sentences from open text corpora and carefully curating test data. We\ndescribe the data collection protocol and present the results of evaluating 25\nlanguage models in various scenarios. We find that the widely used language\nmodels for Russian are sensitive to morphological and agreement-oriented\ncontrasts but fall behind humans on phenomena requiring understanding of\nstructural relations, negation, transitivity, and tense. RuBLiMP, the codebase,\nand other materials are publicly available.\n","authors":["Ekaterina Taktasheva","Maxim Bazhukov","Kirill Koncha","Alena Fenogenova","Ekaterina Artemova","Vladislav Mikhailov"],"pdf_url":"https://arxiv.org/pdf/2406.19232v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17667v2","updated":"2024-06-28T10:40:26Z","published":"2023-11-29T14:30:16Z","title":"TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in\n  Large Language Models","summary":"  Grasping the concept of time is a fundamental facet of human cognition,\nindispensable for truly comprehending the intricacies of the world. Previous\nstudies typically focus on specific aspects of time, lacking a comprehensive\ntemporal reasoning benchmark. To address this, we propose TimeBench, a\ncomprehensive hierarchical temporal reasoning benchmark that covers a broad\nspectrum of temporal reasoning phenomena. TimeBench provides a thorough\nevaluation for investigating the temporal reasoning capabilities of large\nlanguage models. We conduct extensive experiments on GPT-4, LLaMA2, and other\npopular LLMs under various settings. Our experimental results indicate a\nsignificant performance gap between the state-of-the-art LLMs and humans,\nhighlighting that there is still a considerable distance to cover in temporal\nreasoning. Besides, LLMs exhibit capability discrepancies across different\nreasoning categories. Furthermore, we thoroughly analyze the impact of multiple\naspects on temporal reasoning and emphasize the associated challenges. We\naspire for TimeBench to serve as a comprehensive benchmark, fostering research\nin temporal reasoning. Resources are available at:\nhttps://github.com/zchuz/TimeBench\n","authors":["Zheng Chu","Jingchang Chen","Qianglong Chen","Weijiang Yu","Haotian Wang","Ming Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2311.17667v2.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2402.12368v2","updated":"2024-06-28T10:36:27Z","published":"2024-02-19T18:55:16Z","title":"A synthetic data approach for domain generalization of NLI models","summary":"  Natural Language Inference (NLI) remains an important benchmark task for\nLLMs. NLI datasets are a springboard for transfer learning to other semantic\ntasks, and NLI models are standard tools for identifying the faithfulness of\nmodel-generated text. There are several large scale NLI datasets today, and\nmodels have improved greatly by hill-climbing on these collections. Yet their\nrealistic performance on out-of-distribution/domain data is less\nwell-understood. We explore the opportunity for synthetic high-quality datasets\nto adapt NLI models for zero-shot use in downstream applications across new and\nunseen text domains. We demonstrate a new approach for generating NLI data in\ndiverse domains and lengths, so far not covered by existing training sets. The\nresulting examples have meaningful premises, the hypotheses are formed in\ncreative ways rather than simple edits to a few premise tokens, and the labels\nhave high accuracy. We show that models trained on this data ($685$K synthetic\nexamples) have the best generalization to completely new downstream test\nsettings. On the TRUE benchmark, a T5-small model trained with our data\nimproves around $7\\%$ on average compared to training on the best alternative\ndataset. The improvements are more pronounced for smaller models, while still\nmeaningful on a T5 XXL model. We also demonstrate gains on test sets when\nin-domain training data is augmented with our domain-general synthetic data.\n","authors":["Mohammad Javad Hosseini","Andrey Petrov","Alex Fabrikant","Annie Louis"],"pdf_url":"https://arxiv.org/pdf/2402.12368v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15248v3","updated":"2024-06-28T10:27:11Z","published":"2024-02-23T10:27:42Z","title":"Chitchat as Interference: Adding User Backstories to Task-Oriented\n  Dialogues","summary":"  During task-oriented dialogues (TODs), human users naturally introduce\nchitchat that is beyond the immediate scope of the task, interfering with the\nflow of the conversation. To address this issue without the need for expensive\nmanual data creation, we use few-shot prompting with Llama-2-70B to enhance the\nMultiWOZ dataset with user backstories, a typical example of chitchat\ninterference in TODs. We assess the impact of this addition by testing two\nmodels: one trained solely on TODs and another trained on TODs with a\npreliminary chitchat interaction. Our analysis demonstrates that our enhanced\ndataset poses a challenge for these systems. Moreover, we demonstrate that our\ndataset can be effectively used for training purposes, enabling a system to\nconsistently acknowledge the user's backstory while also successfully moving\nthe task forward in the same turn, as confirmed by human evaluation. These\nfindings highlight the benefits of generating novel chitchat-TOD scenarios to\ntest TOD systems more thoroughly and improve their resilience to natural user\ninterferences\n","authors":["Armand Stricker","Patrick Paroubek"],"pdf_url":"https://arxiv.org/pdf/2402.15248v3.pdf","comment":"Accepted @ LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2306.01337v3","updated":"2024-06-28T10:26:27Z","published":"2023-06-02T08:02:15Z","title":"MathChat: Converse to Tackle Challenging Math Problems with LLM Agents","summary":"  Employing Large Language Models (LLMs) to address mathematical problems is an\nintriguing research endeavor, considering the abundance of math problems\nexpressed in natural language across numerous science and engineering fields.\nLLMs, with their generalized ability, are used as a foundation model to build\nAI agents for different tasks. In this paper, we study the effectiveness of\nutilizing LLM agents to solve math problems through conversations. We propose\nMathChat, a conversational problem-solving framework designed for math\nproblems. MathChat consists of an LLM agent and a user proxy agent which is\nresponsible for tool execution and additional guidance. This synergy\nfacilitates a collaborative problem-solving process, where the agents engage in\na dialogue to solve the problems. We perform evaluation on difficult high\nschool competition problems from the MATH dataset. Utilizing Python, we show\nthat MathChat can further improve previous tool-using prompting methods by 6%.\n","authors":["Yiran Wu","Feiran Jia","Shaokun Zhang","Hangyu Li","Erkang Zhu","Yue Wang","Yin Tat Lee","Richard Peng","Qingyun Wu","Chi Wang"],"pdf_url":"https://arxiv.org/pdf/2306.01337v3.pdf","comment":"Update version"},{"id":"http://arxiv.org/abs/2406.19803v1","updated":"2024-06-28T10:24:31Z","published":"2024-06-28T10:24:31Z","title":"Scalable and Domain-General Abstractive Proposition Segmentation","summary":"  Segmenting text into fine-grained units of meaning is important to a wide\nrange of NLP applications. The default approach of segmenting text into\nsentences is often insufficient, especially since sentences are usually complex\nenough to include multiple units of meaning that merit separate treatment in\nthe downstream task. We focus on the task of abstractive proposition\nsegmentation: transforming text into simple, self-contained, well-formed\nsentences. Several recent works have demonstrated the utility of proposition\nsegmentation with few-shot prompted LLMs for downstream tasks such as\nretrieval-augmented grounding and fact verification. However, this approach\ndoes not scale to large amounts of text and may not always extract all the\nfacts from the input text. In this paper, we first introduce evaluation metrics\nfor the task to measure several dimensions of quality. We then propose a\nscalable, yet accurate, proposition segmentation model. We model proposition\nsegmentation as a supervised task by training LLMs on existing annotated\ndatasets and show that training yields significantly improved results. We\nfurther show that by using the fine-tuned LLMs as teachers for annotating large\namounts of multi-domain synthetic distillation data, we can train smaller\nstudent models with results similar to the teacher LLMs. We then demonstrate\nthat our technique leads to effective domain generalization, by annotating data\nin two domains outside the original training data and evaluating on them.\nFinally, as a key contribution of the paper, we share an easy-to-use API for\nNLP practitioners to use.\n","authors":["Mohammad Javad Hosseini","Yang Gao","Tim Baumgärtner","Alex Fabrikant","Reinald Kim Amplayo"],"pdf_url":"https://arxiv.org/pdf/2406.19803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13789v3","updated":"2024-06-28T10:23:29Z","published":"2024-01-24T20:17:11Z","title":"A Unified Approach to Emotion Detection and Task-Oriented Dialogue\n  Modeling","summary":"  In current text-based task-oriented dialogue (TOD) systems, user emotion\ndetection (ED) is often overlooked or is typically treated as a separate and\nindependent task, requiring additional training. In contrast, our work\ndemonstrates that seamlessly unifying ED and TOD modeling brings about mutual\nbenefits, and is therefore an alternative to be considered. Our method consists\nin augmenting SimpleToD, an end-to-end TOD system, by extending belief state\ntracking to include ED, relying on a single language model. We evaluate our\napproach using GPT-2 and Llama-2 on the EmoWOZ benchmark, a version of MultiWOZ\nannotated with emotions. Our results reveal a general increase in performance\nfor ED and task results. Our findings also indicate that user emotions provide\nuseful contextual conditioning for system responses, and can be leveraged to\nfurther refine responses in terms of empathy.\n","authors":["Armand Stricker","Patrick Paroubek"],"pdf_url":"https://arxiv.org/pdf/2401.13789v3.pdf","comment":"Accepted @ IWSDS 2024"},{"id":"http://arxiv.org/abs/2406.16783v2","updated":"2024-06-28T10:14:53Z","published":"2024-06-24T16:45:13Z","title":"M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in\n  Large Language Models","summary":"  Instruction finetuning (IFT) is critical for aligning Large Language Models\n(LLMs) to follow instructions. While many effective IFT datasets have been\nintroduced recently, they predominantly focus on high-resource languages like\nEnglish. To better align LLMs across a broad spectrum of languages and tasks,\nwe propose a fully synthetic, novel taxonomy (Evol) guided Multilingual,\nMulti-turn instruction finetuning dataset, called M2Lingual. It is constructed\nby first selecting a diverse set of seed examples and then utilizing the\nproposed Evol taxonomy to convert these seeds into complex and challenging\nmulti-turn instructions. We demonstrate the effectiveness of M2Lingual by\ntraining LLMs of varying sizes and showcasing the enhanced performance across a\ndiverse set of languages. We contribute the 2 step Evol taxonomy with the\nguided generation code: https://github.com/ServiceNow/M2Lingual, as well as the\nfirst fully synthetic, general and task-oriented, multi-turn, multilingual\ndataset built with Evol - M2Lingual:\nhttps://huggingface.co/datasets/ServiceNow-AI/ M2Lingual - containing 182K\ntotal IFT pairs, covering 70 languages and 17+ NLP tasks.\n","authors":["Rishabh Maheshwary","Vikas Yadav","Hoang Nguyen","Khyati Mahajan","Sathwik Tejaswi Madhusudhan"],"pdf_url":"https://arxiv.org/pdf/2406.16783v2.pdf","comment":"39 pages"},{"id":"http://arxiv.org/abs/2402.03216v4","updated":"2024-06-28T09:55:49Z","published":"2024-02-05T17:26:49Z","title":"BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity\n  Text Embeddings Through Self-Knowledge Distillation","summary":"  In this paper, we present a new embedding model, called M3-Embedding, which\nis distinguished for its versatility in Multi-Linguality, Multi-Functionality,\nand Multi-Granularity. It can support more than 100 working languages, leading\nto new state-of-the-art performances on multi-lingual and cross-lingual\nretrieval tasks. It can simultaneously perform the three common retrieval\nfunctionalities of embedding model: dense retrieval, multi-vector retrieval,\nand sparse retrieval, which provides a unified model foundation for real-world\nIR applications. It is able to process inputs of different granularities,\nspanning from short sentences to long documents of up to 8192 tokens. The\neffective training of M3-Embedding involves the following technical\ncontributions. We propose a novel self-knowledge distillation approach, where\nthe relevance scores from different retrieval functionalities can be integrated\nas the teacher signal to enhance the training quality. We also optimize the\nbatching strategy, enabling a large batch size and high training throughput to\nensure the discriminativeness of embeddings. To the best of our knowledge,\nM3-Embedding is the first embedding model which realizes such a strong\nversatility. The model and code will be publicly available at\nhttps://github.com/FlagOpen/FlagEmbedding.\n","authors":["Jianlv Chen","Shitao Xiao","Peitian Zhang","Kun Luo","Defu Lian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2402.03216v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19783v1","updated":"2024-06-28T09:39:33Z","published":"2024-06-28T09:39:33Z","title":"NLPerturbator: Studying the Robustness of Code LLMs to Natural Language\n  Variations","summary":"  Large language models (LLMs) achieve promising results in code generation\nbased on a given natural language description. They have been integrated into\nopen-source projects and commercial products to facilitate daily coding\nactivities. The natural language description in the prompt is crucial for LLMs\nto comprehend users' requirements. Prior studies uncover that LLMs are\nsensitive to the changes in the prompts, including slight changes that look\ninconspicuous. However, the natural language descriptions often vary in\nreal-world scenarios (e.g., different formats, grammar, and wording). Prior\nstudies on the robustness of LLMs are often based on random perturbations and\nsuch perturbations may not actually happen. In this paper, we conduct a\ncomprehensive study to investigate how are code LLMs robust to variations of\nnatural language description in real-world scenarios. We summarize 18\ncategories of perturbations of natural language and 3 combinations of\nco-occurred categories based on our literature review and an online survey with\npractitioners. We propose an automated framework, NLPerturbator, which can\nperform perturbations of each category given a set of prompts. Through a series\nof experiments on code generation using six code LLMs, we find that the\nperturbed prompts can decrease the performance of code generation by a\nconsiderable margin (e.g., up to 21.2%, and 4.8% to 6.1% on average). Our study\nhighlights the importance of enhancing the robustness of LLMs to real-world\nvariations in the prompts, as well as the essentiality of attentively\nconstructing the prompts.\n","authors":["Junkai Chen","Zhenhao Li","Xing Hu","Xin Xia"],"pdf_url":"https://arxiv.org/pdf/2406.19783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19774v1","updated":"2024-06-28T09:23:40Z","published":"2024-06-28T09:23:40Z","title":"Direct Preference Knowledge Distillation for Large Language Models","summary":"  In the field of large language models (LLMs), Knowledge Distillation (KD) is\na critical technique for transferring capabilities from teacher models to\nstudent models. However, existing KD methods face limitations and challenges in\ndistillation of LLMs, including efficiency and insufficient measurement\ncapabilities of traditional KL divergence. It is shown that LLMs can serve as\nan implicit reward function, which we define as a supplement to KL divergence.\nIn this work, we propose Direct Preference Knowledge Distillation (DPKD) for\nLLMs. DPKD utilizes distribution divergence to represent the preference loss\nand implicit reward function. We re-formulate KD of LLMs into two stages: first\noptimizing and objective consisting of implicit reward and reverse KL\ndivergence and then improving the preference probability of teacher outputs\nover student outputs. We conducted experiments and analysis on various datasets\nwith LLM parameters ranging from 120M to 13B and demonstrate the broad\napplicability and effectiveness of our DPKD approach. Meanwhile, we prove the\nvalue and effectiveness of the introduced implicit reward and output preference\nin KD through experiments and theoretical analysis. The DPKD method outperforms\nthe baseline method in both output response precision and exact match\npercentage. Code and data are available at https://aka.ms/dpkd.\n","authors":["Yixing Li","Yuxian Gu","Li Dong","Dequan Wang","Yu Cheng","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2406.19774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10552v3","updated":"2024-06-28T09:16:28Z","published":"2024-06-15T08:13:47Z","title":"Large Language Model Enhanced Clustering for News Event Detection","summary":"  The news landscape is continuously evolving, with an ever-increasing volume\nof information from around the world. Automated event detection within this\nvast data repository is essential for monitoring, identifying, and categorizing\nsignificant news occurrences across diverse platforms. This paper presents an\nevent detection framework that leverages Large Language Models (LLMs) combined\nwith clustering analysis to detect news events from the Global Database of\nEvents, Language, and Tone (GDELT). The framework enhances event clustering\nthrough both pre-event detection tasks (keyword extraction and text embedding)\nand post-event detection tasks (event summarization and topic labelling). We\nalso evaluate the impact of various textual embeddings on the quality of\nclustering outcomes, ensuring robust news categorization. Additionally, we\nintroduce a novel Cluster Stability Assessment Index (CSAI) to assess the\nvalidity and robustness of clustering results. CSAI utilizes multiple feature\nvectors to provide a new way of measuring clustering quality. Our experiments\nindicate that the use of LLM embedding in the event detection framework has\nsignificantly improved the results, demonstrating greater robustness in terms\nof CSAI scores. Moreover, post-event detection tasks generate meaningful\ninsights, facilitating effective interpretation of event clustering results.\nOverall, our experimental results indicate that the proposed framework offers\nvaluable insights and could enhance the accuracy in news analysis and\nreporting.\n","authors":["Adane Nega Tarekegn"],"pdf_url":"https://arxiv.org/pdf/2406.10552v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19764v1","updated":"2024-06-28T09:09:36Z","published":"2024-06-28T09:09:36Z","title":"Belief Revision: The Adaptability of Large Language Models Reasoning","summary":"  The capability to reason from text is crucial for real-world NLP\napplications. Real-world scenarios often involve incomplete or evolving data.\nIn response, individuals update their beliefs and understandings accordingly.\nHowever, most existing evaluations assume that language models (LMs) operate\nwith consistent information. We introduce Belief-R, a new dataset designed to\ntest LMs' belief revision ability when presented with new evidence. Inspired by\nhow humans suppress prior inferences, this task assesses LMs within the newly\nproposed delta reasoning ($\\Delta R$) framework. Belief-R features sequences of\npremises designed to simulate scenarios where additional information could\nnecessitate prior conclusions drawn by LMs. We evaluate $\\sim$30 LMs across\ndiverse prompting strategies and found that LMs generally struggle to\nappropriately revise their beliefs in response to new information. Further,\nmodels adept at updating often underperformed in scenarios without necessary\nupdates, highlighting a critical trade-off. These insights underscore the\nimportance of improving LMs' adaptiveness to changing information, a step\ntoward more reliable AI systems.\n","authors":["Bryan Wilie","Samuel Cahyawijaya","Etsuko Ishii","Junxian He","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2406.19764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19760v1","updated":"2024-06-28T08:59:45Z","published":"2024-06-28T08:59:45Z","title":"Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case\n  Reformulation","summary":"  Legal case retrieval for sourcing similar cases is critical in upholding\njudicial fairness. Different from general web search, legal case retrieval\ninvolves processing lengthy, complex, and highly specialized legal documents.\nExisting methods in this domain often overlook the incorporation of legal\nexpert knowledge, which is crucial for accurately understanding and modeling\nlegal cases, leading to unsatisfactory retrieval performance. This paper\nintroduces KELLER, a legal knowledge-guided case reformulation approach based\non large language models (LLMs) for effective and interpretable legal case\nretrieval. By incorporating professional legal knowledge about crimes and law\narticles, we enable large language models to accurately reformulate the\noriginal legal case into concise sub-facts of crimes, which contain the\nessential information of the case. Extensive experiments on two legal case\nretrieval benchmarks demonstrate superior retrieval performance and robustness\non complex legal case queries of KELLER over existing methods.\n","authors":["Chenlong Deng","Kelong Mao","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2406.19760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19759v1","updated":"2024-06-28T08:59:24Z","published":"2024-06-28T08:59:24Z","title":"Breaking the Script Barrier in Multilingual Pre-Trained Language Models\n  with Transliteration-Based Post-Training Alignment","summary":"  Multilingual pre-trained models (mPLMs) have shown impressive performance on\ncross-lingual transfer tasks. However, the transfer performance is often\nhindered when a low-resource target language is written in a different script\nthan the high-resource source language, even though the two languages may be\nrelated or share parts of their vocabularies. Inspired by recent work that uses\ntransliteration to address this problem, our paper proposes a\ntransliteration-based post-pretraining alignment (PPA) method aiming to improve\nthe cross-lingual alignment between languages using diverse scripts. We select\ntwo areal language groups, $\\textbf{Mediterranean-Amharic-Farsi}$ and\n$\\textbf{South+East Asian Languages}$, wherein the languages are mutually\ninfluenced but use different scripts. We apply our method to these language\ngroups and conduct extensive experiments on a spectrum of downstream tasks. The\nresults show that after PPA, models consistently outperform the original model\n(up to 50% for some tasks) in English-centric transfer. In addition, when we\nuse languages other than English as sources in transfer, our method obtains\neven larger improvements. We will make our code and models publicly available\nat \\url{https://github.com/cisnlp/Transliteration-PPA}.\n","authors":["Orgest Xhelili","Yihong Liu","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2406.19759v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2406.15486v2","updated":"2024-06-28T08:55:17Z","published":"2024-06-17T11:05:15Z","title":"SampleAttention: Near-Lossless Acceleration of Long Context LLM\n  Inference with Adaptive Structured Sparse Attention","summary":"  Large language models (LLMs) now support extremely long context windows, but\nthe quadratic complexity of vanilla attention results in significantly long\nTime-to-First-Token (TTFT) latency. Existing approaches to address this\ncomplexity require additional pretraining or finetuning, and often sacrifice\nmodel accuracy. In this paper, we first provide both theoretical and empirical\nfoundations for near-lossless sparse attention. We find dynamically capturing\nhead-specific sparse patterns at runtime with low overhead is crucial. To\naddress this, we propose SampleAttention, an adaptive structured and\nnear-lossless sparse attention. Leveraging observed significant sparse\npatterns, SampleAttention attends to a fixed percentage of adjacent tokens to\ncapture local window patterns, and employs a two-stage query-guided key-value\nfiltering approach, which adaptively select a minimum set of key-values with\nlow overhead, to capture column stripe patterns. Comprehensive evaluations show\nthat SampleAttention can seamlessly replace vanilla attention in off-the-shelf\nLLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times$\ncompared with FlashAttention.\n","authors":["Qianchao Zhu","Jiangfei Duan","Chang Chen","Siran Liu","Xiuhong Li","Guanyu Feng","Xin Lv","Huanqi Cao","Xiao Chuanfu","Xingcheng Zhang","Dahua Lin","Chao Yang"],"pdf_url":"https://arxiv.org/pdf/2406.15486v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19736v1","updated":"2024-06-28T08:25:27Z","published":"2024-06-28T08:25:27Z","title":"MM-Instruct: Generated Visual Instructions for Large Multimodal Model\n  Alignment","summary":"  This paper introduces MM-Instruct, a large-scale dataset of diverse and\nhigh-quality visual instruction data designed to enhance the\ninstruction-following capabilities of large multimodal models (LMMs). While\nexisting visual instruction datasets often focus on question-answering, they\nstruggle to generalize to broader application scenarios such as creative\nwriting, summarization, or image analysis. To address these limitations, we\npropose a novel approach to constructing MM-Instruct that leverages the strong\ninstruction-following capabilities of existing LLMs to generate novel visual\ninstruction data from large-scale but conventional image captioning datasets.\nMM-Instruct first leverages ChatGPT to automatically generate diverse\ninstructions from a small set of seed instructions through augmenting and\nsummarization. It then matches these instructions with images and uses an\nopen-sourced large language model (LLM) to generate coherent answers to the\ninstruction-image pairs. The LLM is grounded by the detailed text descriptions\nof images in the whole answer generation process to guarantee the alignment of\nthe instruction data. Moreover, we introduce a benchmark based on the generated\ninstruction data to evaluate the instruction-following capabilities of existing\nLMMs. We demonstrate the effectiveness of MM-Instruct by training a LLaVA-1.5\nmodel on the generated data, denoted as LLaVA-Instruct, which exhibits\nsignificant improvements in instruction-following capabilities compared to\nLLaVA-1.5 models. The MM-Instruct dataset, benchmark, and pre-trained models\nare available at https://github.com/jihaonew/MM-Instruct.\n","authors":["Jihao Liu","Xin Huang","Jinliang Zheng","Boxiao Liu","Jia Wang","Osamu Yoshie","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2406.19736v1.pdf","comment":"Dataset and models are available at\n  https://github.com/jihaonew/MM-Instruct"},{"id":"http://arxiv.org/abs/2307.10635v3","updated":"2024-06-28T08:24:13Z","published":"2023-07-20T07:01:57Z","title":"SciBench: Evaluating College-Level Scientific Problem-Solving Abilities\n  of Large Language Models","summary":"  Most of the existing Large Language Model (LLM) benchmarks on scientific\nproblem reasoning focus on problems grounded in high-school subjects and are\nconfined to elementary algebraic operations. To systematically examine the\nreasoning capabilities required for solving complex scientific problems, we\nintroduce an expansive benchmark suite SciBench for LLMs. SciBench contains a\ncarefully curated dataset featuring a range of collegiate-level scientific\nproblems from mathematics, chemistry, and physics domains. Based on the\ndataset, we conduct an in-depth benchmarking study of representative\nopen-source and proprietary LLMs with various prompting strategies. The results\nreveal that the current LLMs fall short of delivering satisfactory performance,\nwith the best overall score of merely 43.22%. Furthermore, through a detailed\nuser study, we categorize the errors made by LLMs into ten problem-solving\nabilities. Our analysis indicates that no single prompting strategy\nsignificantly outperforms the others and some strategies that demonstrate\nimprovements in certain problem-solving skills could result in declines in\nother skills. We envision that SciBench will catalyze further developments in\nthe reasoning abilities of LLMs, thereby ultimately contributing to scientific\nresearch and discovery.\n","authors":["Xiaoxuan Wang","Ziniu Hu","Pan Lu","Yanqiao Zhu","Jieyu Zhang","Satyen Subramaniam","Arjun R. Loomba","Shichang Zhang","Yizhou Sun","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2307.10635v3.pdf","comment":"To appear at ICML 2024"},{"id":"http://arxiv.org/abs/2402.08114v2","updated":"2024-06-28T08:22:01Z","published":"2024-02-12T23:09:00Z","title":"Active Preference Learning for Large Language Models","summary":"  As large language models (LLMs) become more capable, fine-tuning techniques\nfor aligning with human intent are increasingly important. A key consideration\nfor aligning these models is how to most effectively use human resources, or\nmodel resources in the case where LLMs themselves are used as oracles.\nReinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most\nprominent example of such a technique, but is complex and often unstable.\nDirect Preference Optimization (DPO) has recently been proposed as a simpler\nand more stable alternative. In this work, we develop an active learning\nstrategy for DPO to make better use of preference labels. We propose a\npractical acquisition function for prompt/completion pairs based on the\npredictive entropy of the language model and a measure of certainty of the\nimplicit preference model optimized by DPO. We demonstrate how our approach\nimproves both the rate of learning and final performance of fine-tuning on\npairwise preference data.\n","authors":["William Muldrew","Peter Hayes","Mingtian Zhang","David Barber"],"pdf_url":"https://arxiv.org/pdf/2402.08114v2.pdf","comment":"13 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.19731v1","updated":"2024-06-28T08:20:32Z","published":"2024-06-28T08:20:32Z","title":"Message du troisi{è}me type : irruption d'un tiers dans un dialogue en\n  ligne","summary":"  Our study focuses on Wikipedia talk pages, from a global perspective\nanalyzing contributors' behaviors in online interactions. Using a corpus\ncomprising all Wikipedia talk pages in French, totaling more than 300,000\ndiscussion threads, we examine how discussions with more than two participants\n(multiparty conversation) unfold and we specifically investigate the role of a\nthird participant's intervention when two Wikipedians have already initiated an\nexchange. In this regard, we concentrate on the sequential structure of these\ninteractions in terms of articulation among different participants and aim to\nspecify this third message by exploring its lexical particularities, while also\nproposing an initial typology of the third participant's message role and how\nit aligns with preceding messages.\n","authors":["Ludovic Tanguy","Céline Poudat","Lydia-Mai Ho-Dac"],"pdf_url":"https://arxiv.org/pdf/2406.19731v1.pdf","comment":"in French language. JADT 2024 - 17es Journ{\\'e}es internationales\n  d'Analyse statistique des Donn{\\'e}es Textuelles, SeSLa (S{\\'e}minaire des\n  Sciences du Langage de l'UCLouvain -- Site Saint-Louis); LASLA (Laboratoire\n  d'Analyse statistique des Langues anciennes de l'Universit{\\'e} de\n  Li{\\`e}ge), 2024, Bruxelles, Belgique"},{"id":"http://arxiv.org/abs/2406.19729v1","updated":"2024-06-28T08:19:36Z","published":"2024-06-28T08:19:36Z","title":"Le sens de la famille : analyse du vocabulaire de la parent{é} par les\n  plongements de mots","summary":"  In this study, we propose a corpus analysis of an area of the French lexicon\nthat is both dense and highly structured: the vocabulary of family\nrelationships. Starting with a lexicon of 25 nouns designating the main\nrelationships (son, cousin, mother, grandfather, sister-in-law etc.), we\nexamine how these terms are positioned in relation to each other through\ndistributional analyses based on the use of these terms in corpora. We show\nthat distributional information can capture certain features that organize this\nvocabulary (descent, alliance, siblings, genre), in ways that vary according to\nthe different corpora compared.\n","authors":["Ludovic Tanguy","Cécile Fabre","Nabil Hathout","Lydia-Mai Ho-Dac"],"pdf_url":"https://arxiv.org/pdf/2406.19729v1.pdf","comment":"in French language. JADT 2024 - 17es Journ{\\'e}es internationales\n  d'Analyse statistique des Donn{\\'e}es Textuelles, SeSLa (S{\\'e}minaire des\n  Sciences du Langage de l'UCLouvain -- Site Saint-Louis), 2024, Bruxelles,\n  Belgique"},{"id":"http://arxiv.org/abs/2406.18966v2","updated":"2024-06-28T08:12:28Z","published":"2024-06-27T07:56:44Z","title":"UniGen: A Unified Framework for Textual Dataset Generation Using Large\n  Language Models","summary":"  Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly\nimpacted various fields by enabling high-quality synthetic data generation and\nreducing dependence on expensive human-generated datasets. Despite this,\nchallenges remain in the areas of generalization, controllability, diversity,\nand truthfulness within the existing generative frameworks. To address these\nchallenges, this paper presents UniGen, a comprehensive LLM-powered framework\ndesigned to produce diverse, accurate, and highly controllable datasets. UniGen\nis adaptable, supporting all types of text datasets and enhancing the\ngenerative process through innovative mechanisms. To augment data diversity,\nUniGen incorporates an attribute-guided generation module and a group checking\nfeature. For accuracy, it employs a code-based mathematical assessment for\nlabel verification alongside a retrieval-augmented generation technique for\nfactual validation. The framework also allows for user-specified constraints,\nenabling customization of the data generation process to suit particular\nrequirements. Extensive experiments demonstrate the superior quality of data\ngenerated by UniGen, and each module within UniGen plays a critical role in\nthis enhancement. Additionally, UniGen is applied in two practical scenarios:\nbenchmarking LLMs and data augmentation. The results indicate that UniGen\neffectively supports dynamic and evolving benchmarking, and that data\naugmentation improves LLM capabilities in various domains, including\nagent-oriented abilities and reasoning skills.\n","authors":["Siyuan Wu","Yue Huang","Chujie Gao","Dongping Chen","Qihui Zhang","Yao Wan","Tianyi Zhou","Xiangliang Zhang","Jianfeng Gao","Chaowei Xiao","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2406.18966v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09703v2","updated":"2024-06-28T08:03:19Z","published":"2024-03-08T19:07:47Z","title":"Concept-aware Data Construction Improves In-context Learning of Language\n  Models","summary":"  Many recent language models (LMs) are capable of in-context learning (ICL),\nmanifested in the LMs' ability to perform a new task solely from\nnatural-language instruction. Previous work curating in-context learners\nassumes that ICL emerges from a vast over-parametrization or the scale of\nmulti-task training. However, recent theoretical work attributes the ICL\nability to concept-dependent training data and creates functional in-context\nlearners even in small-scale, synthetic settings.\n  In this work, we practically explore this newly identified axis of ICL\nquality. We propose Concept-aware Training (CoAT), a framework for constructing\ntraining scenarios that make it beneficial for the LM to learn to utilize the\nanalogical reasoning concepts from demonstrations. We find that by using CoAT,\npre-trained transformers can learn to better utilise new latent concepts from\ndemonstrations and that such ability makes ICL more robust to the functional\ndeficiencies of the previous models. Finally, we show that concept-aware\nin-context learning is more effective for a majority of new tasks when compared\nto traditional instruction tuning, resulting in a performance comparable to the\nprevious in-context learners using magnitudes of more training data.\n","authors":["Michal Štefánik","Marek Kadlčík","Petr Sojka"],"pdf_url":"https://arxiv.org/pdf/2403.09703v2.pdf","comment":"Long paper to appear in Findings of ACL 2024"},{"id":"http://arxiv.org/abs/2406.01124v3","updated":"2024-06-28T07:54:19Z","published":"2024-06-03T09:10:42Z","title":"Latent Logic Tree Extraction for Event Sequence Explanation from LLMs","summary":"  Modern high-stakes systems, such as healthcare or robotics, often generate\nvast streaming event sequences. Our goal is to design an efficient,\nplug-and-play tool to elicit logic tree-based explanations from Large Language\nModels (LLMs) to provide customized insights into each observed event sequence.\nBuilt on the temporal point process model for events, our method employs the\nlikelihood function as a score to evaluate generated logic trees. We propose an\namortized Expectation-Maximization (EM) learning framework and treat the logic\ntree as latent variables. In the E-step, we evaluate the posterior distribution\nover the latent logic trees using an LLM prior and the likelihood of the\nobserved event sequences. LLM provides a high-quality prior for the latent\nlogic trees, however, since the posterior is built over a discrete\ncombinatorial space, we cannot get the closed-form solution. We propose to\ngenerate logic tree samples from the posterior using a learnable GFlowNet,\nwhich is a diversity-seeking generator for structured discrete variables. The\nM-step employs the generated logic rules to approximate marginalization over\nthe posterior, facilitating the learning of model parameters and refining the\ntunable LLM prior parameters. In the online setting, our locally built,\nlightweight model will iteratively extract the most relevant rules from LLMs\nfor each sequence using only a few iterations. Empirical demonstrations\nshowcase the promising performance and adaptability of our framework.\n","authors":["Zitao Song","Chao Yang","Chaojie Wang","Bo An","Shuang Li"],"pdf_url":"https://arxiv.org/pdf/2406.01124v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19712v1","updated":"2024-06-28T07:47:34Z","published":"2024-06-28T07:47:34Z","title":"Uncertainty Quantification in Large Language Models Through Convex Hull\n  Analysis","summary":"  Uncertainty quantification approaches have been more critical in large\nlanguage models (LLMs), particularly high-risk applications requiring reliable\noutputs. However, traditional methods for uncertainty quantification, such as\nprobabilistic models and ensemble techniques, face challenges when applied to\nthe complex and high-dimensional nature of LLM-generated outputs. This study\nproposes a novel geometric approach to uncertainty quantification using convex\nhull analysis. The proposed method leverages the spatial properties of response\nembeddings to measure the dispersion and variability of model outputs. The\nprompts are categorized into three types, i.e., `easy', `moderate', and\n`confusing', to generate multiple responses using different LLMs at varying\ntemperature settings. The responses are transformed into high-dimensional\nembeddings via a BERT model and subsequently projected into a two-dimensional\nspace using Principal Component Analysis (PCA). The Density-Based Spatial\nClustering of Applications with Noise (DBSCAN) algorithm is utilized to cluster\nthe embeddings and compute the convex hull for each selected cluster. The\nexperimental results indicate that the uncertainty of the model for LLMs\ndepends on the prompt complexity, the model, and the temperature setting.\n","authors":["Ferhat Ozgur Catak","Murat Kuzlu"],"pdf_url":"https://arxiv.org/pdf/2406.19712v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2402.11622v2","updated":"2024-06-28T07:20:22Z","published":"2024-02-18T15:28:39Z","title":"Logical Closed Loop: Uncovering Object Hallucinations in Large\n  Vision-Language Models","summary":"  Object hallucination has been an Achilles' heel which hinders the broader\napplications of large vision-language models (LVLMs). Object hallucination\nrefers to the phenomenon that the LVLMs claim non-existent objects in the\nimage. To mitigate the object hallucinations, instruction tuning and external\nmodel-based detection methods have been proposed, which either require\nlarge-scare computational resources or depend on the detection result of\nexternal models. However, there remains an under-explored field to utilize the\nLVLM itself to alleviate object hallucinations. In this work, we adopt the\nintuition that the LVLM tends to respond logically consistently for existent\nobjects but inconsistently for hallucinated objects. Therefore, we propose a\nLogical Closed Loop-based framework for Object Hallucination Detection and\nMitigation, namely LogicCheckGPT. In specific, we devise logical consistency\nprobing to raise questions with logical correlations, inquiring about\nattributes from objects and vice versa. Whether their responses can form a\nlogical closed loop serves as an indicator of object hallucination. As a\nplug-and-play method, it can be seamlessly applied to all existing LVLMs.\nComprehensive experiments conducted on three benchmarks across four LVLMs have\ndemonstrated significant improvements brought by our method, indicating its\neffectiveness and generality.\n","authors":["Junfei Wu","Qiang Liu","Ding Wang","Jinghao Zhang","Shu Wu","Liang Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2402.11622v2.pdf","comment":"Accept to ACL 2024; 19 Pages, 15 Figures, 6 Tables"},{"id":"http://arxiv.org/abs/2402.03848v6","updated":"2024-06-28T06:49:39Z","published":"2024-02-06T09:50:08Z","title":"ANLS* -- A Universal Document Processing Metric for Generative Large\n  Language Models","summary":"  Traditionally, discriminative models have been the predominant choice for\ntasks like document classification and information extraction. These models\nmake predictions that fall into a limited number of predefined classes,\nfacilitating a binary true or false evaluation and enabling the direct\ncalculation of metrics such as the F1 score. However, recent advancements in\ngenerative large language models (GLLMs) have prompted a shift in the field due\nto their enhanced zero-shot capabilities, which eliminate the need for a\ndownstream dataset and computationally expensive fine-tuning. However,\nevaluating GLLMs presents a challenge as the binary true or false evaluation\nused for discriminative models is not applicable to the predictions made by\nGLLMs.\n  This paper introduces a new metric for generative models called ANLS* for\nevaluating a wide variety of tasks, including information extraction and\nclassification tasks. The ANLS* metric extends existing ANLS metrics as a\ndrop-in-replacement and is still compatible with previously reported ANLS\nscores. An evaluation of 7 different datasets, and more than 10 different GLLMs\ntogether with 3 different prompting methods using the ANLS* metric is also\nprovided, demonstrating the importance of the proposed metric.\n  We also benchmark a novel approach to generate prompts for documents, called\nSFT, against other prompting techniques such as LATIN. In 6 out of 7 cases, SFT\noutperforms other techniques and improves the state-of-the-art, sometimes by as\nmuch as $10$ percentage points.\n  Sources are available at https://github.com/deepopinion/anls_star_metric\n","authors":["David Peer","Philemon Schöpf","Volckmar Nebendahl","Alexander Rietzler","Sebastian Stabinger"],"pdf_url":"https://arxiv.org/pdf/2402.03848v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17710v2","updated":"2024-06-28T06:24:37Z","published":"2024-05-28T00:00:04Z","title":"Does Geo-co-location Matter? A Case Study of Public Health Conversations\n  during COVID-19","summary":"  Social media platforms like Twitter (now X) have been pivotal in information\ndissemination and public engagement, especially during COVID-19. A key goal for\npublic health experts was to encourage prosocial behavior that could impact\nlocal outcomes such as masking and social distancing. Given the importance of\nlocal news and guidance during COVID-19, the objective of our research is to\nanalyze the effect of localized engagement, on social media conversations. This\nstudy examines the impact of geographic co-location, as a proxy for localized\nengagement between public health experts (PHEs) and the public, on social\nmedia. We analyze a Twitter conversation dataset from January 2020 to November\n2021, comprising over 19 K tweets from nearly five hundred PHEs, along with\napproximately 800 K replies from 350 K participants. Our findings reveal that\ngeo-co-location is associated with higher engagement rates, especially in\nconversations on topics including masking, lockdowns, and education, and in\nconversations with academic and medical professionals. Lexical features\nassociated with emotion and personal experiences were more common in\ngeo-co-located contexts. This research provides insights into how geographic\nco-location influences social media engagement and can inform strategies to\nimprove public health messaging.\n","authors":["Paiheng Xu","Louiqa Raschid","Vanessa Frias-Martinez"],"pdf_url":"https://arxiv.org/pdf/2405.17710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19674v1","updated":"2024-06-28T06:22:23Z","published":"2024-06-28T06:22:23Z","title":"Less is More: Accurate Speech Recognition & Translation without\n  Web-Scale Data","summary":"  Recent advances in speech recognition and translation rely on hundreds of\nthousands of hours of Internet speech data. We argue that state-of-the art\naccuracy can be reached without relying on web-scale data. Canary -\nmultilingual ASR and speech translation model, outperforms current\nstate-of-the-art models - Whisper, OWSM, and Seamless-M4T on English, French,\nSpanish, and German languages, while being trained on an order of magnitude\nless data than these models. Three key factors enables such data-efficient\nmodel: (1) a FastConformer-based attention encoder-decoder architecture (2)\ntraining on synthetic data generated with machine translation and (3) advanced\ntraining techniques: data-balancing, dynamic data blending, dynamic bucketing\nand noise-robust fine-tuning. The model, weights, and training code will be\nopen-sourced.\n","authors":["Krishna C. Puvvada","Piotr Żelasko","He Huang","Oleksii Hrinchuk","Nithin Rao Koluguri","Kunal Dhawan","Somshubra Majumdar","Elena Rastorgueva","Zhehuai Chen","Vitaly Lavrukhin","Jagadeesh Balam","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2406.19674v1.pdf","comment":"Accepted at Interspeech-2024"},{"id":"http://arxiv.org/abs/2403.03640v3","updated":"2024-06-28T06:16:24Z","published":"2024-03-06T11:56:02Z","title":"Apollo: A Lightweight Multilingual Medical LLM towards Democratizing\n  Medical AI to 6B People","summary":"  Despite the vast repository of global medical knowledge predominantly being\nin English, local languages are crucial for delivering tailored healthcare\nservices, particularly in areas with limited medical resources. To extend the\nreach of medical AI advancements to a broader population, we aim to develop\nmedical LLMs across the six most widely spoken languages, encompassing a global\npopulation of 6.1 billion. This effort culminates in the creation of the\nApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the\nmultilingual medical benchmark, the released Apollo models, at various\nrelatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best\nperformance among models of equivalent size. Especially, Apollo-7B is the\nstate-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite\nmodels could be used to improve the multi-lingual medical capabilities of\nlarger models without fine-tuning in a proxy-tuning fashion. We will\nopen-source training corpora, code, model weights and evaluation benchmark.\n","authors":["Xidong Wang","Nuo Chen","Junyin Chen","Yan Hu","Yidong Wang","Xiangbo Wu","Anningzhe Gao","Xiang Wan","Haizhou Li","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2403.03640v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.18118v2","updated":"2024-06-28T06:06:59Z","published":"2024-06-26T07:15:44Z","title":"SafeAligner: Safety Alignment against Jailbreak Attacks via Response\n  Disparity Guidance","summary":"  As the development of large language models (LLMs) rapidly advances, securing\nthese models effectively without compromising their utility has become a\npivotal area of research. However, current defense strategies against jailbreak\nattacks (i.e., efforts to bypass security protocols) often suffer from limited\nadaptability, restricted general capability, and high cost. To address these\nchallenges, we introduce SafeAligner, a methodology implemented at the decoding\nstage to fortify defenses against jailbreak attacks. We begin by developing two\nspecialized models: the Sentinel Model, which is trained to foster safety, and\nthe Intruder Model, designed to generate riskier responses. SafeAligner\nleverages the disparity in security levels between the responses from these\nmodels to differentiate between harmful and beneficial tokens, effectively\nguiding the safety alignment by altering the output token distribution of the\ntarget model. Extensive experiments show that SafeAligner can increase the\nlikelihood of beneficial tokens, while reducing the occurrence of harmful ones,\nthereby ensuring secure alignment with minimal loss to generality.\n","authors":["Caishuang Huang","Wanxu Zhao","Rui Zheng","Huijie Lv","Shihan Dou","Sixian Li","Xiao Wang","Enyu Zhou","Junjie Ye","Yuming Yang","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2406.18118v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19237v2","updated":"2024-06-28T05:43:46Z","published":"2024-06-27T15:01:48Z","title":"FlowVQA: Mapping Multimodal Logic in Visual Question Answering with\n  Flowcharts","summary":"  Existing benchmarks for visual question answering lack in visual grounding\nand complexity, particularly in evaluating spatial reasoning skills. We\nintroduce FlowVQA, a novel benchmark aimed at assessing the capabilities of\nvisual question-answering multimodal language models in reasoning with\nflowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and\nhuman-verified flowchart images from three distinct content sources, along with\n22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,\nincluding information localization, decision-making, and logical progression.\nWe conduct a thorough baseline evaluation on a suite of both open-source and\nproprietary multimodal language models using various strategies, followed by an\nanalysis of directional bias. The results underscore the benchmark's potential\nas a vital tool for advancing the field of multimodal modeling, providing a\nfocused and challenging environment for enhancing model performance in visual\nand logical reasoning tasks.\n","authors":["Shubhankar Singh","Purvi Chaurasia","Yerram Varun","Pranshu Pandya","Vatsal Gupta","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2406.19237v2.pdf","comment":"Accepted in ACL 2024 (Findings), 21 pages, 7 figures, 9 Tables"},{"id":"http://arxiv.org/abs/2406.19650v1","updated":"2024-06-28T04:38:54Z","published":"2024-06-28T04:38:54Z","title":"DECOR: Improving Coherence in L2 English Writing with a Novel Benchmark\n  for Incoherence Detection, Reasoning, and Rewriting","summary":"  Coherence in writing, an aspect that second-language (L2) English learners\noften struggle with, is crucial in assessing L2 English writing. Existing\nautomated writing evaluation systems primarily use basic surface linguistic\nfeatures to detect coherence in writing. However, little effort has been made\nto correct the detected incoherence, which could significantly benefit L2\nlanguage learners seeking to improve their writing. To bridge this gap, we\nintroduce DECOR, a novel benchmark that includes expert annotations for\ndetecting incoherence in L2 English writing, identifying the underlying\nreasons, and rewriting the incoherent sentences. To our knowledge, DECOR is the\nfirst coherence assessment dataset specifically designed for improving L2\nEnglish writing, featuring pairs of original incoherent sentences alongside\ntheir expert-rewritten counterparts. Additionally, we fine-tuned models to\nautomatically detect and rewrite incoherence in student essays. We find that\nincorporating specific reasons for incoherence during fine-tuning consistently\nimproves the quality of the rewrites, achieving a result that is favored in\nboth automatic and human evaluations.\n","authors":["Xuanming Zhang","Anthony Diaz","Zixun Chen","Qingyang Wu","Kun Qian","Erik Voss","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2406.19650v1.pdf","comment":"21 pages, 5 figures, 20 tables"},{"id":"http://arxiv.org/abs/2406.19648v1","updated":"2024-06-28T04:33:41Z","published":"2024-06-28T04:33:41Z","title":"Designing and Evaluating Multi-Chatbot Interface for Human-AI\n  Communication: Preliminary Findings from a Persuasion Task","summary":"  The dynamics of human-AI communication have been reshaped by language models\nsuch as ChatGPT. However, extant research has primarily focused on dyadic\ncommunication, leaving much to be explored regarding the dynamics of human-AI\ncommunication in group settings. The availability of multiple language model\nchatbots presents a unique opportunity for scholars to better understand the\ninteraction between humans and multiple chatbots. This study examines the\nimpact of multi-chatbot communication in a specific persuasion setting:\npromoting charitable donations. We developed an online environment that enables\nmulti-chatbot communication and conducted a pilot experiment utilizing two\nGPT-based chatbots, Save the Children and UNICEF chatbots, to promote\ncharitable donations. In this study, we present our development process of the\nmulti-chatbot interface and present preliminary findings from a pilot\nexperiment. Analysis of qualitative and quantitative feedback are presented,\nand limitations are addressed.\n","authors":["Sion Yoon","Tae Eun Kim","Yoo Jung Oh"],"pdf_url":"https://arxiv.org/pdf/2406.19648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19643v1","updated":"2024-06-28T04:21:20Z","published":"2024-06-28T04:21:20Z","title":"Unlocking Varied Perspectives: A Persona-Based Multi-Agent Framework\n  with Debate-Driven Text Planning for Argument Generation","summary":"  Writing persuasive arguments is a challenging task for both humans and\nmachines. It entails incorporating high-level beliefs from various perspectives\non the topic, along with deliberate reasoning and planning to construct a\ncoherent narrative. Current language models often generate surface tokens\nautoregressively, lacking explicit integration of these underlying controls,\nresulting in limited output diversity and coherence. In this work, we propose a\npersona-based multi-agent framework for argument writing. Inspired by the human\ndebate, we first assign each agent a persona representing its high-level\nbeliefs from a unique perspective, and then design an agent interaction process\nso that the agents can collaboratively debate and discuss the idea to form an\noverall plan for argument writing. Such debate process enables fluid and\nnonlinear development of ideas. We evaluate our framework on argumentative\nessay writing. The results show that our framework can generate more diverse\nand persuasive arguments through both automatic and human evaluations.\n","authors":["Zhe Hu","Hou Pong Chan","Jing Li","Yu Yin"],"pdf_url":"https://arxiv.org/pdf/2406.19643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19642v1","updated":"2024-06-28T04:14:35Z","published":"2024-06-28T04:14:35Z","title":"IDT: Dual-Task Adversarial Attacks for Privacy Protection","summary":"  Natural language processing (NLP) models may leak private information in\ndifferent ways, including membership inference, reconstruction or attribute\ninference attacks. Sensitive information may not be explicit in the text, but\nhidden in underlying writing characteristics. Methods to protect privacy can\ninvolve using representations inside models that are demonstrated not to detect\nsensitive attributes or -- for instance, in cases where users might not trust a\nmodel, the sort of scenario of interest here -- changing the raw text before\nmodels can have access to it. The goal is to rewrite text to prevent someone\nfrom inferring a sensitive attribute (e.g. the gender of the author, or their\nlocation by the writing style) whilst keeping the text useful for its original\nintention (e.g. the sentiment of a product review). The few works tackling this\nhave focused on generative techniques. However, these often create extensively\ndifferent texts from the original ones or face problems such as mode collapse.\nThis paper explores a novel adaptation of adversarial attack techniques to\nmanipulate a text to deceive a classifier w.r.t one task (privacy) whilst\nkeeping the predictions of another classifier trained for another task\n(utility) unchanged. We propose IDT, a method that analyses predictions made by\nauxiliary and interpretable models to identify which tokens are important to\nchange for the privacy task, and which ones should be kept for the utility\ntask. We evaluate different datasets for NLP suitable for different tasks.\nAutomatic and human evaluations show that IDT retains the utility of text,\nwhile also outperforming existing methods when deceiving a classifier w.r.t\nprivacy task.\n","authors":["Pedro Faustini","Shakila Mahjabin Tonni","Annabelle McIver","Qiongkai Xu","Mark Dras"],"pdf_url":"https://arxiv.org/pdf/2406.19642v1.pdf","comment":"28 pages, 1 figure"},{"id":"http://arxiv.org/abs/2406.12058v3","updated":"2024-06-28T04:08:12Z","published":"2024-06-17T19:50:40Z","title":"WellDunn: On the Robustness and Explainability of Language Models and\n  Large Language Models in Identifying Wellness Dimensions","summary":"  Language Models (LMs) are being proposed for mental health applications where\nthe heightened risk of adverse outcomes means predictive performance may not be\na sufficient litmus test of a model's utility in clinical practice. A model\nthat can be trusted for practice should have a correspondence between\nexplanation and clinical determination, yet no prior research has examined the\nattention fidelity of these models and their effect on ground truth\nexplanations. We introduce an evaluation design that focuses on the robustness\nand explainability of LMs in identifying Wellness Dimensions (WD). We focus on\ntwo mental health and well-being datasets: (a) Multi-label Classification-based\nMultiWD, and (b) WellXplain for evaluating attention mechanism veracity against\nexpert-labeled explanations. The labels are based on Halbert Dunn's theory of\nwellness, which gives grounding to our evaluation. We reveal four surprising\nresults about LMs/LLMs: (1) Despite their human-like capabilities, GPT-3.5/4\nlag behind RoBERTa, and MedAlpaca, a fine-tuned LLM fails to deliver any\nremarkable improvements in performance or explanations. (2) Re-examining LMs'\npredictions based on a confidence-oriented loss function reveals a significant\nperformance drop. (3) Across all LMs/LLMs, the alignment between attention and\nexplanations remains low, with LLMs scoring a dismal 0.0. (4) Most mental\nhealth-specific LMs/LLMs overlook domain-specific knowledge and undervalue\nexplanations, causing these discrepancies. This study highlights the need for\nfurther research into their consistency and explanations in mental health and\nwell-being.\n","authors":["Seyedali Mohammadi","Edward Raff","Jinendra Malekar","Vedant Palit","Francis Ferraro","Manas Gaur"],"pdf_url":"https://arxiv.org/pdf/2406.12058v3.pdf","comment":"26 pages, including reference and appendix sections, 8 figures, and\n  16 tables"},{"id":"http://arxiv.org/abs/2402.09742v4","updated":"2024-06-28T03:11:48Z","published":"2024-02-15T06:46:48Z","title":"AI Hospital: Benchmarking Large Language Models in a Multi-agent Medical\n  Interaction Simulator","summary":"  Artificial intelligence has significantly advanced healthcare, particularly\nthrough large language models (LLMs) that excel in medical question answering\nbenchmarks. However, their real-world clinical application remains limited due\nto the complexities of doctor-patient interactions. To address this, we\nintroduce \\textbf{AI Hospital}, a multi-agent framework simulating dynamic\nmedical interactions between \\emph{Doctor} as player and NPCs including\n\\emph{Patient}, \\emph{Examiner}, \\emph{Chief Physician}. This setup allows for\nrealistic assessments of LLMs in clinical scenarios. We develop the Multi-View\nMedical Evaluation (MVME) benchmark, utilizing high-quality Chinese medical\nrecords and NPCs to evaluate LLMs' performance in symptom collection,\nexamination recommendations, and diagnoses. Additionally, a dispute resolution\ncollaborative mechanism is proposed to enhance diagnostic accuracy through\niterative discussions. Despite improvements, current LLMs exhibit significant\nperformance gaps in multi-turn interactions compared to one-step approaches.\nOur findings highlight the need for further research to bridge these gaps and\nimprove LLMs' clinical diagnostic capabilities. Our data, code, and\nexperimental results are all open-sourced at\n\\url{https://github.com/LibertFan/AI_Hospital}.\n","authors":["Zhihao Fan","Jialong Tang","Wei Chen","Siyuan Wang","Zhongyu Wei","Jun Xi","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2402.09742v4.pdf","comment":"https://github.com/LibertFan/AI_Hospital"},{"id":"http://arxiv.org/abs/2406.18841v2","updated":"2024-06-28T02:56:09Z","published":"2024-05-14T15:03:05Z","title":"Navigating LLM Ethics: Advancements, Challenges, and Future Directions","summary":"  This study addresses ethical issues surrounding Large Language Models (LLMs)\nwithin the field of artificial intelligence. It explores the common ethical\nchallenges posed by both LLMs and other AI systems, such as privacy and\nfairness, as well as ethical challenges uniquely arising from LLMs. It\nhighlights challenges such as hallucination, verifiable accountability, and\ndecoding censorship complexity, which are unique to LLMs and distinct from\nthose encountered in traditional AI systems. The study underscores the need to\ntackle these complexities to ensure accountability, reduce biases, and enhance\ntransparency in the influential role that LLMs play in shaping information\ndissemination. It proposes mitigation strategies and future directions for LLM\nethics, advocating for interdisciplinary collaboration. It recommends ethical\nframeworks tailored to specific domains and dynamic auditing systems adapted to\ndiverse contexts. This roadmap aims to guide responsible development and\nintegration of LLMs, envisioning a future where ethical considerations govern\nAI advancements in society.\n","authors":["Junfeng Jiao","Saleh Afroogh","Yiming Xu","Connor Phillips"],"pdf_url":"https://arxiv.org/pdf/2406.18841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18842v2","updated":"2024-06-28T02:54:06Z","published":"2024-05-26T15:28:24Z","title":"The global landscape of academic guidelines for generative AI and Large\n  Language Models","summary":"  The integration of Generative Artificial Intelligence (GAI) and Large\nLanguage Models (LLMs) in academia has spurred a global discourse on their\npotential pedagogical benefits and ethical considerations. Positive reactions\nhighlight some potential, such as collaborative creativity, increased access to\neducation, and empowerment of trainers and trainees. However, negative\nreactions raise concerns about ethical complexities, balancing innovation and\nacademic integrity, unequal access, and misinformation risks. Through a\nsystematic survey and text-mining-based analysis of global and national\ndirectives, insights from independent research, and eighty university-level\nguidelines, this study provides a nuanced understanding of the opportunities\nand challenges posed by GAI and LLMs in education. It emphasizes the importance\nof balanced approaches that harness the benefits of these technologies while\naddressing ethical considerations and ensuring equitable access and educational\noutcomes. The paper concludes with recommendations for fostering responsible\ninnovation and ethical practices to guide the integration of GAI and LLMs in\nacademia.\n","authors":["Junfeng Jiao","Saleh Afroogh","Kevin Chen","David Atkinson","Amit Dhurandhar"],"pdf_url":"https://arxiv.org/pdf/2406.18842v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02990v3","updated":"2024-06-28T02:35:38Z","published":"2024-03-05T14:11:54Z","title":"Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and\n  Challenges","summary":"  In the rapidly evolving field of large language models (LLMs), data\naugmentation (DA) has emerged as a pivotal technique for enhancing model\nperformance by diversifying training examples without the need for additional\ndata collection. This survey explores the transformative impact of LLMs on DA,\nparticularly addressing the unique challenges and opportunities they present in\nthe context of natural language processing (NLP) and beyond. From both data and\nlearning perspectives, we examine various strategies that utilize LLMs for data\naugmentation, including a novel exploration of learning paradigms where\nLLM-generated data is used for diverse forms of further training. Additionally,\nthis paper highlights the primary open challenges faced in this domain, ranging\nfrom controllable data augmentation to multi-modal data augmentation. This\nsurvey highlights a paradigm shift introduced by LLMs in DA, and aims to serve\nas a comprehensive guide for researchers and practitioners.\n","authors":["Bosheng Ding","Chengwei Qin","Ruochen Zhao","Tianze Luo","Xinze Li","Guizhen Chen","Wenhan Xia","Junjie Hu","Anh Tuan Luu","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2403.02990v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10943v4","updated":"2024-06-28T02:05:59Z","published":"2024-03-16T15:14:15Z","title":"MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent\n  Recognition and Out-of-scope Detection in Conversations","summary":"  Multimodal intent recognition poses significant challenges, requiring the\nincorporation of non-verbal modalities from real-world contexts to enhance the\ncomprehension of human intentions. Existing benchmark datasets are limited in\nscale and suffer from difficulties in handling out-of-scope samples that arise\nin multi-turn conversational interactions. We introduce MIntRec2.0, a\nlarge-scale benchmark dataset for multimodal intent recognition in multi-party\nconversations. It contains 1,245 dialogues with 15,040 samples, each annotated\nwithin a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope\nsamples, it also includes 5,736 out-of-scope samples appearing in multi-turn\ncontexts, which naturally occur in real-world scenarios. Furthermore, we\nprovide comprehensive information on the speakers in each utterance, enriching\nits utility for multi-party conversational research. We establish a general\nframework supporting the organization of single-turn and multi-turn dialogue\ndata, modality feature extraction, multimodal fusion, as well as in-scope\nclassification and out-of-scope detection. Evaluation benchmarks are built\nusing classic multimodal fusion methods, ChatGPT, and human evaluators. While\nexisting methods incorporating nonverbal information yield improvements,\neffectively leveraging context information and detecting out-of-scope samples\nremains a substantial challenge. Notably, large language models exhibit a\nsignificant performance gap compared to humans, highlighting the limitations of\nmachine learning methods in the cognitive intent understanding task. We believe\nthat MIntRec2.0 will serve as a valuable resource, providing a pioneering\nfoundation for research in human-machine conversational interactions, and\nsignificantly facilitating related applications. The full dataset and codes are\navailable at https://github.com/thuiar/MIntRec2.0.\n","authors":["Hanlei Zhang","Xin Wang","Hua Xu","Qianrui Zhou","Kai Gao","Jianhua Su","jinyue Zhao","Wenrui Li","Yanting Chen"],"pdf_url":"https://arxiv.org/pdf/2403.10943v4.pdf","comment":"Accepted by ICLR 2024, Long Paper; The abstract is slightly modified\n  due to the length limitation"},{"id":"http://arxiv.org/abs/2406.19598v1","updated":"2024-06-28T01:46:41Z","published":"2024-06-28T01:46:41Z","title":"Mixture of In-Context Experts Enhance LLMs' Long Context Awareness","summary":"  Many studies have revealed that large language models (LLMs) exhibit uneven\nawareness of different contextual positions.Their limited context awareness can\nlead to overlooking critical information and subsequent task failures. While\nseveral approaches have been proposed to enhance LLMs' context awareness,\nachieving both effectiveness and efficiency remains challenging.In this paper,\nfor LLMs utilizing RoPE as position embeddings, we introduce a novel method\ncalled ``Mixture of In-Context Experts'' (MoICE) to address this challenge.\nMoICE comprises two key components: a router integrated into each attention\nhead within LLMs and a lightweight router-only training optimization strategy:\n(1) MoICE views each RoPE angle as an `in-context' expert, demonstrated to be\ncapable of directing the attention of a head to specific contextual positions.\nConsequently, each attention head flexibly processes tokens using multiple RoPE\nangles dynamically selected by the router to attend to the needed positions.\nThis approach mitigates the risk of overlooking essential contextual\ninformation. (2) The router-only training strategy entails freezing LLM\nparameters and exclusively updating routers for only a few steps. When applied\nto open-source LLMs including Llama and Mistral, MoICE surpasses prior methods\nacross multiple tasks on long context understanding and generation, all while\nmaintaining commendable inference efficiency.\n","authors":["Hongzhan Lin","Ang Lv","Yuhan Chen","Chen Zhu","Yang Song","Hengshu Zhu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2406.19598v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.19593v1","updated":"2024-06-28T01:14:43Z","published":"2024-06-28T01:14:43Z","title":"SK-VQA: Synthetic Knowledge Generation at Scale for Training\n  Context-Augmented Multimodal LLMs","summary":"  Synthetic data generation has gained significant attention recently for its\nutility in training large vision and language models. However, the application\nof synthetic data to the training of multimodal context-augmented generation\nsystems has been relatively unexplored. This gap in existing work is important\nbecause existing vision and language models (VLMs) are not trained specifically\nfor context-augmented generation. Resources for adapting such models are\ntherefore crucial for enabling their use in retrieval-augmented generation\n(RAG) settings, where a retriever is used to gather relevant information that\nis then subsequently provided to a generative model via context augmentation.\nTo address this challenging problem, we generate SK-VQA: a large synthetic\nmultimodal dataset containing over 2 million question-answer pairs which\nrequire external knowledge to determine the final answer. Our dataset is both\nlarger and significantly more diverse than existing resources of its kind,\npossessing over 11x more unique questions and containing images from a greater\nvariety of sources than previously-proposed datasets. Through extensive\nexperiments, we demonstrate that our synthetic dataset can not only serve as a\nchallenging benchmark, but is also highly effective for adapting existing\ngenerative multimodal models for context-augmented generation.\n","authors":["Xin Su","Man Luo","Kris W Pan","Tien Pei Chou","Vasudev Lal","Phillip Howard"],"pdf_url":"https://arxiv.org/pdf/2406.19593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19350v5","updated":"2024-06-28T00:26:41Z","published":"2024-02-29T16:56:36Z","title":"Prompting Explicit and Implicit Knowledge for Multi-hop Question\n  Answering Based on Human Reading Process","summary":"  Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to\nsimulate human reasoning and inference processes, achieving proficient\nperformance in multi-hop QA. However, a gap persists between PLMs' reasoning\nabilities and those of humans when tackling complex problems. Psychological\nstudies suggest a vital connection between explicit information in passages and\nhuman prior knowledge during reading. Nevertheless, current research has given\ninsufficient attention to linking input passages and PLMs' pre-training-based\nknowledge from the perspective of human cognition studies. In this study, we\nintroduce a Prompting Explicit and Implicit knowledge (PEI) framework, which\nuses prompts to connect explicit and implicit knowledge, aligning with human\nreading process for multi-hop QA. We consider the input passages as explicit\nknowledge, employing them to elicit implicit knowledge through unified prompt\nreasoning. Furthermore, our model incorporates type-specific reasoning via\nprompts, a form of implicit knowledge. Experimental results show that PEI\nperforms comparably to the state-of-the-art on HotpotQA. Ablation studies\nconfirm the efficacy of our model in bridging and integrating explicit and\nimplicit knowledge.\n","authors":["Guangming Huang","Yunfei Long","Cunjin Luo","Jiaxing Shen","Xia Sun"],"pdf_url":"https://arxiv.org/pdf/2402.19350v5.pdf","comment":"This paper has been accepted at COLING 2024"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2406.20099v1","updated":"2024-06-28T17:59:51Z","published":"2024-06-28T17:59:51Z","title":"Odd-One-Out: Anomaly Detection by Comparing with Neighbors","summary":"  This paper introduces a novel anomaly detection (AD) problem that focuses on\nidentifying `odd-looking' objects relative to the other instances within a\nscene. Unlike the traditional AD benchmarks, in our setting, anomalies in this\ncontext are scene-specific, defined by the regular instances that make up the\nmajority. Since object instances are often partly visible from a single\nviewpoint, our setting provides multiple views of each scene as input. To\nprovide a testbed for future research in this task, we introduce two\nbenchmarks, ToysAD-8K and PartsAD-15K. We propose a novel method that generates\n3D object-centric representations for each instance and detects the anomalous\nones through a cross-examination between the instances. We rigorously analyze\nour method quantitatively and qualitatively in the presented benchmarks.\n","authors":["Ankan Bhunia","Changjian Li","Hakan Bilen"],"pdf_url":"https://arxiv.org/pdf/2406.20099v1.pdf","comment":"Codes & Dataset at https://github.com/VICO-UoE/OddOneOutAD"},{"id":"http://arxiv.org/abs/2406.20098v1","updated":"2024-06-28T17:59:46Z","published":"2024-06-28T17:59:46Z","title":"Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework\n  for Multimodal LLMs","summary":"  Multimodal large language models (MLLMs) have shown impressive success across\nmodalities such as image, video, and audio in a variety of understanding and\ngeneration tasks. However, current MLLMs are surprisingly poor at understanding\nwebpage screenshots and generating their corresponding HTML code. To address\nthis problem, we propose Web2Code, a benchmark consisting of a new large-scale\nwebpage-to-code dataset for instruction tuning and an evaluation framework for\nthe webpage understanding and HTML code translation abilities of MLLMs. For\ndataset construction, we leverage pretrained LLMs to enhance existing\nwebpage-to-code datasets as well as generate a diverse pool of new webpages\nrendered into images. Specifically, the inputs are webpage images and\ninstructions, while the responses are the webpage's HTML code. We further\ninclude diverse natural language QA pairs about the webpage content in the\nresponses to enable a more comprehensive understanding of the web content. To\nevaluate model performance in these tasks, we develop an evaluation framework\nfor testing MLLMs' abilities in webpage understanding and web-to-code\ngeneration. Extensive experiments show that our proposed dataset is beneficial\nnot only to our proposed tasks but also in the general visual domain, while\nprevious datasets result in worse performance. We hope our work will contribute\nto the development of general MLLMs suitable for web-based content generation\nand task automation. Our data and code will be available at\nhttps://github.com/MBZUAI-LLM/web2code.\n","authors":["Sukmin Yun","Haokun Lin","Rusiru Thushara","Mohammad Qazim Bhat","Yongxin Wang","Zutao Jiang","Mingkai Deng","Jinhong Wang","Tianhua Tao","Junbo Li","Haonan Li","Preslav Nakov","Timothy Baldwin","Zhengzhong Liu","Eric P. Xing","Xiaodan Liang","Zhiqiang Shen"],"pdf_url":"https://arxiv.org/pdf/2406.20098v1.pdf","comment":"Website at https://mbzuai-llm.github.io/webpage2code/"},{"id":"http://arxiv.org/abs/2406.20095v1","updated":"2024-06-28T17:59:12Z","published":"2024-06-28T17:59:12Z","title":"LLaRA: Supercharging Robot Learning Data for Vision-Language Policy","summary":"  Large Language Models (LLMs) equipped with extensive world knowledge and\nstrong reasoning skills can tackle diverse tasks across domains, often by\nposing them as conversation-style instruction-response pairs. In this paper, we\npropose LLaRA: Large Language and Robotics Assistant, a framework which\nformulates robot action policy as conversations, and provides improved\nresponses when trained with auxiliary data that complements policy learning.\nLLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity\nto process state information as visual-textual prompts and generate optimal\npolicy decisions in text. To train such action policy VLMs, we first introduce\nan automated pipeline to generate diverse high-quality robotics instruction\ndata from existing behavior cloning data. A VLM finetuned with the resulting\ncollection of datasets based on a conversation-style formulation tailored for\nrobotics tasks, can generate meaningful robot action policy decisions. Our\nexperiments across multiple simulated and real-world environments demonstrate\nthe state-of-the-art performance of the proposed LLaRA framework. The code,\ndatasets, and pretrained models are available at\nhttps://github.com/LostXine/LLaRA.\n","authors":["Xiang Li","Cristina Mata","Jongwoo Park","Kumara Kahatapitiya","Yoo Sung Jang","Jinghuan Shang","Kanchana Ranasinghe","Ryan Burgert","Mu Cai","Yong Jae Lee","Michael S. Ryoo"],"pdf_url":"https://arxiv.org/pdf/2406.20095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20092v1","updated":"2024-06-28T17:57:14Z","published":"2024-06-28T17:57:14Z","title":"LLaVolta: Efficient Multi-modal Models via Stage-wise Visual Context\n  Compression","summary":"  While significant advancements have been made in compressed representations\nfor text embeddings in large language models (LLMs), the compression of visual\ntokens in large multi-modal models (LMMs) has remained a largely overlooked\narea. In this work, we present the study on the analysis of redundancy\nconcerning visual tokens and efficient training within these models. Our\ninitial experiments show that eliminating up to 70% of visual tokens at the\ntesting stage by simply average pooling only leads to a minimal 3% reduction in\nvisual question answering accuracy on the GQA benchmark, indicating significant\nredundancy in visual context. Addressing this, we introduce Visual Context\nCompressor, which reduces the number of visual tokens during training to\nenhance training efficiency without sacrificing performance. To minimize\ninformation loss caused by the compression on visual tokens while maintaining\ntraining efficiency, we develop LLaVolta as a lite training scheme. LLaVolta\nincorporates stage-wise visual context compression to progressively compress\nthe visual tokens from heavily to lightly, and finally no compression at the\nend of training, yielding no loss of information when testing. Extensive\nexperiments demonstrate that our approach enhances the performance of MLLMs in\nboth image-language and video-language understanding, while also significantly\ncutting training costs. Code is available at\nhttps://github.com/Beckschen/LLaVolta\n","authors":["Jieneng Chen","Luoxin Ye","Ju He","Zhao-Yang Wang","Daniel Khashabi","Alan Yuille"],"pdf_url":"https://arxiv.org/pdf/2406.20092v1.pdf","comment":"Code is available at https://github.com/Beckschen/LLaVolta"},{"id":"http://arxiv.org/abs/2406.20085v1","updated":"2024-06-28T17:53:18Z","published":"2024-06-28T17:53:18Z","title":"Auto Cherry-Picker: Learning from High-quality Generative Data Driven by\n  Language","summary":"  Diffusion-based models have shown great potential in generating high-quality\nimages with various layouts, which can benefit downstream perception tasks.\nHowever, a fully automatic layout generation driven only by language and a\nsuitable metric for measuring multiple generated instances has not been well\nexplored. In this work, we present Auto Cherry-Picker (ACP), a novel framework\nthat generates high-quality multi-modal training examples to augment perception\nand multi-modal training. Starting with a simple list of natural language\nconcepts, we prompt large language models (LLMs) to generate a detailed\ndescription and design reasonable layouts. Next, we use an off-the-shelf\ntext-to-image model to generate multiple images. Then, the generated data are\nrefined using a comprehensively designed metric to ensure quality. In\nparticular, we present a new metric, Composite Layout and Image Score (CLIS),\nto evaluate the generated images fairly. Our synthetic high-quality examples\nboost performance in various scenarios by customizing the initial concept list,\nespecially in addressing challenges associated with long-tailed distribution\nand imbalanced datasets. Experiment results on downstream tasks demonstrate\nthat Auto Cherry-Picker can significantly improve the performance of existing\nmodels. In addition, we have thoroughly investigated the correlation between\nCLIS and performance gains in downstream tasks, and we find that a better CLIS\nscore results in better performance. This finding shows the potential for\nevaluation metrics as the role for various visual perception and MLLM tasks.\nCode will be available.\n","authors":["Yicheng Chen","Xiangtai Li","Yining Li","Yanhong Zeng","Jianzong Wu","Xiangyu Zhao","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2406.20085v1.pdf","comment":"19 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.20083v1","updated":"2024-06-28T17:51:10Z","published":"2024-06-28T17:51:10Z","title":"PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful\n  Navigators","summary":"  We present PoliFormer (Policy Transformer), an RGB-only indoor navigation\nagent trained end-to-end with reinforcement learning at scale that generalizes\nto the real-world without adaptation despite being trained purely in\nsimulation. PoliFormer uses a foundational vision transformer encoder with a\ncausal transformer decoder enabling long-term memory and reasoning. It is\ntrained for hundreds of millions of interactions across diverse environments,\nleveraging parallelized, multi-machine rollouts for efficient training with\nhigh throughput. PoliFormer is a masterful navigator, producing\nstate-of-the-art results across two distinct embodiments, the LoCoBot and\nStretch RE-1 robots, and four navigation benchmarks. It breaks through the\nplateaus of previous work, achieving an unprecedented 85.5% success rate in\nobject goal navigation on the CHORES-S benchmark, a 28.5% absolute improvement.\nPoliFormer can also be trivially extended to a variety of downstream\napplications such as object tracking, multi-object navigation, and\nopen-vocabulary navigation with no finetuning.\n","authors":["Kuo-Hao Zeng","Zichen Zhang","Kiana Ehsani","Rose Hendrix","Jordi Salvador","Alvaro Herrasti","Ross Girshick","Aniruddha Kembhavi","Luca Weihs"],"pdf_url":"https://arxiv.org/pdf/2406.20083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20081v1","updated":"2024-06-28T17:47:32Z","published":"2024-06-28T17:47:32Z","title":"Segment Anything without Supervision","summary":"  The Segmentation Anything Model (SAM) requires labor-intensive data labeling.\nWe present Unsupervised SAM (UnSAM) for promptable and automatic whole-image\nsegmentation that does not require human annotations. UnSAM utilizes a\ndivide-and-conquer strategy to \"discover\" the hierarchical structure of visual\nscenes. We first leverage top-down clustering methods to partition an unlabeled\nimage into instance/semantic level segments. For all pixels within a segment, a\nbottom-up clustering method is employed to iteratively merge them into larger\ngroups, thereby forming a hierarchical structure. These unsupervised\nmulti-granular masks are then utilized to supervise model training. Evaluated\nacross seven popular datasets, UnSAM achieves competitive results with the\nsupervised counterpart SAM, and surpasses the previous state-of-the-art in\nunsupervised segmentation by 11% in terms of AR. Moreover, we show that\nsupervised SAM can also benefit from our self-supervised labels. By integrating\nour unsupervised pseudo masks into SA-1B's ground-truth masks and training\nUnSAM with only 1% of SA-1B, a lightly semi-supervised UnSAM can often segment\nentities overlooked by supervised SAM, exceeding SAM's AR by over 6.7% and AP\nby 3.9% on SA-1B.\n","authors":["XuDong Wang","Jingfeng Yang","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2406.20081v1.pdf","comment":"Code: https://github.com/frank-xwang/UnSAM"},{"id":"http://arxiv.org/abs/2406.20078v1","updated":"2024-06-28T17:42:08Z","published":"2024-06-28T17:42:08Z","title":"GM-DF: Generalized Multi-Scenario Deepfake Detection","summary":"  Existing face forgery detection usually follows the paradigm of training\nmodels in a single domain, which leads to limited generalization capacity when\nunseen scenarios and unknown attacks occur. In this paper, we elaborately\ninvestigate the generalization capacity of deepfake detection models when\njointly trained on multiple face forgery detection datasets. We first find a\nrapid degradation of detection accuracy when models are directly trained on\ncombined datasets due to the discrepancy across collection scenarios and\ngeneration methods. To address the above issue, a Generalized Multi-Scenario\nDeepfake Detection framework (GM-DF) is proposed to serve multiple real-world\nscenarios by a unified model. First, we propose a hybrid expert modeling\napproach for domain-specific real/forgery feature extraction. Besides, as for\nthe commonality representation, we use CLIP to extract the common features for\nbetter aligning visual and textual features across domains. Meanwhile, we\nintroduce a masked image reconstruction mechanism to force models to capture\nrich forged details. Finally, we supervise the models via a domain-aware\nmeta-learning strategy to further enhance their generalization capacities.\nSpecifically, we design a novel domain alignment loss to strongly align the\ndistributions of the meta-test domains and meta-train domains. Thus, the\nupdated models are able to represent both specific and common real/forgery\nfeatures across multiple datasets. In consideration of the lack of study of\nmulti-dataset training, we establish a new benchmark leveraging multi-source\ndata to fairly evaluate the models' generalization capacity on unseen\nscenarios. Both qualitative and quantitative experiments on five datasets\nconducted on traditional protocols as well as the proposed benchmark\ndemonstrate the effectiveness of our approach.\n","authors":["Yingxin Lai","Zitong Yu","Jing Yang","Bin Li","Xiangui Kang","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2406.20078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20077v1","updated":"2024-06-28T17:39:38Z","published":"2024-06-28T17:39:38Z","title":"HouseCrafter: Lifting Floorplans to 3D Scenes with 2D Diffusion Model","summary":"  We introduce HouseCrafter, a novel approach that can lift a floorplan into a\ncomplete large 3D indoor scene (e.g., a house). Our key insight is to adapt a\n2D diffusion model, which is trained on web-scale images, to generate\nconsistent multi-view color (RGB) and depth (D) images across different\nlocations of the scene. Specifically, the RGB-D images are generated\nautoregressively in a batch-wise manner along sampled locations based on the\nfloorplan, where previously generated images are used as condition to the\ndiffusion model to produce images at nearby locations. The global floorplan and\nattention design in the diffusion model ensures the consistency of the\ngenerated images, from which a 3D scene can be reconstructed. Through extensive\nevaluation on the 3D-Front dataset, we demonstrate that HouseCraft can generate\nhigh-quality house-scale 3D scenes. Ablation studies also validate the\neffectiveness of different design choices. We will release our code and model\nweights. Project page: https://neu-vi.github.io/houseCrafter/\n","authors":["Hieu T. Nguyen","Yiwen Chen","Vikram Voleti","Varun Jampani","Huaizu Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.20077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20076v1","updated":"2024-06-28T17:38:18Z","published":"2024-06-28T17:38:18Z","title":"EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything\n  Model","summary":"  Segment Anything Model (SAM) has attracted widespread attention for its\nsuperior interactive segmentation capabilities with visual prompts while\nlacking further exploration of text prompts. In this paper, we empirically\ninvestigate what text prompt encoders (e.g., CLIP or LLM) are good for adapting\nSAM for referring expression segmentation and introduce the Early\nVision-language Fusion-based SAM (EVF-SAM). EVF-SAM is a simple yet effective\nreferring segmentation method which exploits multimodal prompts (i.e., image\nand text) and comprises a pre-trained vision-language model to generate\nreferring prompts and a SAM model for segmentation. Surprisingly, we observe\nthat: (1) multimodal prompts and (2) vision-language models with early fusion\n(e.g., BEIT-3) are beneficial for prompting SAM for accurate referring\nsegmentation. Our experiments show that the proposed EVF-SAM based on BEIT-3\ncan obtain state-of-the-art performance on RefCOCO/+/g for referring expression\nsegmentation and demonstrate the superiority of prompting SAM with early\nvision-language fusion. In addition, the proposed EVF-SAM with 1.32B parameters\nachieves remarkably higher performance while reducing nearly 82% of parameters\ncompared to previous SAM methods based on large multimodal models.\n","authors":["Yuxuan Zhang","Tianheng Cheng","Rui Hu","ei Liu","Heng Liu","Longjin Ran","Xiaoxin Chen","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2406.20076v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.20066v1","updated":"2024-06-28T17:22:33Z","published":"2024-06-28T17:22:33Z","title":"ASSR-NeRF: Arbitrary-Scale Super-Resolution on Voxel Grid for\n  High-Quality Radiance Fields Reconstruction","summary":"  NeRF-based methods reconstruct 3D scenes by building a radiance field with\nimplicit or explicit representations. While NeRF-based methods can perform\nnovel view synthesis (NVS) at arbitrary scale, the performance in\nhigh-resolution novel view synthesis (HRNVS) with low-resolution (LR)\noptimization often results in oversmoothing. On the other hand, single-image\nsuper-resolution (SR) aims to enhance LR images to HR counterparts but lacks\nmulti-view consistency. To address these challenges, we propose Arbitrary-Scale\nSuper-Resolution NeRF (ASSR-NeRF), a novel framework for super-resolution novel\nview synthesis (SRNVS). We propose an attention-based VoxelGridSR model to\ndirectly perform 3D super-resolution (SR) on the optimized volume. Our model is\ntrained on diverse scenes to ensure generalizability. For unseen scenes trained\nwith LR views, we then can directly apply our VoxelGridSR to further refine the\nvolume and achieve multi-view consistent SR. We demonstrate quantitative and\nqualitatively that the proposed method achieves significant performance in\nSRNVS.\n","authors":["Ding-Jiun Huang","Zi-Ting Chou","Yu-Chiang Frank Wang","Cheng Sun"],"pdf_url":"https://arxiv.org/pdf/2406.20066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.07015v4","updated":"2024-06-28T17:14:13Z","published":"2023-05-11T17:55:25Z","title":"Exploiting Diffusion Prior for Real-World Image Super-Resolution","summary":"  We present a novel approach to leverage prior knowledge encapsulated in\npre-trained text-to-image diffusion models for blind super-resolution (SR).\nSpecifically, by employing our time-aware encoder, we can achieve promising\nrestoration results without altering the pre-trained synthesis model, thereby\npreserving the generative prior and minimizing training cost. To remedy the\nloss of fidelity caused by the inherent stochasticity of diffusion models, we\nemploy a controllable feature wrapping module that allows users to balance\nquality and fidelity by simply adjusting a scalar value during the inference\nprocess. Moreover, we develop a progressive aggregation sampling strategy to\novercome the fixed-size constraints of pre-trained diffusion models, enabling\nadaptation to resolutions of any size. A comprehensive evaluation of our method\nusing both synthetic and real-world benchmarks demonstrates its superiority\nover current state-of-the-art approaches. Code and models are available at\nhttps://github.com/IceClear/StableSR.\n","authors":["Jianyi Wang","Zongsheng Yue","Shangchen Zhou","Kelvin C. K. Chan","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2305.07015v4.pdf","comment":"Accepted by IJCV'2024. Some Figs are compressed due to size limits.\n  Uncompressed ver.:\n  https://github.com/IceClear/StableSR/releases/download/UncompressedPDF/StableSR_IJCV_Uncompressed.pdf.\n  Project page: https://iceclear.github.io/projects/stablesr/"},{"id":"http://arxiv.org/abs/2406.20055v1","updated":"2024-06-28T17:07:11Z","published":"2024-06-28T17:07:11Z","title":"SpotlessSplats: Ignoring Distractors in 3D Gaussian Splatting","summary":"  3D Gaussian Splatting (3DGS) is a promising technique for 3D reconstruction,\noffering efficient training and rendering speeds, making it suitable for\nreal-time applications.However, current methods require highly controlled\nenvironments (no moving people or wind-blown elements, and consistent lighting)\nto meet the inter-view consistency assumption of 3DGS. This makes\nreconstruction of real-world captures problematic. We present SpotlessSplats,\nan approach that leverages pre-trained and general-purpose features coupled\nwith robust optimization to effectively ignore transient distractors. Our\nmethod achieves state-of-the-art reconstruction quality both visually and\nquantitatively, on casual captures.\n","authors":["Sara Sabour","Lily Goli","George Kopanas","Mark Matthews","Dmitry Lagun","Leonidas Guibas","Alec Jacobson","David J. Fleet","Andrea Tagliasacchi"],"pdf_url":"https://arxiv.org/pdf/2406.20055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20042v1","updated":"2024-06-28T16:40:57Z","published":"2024-06-28T16:40:57Z","title":"HAITCH: A Framework for Distortion and Motion Correction in Fetal\n  Multi-Shell Diffusion-Weighted MRI","summary":"  Diffusion magnetic resonance imaging (dMRI) is pivotal for probing the\nmicrostructure of the rapidly-developing fetal brain. However, fetal motion\nduring scans and its interaction with magnetic field inhomogeneities result in\nartifacts and data scattering across spatial and angular domains. The effects\nof those artifacts are more pronounced in high-angular resolution fetal dMRI,\nwhere signal-to-noise ratio is very low. Those effects lead to biased estimates\nand compromise the consistency and reliability of dMRI analysis. This work\npresents HAITCH, the first and the only publicly available tool to correct and\nreconstruct multi-shell high-angular resolution fetal dMRI data. HAITCH offers\nseveral technical advances that include a blip-reversed dual-echo acquisition\nfor dynamic distortion correction, advanced motion correction for model-free\nand robust reconstruction, optimized multi-shell design for enhanced\ninformation capture and increased tolerance to motion, and outlier detection\nfor improved reconstruction fidelity. The framework is open-source, flexible,\nand can be used to process any type of fetal dMRI data including single-echo or\nsingle-shell acquisitions, but is most effective when used with multi-shell\nmulti-echo fetal dMRI data that cannot be processed with any of the existing\ntools. Validation experiments on real fetal dMRI scans demonstrate significant\nimprovements and accurate correction across diverse fetal ages and motion\nlevels. HAITCH successfully removes artifacts and reconstructs high-fidelity\nfetal dMRI data suitable for advanced diffusion modeling, including fiber\norientation distribution function estimation. These advancements pave the way\nfor more reliable analysis of the fetal brain microstructure and tractography\nunder challenging imaging conditions.\n","authors":["Haykel Snoussi","Davood Karimi","Onur Afacan","Mustafa Utkur","Ali Gholipour"],"pdf_url":"https://arxiv.org/pdf/2406.20042v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15180v2","updated":"2024-06-28T16:40:39Z","published":"2023-07-27T20:19:11Z","title":"EnSolver: Uncertainty-Aware Ensemble CAPTCHA Solvers with Theoretical\n  Guarantees","summary":"  The popularity of text-based CAPTCHA as a security mechanism to protect\nwebsites from automated bots has prompted researches in CAPTCHA solvers, with\nthe aim of understanding its failure cases and subsequently making CAPTCHAs\nmore secure. Recently proposed solvers, built on advances in deep learning, are\nable to crack even the very challenging CAPTCHAs with high accuracy. However,\nthese solvers often perform poorly on out-of-distribution samples that contain\nvisual features different from those in the training set. Furthermore, they\nlack the ability to detect and avoid such samples, making them susceptible to\nbeing locked out by defense systems after a certain number of failed attempts.\nIn this paper, we propose EnSolver, a family of CAPTCHA solvers that use deep\nensemble uncertainty to detect and skip out-of-distribution CAPTCHAs, making it\nharder to be detected. We prove novel theoretical bounds on the effectiveness\nof our solvers and demonstrate their use with state-of-the-art CAPTCHA solvers.\nOur experiments show that the proposed approaches perform well when cracking\nCAPTCHA datasets that contain both in-distribution and out-of-distribution\nsamples.\n","authors":["Duc C. Hoang","Behzad Ousat","Amin Kharraz","Cuong V. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2307.15180v2.pdf","comment":"A previous version of this paper was presented at the Epistemic\n  Uncertainty - E-pi UAI 2023 Workshop"},{"id":"http://arxiv.org/abs/2406.20024v1","updated":"2024-06-28T16:13:55Z","published":"2024-06-28T16:13:55Z","title":"eMoE-Tracker: Environmental MoE-based Transformer for Robust\n  Event-guided Object Tracking","summary":"  The unique complementarity of frame-based and event cameras for high frame\nrate object tracking has recently inspired some research attempts to develop\nmulti-modal fusion approaches. However, these methods directly fuse both\nmodalities and thus ignore the environmental attributes, e.g., motion blur,\nillumination variance, occlusion, scale variation, etc. Meanwhile, no\ninteraction between search and template features makes distinguishing target\nobjects and backgrounds difficult. As a result, performance degradation is\ninduced especially in challenging conditions. This paper proposes a novel and\neffective Transformer-based event-guided tracking framework, called\neMoE-Tracker, which achieves new SOTA performance under various conditions. Our\nkey idea is to disentangle the environment into several learnable attributes to\ndynamically learn the attribute-specific features for better interaction and\ndiscriminability between the target information and background. To achieve the\ngoal, we first propose an environmental Mix-of-Experts (eMoE) module that is\nbuilt upon the environmental Attributes Disentanglement to learn\nattribute-specific features and environmental Attributes Gating to assemble the\nattribute-specific features by the learnable attribute scores dynamically. The\neMoE module is a subtle router that fine-tunes the transformer backbone more\nefficiently. We then introduce a contrastive relation modeling (CRM) module to\nimprove interaction and discriminability between the target information and\nbackground. Extensive experiments on diverse event-based benchmark datasets\nshowcase the superior performance of our eMoE-Tracker compared to the prior\narts.\n","authors":["Yucheng Chen","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2406.20024v1.pdf","comment":"RGB-event single object tracking"},{"id":"http://arxiv.org/abs/2406.20005v1","updated":"2024-06-28T15:44:55Z","published":"2024-06-28T15:44:55Z","title":"Malaria Cell Detection Using Deep Neural Networks","summary":"  Malaria remains one of the most pressing public health concerns globally,\ncausing significant morbidity and mortality, especially in sub-Saharan Africa.\nRapid and accurate diagnosis is crucial for effective treatment and disease\nmanagement. Traditional diagnostic methods, such as microscopic examination of\nblood smears, are labor-intensive and require significant expertise, which may\nnot be readily available in resource-limited settings. This project aims to\nautomate the detection of malaria-infected cells using a deep learning\napproach. We employed a convolutional neural network (CNN) based on the\nResNet50 architecture, leveraging transfer learning to enhance performance. The\nMalaria Cell Images Dataset from Kaggle, containing 27,558 images categorized\ninto infected and uninfected cells, was used for training and evaluation. Our\nmodel demonstrated high accuracy, precision, and recall, indicating its\npotential as a reliable tool for assisting in malaria diagnosis. Additionally,\na web application was developed using Streamlit to allow users to upload cell\nimages and receive predictions about malaria infection, making the technology\naccessible and user-friendly. This paper provides a comprehensive overview of\nthe methodology, experiments, and results, highlighting the effectiveness of\ndeep learning in medical image analysis.\n","authors":["Saurabh Sawant","Anurag Singh"],"pdf_url":"https://arxiv.org/pdf/2406.20005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00035v3","updated":"2024-06-28T15:42:12Z","published":"2024-01-08T12:19:46Z","title":"Robustness Assessment of a Runway Object Classifier for Safe Aircraft\n  Taxiing","summary":"  As deep neural networks (DNNs) are becoming the prominent solution for many\ncomputational problems, the aviation industry seeks to explore their potential\nin alleviating pilot workload and in improving operational safety. However, the\nuse of DNNs in this type of safety-critical applications requires a thorough\ncertification process. This need can be addressed through formal verification,\nwhich provides rigorous assurances -- e.g.,~by proving the absence of certain\nmispredictions. In this case-study paper, we demonstrate this process using an\nimage-classifier DNN currently under development at Airbus and intended for use\nduring the aircraft taxiing phase. We use formal methods to assess this DNN's\nrobustness to three common image perturbation types: noise, brightness and\ncontrast, and some of their combinations. This process entails multiple\ninvocations of the underlying verifier, which might be computationally\nexpensive; and we therefore propose a method that leverages the monotonicity of\nthese robustness properties, as well as the results of past verification\nqueries, in order to reduce the overall number of verification queries required\nby nearly 60%. Our results provide an indication of the level of robustness\nachieved by the DNN classifier under study, and indicate that it is\nconsiderably more vulnerable to noise than to brightness or contrast\nperturbations.\n","authors":["Yizhak Elboher","Raya Elsaleh","Omri Isac","Mélanie Ducoffe","Audrey Galametz","Guillaume Povéda","Ryma Boumazouza","Noémie Cohen","Guy Katz"],"pdf_url":"https://arxiv.org/pdf/2402.00035v3.pdf","comment":"This is a preprint version of the paper in the proceedings of 43rd\n  Digital Avionics Systems Conference (DASC)"},{"id":"http://arxiv.org/abs/2406.19997v1","updated":"2024-06-28T15:32:59Z","published":"2024-06-28T15:32:59Z","title":"Wavelets Are All You Need for Autoregressive Image Generation","summary":"  In this paper, we take a new approach to autoregressive image generation that\nis based on two main ingredients. The first is wavelet image coding, which\nallows to tokenize the visual details of an image from coarse to fine details\nby ordering the information starting with the most significant bits of the most\nsignificant wavelet coefficients. The second is a variant of a language\ntransformer whose architecture is re-designed and optimized for token sequences\nin this 'wavelet language'. The transformer learns the significant statistical\ncorrelations within a token sequence, which are the manifestations of\nwell-known correlations between the wavelet subbands at various resolutions. We\nshow experimental results with conditioning on the generation process.\n","authors":["Wael Mattar","Idan Levy","Nir Sharon","Shai Dekel"],"pdf_url":"https://arxiv.org/pdf/2406.19997v1.pdf","comment":"16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.19973v1","updated":"2024-06-28T15:01:23Z","published":"2024-06-28T15:01:23Z","title":"STLLaVA-Med: Self-Training Large Language and Vision Assistant for\n  Medical","summary":"  Large Vision-Language Models (LVLMs) have shown significant potential in\nassisting medical diagnosis by leveraging extensive biomedical datasets.\nHowever, the advancement of medical image understanding and reasoning\ncritically depends on building high-quality visual instruction data, which is\ncostly and labor-intensive to obtain, particularly in the medical domain. To\nmitigate this data-starving issue, we introduce Self-Training Large Language\nand Vision Assistant for Medical (STLLaVA-Med). The proposed method is designed\nto train a policy model (an LVLM) capable of auto-generating medical visual\ninstruction data to improve data efficiency, guided through Direct Preference\nOptimization (DPO). Specifically, a more powerful and larger LVLM (e.g.,\nGPT-4o) is involved as a biomedical expert to oversee the DPO fine-tuning\nprocess on the auto-generated data, encouraging the policy model to align\nefficiently with human preferences. We validate the efficacy and data\nefficiency of STLLaVA-Med across three major medical Visual Question Answering\n(VQA) benchmarks, demonstrating competitive zero-shot performance with the\nutilization of only 9% of the medical data.\n","authors":["Guohao Sun","Can Qin","Huazhu Fu","Linwei Wang","Zhiqiang Tao"],"pdf_url":"https://arxiv.org/pdf/2406.19973v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.05779v3","updated":"2024-06-28T14:53:06Z","published":"2024-06-09T13:25:02Z","title":"Learning to utilize image second-order derivative information for crisp\n  edge detection","summary":"  Edge detection is a fundamental task in computer vision. It has made great\nprogress under the development of deep convolutional neural networks (DCNNs),\nsome of which have achieved a beyond human-level performance. However, recent\ntop-performing edge detection methods tend to generate thick and noisy edge\nlines. In this work, we solve this problem from two aspects: (1) the lack of\nprior knowledge regarding image edges, and (2) the issue of imbalanced pixel\ndistribution. We propose a second-order derivative-based multi-scale contextual\nenhancement module (SDMCM) to help the model locate true edge pixels accurately\nby introducing the edge prior knowledge. We also construct a hybrid focal loss\nfunction (HFL) to alleviate the imbalanced distribution issue. In addition, we\nemploy the conditionally parameterized convolution (CondConv) to develop a\nnovel boundary refinement module (BRM), which can further refine the final\noutput edge maps. In the end, we propose a U-shape network named LUS-Net which\nis based on the SDMCM and BRM for crisp edge detection. We perform extensive\nexperiments on three standard benchmarks, and the experiment results illustrate\nthat our method can predict crisp and clean edge maps and achieves\nstate-of-the-art performance on the BSDS500 dataset (ODS=0.829), NYUD-V2\ndataset (ODS=0.768), and BIPED dataset (ODS=0.903).\n","authors":["Changsong Liu","Wei Zhang","Yanyan Liu","Yimeng Fan","Mingyang Li","Wenlin Li"],"pdf_url":"https://arxiv.org/pdf/2406.05779v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17032v2","updated":"2024-06-28T14:34:00Z","published":"2024-06-24T18:00:11Z","title":"DWARF: Disease-weighted network for attention map refinement","summary":"  The interpretability of deep learning is crucial for evaluating the\nreliability of medical imaging models and reducing the risks of inaccurate\npatient recommendations. This study addresses the \"human out of the loop\" and\n\"trustworthiness\" issues in medical image analysis by integrating medical\nprofessionals into the interpretability process. We propose a disease-weighted\nattention map refinement network (DWARF) that leverages expert feedback to\nenhance model relevance and accuracy. Our method employs cyclic training to\niteratively improve diagnostic performance, generating precise and\ninterpretable feature maps. Experimental results demonstrate significant\nimprovements in interpretability and diagnostic accuracy across multiple\nmedical imaging datasets. This approach fosters effective collaboration between\nAI systems and healthcare professionals, ultimately aiming to improve patient\noutcomes\n","authors":["Haozhe Luo","Aurélie Pahud de Mortanges","Oana Inel","Abraham Bernstein","Mauricio Reyes"],"pdf_url":"https://arxiv.org/pdf/2406.17032v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19943v1","updated":"2024-06-28T14:22:30Z","published":"2024-06-28T14:22:30Z","title":"Impact of Initialization on Intra-subject Pediatric Brain MR Image\n  Registration: A Comparative Analysis between SyN ANTs and Deep Learning-Based\n  Approaches","summary":"  This study evaluates the performance of conventional SyN ANTs and\nlearning-based registration methods in the context of pediatric neuroimaging,\nspecifically focusing on intrasubject deformable registration. The comparison\ninvolves three approaches: without (NR), with rigid (RR), and with rigid and\naffine (RAR) initializations. In addition to initialization, performances are\nevaluated in terms of accuracy, speed, and the impact of age intervals and sex\nper pair. Data consists of the publicly available MRI scans from the Calgary\nPreschool dataset, which includes 63 children aged 2-7 years, allowing for 431\nregistration pairs. We implemented the unsupervised DL framework with a U-Net\narchitecture using DeepReg and it was 5-fold cross-validated. Evaluation\nincludes Dice scores for tissue segmentation from 18 smaller regions obtained\nby SynthSeg, analysis of log Jacobian determinants, and registration pro-rated\ntraining and inference times. Learning-based approaches, with or without linear\ninitializations, exhibit slight superiority over SyN ANTs in terms of Dice\nscores. Indeed, DL-based implementations with RR and RAR initializations\nsignificantly outperform SyN ANTs. Both SyN ANTs and DL-based registration\ninvolve parameter optimization, but the choice between these methods depends on\nthe scale of registration: network-based for broader coverage or SyN ANTs for\nspecific structures. Both methods face challenges with larger age intervals due\nto greater growth changes. The main takeaway is that while DL-based methods\nshow promise with faster and more accurate registrations, SyN ANTs remains\nrobust and generalizable without the need for extensive training, highlighting\nthe importance of method selection based on specific registration needs in the\npediatric context. Our code is available at\nhttps://github.com/neuropoly/pediatric-DL-registration\n","authors":["Andjela Dimitrijevic","Vincent Noblet","Benjamin De Leener"],"pdf_url":"https://arxiv.org/pdf/2406.19943v1.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:013"},{"id":"http://arxiv.org/abs/2406.19941v1","updated":"2024-06-28T14:17:16Z","published":"2024-06-28T14:17:16Z","title":"GRACE: Graph-Regularized Attentive Convolutional Entanglement with\n  Laplacian Smoothing for Robust DeepFake Video Detection","summary":"  As DeepFake video manipulation techniques escalate, posing profound threats,\nthe urgent need to develop efficient detection strategies is underscored.\nHowever, one particular issue lies with facial images being mis-detected, often\noriginating from degraded videos or adversarial attacks, leading to unexpected\ntemporal artifacts that can undermine the efficacy of DeepFake video detection\ntechniques. This paper introduces a novel method for robust DeepFake video\ndetection, harnessing the power of the proposed Graph-Regularized Attentive\nConvolutional Entanglement (GRACE) based on the graph convolutional network\nwith graph Laplacian to address the aforementioned challenges. First,\nconventional Convolution Neural Networks are deployed to perform spatiotemporal\nfeatures for the entire video. Then, the spatial and temporal features are\nmutually entangled by constructing a graph with sparse constraint, enforcing\nessential features of valid face images in the noisy face sequences remaining,\nthus augmenting stability and performance for DeepFake video detection.\nFurthermore, the Graph Laplacian prior is proposed in the graph convolutional\nnetwork to remove the noise pattern in the feature space to further improve the\nperformance. Comprehensive experiments are conducted to illustrate that our\nproposed method delivers state-of-the-art performance in DeepFake video\ndetection under noisy face sequences. The source code is available at\nhttps://github.com/ming053l/GRACE.\n","authors":["Chih-Chung Hsu","Shao-Ning Chen","Mei-Hsuan Wu","Yi-Fang Wang","Chia-Ming Lee","Yi-Shiuan Chou"],"pdf_url":"https://arxiv.org/pdf/2406.19941v1.pdf","comment":"Submitted to TPAMI 2024"},{"id":"http://arxiv.org/abs/2404.00548v2","updated":"2024-06-28T14:13:18Z","published":"2024-03-31T03:30:37Z","title":"Modeling State Shifting via Local-Global Distillation for Event-Frame\n  Gaze Tracking","summary":"  This paper tackles the problem of passive gaze estimation using both event\nand frame data. Considering the inherently different physiological structures,\nit is intractable to accurately estimate gaze purely based on a given state.\nThus, we reformulate gaze estimation as the quantification of the state\nshifting from the current state to several prior registered anchor states.\nSpecifically, we propose a two-stage learning-based gaze estimation framework\nthat divides the whole gaze estimation process into a coarse-to-fine approach\ninvolving anchor state selection and final gaze location. Moreover, to improve\nthe generalization ability, instead of learning a large gaze estimation network\ndirectly, we align a group of local experts with a student network, where a\nnovel denoising distillation algorithm is introduced to utilize denoising\ndiffusion techniques to iteratively remove inherent noise in event data.\nExtensive experiments demonstrate the effectiveness of the proposed method,\nwhich surpasses state-of-the-art methods by a large margin of 15$\\%$. The code\nwill be publicly available at\nhttps://github.com/jdjdli/Denoise_distill_EF_gazetracker.\n","authors":["Jiading Li","Zhiyu Zhu","Jinhui Hou","Junhui Hou","Jinjian Wu"],"pdf_url":"https://arxiv.org/pdf/2404.00548v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00592v2","updated":"2024-06-28T14:02:06Z","published":"2023-12-01T13:56:28Z","title":"Tracking Object Positions in Reinforcement Learning: A Metric for\n  Keypoint Detection (extended version)","summary":"  Reinforcement learning (RL) for robot control typically requires a detailed\nrepresentation of the environment state, including information about\ntask-relevant objects not directly measurable. Keypoint detectors, such as\nspatial autoencoders (SAEs), are a common approach to extracting a\nlow-dimensional representation from high-dimensional image data. SAEs aim at\nspatial features such as object positions, which are often useful\nrepresentations in robotic RL. However, whether an SAE is actually able to\ntrack objects in the scene and thus yields a spatial state representation well\nsuited for RL tasks has rarely been examined due to a lack of established\nmetrics. In this paper, we propose to assess the performance of an SAE instance\nby measuring how well keypoints track ground truth objects in images. We\npresent a computationally lightweight metric and use it to evaluate common\nbaseline SAE architectures on image data from a simulated robot task. We find\nthat common SAEs differ substantially in their spatial extraction capability.\nFurthermore, we validate that SAEs that perform well in our metric achieve\nsuperior performance when used in downstream RL. Thus, our metric is an\neffective and lightweight indicator of RL performance before executing\nexpensive RL training. Building on these insights, we identify three key\nmodifications of SAE architectures to improve tracking performance. We make our\ncode available at anonymous.4open.science/r/sae-rl.\n","authors":["Emma Cramer","Jonas Reiher","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2312.00592v2.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2406.19922v1","updated":"2024-06-28T13:51:59Z","published":"2024-06-28T13:51:59Z","title":"Parallax-tolerant Image Stitching via Segmentation-guided\n  Multi-homography Warping","summary":"  Large parallax between images is an intractable issue in image stitching.\nVarious warping-based methods are proposed to address it, yet the results are\nunsatisfactory. In this paper, we propose a novel image stitching method using\nmulti-homography warping guided by image segmentation. Specifically, we\nleverage the Segment Anything Model to segment the target image into numerous\ncontents and partition the feature points into multiple subsets via the\nenergy-based multi-homography fitting algorithm. The multiple subsets of\nfeature points are used to calculate the corresponding multiple homographies.\nFor each segmented content in the overlapping region, we select its\nbest-fitting homography with the lowest photometric error. For each segmented\ncontent in the non-overlapping region, we calculate a weighted combination of\nthe linearized homographies. Finally, the target image is warped via the\nbest-fitting homographies to align with the reference image, and the final\npanorama is generated via linear blending. Comprehensive experimental results\non the public datasets demonstrate that our method provides the best alignment\naccuracy by a large margin, compared with the state-of-the-art methods. The\nsource code is available at https://github.com/tlliao/multi-homo-warp.\n","authors":["Tianli Liao","Ce Wang","Lei Li","Guangen Liu","Nan Li"],"pdf_url":"https://arxiv.org/pdf/2406.19922v1.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.19905v1","updated":"2024-06-28T13:20:17Z","published":"2024-06-28T13:20:17Z","title":"Solving Token Gradient Conflict in Mixture-of-Experts for Large\n  Vision-Language Model","summary":"  The Mixture-of-Experts (MoE) has gained increasing attention in the study of\nLarge Vision-Language Models (LVLMs). It uses a sparse model to replace the\ndense model, achieving comparable performance while activating fewer parameters\nduring inference, thus significantly reducing the inference cost. Existing MoE\nmethods in LVLMs encourage different experts to handle different tokens, and\nthus they employ a router to predict the routing for each token. However, the\npredictions are based solely on sample features and do not truly reveal the\noptimization direction of tokens. This can lead to severe optimization\nconflicts between different tokens within an expert. To address this problem,\nthis paper proposes a novel method based on token-level gradient analysis.\nSpecifically, we first use token-level gradients to identify conflicting tokens\nin experts. Then, we add a specialized loss tailored to eliminate conflicts\namong tokens within each expert. Our method can serve as a plug-in for diverse\nLarge Vision-Language Models, and extensive experimental results demonstrate\nthe effectiveness of our method. The code will be publicly available at\nhttps://github.com/longrongyang/STGC.\n","authors":["Longrong Yang","Dong Sheng","Chaoxiang Cai","Fan Yang","Size Li","Di Zhang","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2406.19905v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14862v3","updated":"2024-06-28T13:19:37Z","published":"2024-06-21T04:39:03Z","title":"LatentExplainer: Explaining Latent Representations in Deep Generative\n  Models with Multi-modal Foundation Models","summary":"  Deep generative models like VAEs and diffusion models have advanced various\ngeneration tasks by leveraging latent variables to learn data distributions and\ngenerate high-quality samples. Despite the field of explainable AI making\nstrides in interpreting machine learning models, understanding latent variables\nin generative models remains challenging. This paper introduces\nLatentExplainer, a framework for automatically generating semantically\nmeaningful explanations of latent variables in deep generative models.\nLatentExplainer tackles three main challenges: inferring the meaning of latent\nvariables, aligning explanations with inductive biases, and handling varying\ndegrees of explainability. By perturbing latent variables and interpreting\nchanges in generated data, the framework provides a systematic approach to\nunderstanding and controlling the data generation process, enhancing the\ntransparency and interpretability of deep generative models. We evaluate our\nproposed method on several real-world and synthetic datasets, and the results\ndemonstrate superior performance in generating high-quality explanations of\nlatent variables.\n","authors":["Mengdan Zhu","Raasikh Kanjiani","Jiahui Lu","Andrew Choi","Qirui Ye","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.14862v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19899v1","updated":"2024-06-28T13:07:43Z","published":"2024-06-28T13:07:43Z","title":"On the Value of PHH3 for Mitotic Figure Detection on H&E-stained Images","summary":"  The count of mitotic figures (MFs) observed in hematoxylin and eosin\n(H&E)-stained slides is an important prognostic marker as it is a measure for\ntumor cell proliferation. However, the identification of MFs has a known low\ninter-rater agreement. Deep learning algorithms can standardize this task, but\nthey require large amounts of annotated data for training and validation.\nFurthermore, label noise introduced during the annotation process may impede\nthe algorithm's performance. Unlike H&E, the mitosis-specific antibody\nphospho-histone H3 (PHH3) specifically highlights MFs. Counting MFs on slides\nstained against PHH3 leads to higher agreement among raters and has therefore\nrecently been used as a ground truth for the annotation of MFs in H&E. However,\nas PHH3 facilitates the recognition of cells indistinguishable from H&E stain\nalone, the use of this ground truth could potentially introduce noise into the\nH&E-related dataset, impacting model performance. This study analyzes the\nimpact of PHH3-assisted MF annotation on inter-rater reliability and object\nlevel agreement through an extensive multi-rater experiment. We found that the\nannotators' object-level agreement increased when using PHH3-assisted labeling.\nSubsequently, MF detectors were evaluated on the resulting datasets to\ninvestigate the influence of PHH3-assisted labeling on the models' performance.\nAdditionally, a novel dual-stain MF detector was developed to investigate the\ninterpretation-shift of PHH3-assisted labels used in H&E, which clearly\noutperformed single-stain detectors. However, the PHH3-assisted labels did not\nhave a positive effect on solely H&E-based models. The high performance of our\ndual-input detector reveals an information mismatch between the H&E and\nPHH3-stained images as the cause of this effect.\n","authors":["Jonathan Ganz","Christian Marzahl","Jonas Ammeling","Barbara Richter","Chloé Puget","Daniela Denk","Elena A. Demeter","Flaviu A. Tabaran","Gabriel Wasinger","Karoline Lipnik","Marco Tecilla","Matthew J. Valentine","Michael J. Dark","Niklas Abele","Pompei Bolfa","Ramona Erber","Robert Klopfleisch","Sophie Merz","Taryn A. Donovan","Samir Jabari","Christof A. Bertram","Katharina Breininger","Marc Aubreville"],"pdf_url":"https://arxiv.org/pdf/2406.19899v1.pdf","comment":"10 pages, 5 figures, 1 Table"},{"id":"http://arxiv.org/abs/2406.19875v1","updated":"2024-06-28T12:35:01Z","published":"2024-06-28T12:35:01Z","title":"InfiniBench: A Comprehensive Benchmark for Large Multimodal Models in\n  Very Long Video Understanding","summary":"  Understanding long videos, ranging from tens of minutes to several hours,\npresents unique challenges in video comprehension. Despite the increasing\nimportance of long-form video content, existing benchmarks primarily focus on\nshorter clips. To address this gap, we introduce InfiniBench a comprehensive\nbenchmark for very long video understanding which presents 1)The longest video\nduration, averaging 76.34 minutes; 2) The largest number of question-answer\npairs, 108.2K; 3) Diversity in questions that examine nine different skills and\ninclude both multiple-choice questions and open-ended questions; 4)\nHumancentric, as the video sources come from movies and daily TV shows, with\nspecific human-level question designs such as Movie Spoiler Questions that\nrequire critical thinking and comprehensive understanding. Using InfiniBench,\nwe comprehensively evaluate existing Large MultiModality Models (LMMs) on each\nskill, including the commercial model Gemini 1.5 Flash and the open-source\nmodels. The evaluation shows significant challenges in our benchmark.Our\nresults show that the best AI models such Gemini struggles to perform well with\n42.72% average accuracy and 2.71 out of 5 average score. We hope this benchmark\nwill stimulate the LMMs community towards long video and human-level\nunderstanding. Our benchmark can be accessed at\nhttps://vision-cair.github.io/InfiniBench/\n","authors":["Kirolos Ataallah","Chenhui Gou","Eslam Abdelrahman","Khushbu Pahwa","Jian Ding","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2406.19875v1.pdf","comment":"16 page ,17 figures"},{"id":"http://arxiv.org/abs/2406.11252v2","updated":"2024-06-28T11:59:01Z","published":"2024-06-17T06:28:58Z","title":"Mining Open Semantics from CLIP: A Relation Transition Perspective for\n  Few-Shot Learning","summary":"  Contrastive Vision-Language Pre-training(CLIP) demonstrates impressive\nzero-shot capability. The key to improve the adaptation of CLIP to downstream\ntask with few exemplars lies in how to effectively model and transfer the\nuseful knowledge embedded in CLIP. Previous work mines the knowledge typically\nbased on the limited visual samples and close-set semantics (i.e., within\ntarget category set of downstream task). However, the aligned CLIP image/text\nencoders contain abundant relationships between visual features and almost\ninfinite open semantics, which may benefit the few-shot learning but remains\nunexplored. In this paper, we propose to mine open semantics as anchors to\nperform a relation transition from image-anchor relationship to image-target\nrelationship to make predictions. Specifically, we adopt a transformer module\nwhich takes the visual feature as \"Query\", the text features of the anchors as\n\"Key\" and the similarity matrix between the text features of anchor and target\nclasses as \"Value\". In this way, the output of such a transformer module\nrepresents the relationship between the image and target categories, i.e., the\nclassification predictions. To avoid manually selecting the open semantics, we\nmake the [CLASS] token of input text embedding learnable. We conduct extensive\nexperiments on eleven representative classification datasets. The results show\nthat our method performs favorably against previous state-of-the-arts\nconsidering few-shot classification settings.\n","authors":["Cilin Yan","Haochen Wang","Xiaolong Jiang","Yao Hu","Xu Tang","Guoliang Kang","Efstratios Gavves"],"pdf_url":"https://arxiv.org/pdf/2406.11252v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19852v1","updated":"2024-06-28T11:49:59Z","published":"2024-06-28T11:49:59Z","title":"FootBots: A Transformer-based Architecture for Motion Prediction in\n  Soccer","summary":"  Motion prediction in soccer involves capturing complex dynamics from player\nand ball interactions. We present FootBots, an encoder-decoder\ntransformer-based architecture addressing motion prediction and conditioned\nmotion prediction through equivariance properties. FootBots captures temporal\nand social dynamics using set attention blocks and multi-attention block\ndecoder. Our evaluation utilizes two datasets: a real soccer dataset and a\ntailored synthetic one. Insights from the synthetic dataset highlight the\neffectiveness of FootBots' social attention mechanism and the significance of\nconditioned motion prediction. Empirical results on real soccer data\ndemonstrate that FootBots outperforms baselines in motion prediction and excels\nin conditioned tasks, such as predicting the players based on the ball\nposition, predicting the offensive (defensive) team based on the ball and the\ndefensive (offensive) team, and predicting the ball position based on all\nplayers. Our evaluation connects quantitative and qualitative findings.\nhttps://youtu.be/9kaEkfzG3L8\n","authors":["Guillem Capellera","Luis Ferraz","Antonio Rubio","Antonio Agudo","Francesc Moreno-Noguer"],"pdf_url":"https://arxiv.org/pdf/2406.19852v1.pdf","comment":"Published as a conference paper at IEEE ICIP 2024"},{"id":"http://arxiv.org/abs/2406.19844v1","updated":"2024-06-28T11:35:35Z","published":"2024-06-28T11:35:35Z","title":"StreamMOTP: Streaming and Unified Framework for Joint 3D Multi-Object\n  Tracking and Trajectory Prediction","summary":"  3D multi-object tracking and trajectory prediction are two crucial modules in\nautonomous driving systems. Generally, the two tasks are handled separately in\ntraditional paradigms and a few methods have started to explore modeling these\ntwo tasks in a joint manner recently. However, these approaches suffer from the\nlimitations of single-frame training and inconsistent coordinate\nrepresentations between tracking and prediction tasks. In this paper, we\npropose a streaming and unified framework for joint 3D Multi-Object Tracking\nand trajectory Prediction (StreamMOTP) to address the above challenges.\nFirstly, we construct the model in a streaming manner and exploit a memory bank\nto preserve and leverage the long-term latent features for tracked objects more\neffectively. Secondly, a relative spatio-temporal positional encoding strategy\nis introduced to bridge the gap of coordinate representations between the two\ntasks and maintain the pose-invariance for trajectory prediction. Thirdly, we\nfurther improve the quality and consistency of predicted trajectories with a\ndual-stream predictor. We conduct extensive experiments on popular nuSences\ndataset and the experimental results demonstrate the effectiveness and\nsuperiority of StreamMOTP, which outperforms previous methods significantly on\nboth tasks. Furthermore, we also prove that the proposed framework has great\npotential and advantages in actual applications of autonomous driving.\n","authors":["Jiaheng Zhuang","Guoan Wang","Siyu Zhang","Xiyang Wang","Hangning Zhou","Ziyao Xu","Chi Zhang","Zhiheng Li"],"pdf_url":"https://arxiv.org/pdf/2406.19844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03511v3","updated":"2024-06-28T11:23:11Z","published":"2023-12-06T14:13:38Z","title":"Kandinsky 3.0 Technical Report","summary":"  We present Kandinsky 3.0, a large-scale text-to-image generation model based\non latent diffusion, continuing the series of text-to-image Kandinsky models\nand reflecting our progress to achieve higher quality and realism of image\ngeneration. In this report we describe the architecture of the model, the data\ncollection procedure, the training technique, and the production system for\nuser interaction. We focus on the key components that, as we have identified as\na result of a large number of experiments, had the most significant impact on\nimproving the quality of our model compared to the others. We also describe\nextensions and applications of our model, including super resolution,\ninpainting, image editing, image-to-video generation, and a distilled version\nof Kandinsky 3.0 - Kandinsky 3.1, which does inference in 4 steps of the\nreverse process and 20 times faster without visual quality decrease. By\nside-by-side human preferences comparison, Kandinsky becomes better in text\nunderstanding and works better on specific domains. The code is available at\nhttps://github.com/ai-forever/Kandinsky-3\n","authors":["Vladimir Arkhipkin","Andrei Filatov","Viacheslav Vasilev","Anastasia Maltseva","Said Azizov","Igor Pavlov","Julia Agafonova","Andrey Kuznetsov","Denis Dimitrov"],"pdf_url":"https://arxiv.org/pdf/2312.03511v3.pdf","comment":"Project page: https://ai-forever.github.io/Kandinsky-3"},{"id":"http://arxiv.org/abs/2406.19833v1","updated":"2024-06-28T11:11:24Z","published":"2024-06-28T11:11:24Z","title":"LightStereo: Channel Boost Is All Your Need for Efficient 2D Cost\n  Aggregation","summary":"  We present LightStereo, a cutting-edge stereo-matching network crafted to\naccelerate the matching process. Departing from conventional methodologies that\nrely on aggregating computationally intensive 4D costs, LightStereo adopts the\n3D cost volume as a lightweight alternative. While similar approaches have been\nexplored previously, our breakthrough lies in enhancing performance through a\ndedicated focus on the channel dimension of the 3D cost volume, where the\ndistribution of matching costs is encapsulated. Our exhaustive exploration has\nyielded plenty of strategies to amplify the capacity of the pivotal dimension,\nensuring both precision and efficiency. We compare the proposed LightStereo\nwith existing state-of-the-art methods across various benchmarks, which\ndemonstrate its superior performance in speed, accuracy, and resource\nutilization. LightStereo achieves a competitive EPE metric in the SceneFlow\ndatasets while demanding a minimum of only 22 GFLOPs, with an inference time of\njust 17 ms. Our comprehensive analysis reveals the effect of 2D cost\naggregation for stereo matching, paving the way for real-world applications of\nefficient stereo systems. Code will be available at\n\\url{https://github.com/XiandaGuo/OpenStereo}.\n","authors":["Xianda Guo","Chenming Zhang","Dujun Nie","Wenzhao Zheng","Youmin Zhang","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2406.19833v1.pdf","comment":"Code will be available at\n  \\url{https://github.com/XiandaGuo/OpenStereo}"},{"id":"http://arxiv.org/abs/2406.19815v1","updated":"2024-06-28T10:45:37Z","published":"2024-06-28T10:45:37Z","title":"Emotion Loss Attacking: Adversarial Attack Perception for Skeleton based\n  on Multi-dimensional Features","summary":"  Adversarial attack on skeletal motion is a hot topic. However, existing\nresearches only consider part of dynamic features when measuring distance\nbetween skeleton graph sequences, which results in poor imperceptibility. To\nthis end, we propose a novel adversarial attack method to attack action\nrecognizers for skeletal motions. Firstly, our method systematically proposes a\ndynamic distance function to measure the difference between skeletal motions.\nMeanwhile, we innovatively introduce emotional features for complementary\ninformation. In addition, we use Alternating Direction Method of\nMultipliers(ADMM) to solve the constrained optimization problem, which\ngenerates adversarial samples with better imperceptibility to deceive the\nclassifiers. Experiments show that our method is effective on multiple action\nclassifiers and datasets. When the perturbation magnitude measured by l norms\nis the same, the dynamic perturbations generated by our method are much lower\nthan that of other methods. What's more, we are the first to prove the\neffectiveness of emotional features, and provide a new idea for measuring the\ndistance between skeletal motions.\n","authors":["Feng Liu","Qing Xu","Qijian Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.19815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19814v1","updated":"2024-06-28T10:45:25Z","published":"2024-06-28T10:45:25Z","title":"Extract More from Less: Efficient Fine-Grained Visual Recognition in\n  Low-Data Regimes","summary":"  The emerging task of fine-grained image classification in low-data regimes\nassumes the presence of low inter-class variance and large intra-class\nvariation along with a highly limited amount of training samples per class.\nHowever, traditional ways of separately dealing with fine-grained\ncategorisation and extremely scarce data may be inefficient under both these\nharsh conditions presented together. In this paper, we present a novel\nframework, called AD-Net, aiming to enhance deep neural network performance on\nthis challenge by leveraging the power of Augmentation and Distillation\ntechniques. Specifically, our approach is designed to refine learned features\nthrough self-distillation on augmented samples, mitigating harmful overfitting.\nWe conduct comprehensive experiments on popular fine-grained image\nclassification benchmarks where our AD-Net demonstrates consistent improvement\nover traditional fine-tuning and state-of-the-art low-data techniques.\nRemarkably, with the smallest data available, our framework shows an\noutstanding relative accuracy increase of up to 45 % compared to standard\nResNet-50 and up to 27 % compared to the closest SOTA runner-up. We emphasise\nthat our approach is practically architecture-independent and adds zero extra\ncost at inference time. Additionally, we provide an extensive study on the\nimpact of every framework's component, highlighting the importance of each in\nachieving optimal performance. Source code and trained models are publicly\navailable at github.com/demidovd98/fgic_lowd.\n","authors":["Dmitry Demidov","Abduragim Shtanchaev","Mihail Mihaylov","Mohammad Almansoori"],"pdf_url":"https://arxiv.org/pdf/2406.19814v1.pdf","comment":"Main paper and Appendices"},{"id":"http://arxiv.org/abs/2406.19811v1","updated":"2024-06-28T10:39:36Z","published":"2024-06-28T10:39:36Z","title":"EgoGaussian: Dynamic Scene Understanding from Egocentric Video with 3D\n  Gaussian Splatting","summary":"  Human activities are inherently complex, and even simple household tasks\ninvolve numerous object interactions. To better understand these activities and\nbehaviors, it is crucial to model their dynamic interactions with the\nenvironment. The recent availability of affordable head-mounted cameras and\negocentric data offers a more accessible and efficient means to understand\ndynamic human-object interactions in 3D environments. However, most existing\nmethods for human activity modeling either focus on reconstructing 3D models of\nhand-object or human-scene interactions or on mapping 3D scenes, neglecting\ndynamic interactions with objects. The few existing solutions often require\ninputs from multiple sources, including multi-camera setups, depth-sensing\ncameras, or kinesthetic sensors. To this end, we introduce EgoGaussian, the\nfirst method capable of simultaneously reconstructing 3D scenes and dynamically\ntracking 3D object motion from RGB egocentric input alone. We leverage the\nuniquely discrete nature of Gaussian Splatting and segment dynamic interactions\nfrom the background. Our approach employs a clip-level online learning pipeline\nthat leverages the dynamic nature of human activities, allowing us to\nreconstruct the temporal evolution of the scene in chronological order and\ntrack rigid object motion. Additionally, our method automatically segments\nobject and background Gaussians, providing 3D representations for both static\nscenes and dynamic objects. EgoGaussian outperforms previous NeRF and Dynamic\nGaussian methods in challenging in-the-wild videos and we also qualitatively\ndemonstrate the high quality of the reconstructed models.\n","authors":["Daiwei Zhang","Gengyan Li","Jiajie Li","Mickaël Bressieux","Otmar Hilliges","Marc Pollefeys","Luc Van Gool","Xi Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19796v1","updated":"2024-06-28T10:05:58Z","published":"2024-06-28T10:05:58Z","title":"Comprehensive Generative Replay for Task-Incremental Segmentation with\n  Concurrent Appearance and Semantic Forgetting","summary":"  Generalist segmentation models are increasingly favored for diverse tasks\ninvolving various objects from different image sources. Task-Incremental\nLearning (TIL) offers a privacy-preserving training paradigm using tasks\narriving sequentially, instead of gathering them due to strict data sharing\npolicies. However, the task evolution can span a wide scope that involves\nshifts in both image appearance and segmentation semantics with intricate\ncorrelation, causing concurrent appearance and semantic forgetting. To solve\nthis issue, we propose a Comprehensive Generative Replay (CGR) framework that\nrestores appearance and semantic knowledge by synthesizing image-mask pairs to\nmimic past task data, which focuses on two aspects: modeling image-mask\ncorrespondence and promoting scalability for diverse tasks. Specifically, we\nintroduce a novel Bayesian Joint Diffusion (BJD) model for high-quality\nsynthesis of image-mask pairs with their correspondence explicitly preserved by\nconditional denoising. Furthermore, we develop a Task-Oriented Adapter (TOA)\nthat recalibrates prompt embeddings to modulate the diffusion model, making the\ndata synthesis compatible with different tasks. Experiments on incremental\ntasks (cardiac, fundus and prostate segmentation) show its clear advantage for\nalleviating concurrent appearance and semantic forgetting. Code is available at\nhttps://github.com/jingyzhang/CGR.\n","authors":["Wei Li","Jingyang Zhang","Pheng-Ann Heng","Lixu Gu"],"pdf_url":"https://arxiv.org/pdf/2406.19796v1.pdf","comment":"Accepted by MICCAI24"},{"id":"http://arxiv.org/abs/2404.09666v2","updated":"2024-06-28T09:25:25Z","published":"2024-04-15T10:57:16Z","title":"Deformable MRI Sequence Registration for AI-based Prostate Cancer\n  Diagnosis","summary":"  The PI-CAI (Prostate Imaging: Cancer AI) challenge led to expert-level\ndiagnostic algorithms for clinically significant prostate cancer detection. The\nalgorithms receive biparametric MRI scans as input, which consist of\nT2-weighted and diffusion-weighted scans. These scans can be misaligned due to\nmultiple factors in the scanning process. Image registration can alleviate this\nissue by predicting the deformation between the sequences. We investigate the\neffect of image registration on the diagnostic performance of AI-based prostate\ncancer diagnosis. First, the image registration algorithm, developed in\nMeVisLab, is analyzed using a dataset with paired lesion annotations. Second,\nthe effect on diagnosis is evaluated by comparing case-level cancer diagnosis\nperformance between using the original dataset, rigidly aligned\ndiffusion-weighted scans, or deformably aligned diffusion-weighted scans. Rigid\nregistration showed no improvement. Deformable registration demonstrated a\nsubstantial improvement in lesion overlap (+10% median Dice score) and a\npositive yet non-significant improvement in diagnostic performance (+0.3%\nAUROC, p=0.18). Our investigation shows that a substantial improvement in\nlesion alignment does not directly lead to a significant improvement in\ndiagnostic performance. Qualitative analysis indicated that jointly developing\nimage registration methods and diagnostic AI algorithms could enhance\ndiagnostic accuracy and patient outcomes.\n","authors":["Alessa Hering","Sarah de Boer","Anindo Saha","Jasper J. Twilt","Mattias P. Heinrich","Derya Yakar","Maarten de Rooij","Henkjan Huisman","Joeran S. Bosma"],"pdf_url":"https://arxiv.org/pdf/2404.09666v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15613v2","updated":"2024-06-28T09:22:38Z","published":"2024-05-24T14:58:51Z","title":"Automatic Data Curation for Self-Supervised Learning: A Clustering-Based\n  Approach","summary":"  Self-supervised features are the cornerstone of modern machine learning\nsystems. They are typically pre-trained on data collections whose construction\nand curation typically require extensive human effort. This manual process has\nsome limitations similar to those encountered in supervised learning, e.g., the\ncrowd-sourced selection of data is costly and time-consuming, preventing\nscaling the dataset size. In this work, we consider the problem of automatic\ncuration of high-quality datasets for self-supervised pre-training. We posit\nthat such datasets should be large, diverse and balanced, and propose a\nclustering-based approach for building ones satisfying all these criteria. Our\nmethod involves successive and hierarchical applications of $k$-means on a\nlarge and diverse data repository to obtain clusters that distribute uniformly\namong data concepts, followed by a hierarchical, balanced sampling step from\nthese clusters. Extensive experiments on three different data domains including\nweb-based images, satellite images and text show that features trained on our\nautomatically curated datasets outperform those trained on uncurated data while\nbeing on par or better than ones trained on manually curated data. Code is\navailable at https://github.com/facebookresearch/ssl-data-curation.\n","authors":["Huy V. Vo","Vasil Khalidov","Timothée Darcet","Théo Moutakanni","Nikita Smetanin","Marc Szafraniec","Hugo Touvron","Camille Couprie","Maxime Oquab","Armand Joulin","Hervé Jégou","Patrick Labatut","Piotr Bojanowski"],"pdf_url":"https://arxiv.org/pdf/2405.15613v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19336v2","updated":"2024-06-28T09:20:01Z","published":"2024-06-27T17:10:10Z","title":"LiverUSRecon: Automatic 3D Reconstruction and Volumetry of the Liver\n  with a Few Partial Ultrasound Scans","summary":"  3D reconstruction of the liver for volumetry is important for qualitative\nanalysis and disease diagnosis. Liver volumetry using ultrasound (US) scans,\nalthough advantageous due to less acquisition time and safety, is challenging\ndue to the inherent noisiness in US scans, blurry boundaries, and partial liver\nvisibility. We address these challenges by using the segmentation masks of a\nfew incomplete sagittal-plane US scans of the liver in conjunction with a\nstatistical shape model (SSM) built using a set of CT scans of the liver. We\ncompute the shape parameters needed to warp this canonical SSM to fit the US\nscans through a parametric regression network. The resulting 3D liver\nreconstruction is accurate and leads to automatic liver volume calculation. We\nevaluate the accuracy of the estimated liver volumes with respect to CT\nsegmentation volumes using RMSE. Our volume computation is statistically much\ncloser to the volume estimated using CT scans than the volume computed using\nChilds' method by radiologists: p-value of 0.094 (>0.05) says that there is no\nsignificant difference between CT segmentation volumes and ours in contrast to\nChilds' method. We validate our method using investigations (ablation studies)\non the US image resolution, the number of CT scans used for SSM, the number of\nprincipal components, and the number of input US scans. To the best of our\nknowledge, this is the first automatic liver volumetry system using a few\nincomplete US scans given a set of CT scans of livers for SSM.\n","authors":["Kaushalya Sivayogaraj","Sahan T. Guruge","Udari Liyanage","Jeevani Udupihille","Saroj Jayasinghe","Gerard Fernando","Ranga Rodrigo","M. Rukshani Liyanaarachchi"],"pdf_url":"https://arxiv.org/pdf/2406.19336v2.pdf","comment":"10 pages, Accepted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2304.10839v4","updated":"2024-06-28T09:01:03Z","published":"2023-04-21T09:30:22Z","title":"Cross-domain Denoising for Low-dose Multi-frame Spiral Computed\n  Tomography","summary":"  Computed tomography (CT) has been used worldwide as a non-invasive test to\nassist in diagnosis. However, the ionizing nature of X-ray exposure raises\nconcerns about potential health risks such as cancer. The desire for lower\nradiation doses has driven researchers to improve reconstruction quality.\nAlthough previous studies on low-dose computed tomography (LDCT) denoising have\ndemonstrated the effectiveness of learning-based methods, most were developed\non the simulated data. However, the real-world scenario differs significantly\nfrom the simulation domain, especially when using the multi-slice spiral\nscanner geometry. This paper proposes a two-stage method for the commercially\navailable multi-slice spiral CT scanners that better exploits the complete\nreconstruction pipeline for LDCT denoising across different domains. Our\napproach makes good use of the high redundancy of multi-slice projections and\nthe volumetric reconstructions while leveraging the over-smoothing problem in\nconventional cascaded frameworks caused by aggressive denoising. The dedicated\ndesign also provides a more explicit interpretation of the data flow. Extensive\nexperiments on various datasets showed that the proposed method could remove up\nto 70\\% of noise without compromised spatial resolution, and subjective\nevaluations by two experienced radiologists further supported its superior\nperformance against state-of-the-art methods in clinical practice.\n","authors":["Yucheng Lu","Zhixin Xu","Moon Hyung Choi","Jimin Kim","Seung-Won Jung"],"pdf_url":"https://arxiv.org/pdf/2304.10839v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19756v1","updated":"2024-06-28T08:54:44Z","published":"2024-06-28T08:54:44Z","title":"Structure-aware World Model for Probe Guidance via Large-scale\n  Self-supervised Pre-train","summary":"  The complex structure of the heart leads to significant challenges in\nechocardiography, especially in acquisition cardiac ultrasound images.\nSuccessful echocardiography requires a thorough understanding of the structures\non the two-dimensional plane and the spatial relationships between planes in\nthree-dimensional space. In this paper, we innovatively propose a large-scale\nself-supervised pre-training method to acquire a cardiac structure-aware world\nmodel. The core innovation lies in constructing a self-supervised task that\nrequires structural inference by predicting masked structures on a 2D plane and\nimagining another plane based on pose transformation in 3D space. To support\nlarge-scale pre-training, we collected over 1.36 million echocardiograms from\nten standard views, along with their 3D spatial poses. In the downstream probe\nguidance task, we demonstrate that our pre-trained model consistently reduces\nguidance errors across the ten most common standard views on the test set with\n0.29 million samples from 74 routine clinical scans, indicating that\nstructure-aware pre-training benefits the scanning.\n","authors":["Haojun Jiang","Meng Li","Zhenguo Sun","Ning Jia","Yu Sun","Shaqi Luo","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2406.19756v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2406.19749v1","updated":"2024-06-28T08:48:14Z","published":"2024-06-28T08:48:14Z","title":"SPIRONet: Spatial-Frequency Learning and Topological Channel Interaction\n  Network for Vessel Segmentation","summary":"  Automatic vessel segmentation is paramount for developing next-generation\ninterventional navigation systems. However, current approaches suffer from\nsuboptimal segmentation performances due to significant challenges in\nintraoperative images (i.e., low signal-to-noise ratio, small or slender\nvessels, and strong interference). In this paper, a novel spatial-frequency\nlearning and topological channel interaction network (SPIRONet) is proposed to\naddress the above issues. Specifically, dual encoders are utilized to\ncomprehensively capture local spatial and global frequency vessel features.\nThen, a cross-attention fusion module is introduced to effectively fuse spatial\nand frequency features, thereby enhancing feature discriminability.\nFurthermore, a topological channel interaction module is designed to filter out\ntask-irrelevant responses based on graph neural networks. Extensive\nexperimental results on several challenging datasets (CADSA, CAXF, DCA1, and\nXCAD) demonstrate state-of-the-art performances of our method. Moreover, the\ninference speed of SPIRONet is 21 FPS with a 512x512 input size, surpassing\nclinical real-time requirements (6~12FPS). These promising outcomes indicate\nSPIRONet's potential for integration into vascular interventional navigation\nsystems. Code is available at https://github.com/Dxhuang-CASIA/SPIRONet.\n","authors":["De-Xing Huang","Xiao-Hu Zhou","Xiao-Liang Xie","Shi-Qi Liu","Shuang-Yi Wang","Zhen-Qiu Feng","Mei-Jiang Gui","Hao Li","Tian-Yu Xiang","Bo-Xian Yao","Zeng-Guang Hou"],"pdf_url":"https://arxiv.org/pdf/2406.19749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19736v1","updated":"2024-06-28T08:25:27Z","published":"2024-06-28T08:25:27Z","title":"MM-Instruct: Generated Visual Instructions for Large Multimodal Model\n  Alignment","summary":"  This paper introduces MM-Instruct, a large-scale dataset of diverse and\nhigh-quality visual instruction data designed to enhance the\ninstruction-following capabilities of large multimodal models (LMMs). While\nexisting visual instruction datasets often focus on question-answering, they\nstruggle to generalize to broader application scenarios such as creative\nwriting, summarization, or image analysis. To address these limitations, we\npropose a novel approach to constructing MM-Instruct that leverages the strong\ninstruction-following capabilities of existing LLMs to generate novel visual\ninstruction data from large-scale but conventional image captioning datasets.\nMM-Instruct first leverages ChatGPT to automatically generate diverse\ninstructions from a small set of seed instructions through augmenting and\nsummarization. It then matches these instructions with images and uses an\nopen-sourced large language model (LLM) to generate coherent answers to the\ninstruction-image pairs. The LLM is grounded by the detailed text descriptions\nof images in the whole answer generation process to guarantee the alignment of\nthe instruction data. Moreover, we introduce a benchmark based on the generated\ninstruction data to evaluate the instruction-following capabilities of existing\nLMMs. We demonstrate the effectiveness of MM-Instruct by training a LLaVA-1.5\nmodel on the generated data, denoted as LLaVA-Instruct, which exhibits\nsignificant improvements in instruction-following capabilities compared to\nLLaVA-1.5 models. The MM-Instruct dataset, benchmark, and pre-trained models\nare available at https://github.com/jihaonew/MM-Instruct.\n","authors":["Jihao Liu","Xin Huang","Jinliang Zheng","Boxiao Liu","Jia Wang","Osamu Yoshie","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2406.19736v1.pdf","comment":"Dataset and models are available at\n  https://github.com/jihaonew/MM-Instruct"},{"id":"http://arxiv.org/abs/2406.19726v1","updated":"2024-06-28T08:16:54Z","published":"2024-06-28T08:16:54Z","title":"EPOCH: Jointly Estimating the 3D Pose of Cameras and Humans","summary":"  Monocular Human Pose Estimation (HPE) aims at determining the 3D positions of\nhuman joints from a single 2D image captured by a camera. However, a single 2D\npoint in the image may correspond to multiple points in 3D space. Typically,\nthe uniqueness of the 2D-3D relationship is approximated using an orthographic\nor weak-perspective camera model. In this study, instead of relying on\napproximations, we advocate for utilizing the full perspective camera model.\nThis involves estimating camera parameters and establishing a precise,\nunambiguous 2D-3D relationship. To do so, we introduce the EPOCH framework,\ncomprising two main components: the pose lifter network (LiftNet) and the pose\nregressor network (RegNet). LiftNet utilizes the full perspective camera model\nto precisely estimate the 3D pose in an unsupervised manner. It takes a 2D pose\nand camera parameters as inputs and produces the corresponding 3D pose\nestimation. These inputs are obtained from RegNet, which starts from a single\nimage and provides estimates for the 2D pose and camera parameters. RegNet\nutilizes only 2D pose data as weak supervision. Internally, RegNet predicts a\n3D pose, which is then projected to 2D using the estimated camera parameters.\nThis process enables RegNet to establish the unambiguous 2D-3D relationship.\nOur experiments show that modeling the lifting as an unsupervised task with a\ncamera in-the-loop results in better generalization to unseen data. We obtain\nstate-of-the-art results for the 3D HPE on the Human3.6M and MPI-INF-3DHP\ndatasets. Our code is available at: [Github link upon acceptance, see\nsupplementary materials].\n","authors":["Nicola Garau","Giulia Martinelli","Niccolò Bisagno","Denis Tomè","Carsten Stoll"],"pdf_url":"https://arxiv.org/pdf/2406.19726v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2309.01469v2","updated":"2024-06-28T08:13:48Z","published":"2023-09-04T09:26:04Z","title":"Defect Detection in Synthetic Fibre Ropes using Detectron2 Framework","summary":"  Fibre ropes with the latest technology have emerged as an appealing\nalternative to steel ropes for offshore industries due to their lightweight and\nhigh tensile strength. At the same time, frequent inspection of these ropes is\nessential to ensure the proper functioning and safety of the entire system. The\ndevelopment of deep learning (DL) models in condition monitoring (CM)\napplications offers a simpler and more effective approach for defect detection\nin synthetic fibre ropes (SFRs). The present paper investigates the performance\nof Detectron2, a state-of-the-art library for defect detection and instance\nsegmentation. Detectron2 with Mask R-CNN architecture is used for segmenting\ndefects in SFRs. Mask R-CNN with various backbone configurations has been\ntrained and tested on an experimentally obtained dataset comprising 1,803\nhigh-dimensional images containing seven damage classes (placking high,\nplacking medium, placking low, compression, core out, chafing, and normal\nrespectively) for SFRs. By leveraging the capabilities of Detectron2, this\nstudy aims to develop an automated and efficient method for detecting defects\nin SFRs, enhancing the inspection process, and ensuring the safety of the fibre\nropes.\n","authors":["Anju Rani","Daniel O. Arroyo","Petar Durdevic"],"pdf_url":"https://arxiv.org/pdf/2309.01469v2.pdf","comment":"12 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2406.18584v2","updated":"2024-06-28T07:34:25Z","published":"2024-06-06T06:22:06Z","title":"Assessment of Sentinel-2 spatial and temporal coverage based on the\n  scene classification layer","summary":"  Since the launch of the Sentinel-2 (S2) satellites, many ML models have used\nthe data for diverse applications. The scene classification layer (SCL) inside\nthe S2 product provides rich information for training, such as filtering images\nwith high cloud coverage. However, there is more potential in this. We propose\na technique to assess the clean optical coverage of a region, expressed by a\nSITS and calculated with the S2-based SCL data. With a manual threshold and\nspecific labels in the SCL, the proposed technique assigns a percentage of\nspatial and temporal coverage across the time series and a high/low assessment.\nBy evaluating the AI4EO challenge for Enhanced Agriculture, we show that the\nassessment is correlated to the predictive results of ML models. The\nclassification results in a region with low spatial and temporal coverage is\nworse than in a region with high coverage. Finally, we applied the technique\nacross all continents of the global dataset LandCoverNet.\n","authors":["Cristhian Sanchez","Francisco Mena","Marcela Charfuelan","Marlon Nuske","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2406.18584v2.pdf","comment":"Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium 2024"},{"id":"http://arxiv.org/abs/2406.19703v1","updated":"2024-06-28T07:28:50Z","published":"2024-06-28T07:28:50Z","title":"Vision Transformer with Key-select Routing Attention for Single Image\n  Dehazing","summary":"  We present Ksformer, utilizing Multi-scale Key-select Routing Attention\n(MKRA) for intelligent selection of key areas through multi-channel,\nmulti-scale windows with a top-k operator, and Lightweight Frequency Processing\nModule (LFPM) to enhance high-frequency features, outperforming other dehazing\nmethods in tests.\n","authors":["Lihan Tong","Weijia Li","Qingxia Yang","Liyuan Chen","Peng Chen"],"pdf_url":"https://arxiv.org/pdf/2406.19703v1.pdf","comment":"5 pages,4 figures,IEICE Trans. Information and Systems"},{"id":"http://arxiv.org/abs/2402.11622v2","updated":"2024-06-28T07:20:22Z","published":"2024-02-18T15:28:39Z","title":"Logical Closed Loop: Uncovering Object Hallucinations in Large\n  Vision-Language Models","summary":"  Object hallucination has been an Achilles' heel which hinders the broader\napplications of large vision-language models (LVLMs). Object hallucination\nrefers to the phenomenon that the LVLMs claim non-existent objects in the\nimage. To mitigate the object hallucinations, instruction tuning and external\nmodel-based detection methods have been proposed, which either require\nlarge-scare computational resources or depend on the detection result of\nexternal models. However, there remains an under-explored field to utilize the\nLVLM itself to alleviate object hallucinations. In this work, we adopt the\nintuition that the LVLM tends to respond logically consistently for existent\nobjects but inconsistently for hallucinated objects. Therefore, we propose a\nLogical Closed Loop-based framework for Object Hallucination Detection and\nMitigation, namely LogicCheckGPT. In specific, we devise logical consistency\nprobing to raise questions with logical correlations, inquiring about\nattributes from objects and vice versa. Whether their responses can form a\nlogical closed loop serves as an indicator of object hallucination. As a\nplug-and-play method, it can be seamlessly applied to all existing LVLMs.\nComprehensive experiments conducted on three benchmarks across four LVLMs have\ndemonstrated significant improvements brought by our method, indicating its\neffectiveness and generality.\n","authors":["Junfei Wu","Qiang Liu","Ding Wang","Jinghao Zhang","Shu Wu","Liang Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2402.11622v2.pdf","comment":"Accept to ACL 2024; 19 Pages, 15 Figures, 6 Tables"},{"id":"http://arxiv.org/abs/2406.19693v1","updated":"2024-06-28T07:09:06Z","published":"2024-06-28T07:09:06Z","title":"MMRo: Are Multimodal LLMs Eligible as the Brain for In-Home Robotics?","summary":"  It is fundamentally challenging for robots to serve as useful assistants in\nhuman environments because this requires addressing a spectrum of sub-problems\nacross robotics, including perception, language understanding, reasoning, and\nplanning. The recent advancements in Multimodal Large Language Models (MLLMs)\nhave demonstrated their exceptional abilities in solving complex mathematical\nproblems, mastering commonsense and abstract reasoning. This has led to the\nrecent utilization of MLLMs as the brain in robotic systems, enabling these\nmodels to conduct high-level planning prior to triggering low-level control\nactions for task execution. However, it remains uncertain whether existing\nMLLMs are reliable in serving the brain role of robots. In this study, we\nintroduce the first benchmark for evaluating Multimodal LLM for Robotic (MMRo)\nbenchmark, which tests the capability of MLLMs for robot applications.\nSpecifically, we identify four essential capabilities perception, task\nplanning, visual reasoning, and safety measurement that MLLMs must possess to\nqualify as the robot's central processing unit. We have developed several\nscenarios for each capability, resulting in a total of 14 metrics for\nevaluation. We present experimental results for various MLLMs, including both\ncommercial and open-source models, to assess the performance of existing\nsystems. Our findings indicate that no single model excels in all areas,\nsuggesting that current MLLMs are not yet trustworthy enough to serve as the\ncognitive core for robots. Our data can be found in\nhttps://mm-robobench.github.io/.\n","authors":["Jinming Li","Yichen Zhu","Zhiyuan Xu","Jindong Gu","Minjie Zhu","Xin Liu","Ning Liu","Yaxin Peng","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2406.19693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19690v1","updated":"2024-06-28T07:06:02Z","published":"2024-06-28T07:06:02Z","title":"Deep Fusion Model for Brain Tumor Classification Using Fine-Grained\n  Gradient Preservation","summary":"  Brain tumors are one of the most common diseases that lead to early death if\nnot diagnosed at an early stage. Traditional diagnostic approaches are\nextremely time-consuming and prone to errors. In this context, computer\nvision-based approaches have emerged as an effective tool for accurate brain\ntumor classification. While some of the existing solutions demonstrate\nnoteworthy accuracy, the models become infeasible to deploy in areas where\ncomputational resources are limited. This research addresses the need for\naccurate and fast classification of brain tumors with a priority of deploying\nthe model in technologically underdeveloped regions. The research presents a\nnovel architecture for precise brain tumor classification fusing pretrained\nResNet152V2 and modified VGG16 models. The proposed architecture undergoes a\ndiligent fine-tuning process that ensures fine gradients are preserved in deep\nneural networks, which are essential for effective brain tumor classification.\nThe proposed solution incorporates various image processing techniques to\nimprove image quality and achieves an astounding accuracy of 98.36% and 98.04%\nin Figshare and Kaggle datasets respectively. This architecture stands out for\nhaving a streamlined profile, with only 2.8 million trainable parameters. We\nhave leveraged 8-bit quantization to produce a model of size 73.881 MB,\nsignificantly reducing it from the previous size of 289.45 MB, ensuring smooth\ndeployment in edge devices even in resource-constrained areas. Additionally,\nthe use of Grad-CAM improves the interpretability of the model, offering\ninsightful information regarding its decision-making process. Owing to its high\ndiscriminative ability, this model can be a reliable option for accurate brain\ntumor classification.\n","authors":["Niful Islam","Mohaiminul Islam Bhuiyan","Jarin Tasnim Raya","Nur Shazwani Kamarudin","Khan Md Hasib","M. F. Mridha","Dewan Md. Farid"],"pdf_url":"https://arxiv.org/pdf/2406.19690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16462v2","updated":"2024-06-28T07:04:21Z","published":"2023-11-28T03:45:29Z","title":"Viewport Prediction for Volumetric Video Streaming by Exploring Video\n  Saliency and Trajectory Information","summary":"  Volumetric video, also known as hologram video, is a novel medium that\nportrays natural content in Virtual Reality (VR), Augmented Reality (AR), and\nMixed Reality (MR). It is expected to be the next-gen video technology and a\nprevalent use case for 5G and beyond wireless communication. Considering that\neach user typically only watches a section of the volumetric video, known as\nthe viewport, it is essential to have precise viewport prediction for optimal\nperformance. However, research on this topic is still in its infancy. In the\nend, this paper presents and proposes a novel approach, named Saliency and\nTrajectory Viewport Prediction (STVP), which aims to improve the precision of\nviewport prediction in volumetric video streaming. The STVP extensively\nutilizes video saliency information and viewport trajectory. To our knowledge,\nthis is the first comprehensive study of viewport prediction in volumetric\nvideo streaming. In particular, we introduce a novel sampling method, Uniform\nRandom Sampling (URS), to reduce computational complexity while still\npreserving video features in an efficient manner. Then we present a saliency\ndetection technique that incorporates both spatial and temporal information for\ndetecting static, dynamic geometric, and color salient regions. Finally, we\nintelligently fuse saliency and trajectory information to achieve more accurate\nviewport prediction. We conduct extensive simulations to evaluate the\neffectiveness of our proposed viewport prediction methods using\nstate-of-the-art volumetric video sequences. The experimental results show the\nsuperiority of the proposed method over existing schemes. The dataset and\nsource code will be publicly accessible after acceptance.\n","authors":["Jie Li","Zhixin Li","Zhi Liu","Pengyuan Zhou","Richang Hong","Qiyue Li","Han Hu"],"pdf_url":"https://arxiv.org/pdf/2311.16462v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19686v1","updated":"2024-06-28T06:51:38Z","published":"2024-06-28T06:51:38Z","title":"Enhancing Radiological Diagnosis: A Collaborative Approach Integrating\n  AI and Human Expertise for Visual Miss Correction","summary":"  Human-AI collaboration to identify and correct perceptual errors in chest\nradiographs has not been previously explored. This study aimed to develop a\ncollaborative AI system, CoRaX, which integrates eye gaze data and radiology\nreports to enhance diagnostic accuracy in chest radiology by pinpointing\nperceptual errors and refining the decision-making process. Using public\ndatasets REFLACX and EGD-CXR, the study retrospectively developed CoRaX,\nemploying a large multimodal model to analyze image embeddings, eye gaze data,\nand radiology reports. The system's effectiveness was evaluated based on its\nreferral-making process, the quality of referrals, and performance in\ncollaborative diagnostic settings. CoRaX was tested on a simulated error\ndataset of 271 samples with 28% (93 of 332) missed abnormalities. The system\ncorrected 21% (71 of 332) of these errors, leaving 7% (22 of 312) unresolved.\nThe Referral-Usefulness score, indicating the accuracy of predicted regions for\nall true referrals, was 0.63 (95% CI 0.59, 0.68). The Total-Usefulness score,\nreflecting the diagnostic accuracy of CoRaX's interactions with radiologists,\nshowed that 84% (237 of 280) of these interactions had a score above 0.40. In\nconclusion, CoRaX efficiently collaborates with radiologists to address\nperceptual errors across various abnormalities, with potential applications in\nthe education and training of novice radiologists.\n","authors":["Akash Awasthi","Ngan Le","Zhigang Deng","Carol C. Wu","Hien Van Nguyen"],"pdf_url":"https://arxiv.org/pdf/2406.19686v1.pdf","comment":"Under Review in Journal"},{"id":"http://arxiv.org/abs/2406.19070v2","updated":"2024-06-28T06:47:10Z","published":"2024-06-27T10:40:35Z","title":"FAGhead: Fully Animate Gaussian Head from Monocular Videos","summary":"  High-fidelity reconstruction of 3D human avatars has a wild application in\nvisual reality. In this paper, we introduce FAGhead, a method that enables\nfully controllable human portraits from monocular videos. We explicit the\ntraditional 3D morphable meshes (3DMM) and optimize the neutral 3D Gaussians to\nreconstruct with complex expressions. Furthermore, we employ a novel\nPoint-based Learnable Representation Field (PLRF) with learnable Gaussian point\npositions to enhance reconstruction performance. Meanwhile, to effectively\nmanage the edges of avatars, we introduced the alpha rendering to supervise the\nalpha value of each pixel. Extensive experimental results on the open-source\ndatasets and our capturing datasets demonstrate that our approach is able to\ngenerate high-fidelity 3D head avatars and fully control the expression and\npose of the virtual avatars, which is outperforming than existing works.\n","authors":["Yixin Xuan","Xinyang Li","Gongxin Yao","Shiwei Zhou","Donghui Sun","Xiaoxin Chen","Yu Pan"],"pdf_url":"https://arxiv.org/pdf/2406.19070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18146v2","updated":"2024-06-28T06:43:39Z","published":"2024-06-26T07:56:17Z","title":"A Refer-and-Ground Multimodal Large Language Model for Biomedicine","summary":"  With the rapid development of multimodal large language models (MLLMs),\nespecially their capabilities in visual chat through refer and ground\nfunctionalities, their significance is increasingly recognized. However, the\nbiomedical field currently exhibits a substantial gap in this area, primarily\ndue to the absence of a dedicated refer and ground dataset for biomedical\nimages. To address this challenge, we devised the Med-GRIT-270k dataset. It\ncomprises 270k question-and-answer pairs and spans eight distinct medical\nimaging modalities. Most importantly, it is the first dedicated to the\nbiomedical domain and integrating refer and ground conversations. The key idea\nis to sample large-scale biomedical image-mask pairs from medical segmentation\ndatasets and generate instruction datasets from text using chatGPT.\nAdditionally, we introduce a Refer-and-Ground Multimodal Large Language Model\nfor Biomedicine (BiRD) by using this dataset and multi-task instruction\nlearning. Extensive experiments have corroborated the efficacy of the\nMed-GRIT-270k dataset and the multi-modal, fine-grained interactive\ncapabilities of the BiRD model. This holds significant reference value for the\nexploration and development of intelligent biomedical assistants.\n","authors":["Xiaoshuang Huang","Haifeng Huang","Lingdong Shen","Yehui Yang","Fangxin Shang","Junwei Liu","Jia Liu"],"pdf_url":"https://arxiv.org/pdf/2406.18146v2.pdf","comment":"Accepted by MICCAI2024"},{"id":"http://arxiv.org/abs/2406.19680v1","updated":"2024-06-28T06:40:53Z","published":"2024-06-28T06:40:53Z","title":"MimicMotion: High-Quality Human Motion Video Generation with\n  Confidence-aware Pose Guidance","summary":"  In recent years, generative artificial intelligence has achieved significant\nadvancements in the field of image generation, spawning a variety of\napplications. However, video generation still faces considerable challenges in\nvarious aspects, such as controllability, video length, and richness of\ndetails, which hinder the application and popularization of this technology. In\nthis work, we propose a controllable video generation framework, dubbed\nMimicMotion, which can generate high-quality videos of arbitrary length\nmimicking specific motion guidance. Compared with previous methods, our\napproach has several highlights. Firstly, we introduce confidence-aware pose\nguidance that ensures high frame quality and temporal smoothness. Secondly, we\nintroduce regional loss amplification based on pose confidence, which\nsignificantly reduces image distortion. Lastly, for generating long and smooth\nvideos, we propose a progressive latent fusion strategy. By this means, we can\nproduce videos of arbitrary length with acceptable resource consumption. With\nextensive experiments and user studies, MimicMotion demonstrates significant\nimprovements over previous approaches in various aspects. Detailed results and\ncomparisons are available on our project page:\nhttps://tencent.github.io/MimicMotion .\n","authors":["Yuang Zhang","Jiaxi Gu","Li-Wen Wang","Han Wang","Junqi Cheng","Yuefeng Zhu","Fangyuan Zou"],"pdf_url":"https://arxiv.org/pdf/2406.19680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19675v1","updated":"2024-06-28T06:25:21Z","published":"2024-06-28T06:25:21Z","title":"Deep Learning-based Depth Estimation Methods from Monocular Image and\n  Videos: A Comprehensive Survey","summary":"  Estimating depth from single RGB images and videos is of widespread interest\ndue to its applications in many areas, including autonomous driving, 3D\nreconstruction, digital entertainment, and robotics. More than 500 deep\nlearning-based papers have been published in the past 10 years, which indicates\nthe growing interest in the task. This paper presents a comprehensive survey of\nthe existing deep learning-based methods, the challenges they address, and how\nthey have evolved in their architecture and supervision methods. It provides a\ntaxonomy for classifying the current work based on their input and output\nmodalities, network architectures, and learning methods. It also discusses the\nmajor milestones in the history of monocular depth estimation, and different\npipelines, datasets, and evaluation metrics used in existing methods.\n","authors":["Uchitha Rajapaksha","Ferdous Sohel","Hamid Laga","Dean Diepeveen","Mohammed Bennamoun"],"pdf_url":"https://arxiv.org/pdf/2406.19675v1.pdf","comment":"46 pages, 10 figures, The paper has been accepted for publication in\n  ACM Computing Surveys 2024"},{"id":"http://arxiv.org/abs/2405.05164v2","updated":"2024-06-28T06:11:11Z","published":"2024-05-08T15:54:57Z","title":"ProbRadarM3F: mmWave Radar based Human Skeletal Pose Estimation with\n  Probability Map Guided Multi-Format Feature Fusion","summary":"  Millimeter wave (mmWave) radar is a non-intrusive privacy and relatively\nconvenient and inexpensive device, which has been demonstrated to be applicable\nin place of RGB cameras in human indoor pose estimation tasks. However, mmWave\nradar relies on the collection of reflected signals from the target, and the\nradar signals containing information is difficult to be fully applied. This has\nbeen a long-standing hindrance to the improvement of pose estimation accuracy.\nTo address this major challenge, this paper introduces a probability map guided\nmulti-format feature fusion model, ProbRadarM3F. This is a novel radar feature\nextraction framework using a traditional FFT method in parallel with a\nprobability map based positional encoding method. ProbRadarM3F fuses the\ntraditional heatmap features and the positional features, then effectively\nachieves the estimation of 14 keypoints of the human body. Experimental\nevaluation on the HuPR dataset proves the effectiveness of the model proposed\nin this paper, outperforming other methods experimented on this dataset with an\nAP of 69.9 %. The emphasis of our study is focusing on the position information\nthat is not exploited before in radar singal. This provides direction to\ninvestigate other potential non-redundant information from mmWave rader.\n","authors":["Bing Zhu","Zixin He","Weiyi Xiong","Guanhua Ding","Jianan Liu","Tao Huang","Wei Chen","Wei Xiang"],"pdf_url":"https://arxiv.org/pdf/2405.05164v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19672v1","updated":"2024-06-28T06:06:01Z","published":"2024-06-28T06:06:01Z","title":"Beyond First-Order: A Multi-Scale Approach to Finger Knuckle Print\n  Biometrics","summary":"  Recently, finger knuckle prints (FKPs) have gained attention due to their\nrich textural patterns, positioning them as a promising biometric for identity\nrecognition. Prior FKP recognition methods predominantly leverage first-order\nfeature descriptors, which capture intricate texture details but fail to\naccount for structural information. Emerging research, however, indicates that\nsecond-order textures, which describe the curves and arcs of the textures,\nencompass this overlooked structural information. This paper introduces a novel\nFKP recognition approach, the Dual-Order Texture Competition Network (DOTCNet),\ndesigned to capture texture information in FKP images comprehensively. DOTCNet\nincorporates three dual-order texture competitive modules (DTCMs), each\ntargeting textures at different scales. Each DTCM employs a learnable texture\ndescriptor, specifically a learnable Gabor filter (LGF), to extract texture\nfeatures. By leveraging LGFs, the network extracts first and second order\ntextures to describe fine textures and structural features thoroughly.\nFurthermore, an attention mechanism enhances relevant features in the\nfirst-order features, thereby highlighting significant texture details. For\nsecond-order features, a competitive mechanism emphasizes structural\ninformation while reducing noise from higher-order features. Extensive\nexperimental results reveal that DOTCNet significantly outperforms several\nstandard algorithms on the publicly available PolyU-FKP dataset.\n","authors":["Chengrui Gao","Ziyuan Yang","Andrew Beng Jin Teoh","Min Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.19672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19364v2","updated":"2024-06-28T05:56:08Z","published":"2024-06-27T17:46:13Z","title":"SimTxtSeg: Weakly-Supervised Medical Image Segmentation with Simple Text\n  Cues","summary":"  Weakly-supervised medical image segmentation is a challenging task that aims\nto reduce the annotation cost while keep the segmentation performance. In this\npaper, we present a novel framework, SimTxtSeg, that leverages simple text cues\nto generate high-quality pseudo-labels and study the cross-modal fusion in\ntraining segmentation models, simultaneously. Our contribution consists of two\nkey components: an effective Textual-to-Visual Cue Converter that produces\nvisual prompts from text prompts on medical images, and a text-guided\nsegmentation model with Text-Vision Hybrid Attention that fuses text and image\nfeatures. We evaluate our framework on two medical image segmentation tasks:\ncolonic polyp segmentation and MRI brain tumor segmentation, and achieve\nconsistent state-of-the-art performance.\n","authors":["Yuxin Xie","Tao Zhou","Yi Zhou","Geng Chen"],"pdf_url":"https://arxiv.org/pdf/2406.19364v2.pdf","comment":"accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.17051v2","updated":"2024-06-28T05:50:11Z","published":"2024-06-24T18:13:09Z","title":"Leveraging Knowledge Distillation for Lightweight Skin Cancer\n  Classification: Balancing Accuracy and Computational Efficiency","summary":"  Skin cancer is a major concern to public health, accounting for one-third of\nthe reported cancers. If not detected early, the cancer has the potential for\nsevere consequences. Recognizing the critical need for effective skin cancer\nclassification, we address the limitations of existing models, which are often\ntoo large to deploy in areas with limited computational resources. In response,\nwe present a knowledge distillation based approach for creating a lightweight\nyet high-performing classifier. The proposed solution involves fusing three\nmodels, namely ResNet152V2, ConvNeXtBase, and ViT Base, to create an effective\nteacher model. The teacher model is then employed to guide a lightweight\nstudent model of size 2.03 MB. This student model is further compressed to\n469.77 KB using 16-bit quantization, enabling smooth incorporation into edge\ndevices. With six-stage image preprocessing, data augmentation, and a rigorous\nablation study, the model achieves an impressive accuracy of 98.75% on the\nHAM10000 dataset and 98.94% on the Kaggle dataset in classifying benign and\nmalignant skin cancers. With its high accuracy and compact size, our model\nappears to be a potential choice for accurate skin cancer classification,\nparticularly in resource-constrained settings.\n","authors":["Niful Islam","Khan Md Hasib","Fahmida Akter Joti","Asif Karim","Sami Azam"],"pdf_url":"https://arxiv.org/pdf/2406.17051v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19237v2","updated":"2024-06-28T05:43:46Z","published":"2024-06-27T15:01:48Z","title":"FlowVQA: Mapping Multimodal Logic in Visual Question Answering with\n  Flowcharts","summary":"  Existing benchmarks for visual question answering lack in visual grounding\nand complexity, particularly in evaluating spatial reasoning skills. We\nintroduce FlowVQA, a novel benchmark aimed at assessing the capabilities of\nvisual question-answering multimodal language models in reasoning with\nflowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and\nhuman-verified flowchart images from three distinct content sources, along with\n22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,\nincluding information localization, decision-making, and logical progression.\nWe conduct a thorough baseline evaluation on a suite of both open-source and\nproprietary multimodal language models using various strategies, followed by an\nanalysis of directional bias. The results underscore the benchmark's potential\nas a vital tool for advancing the field of multimodal modeling, providing a\nfocused and challenging environment for enhancing model performance in visual\nand logical reasoning tasks.\n","authors":["Shubhankar Singh","Purvi Chaurasia","Yerram Varun","Pranshu Pandya","Vatsal Gupta","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2406.19237v2.pdf","comment":"Accepted in ACL 2024 (Findings), 21 pages, 7 figures, 9 Tables"},{"id":"http://arxiv.org/abs/2406.18684v2","updated":"2024-06-28T05:43:41Z","published":"2024-06-26T18:42:22Z","title":"CSI4Free: GAN-Augmented mmWave CSI for Improved Pose Classification","summary":"  In recent years, Joint Communication and Sensing (JC&S), has demonstrated\nsignificant success, particularly in utilizing sub-6 GHz frequencies with\ncommercial-off-the-shelf (COTS) Wi-Fi devices for applications such as\nlocalization, gesture recognition, and pose classification. Deep learning and\nthe existence of large public datasets has been pivotal in achieving such\nresults. However, at mmWave frequencies (30-300 GHz), which has shown potential\nfor more accurate sensing performance, there is a noticeable lack of research\nin the domain of COTS Wi-Fi sensing. Challenges such as limited research\nhardware, the absence of large datasets, limited functionality in COTS\nhardware, and the complexities of data collection present obstacles to a\ncomprehensive exploration of this field. In this work, we aim to address these\nchallenges by developing a method that can generate synthetic mmWave channel\nstate information (CSI) samples. In particular, we use a generative adversarial\nnetwork (GAN) on an existing dataset, to generate 30,000 additional CSI\nsamples. The augmented samples exhibit a remarkable degree of consistency with\nthe original data, as indicated by the notably high GAN-train and GAN-test\nscores. Furthermore, we integrate the augmented samples in training a pose\nclassification model. We observe that the augmented samples complement the real\ndata and improve the generalization of the classification model.\n","authors":["Nabeel Nisar Bhat","Rafael Berkvens","Jeroen Famaey"],"pdf_url":"https://arxiv.org/pdf/2406.18684v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19668v1","updated":"2024-06-28T05:38:32Z","published":"2024-06-28T05:38:32Z","title":"PopAlign: Population-Level Alignment for Fair Text-to-Image Generation","summary":"  Text-to-image (T2I) models achieve high-fidelity generation through extensive\ntraining on large datasets. However, these models may unintentionally pick up\nundesirable biases of their training data, such as over-representation of\nparticular identities in gender or ethnicity neutral prompts. Existing\nalignment methods such as Reinforcement Learning from Human Feedback (RLHF) and\nDirect Preference Optimization (DPO) fail to address this problem effectively\nbecause they operate on pairwise preferences consisting of individual samples,\nwhile the aforementioned biases can only be measured at a population level. For\nexample, a single sample for the prompt \"doctor\" could be male or female, but a\nmodel generating predominantly male doctors even with repeated sampling\nreflects a gender bias. To address this limitation, we introduce PopAlign, a\nnovel approach for population-level preference optimization, while standard\noptimization would prefer entire sets of samples over others. We further derive\na stochastic lower bound that directly optimizes for individual samples from\npreferred populations over others for scalable training. Using human evaluation\nand standard image quality and bias metrics, we show that PopAlign\nsignificantly mitigates the bias of pretrained T2I models while largely\npreserving the generation quality. Code is available at\nhttps://github.com/jacklishufan/PopAlignSDXL.\n","authors":["Shufan Li","Harkanwar Singh","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2406.19668v1.pdf","comment":"18 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.19666v1","updated":"2024-06-28T05:25:57Z","published":"2024-06-28T05:25:57Z","title":"CSAKD: Knowledge Distillation with Cross Self-Attention for\n  Hyperspectral and Multispectral Image Fusion","summary":"  Hyperspectral imaging, capturing detailed spectral information for each\npixel, is pivotal in diverse scientific and industrial applications. Yet, the\nacquisition of high-resolution (HR) hyperspectral images (HSIs) often needs to\nbe addressed due to the hardware limitations of existing imaging systems. A\nprevalent workaround involves capturing both a high-resolution multispectral\nimage (HR-MSI) and a low-resolution (LR) HSI, subsequently fusing them to yield\nthe desired HR-HSI. Although deep learning-based methods have shown promising\nin HR-MSI/LR-HSI fusion and LR-HSI super-resolution (SR), their substantial\nmodel complexities hinder deployment on resource-constrained imaging devices.\nThis paper introduces a novel knowledge distillation (KD) framework for\nHR-MSI/LR-HSI fusion to achieve SR of LR-HSI. Our KD framework integrates the\nproposed Cross-Layer Residual Aggregation (CLRA) block to enhance efficiency\nfor constructing Dual Two-Streamed (DTS) network structure, designed to extract\njoint and distinct features from LR-HSI and HR-MSI simultaneously. To fully\nexploit the spatial and spectral feature representations of LR-HSI and HR-MSI,\nwe propose a novel Cross Self-Attention (CSA) fusion module to adaptively fuse\nthose features to improve the spatial and spectral quality of the reconstructed\nHR-HSI. Finally, the proposed KD-based joint loss function is employed to\nco-train the teacher and student networks. Our experimental results demonstrate\nthat the student model not only achieves comparable or superior LR-HSI SR\nperformance but also significantly reduces the model-size and computational\nrequirements. This marks a substantial advancement over existing\nstate-of-the-art methods. The source code is available at\nhttps://github.com/ming053l/CSAKD.\n","authors":["Chih-Chung Hsu","Chih-Chien Ni","Chia-Ming Lee","Li-Wei Kang"],"pdf_url":"https://arxiv.org/pdf/2406.19666v1.pdf","comment":"Submitted to TIP 2024"},{"id":"http://arxiv.org/abs/2405.19769v2","updated":"2024-06-28T05:25:19Z","published":"2024-05-30T07:34:05Z","title":"All-In-One Medical Image Restoration via Task-Adaptive Routing","summary":"  Although single-task medical image restoration (MedIR) has witnessed\nremarkable success, the limited generalizability of these methods poses a\nsubstantial obstacle to wider application. In this paper, we focus on the task\nof all-in-one medical image restoration, aiming to address multiple distinct\nMedIR tasks with a single universal model. Nonetheless, due to significant\ndifferences between different MedIR tasks, training a universal model often\nencounters task interference issues, where different tasks with shared\nparameters may conflict with each other in the gradient update direction. This\ntask interference leads to deviation of the model update direction from the\noptimal path, thereby affecting the model's performance. To tackle this issue,\nwe propose a task-adaptive routing strategy, allowing conflicting tasks to\nselect different network paths in spatial and channel dimensions, thereby\nmitigating task interference. Experimental results demonstrate that our\nproposed \\textbf{A}ll-in-one \\textbf{M}edical \\textbf{I}mage\n\\textbf{R}estoration (\\textbf{AMIR}) network achieves state-of-the-art\nperformance in three MedIR tasks: MRI super-resolution, CT denoising, and PET\nsynthesis, both in single-task and all-in-one settings. The code and data will\nbe available at\n\\href{https://github.com/Yaziwel/All-In-One-Medical-Image-Restoration-via-Task-Adaptive-Routing.git}{https://github.com/Yaziwel/AMIR}.\n","authors":["Zhiwen Yang","Haowei Chen","Ziniu Qian","Yang Yi","Hui Zhang","Dan Zhao","Bingzheng Wei","Yan Xu"],"pdf_url":"https://arxiv.org/pdf/2405.19769v2.pdf","comment":"This article has been early accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.19665v1","updated":"2024-06-28T05:22:39Z","published":"2024-06-28T05:22:39Z","title":"PM-VIS+: High-Performance Video Instance Segmentation without Video\n  Annotation","summary":"  Video instance segmentation requires detecting, segmenting, and tracking\nobjects in videos, typically relying on costly video annotations. This paper\nintroduces a method that eliminates video annotations by utilizing image\ndatasets. The PM-VIS algorithm is adapted to handle both bounding box and\ninstance-level pixel annotations dynamically. We introduce ImageNet-bbox to\nsupplement missing categories in video datasets and propose the PM-VIS+\nalgorithm to adjust supervision based on annotation types. To enhance accuracy,\nwe use pseudo masks and semi-supervised optimization techniques on unannotated\nvideo data. This method achieves high video instance segmentation performance\nwithout manual video annotations, offering a cost-effective solution and new\nperspectives for video instance segmentation applications. The code will be\navailable in https://github.com/ldknight/PM-VIS-plus\n","authors":["Zhangjing Yang","Dun Liu","Xin Wang","Zhe Li","Barathwaj Anandan","Yi Wu"],"pdf_url":"https://arxiv.org/pdf/2406.19665v1.pdf","comment":"MIPR 2024"},{"id":"http://arxiv.org/abs/2406.18844v2","updated":"2024-06-28T05:21:13Z","published":"2024-06-27T02:31:03Z","title":"Revisiting Backdoor Attacks against Large Vision-Language Models","summary":"  Instruction tuning enhances large vision-language models (LVLMs) but raises\nsecurity risks through potential backdoor attacks due to their openness.\nPrevious backdoor studies focus on enclosed scenarios with consistent training\nand testing instructions, neglecting the practical domain gaps that could\naffect attack effectiveness. This paper empirically examines the\ngeneralizability of backdoor attacks during the instruction tuning of LVLMs for\nthe first time, revealing certain limitations of most backdoor strategies in\npractical scenarios. We quantitatively evaluate the generalizability of six\ntypical backdoor attacks on image caption benchmarks across multiple LVLMs,\nconsidering both visual and textual domain offsets. Our findings indicate that\nattack generalizability is positively correlated with the backdoor trigger's\nirrelevance to specific images/models and the preferential correlation of the\ntrigger pattern. Additionally, we modify existing backdoor attacks based on the\nabove key observations, demonstrating significant improvements in cross-domain\nscenario generalizability (+86% attack success rate). Notably, even without\naccess to the instruction datasets, a multimodal instruction set can be\nsuccessfully poisoned with a very low poisoning rate (0.2%), achieving an\nattack success rate of over 97%. This paper underscores that even simple\ntraditional backdoor strategies pose a serious threat to LVLMs, necessitating\nmore attention and in-depth research.\n","authors":["Siyuan Liang","Jiawei Liang","Tianyu Pang","Chao Du","Aishan Liu","Ee-Chien Chang","Xiaochun Cao"],"pdf_url":"https://arxiv.org/pdf/2406.18844v2.pdf","comment":"23 pages, 8 figures"},{"id":"http://arxiv.org/abs/2406.19655v1","updated":"2024-06-28T04:49:57Z","published":"2024-06-28T04:49:57Z","title":"Basketball-SORT: An Association Method for Complex Multi-object\n  Occlusion Problems in Basketball Multi-object Tracking","summary":"  Recent deep learning-based object detection approaches have led to\nsignificant progress in multi-object tracking (MOT) algorithms. The current MOT\nmethods mainly focus on pedestrian or vehicle scenes, but basketball sports\nscenes are usually accompanied by three or more object occlusion problems with\nsimilar appearances and high-intensity complex motions, which we call complex\nmulti-object occlusion (CMOO). Here, we propose an online and robust MOT\napproach, named Basketball-SORT, which focuses on the CMOO problems in\nbasketball videos. To overcome the CMOO problem, instead of using the\nintersection-over-union-based (IoU-based) approach, we use the trajectories of\nneighboring frames based on the projected positions of the players. Our method\ndesigns the basketball game restriction (BGR) and reacquiring Long-Lost IDs\n(RLLI) based on the characteristics of basketball scenes, and we also solve the\nocclusion problem based on the player trajectories and appearance features.\nExperimental results show that our method achieves a Higher Order Tracking\nAccuracy (HOTA) score of 63.48$\\%$ on the basketball fixed video dataset and\noutperforms other recent popular approaches. Overall, our approach solved the\nCMOO problem more effectively than recent MOT algorithms.\n","authors":["Qingrui Hu","Atom Scott","Calvin Yeung","Keisuke Fujii"],"pdf_url":"https://arxiv.org/pdf/2406.19655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19649v1","updated":"2024-06-28T04:38:12Z","published":"2024-06-28T04:38:12Z","title":"AstMatch: Adversarial Self-training Consistency Framework for\n  Semi-Supervised Medical Image Segmentation","summary":"  Semi-supervised learning (SSL) has shown considerable potential in medical\nimage segmentation, primarily leveraging consistency regularization and\npseudo-labeling. However, many SSL approaches only pay attention to low-level\nconsistency and overlook the significance of pseudo-label reliability.\nTherefore, in this work, we propose an adversarial self-training consistency\nframework (AstMatch). Firstly, we design an adversarial consistency\nregularization (ACR) approach to enhance knowledge transfer and strengthen\nprediction consistency under varying perturbation intensities. Second, we apply\na feature matching loss for adversarial training to incorporate high-level\nconsistency regularization. Additionally, we present the pyramid channel\nattention (PCA) and efficient channel and spatial attention (ECSA) modules to\nimprove the discriminator's performance. Finally, we propose an adaptive\nself-training (AST) approach to ensure the pseudo-labels' quality. The proposed\nAstMatch has been extensively evaluated with cutting-edge SSL methods on three\npublic-available datasets. The experimental results under different labeled\nratios indicate that AstMatch outperforms other existing methods, achieving new\nstate-of-the-art performance. Our code will be available at\nhttps://github.com/GuanghaoZhu663/AstMatch.\n","authors":["Guanghao Zhu","Jing Zhang","Juanxiu Liu","Xiaohui Du","Ruqian Hao","Yong Liu","Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2406.19649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19640v1","updated":"2024-06-28T04:10:21Z","published":"2024-06-28T04:10:21Z","title":"Efficient Event Stream Super-Resolution with Recursive Multi-Branch\n  Fusion","summary":"  Current Event Stream Super-Resolution (ESR) methods overlook the redundant\nand complementary information present in positive and negative events within\nthe event stream, employing a direct mixing approach for super-resolution,\nwhich may lead to detail loss and inefficiency. To address these issues, we\npropose an efficient Recursive Multi-Branch Information Fusion Network (RMFNet)\nthat separates positive and negative events for complementary information\nextraction, followed by mutual supplementation and refinement. Particularly, we\nintroduce Feature Fusion Modules (FFM) and Feature Exchange Modules (FEM). FFM\nis designed for the fusion of contextual information within neighboring event\nstreams, leveraging the coupling relationship between positive and negative\nevents to alleviate the misleading of noises in the respective branches. FEM\nefficiently promotes the fusion and exchange of information between positive\nand negative branches, enabling superior local information enhancement and\nglobal information complementation. Experimental results demonstrate that our\napproach achieves over 17% and 31% improvement on synthetic and real datasets,\naccompanied by a 2.3X acceleration. Furthermore, we evaluate our method on two\ndownstream event-driven applications, \\emph{i.e.}, object recognition and video\nreconstruction, achieving remarkable results that outperform existing methods.\nOur code and Supplementary Material are available at\nhttps://github.com/Lqm26/RMFNet.\n","authors":["Quanmin Liang","Zhilin Huang","Xiawu Zheng","Feidiao Yang","Jun Peng","Kai Huang","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2406.19640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19638v1","updated":"2024-06-28T03:58:02Z","published":"2024-06-28T03:58:02Z","title":"Precision matters: Precision-aware ensemble for weakly supervised\n  semantic segmentation","summary":"  Weakly Supervised Semantic Segmentation (WSSS) employs weak supervision, such\nas image-level labels, to train the segmentation model. Despite the impressive\nachievement in recent WSSS methods, we identify that introducing weak labels\nwith high mean Intersection of Union (mIoU) does not guarantee high\nsegmentation performance. Existing studies have emphasized the importance of\nprioritizing precision and reducing noise to improve overall performance. In\nthe same vein, we propose ORANDNet, an advanced ensemble approach tailored for\nWSSS. ORANDNet combines Class Activation Maps (CAMs) from two different\nclassifiers to increase the precision of pseudo-masks (PMs). To further\nmitigate small noise in the PMs, we incorporate curriculum learning. This\ninvolves training the segmentation model initially with pairs of smaller-sized\nimages and corresponding PMs, gradually transitioning to the original-sized\npairs. By combining the original CAMs of ResNet-50 and ViT, we significantly\nimprove the segmentation performance over the single-best model and the naive\nensemble model, respectively. We further extend our ensemble method to CAMs\nfrom AMN (ResNet-like) and MCTformer (ViT-like) models, achieving performance\nbenefits in advanced WSSS models. It highlights the potential of our ORANDNet\nas a final add-on module for WSSS models.\n","authors":["Junsung Park","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2406.19638v1.pdf","comment":"5 pages, 5 figures, accepted in AAAI 2024 Edge Intelligence Workshop"},{"id":"http://arxiv.org/abs/2310.01712v2","updated":"2024-06-28T03:53:56Z","published":"2023-10-03T00:54:13Z","title":"Generative Autoencoding of Dropout Patterns","summary":"  We propose a generative model termed Deciphering Autoencoders. In this model,\nwe assign a unique random dropout pattern to each data point in the training\ndataset and then train an autoencoder to reconstruct the corresponding data\npoint using this pattern as information to be encoded. Even if a completely\nrandom dropout pattern is assigned to each data point regardless of their\nsimilarities, a sufficiently large encoder can smoothly map them to a\nlow-dimensional latent space to reconstruct individual training data points.\nDuring inference, using a dropout pattern different from those used during\ntraining allows the model to function as a generator. Since the training of\nDeciphering Autoencoders relies solely on reconstruction error, it offers more\nstable training compared to other generative models. Despite their simplicity,\nDeciphering Autoencoders show sampling quality comparable to DCGAN on the\nCIFAR-10 dataset.\n","authors":["Shunta Maeda"],"pdf_url":"https://arxiv.org/pdf/2310.01712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18070v3","updated":"2024-06-28T03:50:19Z","published":"2024-06-26T05:01:37Z","title":"EgoVideo: Exploring Egocentric Foundation Model and Downstream\n  Adaptation","summary":"  In this report, we present our solutions to the EgoVis Challenges in CVPR\n2024, including five tracks in the Ego4D challenge and three tracks in the\nEPIC-Kitchens challenge. Building upon the video-language two-tower model and\nleveraging our meticulously organized egocentric video data, we introduce a\nnovel foundation model called EgoVideo. This model is specifically designed to\ncater to the unique characteristics of egocentric videos and provides strong\nsupport for our competition submissions. In the Ego4D challenges, we tackle\nvarious tasks including Natural Language Queries, Step Grounding, Moment\nQueries, Short-term Object Interaction Anticipation, and Long-term Action\nAnticipation. In addition, we also participate in the EPIC-Kitchens challenge,\nwhere we engage in the Action Recognition, Multiple Instance Retrieval, and\nDomain Adaptation for Action Recognition tracks. By adapting EgoVideo to these\ndiverse tasks, we showcase its versatility and effectiveness in different\negocentric video analysis scenarios, demonstrating the powerful representation\nability of EgoVideo as an egocentric foundation model. Our codebase and\npretrained models are publicly available at\nhttps://github.com/OpenGVLab/EgoVideo.\n","authors":["Baoqi Pei","Guo Chen","Jilan Xu","Yuping He","Yicheng Liu","Kanghua Pan","Yifei Huang","Yali Wang","Tong Lu","Limin Wang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2406.18070v3.pdf","comment":"Champion solutions in the EgoVis CVPR 2024 workshop"},{"id":"http://arxiv.org/abs/2406.19635v1","updated":"2024-06-28T03:46:53Z","published":"2024-06-28T03:46:53Z","title":"Model Predictive Simulation Using Structured Graphical Models and\n  Transformers","summary":"  We propose an approach to simulating trajectories of multiple interacting\nagents (road users) based on transformers and probabilistic graphical models\n(PGMs), and apply it to the Waymo SimAgents challenge. The transformer baseline\nis based on the MTR model, which predicts multiple future trajectories\nconditioned on the past trajectories and static road layout features. We then\nimprove upon these generated trajectories using a PGM, which contains factors\nwhich encode prior knowledge, such as a preference for smooth trajectories, and\navoidance of collisions with static obstacles and other moving agents. We\nperform (approximate) MAP inference in this PGM using the Gauss-Newton method.\nFinally we sample $K=32$ trajectories for each of the $N \\sim 100$ agents for\nthe next $T=8 \\Delta$ time steps, where $\\Delta=10$ is the sampling rate per\nsecond. Following the Model Predictive Control (MPC) paradigm, we only return\nthe first element of our forecasted trajectories at each step, and then we\nreplan, so that the simulation can constantly adapt to its changing\nenvironment. We therefore call our approach \"Model Predictive Simulation\" or\nMPS. We show that MPS improves upon the MTR baseline, especially in safety\ncritical metrics such as collision rate. Furthermore, our approach is\ncompatible with any underlying forecasting model, and does not require extra\ntraining, so we believe it is a valuable contribution to the community.\n","authors":["Xinghua Lou","Meet Dave","Shrinu Kushagra","Miguel Lazaro-Gredilla","Kevin Murphy"],"pdf_url":"https://arxiv.org/pdf/2406.19635v1.pdf","comment":"Special Mention at the Waymo Sim Agents Challenge 2024"},{"id":"http://arxiv.org/abs/2406.19632v1","updated":"2024-06-28T03:43:49Z","published":"2024-06-28T03:43:49Z","title":"PPTFormer: Pseudo Multi-Perspective Transformer for UAV Segmentation","summary":"  The ascension of Unmanned Aerial Vehicles (UAVs) in various fields\nnecessitates effective UAV image segmentation, which faces challenges due to\nthe dynamic perspectives of UAV-captured images. Traditional segmentation\nalgorithms falter as they cannot accurately mimic the complexity of UAV\nperspectives, and the cost of obtaining multi-perspective labeled datasets is\nprohibitive. To address these issues, we introduce the PPTFormer, a novel\n\\textbf{P}seudo Multi-\\textbf{P}erspective \\textbf{T}rans\\textbf{former}\nnetwork that revolutionizes UAV image segmentation. Our approach circumvents\nthe need for actual multi-perspective data by creating pseudo perspectives for\nenhanced multi-perspective learning. The PPTFormer network boasts Perspective\nDecomposition, novel Perspective Prototypes, and a specialized encoder and\ndecoder that together achieve superior segmentation results through Pseudo\nMulti-Perspective Attention (PMP Attention) and fusion. Our experiments\ndemonstrate that PPTFormer achieves state-of-the-art performance across five\nUAV segmentation datasets, confirming its capability to effectively simulate\nUAV flight perspectives and significantly advance segmentation precision. This\nwork presents a pioneering leap in UAV scene understanding and sets a new\nbenchmark for future developments in semantic segmentation.\n","authors":["Deyi Ji","Wenwei Jin","Hongtao Lu","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.19632v1.pdf","comment":"IJCAI 2024"},{"id":"http://arxiv.org/abs/2406.19630v1","updated":"2024-06-28T03:36:38Z","published":"2024-06-28T03:36:38Z","title":"Optimal Video Compression using Pixel Shift Tracking","summary":"  The Video comprises approximately ~85\\% of all internet traffic, but video\nencoding/compression is being historically done with hard coded rules, which\nhas worked well but only to a certain limit. We have seen a surge in video\ncompression algorithms using ML-based models in the last few years and many of\nthem have outperformed several legacy codecs. The models range from encoding\nvideo end to end using an ML approach or replacing some intermediate steps in\nlegacy codecs using ML models to increase the efficiency of those steps.\n  Optimizing video storage is an essential aspect of video processing, so we\nare proposing one of the possible approaches to achieve it is by avoiding\nredundant data at each frame. In this paper, we want to introduce the approach\nof redundancies removal in subsequent frames for a given video as a main\napproach for video compression. We call this method Redundancy Removal using\nShift (R\\textsuperscript2S). This method can be utilized across various Machine\nLearning model algorithms, and make the compression more accessible and\nadaptable. In this study, we have utilized a computer vision-based pixel point\ntracking method to identify redundant pixels to encode video for optimal\nstorage.\n","authors":["Hitesh Saai Mananchery Panneerselvam","Smit Anand"],"pdf_url":"https://arxiv.org/pdf/2406.19630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18893v2","updated":"2024-06-28T03:22:33Z","published":"2024-06-27T05:08:46Z","title":"AlignIT: Enhancing Prompt Alignment in Customization of Text-to-Image\n  Models","summary":"  We consider the problem of customizing text-to-image diffusion models with\nuser-supplied reference images. Given new prompts, the existing methods can\ncapture the key concept from the reference images but fail to align the\ngenerated image with the prompt. In this work, we seek to address this key\nissue by proposing new methods that can easily be used in conjunction with\nexisting customization methods that optimize the embeddings/weights at various\nintermediate stages of the text encoding process.\n  The first contribution of this paper is a dissection of the various stages of\nthe text encoding process leading up to the conditioning vector for\ntext-to-image models. We take a holistic view of existing customization methods\nand notice that key and value outputs from this process differs substantially\nfrom their corresponding baseline (non-customized) models (e.g., baseline\nstable diffusion). While this difference does not impact the concept being\ncustomized, it leads to other parts of the generated image not being aligned\nwith the prompt. Further, we also observe that these keys and values allow\nindependent control various aspects of the final generation, enabling semantic\nmanipulation of the output. Taken together, the features spanning these keys\nand values, serve as the basis for our next contribution where we fix the\naforementioned issues with existing methods. We propose a new post-processing\nalgorithm, AlignIT, that infuses the keys and values for the concept of\ninterest while ensuring the keys and values for all other tokens in the input\nprompt are unchanged.\n  Our proposed method can be plugged in directly to existing customization\nmethods, leading to a substantial performance improvement in the alignment of\nthe final result with the input prompt while retaining the customization\nquality.\n","authors":["Aishwarya Agarwal","Srikrishna Karanam","Balaji Vasan Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2406.18893v2.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.16537v2","updated":"2024-06-28T03:21:15Z","published":"2024-06-24T11:16:37Z","title":"Character-Adapter: Prompt-Guided Region Control for High-Fidelity\n  Character Customization","summary":"  Customized image generation, which seeks to synthesize images with consistent\ncharacters, holds significant relevance for applications such as storytelling,\nportrait generation, and character design. However, previous approaches have\nencountered challenges in preserving characters with high-fidelity consistency\ndue to inadequate feature extraction and concept confusion of reference\ncharacters. Therefore, we propose Character-Adapter, a plug-and-play framework\ndesigned to generate images that preserve the details of reference characters,\nensuring high-fidelity consistency. Character-Adapter employs prompt-guided\nsegmentation to ensure fine-grained regional features of reference characters\nand dynamic region-level adapters to mitigate concept confusion. Extensive\nexperiments are conducted to validate the effectiveness of Character-Adapter.\nBoth quantitative and qualitative results demonstrate that Character-Adapter\nachieves the state-of-the-art performance of consistent character generation,\nwith an improvement of 24.8% compared with other methods. Our code will be\nreleased at https://github.com/Character-Adapter/Character-Adapte\n","authors":["Yuhang Ma","Wenting Xu","Jiji Tang","Qinfeng Jin","Rongsheng Zhang","Zeng Zhao","Changjie Fan","Zhipeng Hu"],"pdf_url":"https://arxiv.org/pdf/2406.16537v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06777v3","updated":"2024-06-28T03:07:29Z","published":"2024-06-10T20:25:18Z","title":"MolX: Enhancing Large Language Models for Molecular Learning with A\n  Multi-Modal Extension","summary":"  Recently, Large Language Models (LLMs) with their strong task-handling\ncapabilities have shown remarkable advancements across a spectrum of fields,\nmoving beyond natural language understanding. However, their proficiency within\nthe chemistry domain remains restricted, especially in solving professional\nmolecule-related tasks. This challenge is attributed to their inherent\nlimitations in comprehending molecules using only common textual\nrepresentations, i.e., SMILES strings. In this study, we seek to enhance the\nability of LLMs to comprehend molecules by designing and equipping them with a\nmulti-modal external module, namely MolX. In particular, instead of directly\nusing a SMILES string to represent a molecule, we utilize specific encoders to\nextract fine-grained features from both SMILES string and 2D molecular graph\nrepresentations for feeding into an LLM. Moreover, a human-defined molecular\nfingerprint is incorporated to leverage its embedded domain knowledge. Then, to\nestablish an alignment between MolX and the LLM's textual input space, the\nwhole model in which the LLM is frozen, is pre-trained with a versatile\nstrategy including a diverse set of tasks. Extensive experimental evaluations\ndemonstrate that our proposed method only introduces a small number of\ntrainable parameters while outperforming baselines on various downstream\nmolecule-related tasks ranging from molecule-to-text translation to\nretrosynthesis, with and without fine-tuning the LLM.\n","authors":["Khiem Le","Zhichun Guo","Kaiwen Dong","Xiaobao Huang","Bozhao Nan","Roshni Iyer","Xiangliang Zhang","Olaf Wiest","Wei Wang","Nitesh V. Chawla"],"pdf_url":"https://arxiv.org/pdf/2406.06777v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18958v2","updated":"2024-06-28T02:47:07Z","published":"2024-06-27T07:40:59Z","title":"AnyControl: Create Your Artwork with Versatile Control on Text-to-Image\n  Generation","summary":"  The field of text-to-image (T2I) generation has made significant progress in\nrecent years, largely driven by advancements in diffusion models. Linguistic\ncontrol enables effective content creation, but struggles with fine-grained\ncontrol over image generation. This challenge has been explored, to a great\nextent, by incorporating additional user-supplied spatial conditions, such as\ndepth maps and edge maps, into pre-trained T2I models through extra encoding.\nHowever, multi-control image synthesis still faces several challenges.\nSpecifically, current approaches are limited in handling free combinations of\ndiverse input control signals, overlook the complex relationships among\nmultiple spatial conditions, and often fail to maintain semantic alignment with\nprovided textual prompts. This can lead to suboptimal user experiences. To\naddress these challenges, we propose AnyControl, a multi-control image\nsynthesis framework that supports arbitrary combinations of diverse control\nsignals. AnyControl develops a novel Multi-Control Encoder that extracts a\nunified multi-modal embedding to guide the generation process. This approach\nenables a holistic understanding of user inputs, and produces high-quality,\nfaithful results under versatile control signals, as demonstrated by extensive\nquantitative and qualitative evaluations. Our project page is available in\nhttps://any-control.github.io.\n","authors":["Yanan Sun","Yanchen Liu","Yinhao Tang","Wenjie Pei","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2406.18958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19602v1","updated":"2024-06-28T02:18:16Z","published":"2024-06-28T02:18:16Z","title":"A Survey on Deep Clustering: From the Prior Perspective","summary":"  Facilitated by the powerful feature extraction ability of neural networks,\ndeep clustering has achieved great success in analyzing high-dimensional and\ncomplex real-world data. The performance of deep clustering methods is affected\nby various factors such as network structures and learning objectives. However,\nas pointed out in this survey, the essence of deep clustering lies in the\nincorporation and utilization of prior knowledge, which is largely ignored by\nexisting works. From pioneering deep clustering methods based on data structure\nassumptions to recent contrastive clustering methods based on data augmentation\ninvariances, the development of deep clustering intrinsically corresponds to\nthe evolution of prior knowledge. In this survey, we provide a comprehensive\nreview of deep clustering methods by categorizing them into six types of prior\nknowledge. We find that in general the prior innovation follows two trends,\nnamely, i) from mining to constructing, and ii) from internal to external.\nBesides, we provide a benchmark on five widely-used datasets and analyze the\nperformance of methods with diverse priors. By providing a novel prior\nknowledge perspective, we hope this survey could provide some novel insights\nand inspire future research in the deep clustering community.\n","authors":["Yiding Lu","Haobin Li","Yunfan Li","Yijie Lin","Xi Peng"],"pdf_url":"https://arxiv.org/pdf/2406.19602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18915v2","updated":"2024-06-28T02:13:22Z","published":"2024-06-27T06:12:01Z","title":"Manipulate-Anything: Automating Real-World Robots using Vision-Language\n  Models","summary":"  Large-scale endeavors like RT-1 and widespread community efforts such as\nOpen-X-Embodiment have contributed to growing the scale of robot demonstration\ndata. However, there is still an opportunity to improve the quality, quantity,\nand diversity of robot demonstration data. Although vision-language models have\nbeen shown to automatically generate demonstration data, their utility has been\nlimited to environments with privileged state information, they require\nhand-designed skills, and are limited to interactions with few object\ninstances. We propose Manipulate-Anything, a scalable automated generation\nmethod for real-world robotic manipulation. Unlike prior work, our method can\noperate in real-world environments without any privileged state information,\nhand-designed skills, and can manipulate any static object. We evaluate our\nmethod using two setups. First, Manipulate-Anything successfully generates\ntrajectories for all 5 real-world and 12 simulation tasks, significantly\noutperforming existing methods like VoxPoser. Second, Manipulate-Anything's\ndemonstrations can train more robust behavior cloning policies than training\nwith human demonstrations, or from data generated by VoxPoser and\nCode-As-Policies. We believe Manipulate-Anything can be the scalable method for\nboth generating data for robotics and solving novel tasks in a zero-shot\nsetting.\n","authors":["Jiafei Duan","Wentao Yuan","Wilbert Pumacay","Yi Ru Wang","Kiana Ehsani","Dieter Fox","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2406.18915v2.pdf","comment":"Project page: https://robot-ma.github.io/"},{"id":"http://arxiv.org/abs/2406.14534v2","updated":"2024-06-28T02:12:20Z","published":"2024-06-20T17:47:30Z","title":"Epicardium Prompt-guided Real-time Cardiac Ultrasound Frame-to-volume\n  Registration","summary":"  A comprehensive guidance view for cardiac interventional surgery can be\nprovided by the real-time fusion of the intraoperative 2D images and\npreoperative 3D volume based on the ultrasound frame-to-volume registration.\nHowever, cardiac ultrasound images are characterized by a low signal-to-noise\nratio and small differences between adjacent frames, coupled with significant\ndimension variations between 2D frames and 3D volumes to be registered,\nresulting in real-time and accurate cardiac ultrasound frame-to-volume\nregistration being a very challenging task. This paper introduces a lightweight\nend-to-end Cardiac Ultrasound frame-to-volume Registration network, termed\nCU-Reg. Specifically, the proposed model leverages epicardium prompt-guided\nanatomical clues to reinforce the interaction of 2D sparse and 3D dense\nfeatures, followed by a voxel-wise local-global aggregation of enhanced\nfeatures, thereby boosting the cross-dimensional matching effectiveness of\nlow-quality ultrasound modalities. We further embed an inter-frame\ndiscriminative regularization term within the hybrid supervised learning to\nincrease the distinction between adjacent slices in the same ultrasound volume\nto ensure registration stability. Experimental results on the reprocessed CAMUS\ndataset demonstrate that our CU-Reg surpasses existing methods in terms of\nregistration accuracy and efficiency, meeting the guidance requirements of\nclinical cardiac interventional surgery.\n","authors":["Long Lei","Jun Zhou","Jialun Pei","Baoliang Zhao","Yueming Jin","Yuen-Chun Jeremy Teoh","Jing Qin","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2406.14534v2.pdf","comment":"This paper has been accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.11445v2","updated":"2024-06-28T01:24:45Z","published":"2024-06-17T11:57:14Z","title":"Solving the Inverse Problem of Electrocardiography for Cardiac Digital\n  Twins: A Survey","summary":"  Cardiac digital twins are personalized virtual representations used to\nunderstand complex heart mechanisms. Solving the ECG inverse problem is crucial\nfor accurate virtual heart modelling, enabling the derivation of internal\nelectrical activity information from recorded surface potentials. Despite\nchallenges from cardiac complexity, noisy ECG data, and computational\nefficiency, recent advancements hold significant promise for enhancing virtual\nheart modelling, ultimately advancing precision medicine in cardiology. This\npaper aims to provide a comprehensive review of the methods of solving ECG\ninverse problem, the validation strategies, the clinical applications, and\nfuture perspectives. For the computing methodologies, we broadly classify\nstate-of-the-art approaches into two categories: deterministic and\nprobabilistic methods, including conventional and deep learning-based\ntechniques. Integrating physics laws with deep learning models holds promise,\nbut challenges such as capturing dynamic electrophysiology accurately,\naccessing accurate domain knowledge, and quantifying prediction uncertainty\npersist. Integrating models into clinical workflows while ensuring\ninterpretability and usability for healthcare professionals is essential.\nOvercoming these challenges will drive further research in cardiac digital\ntwins.\n","authors":["Lei Li","Julia Camps","Blanca Rodriguez","Vicente Grau"],"pdf_url":"https://arxiv.org/pdf/2406.11445v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.04940v2","updated":"2024-06-28T01:23:10Z","published":"2024-05-08T10:15:04Z","title":"Harnessing the Power of MLLMs for Transferable Text-to-Image Person ReID","summary":"  Text-to-image person re-identification (ReID) retrieves pedestrian images\naccording to textual descriptions. Manually annotating textual descriptions is\ntime-consuming, restricting the scale of existing datasets and therefore the\ngeneralization ability of ReID models. As a result, we study the transferable\ntext-to-image ReID problem, where we train a model on our proposed large-scale\ndatabase and directly deploy it to various datasets for evaluation. We obtain\nsubstantial training data via Multi-modal Large Language Models (MLLMs).\nMoreover, we identify and address two key challenges in utilizing the obtained\ntextual descriptions. First, an MLLM tends to generate descriptions with\nsimilar structures, causing the model to overfit specific sentence patterns.\nThus, we propose a novel method that uses MLLMs to caption images according to\nvarious templates. These templates are obtained using a multi-turn dialogue\nwith a Large Language Model (LLM). Therefore, we can build a large-scale\ndataset with diverse textual descriptions. Second, an MLLM may produce\nincorrect descriptions. Hence, we introduce a novel method that automatically\nidentifies words in a description that do not correspond with the image. This\nmethod is based on the similarity between one text and all patch token\nembeddings in the image. Then, we mask these words with a larger probability in\nthe subsequent training epoch, alleviating the impact of noisy textual\ndescriptions. The experimental results demonstrate that our methods\nsignificantly boost the direct transfer text-to-image ReID performance.\nBenefiting from the pre-trained model weights, we also achieve state-of-the-art\nperformance in the traditional evaluation settings.\n","authors":["Wentao Tan"],"pdf_url":"https://arxiv.org/pdf/2405.04940v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2406.19593v1","updated":"2024-06-28T01:14:43Z","published":"2024-06-28T01:14:43Z","title":"SK-VQA: Synthetic Knowledge Generation at Scale for Training\n  Context-Augmented Multimodal LLMs","summary":"  Synthetic data generation has gained significant attention recently for its\nutility in training large vision and language models. However, the application\nof synthetic data to the training of multimodal context-augmented generation\nsystems has been relatively unexplored. This gap in existing work is important\nbecause existing vision and language models (VLMs) are not trained specifically\nfor context-augmented generation. Resources for adapting such models are\ntherefore crucial for enabling their use in retrieval-augmented generation\n(RAG) settings, where a retriever is used to gather relevant information that\nis then subsequently provided to a generative model via context augmentation.\nTo address this challenging problem, we generate SK-VQA: a large synthetic\nmultimodal dataset containing over 2 million question-answer pairs which\nrequire external knowledge to determine the final answer. Our dataset is both\nlarger and significantly more diverse than existing resources of its kind,\npossessing over 11x more unique questions and containing images from a greater\nvariety of sources than previously-proposed datasets. Through extensive\nexperiments, we demonstrate that our synthetic dataset can not only serve as a\nchallenging benchmark, but is also highly effective for adapting existing\ngenerative multimodal models for context-augmented generation.\n","authors":["Xin Su","Man Luo","Kris W Pan","Tien Pei Chou","Vasudev Lal","Phillip Howard"],"pdf_url":"https://arxiv.org/pdf/2406.19593v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2311.09735v3","updated":"2024-06-28T17:59:26Z","published":"2023-11-16T10:06:09Z","title":"GEO: Generative Engine Optimization","summary":"  The advent of large language models (LLMs) has ushered in a new paradigm of\nsearch engines that use generative models to gather and summarize information\nto answer user queries. This emerging technology, which we formalize under the\nunified framework of generative engines (GEs), can generate accurate and\npersonalized responses, rapidly replacing traditional search engines like\nGoogle and Bing. Generative Engines typically satisfy queries by synthesizing\ninformation from multiple sources and summarizing them using LLMs. While this\nshift significantly improves $\\textit{user}$ utility and $\\textit{generative\nsearch engine}$ traffic, it poses a huge challenge for the third stakeholder --\nwebsite and content creators. Given the black-box and fast-moving nature of\ngenerative engines, content creators have little to no control over\n$\\textit{when}$ and $\\textit{how}$ their content is displayed. With generative\nengines here to stay, we must ensure the creator economy is not disadvantaged.\nTo address this, we introduce Generative Engine Optimization (GEO), the first\nnovel paradigm to aid content creators in improving their content visibility in\ngenerative engine responses through a flexible black-box optimization framework\nfor optimizing and defining visibility metrics. We facilitate systematic\nevaluation by introducing GEO-bench, a large-scale benchmark of diverse user\nqueries across multiple domains, along with relevant web sources to answer\nthese queries. Through rigorous evaluation, we demonstrate that GEO can boost\nvisibility by up to $40\\%$ in generative engine responses. Moreover, we show\nthe efficacy of these strategies varies across domains, underscoring the need\nfor domain-specific optimization methods. Our work opens a new frontier in\ninformation discovery systems, with profound implications for both developers\nof generative engines and content creators.\n","authors":["Pranjal Aggarwal","Vishvak Murahari","Tanmay Rajpurohit","Ashwin Kalyan","Karthik Narasimhan","Ameet Deshpande"],"pdf_url":"https://arxiv.org/pdf/2311.09735v3.pdf","comment":"Accepted to KDD 2024"},{"id":"http://arxiv.org/abs/2310.03812v2","updated":"2024-06-28T17:59:14Z","published":"2023-10-05T18:01:04Z","title":"Fishnets: Information-Optimal, Scalable Aggregation for Sets and Graphs","summary":"  Set-based learning is an essential component of modern deep learning and\nnetwork science. Graph Neural Networks (GNNs) and their edge-free counterparts\nDeepsets have proven remarkably useful on ragged and topologically challenging\ndatasets. The key to learning informative embeddings for set members is a\nspecified aggregation function, usually a sum, max, or mean. We propose\nFishnets, an aggregation strategy for learning information-optimal embeddings\nfor sets of data for both Bayesian inference and graph aggregation. We\ndemonstrate that i) Fishnets neural summaries can be scaled optimally to an\narbitrary number of data objects, ii) Fishnets aggregations are robust to\nchanges in data distribution, unlike standard deepsets, iii) Fishnets saturate\nBayesian information content and extend to regimes where MCMC techniques fail\nand iv) Fishnets can be used as a drop-in aggregation scheme within GNNs. We\nshow that by adopting a Fishnets aggregation scheme for message passing, GNNs\ncan achieve state-of-the-art performance versus architecture size on\nogbn-protein data over existing benchmarks with a fraction of learnable\nparameters and faster training time.\n","authors":["T. Lucas Makinen","Justin Alsing","Benjamin D. Wandelt"],"pdf_url":"https://arxiv.org/pdf/2310.03812v2.pdf","comment":"15 pages, 6 figures, 2 tables. Submitted to JMLR"},{"id":"http://arxiv.org/abs/2406.20095v1","updated":"2024-06-28T17:59:12Z","published":"2024-06-28T17:59:12Z","title":"LLaRA: Supercharging Robot Learning Data for Vision-Language Policy","summary":"  Large Language Models (LLMs) equipped with extensive world knowledge and\nstrong reasoning skills can tackle diverse tasks across domains, often by\nposing them as conversation-style instruction-response pairs. In this paper, we\npropose LLaRA: Large Language and Robotics Assistant, a framework which\nformulates robot action policy as conversations, and provides improved\nresponses when trained with auxiliary data that complements policy learning.\nLLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity\nto process state information as visual-textual prompts and generate optimal\npolicy decisions in text. To train such action policy VLMs, we first introduce\nan automated pipeline to generate diverse high-quality robotics instruction\ndata from existing behavior cloning data. A VLM finetuned with the resulting\ncollection of datasets based on a conversation-style formulation tailored for\nrobotics tasks, can generate meaningful robot action policy decisions. Our\nexperiments across multiple simulated and real-world environments demonstrate\nthe state-of-the-art performance of the proposed LLaRA framework. The code,\ndatasets, and pretrained models are available at\nhttps://github.com/LostXine/LLaRA.\n","authors":["Xiang Li","Cristina Mata","Jongwoo Park","Kumara Kahatapitiya","Yoo Sung Jang","Jinghuan Shang","Kanchana Ranasinghe","Ryan Burgert","Mu Cai","Yong Jae Lee","Michael S. Ryoo"],"pdf_url":"https://arxiv.org/pdf/2406.20095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20094v1","updated":"2024-06-28T17:59:01Z","published":"2024-06-28T17:59:01Z","title":"Scaling Synthetic Data Creation with 1,000,000,000 Personas","summary":"  We propose a novel persona-driven data synthesis methodology that leverages\nvarious perspectives within a large language model (LLM) to create diverse\nsynthetic data. To fully exploit this methodology at scale, we introduce\nPersona Hub -- a collection of 1 billion diverse personas automatically curated\nfrom web data. These 1 billion personas (~13% of the world's total population),\nacting as distributed carriers of world knowledge, can tap into almost every\nperspective encapsulated within the LLM, thereby facilitating the creation of\ndiverse synthetic data at scale for various scenarios. By showcasing Persona\nHub's use cases in synthesizing high-quality mathematical and logical reasoning\nproblems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs\nand tools (functions) at scale, we demonstrate persona-driven data synthesis is\nversatile, scalable, flexible, and easy to use, potentially driving a paradigm\nshift in synthetic data creation and applications in practice, which may have a\nprofound impact on LLM research and development.\n","authors":["Xin Chan","Xiaoyang Wang","Dian Yu","Haitao Mi","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2406.20094v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2406.12909v2","updated":"2024-06-28T17:58:27Z","published":"2024-06-12T21:21:42Z","title":"Scalable Training of Graph Foundation Models for Atomistic Materials\n  Modeling: A Case Study with HydraGNN","summary":"  We present our work on developing and training scalable graph foundation\nmodels (GFM) using HydraGNN, a multi-headed graph convolutional neural network\narchitecture. HydraGNN expands the boundaries of graph neural network (GNN) in\nboth training scale and data diversity. It abstracts over message passing\nalgorithms, allowing both reproduction of and comparison across algorithmic\ninnovations that define convolution in GNNs. This work discusses a series of\noptimizations that have allowed scaling up the GFM training to tens of\nthousands of GPUs on datasets that consist of hundreds of millions of graphs.\nOur GFMs use multi-task learning (MTL) to simultaneously learn graph-level and\nnode-level properties of atomistic structures, such as the total energy and\natomic forces. Using over 150 million atomistic structures for training, we\nillustrate the performance of our approach along with the lessons learned on\ntwo United States Department of Energy (US-DOE) supercomputers, namely the\nPerlmutter petascale system at the National Energy Research Scientific\nComputing Center and the Frontier exascale system at Oak Ridge National\nLaboratory. The HydraGNN architecture enables the GFM to achieve near-linear\nstrong scaling performance using more than 2,000 GPUs on Perlmutter and 16,000\nGPUs on Frontier. Hyperparameter optimization (HPO) was performed on over\n64,000 GPUs on Frontier to select GFM architectures with high accuracy. Early\nstopping was applied on each GFM architecture for energy awareness in\nperforming such an extreme-scale task. The training of an ensemble of\nhighest-ranked GFM architectures continued until convergence to establish\nuncertainty quantification (UQ) capabilities with ensemble learning. Our\ncontribution opens the door for rapidly developing, training, and deploying\nGFMs using large-scale computational resources to enable AI-accelerated\nmaterials discovery and design.\n","authors":["Massimiliano Lupo Pasini","Jong Youl Choi","Kshitij Mehta","Pei Zhang","David Rogers","Jonghyun Bae","Khaled Z. Ibrahim","Ashwin M. Aji","Karl W. Schulz","Jorda Polo","Prasanna Balaprakash"],"pdf_url":"https://arxiv.org/pdf/2406.12909v2.pdf","comment":"16 pages, 13 figures"},{"id":"http://arxiv.org/abs/2406.20087v1","updated":"2024-06-28T17:55:24Z","published":"2024-06-28T17:55:24Z","title":"ProgressGym: Alignment with a Millennium of Moral Progress","summary":"  Frontier AI systems, including large language models (LLMs), hold increasing\ninfluence over the epistemology of human users. Such influence can reinforce\nprevailing societal values, potentially contributing to the lock-in of\nmisguided moral beliefs and, consequently, the perpetuation of problematic\nmoral practices on a broad scale. We introduce progress alignment as a\ntechnical solution to mitigate this imminent risk. Progress alignment\nalgorithms learn to emulate the mechanics of human moral progress, thereby\naddressing the susceptibility of existing alignment methods to contemporary\nmoral blindspots. To empower research in progress alignment, we introduce\nProgressGym, an experimental framework allowing the learning of moral progress\nmechanics from history, in order to facilitate future progress in real-world\nmoral decisions. Leveraging 9 centuries of historical text and 18 historical\nLLMs, ProgressGym enables codification of real-world progress alignment\nchallenges into concrete benchmarks. Specifically, we introduce three core\nchallenges: tracking evolving values (PG-Follow), preemptively anticipating\nmoral progress (PG-Predict), and regulating the feedback loop between human and\nAI value shifts (PG-Coevolve). Alignment methods without a temporal dimension\nare inapplicable to these tasks. In response, we present lifelong and\nextrapolative algorithms as baseline methods of progress alignment, and build\nan open leaderboard soliciting novel algorithms and challenges. The framework\nand the leaderboard are available at\nhttps://github.com/PKU-Alignment/ProgressGym and\nhttps://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard\nrespectively.\n","authors":["Tianyi Qiu","Yang Zhang","Xuchuan Huang","Jasmine Xinze Li","Jiaming Ji","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2406.20087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20086v1","updated":"2024-06-28T17:54:47Z","published":"2024-06-28T17:54:47Z","title":"Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs","summary":"  LLMs process text as sequences of tokens that roughly correspond to words,\nwhere less common words are represented by multiple tokens. However, individual\ntokens are often semantically unrelated to the meanings of the words/concepts\nthey comprise. For example, Llama-2-7b's tokenizer splits the word\n\"northeastern\" into the tokens ['_n', 'ort', 'he', 'astern'], none of which\ncorrespond to semantically meaningful units like \"north\" or \"east.\" Similarly,\nthe overall meanings of named entities like \"Neil Young\" and multi-word\nexpressions like \"break a leg\" cannot be directly inferred from their\nconstituent tokens. Mechanistically, how do LLMs convert such arbitrary groups\nof tokens into useful higher-level representations? In this work, we find that\nlast token representations of named entities and multi-token words exhibit a\npronounced \"erasure\" effect, where information about previous and current\ntokens is rapidly forgotten in early layers. Using this observation, we propose\na method to \"read out\" the implicit vocabulary of an autoregressive LLM by\nexamining differences in token representations across layers, and present\nresults of this method for Llama-2-7b and Llama-3-8B. To our knowledge, this is\nthe first attempt to probe the implicit vocabulary of an LLM.\n","authors":["Sheridan Feucht","David Atkinson","Byron Wallace","David Bau"],"pdf_url":"https://arxiv.org/pdf/2406.20086v1.pdf","comment":"13 pages, 14 figures. Code and data at\n  https://footprints.baulab.info/"},{"id":"http://arxiv.org/abs/2406.20081v1","updated":"2024-06-28T17:47:32Z","published":"2024-06-28T17:47:32Z","title":"Segment Anything without Supervision","summary":"  The Segmentation Anything Model (SAM) requires labor-intensive data labeling.\nWe present Unsupervised SAM (UnSAM) for promptable and automatic whole-image\nsegmentation that does not require human annotations. UnSAM utilizes a\ndivide-and-conquer strategy to \"discover\" the hierarchical structure of visual\nscenes. We first leverage top-down clustering methods to partition an unlabeled\nimage into instance/semantic level segments. For all pixels within a segment, a\nbottom-up clustering method is employed to iteratively merge them into larger\ngroups, thereby forming a hierarchical structure. These unsupervised\nmulti-granular masks are then utilized to supervise model training. Evaluated\nacross seven popular datasets, UnSAM achieves competitive results with the\nsupervised counterpart SAM, and surpasses the previous state-of-the-art in\nunsupervised segmentation by 11% in terms of AR. Moreover, we show that\nsupervised SAM can also benefit from our self-supervised labels. By integrating\nour unsupervised pseudo masks into SA-1B's ground-truth masks and training\nUnSAM with only 1% of SA-1B, a lightly semi-supervised UnSAM can often segment\nentities overlooked by supervised SAM, exceeding SAM's AR by over 6.7% and AP\nby 3.9% on SA-1B.\n","authors":["XuDong Wang","Jingfeng Yang","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2406.20081v1.pdf","comment":"Code: https://github.com/frank-xwang/UnSAM"},{"id":"http://arxiv.org/abs/2402.00093v3","updated":"2024-06-28T17:46:19Z","published":"2024-01-31T12:41:27Z","title":"ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation","summary":"  System Verilog Assertion (SVA) formulation -- a critical yet complex task is\na prerequisite in the Assertion Based Verification (ABV) process.\nTraditionally, SVA formulation involves expert-driven interpretation of\nspecifications, which is time-consuming and prone to human error. Recently,\nLLM-informed automatic assertion generation is gaining interest. We designed a\nnovel framework called ChIRAAG, based on OpenAI GPT4, to generate SVA from\nnatural language specifications of a design. ChIRAAG constitutes the systematic\nbreakdown of design specifications into a standardized format, further\ngenerating assertions from formatted specifications using LLM. Furthermore, we\nused few test cases to validate the LLM-generated assertions. Automatic\nfeedback of log messages from the simulation tool to the LLM ensures that the\nframework can generate correct SVAs. In our experiments, only 27% of\nLLM-generated raw assertions had errors, which was rectified in few iterations\nbased on the simulation log. Our results on OpenTitan designs show that LLMs\ncan streamline and assist engineers in the assertion generation process,\nreshaping verification workflows.\n","authors":["Bhabesh Mali","Karthik Maddala","Vatsal Gupta","Sweeya Reddy","Chandan Karfa","Ramesh Karri"],"pdf_url":"https://arxiv.org/pdf/2402.00093v3.pdf","comment":"4 pages, 2 figures and 2 tables"},{"id":"http://arxiv.org/abs/2406.03472v2","updated":"2024-06-28T17:44:28Z","published":"2024-06-05T17:25:29Z","title":"Solving Differential Equations using Physics-Informed Deep Equilibrium\n  Models","summary":"  This paper introduces Physics-Informed Deep Equilibrium Models (PIDEQs) for\nsolving initial value problems (IVPs) of ordinary differential equations\n(ODEs). Leveraging recent advancements in deep equilibrium models (DEQs) and\nphysics-informed neural networks (PINNs), PIDEQs combine the implicit output\nrepresentation of DEQs with physics-informed training techniques. We validate\nPIDEQs using the Van der Pol oscillator as a benchmark problem, demonstrating\ntheir efficiency and effectiveness in solving IVPs. Our analysis includes key\nhyperparameter considerations for optimizing PIDEQ performance. By bridging\ndeep learning and physics-based modeling, this work advances computational\ntechniques for solving IVPs, with implications for scientific computing and\nengineering applications.\n","authors":["Bruno Machado Pacheco","Eduardo Camponogara"],"pdf_url":"https://arxiv.org/pdf/2406.03472v2.pdf","comment":"Accepted at CASE 2024; Extended Sec. III.B"},{"id":"http://arxiv.org/abs/2406.20062v1","updated":"2024-06-28T17:20:13Z","published":"2024-06-28T17:20:13Z","title":"Cost-aware Bayesian optimization via the Pandora's Box Gittins index","summary":"  Bayesian optimization is a technique for efficiently optimizing unknown\nfunctions in a black-box manner. To handle practical settings where gathering\ndata requires use of finite resources, it is desirable to explicitly\nincorporate function evaluation costs into Bayesian optimization policies. To\nunderstand how to do so, we develop a previously-unexplored connection between\ncost-aware Bayesian optimization and the Pandora's Box problem, a decision\nproblem from economics. The Pandora's Box problem admits a Bayesian-optimal\nsolution based on an expression called the Gittins index, which can be\nreinterpreted as an acquisition function. We study the use of this acquisition\nfunction for cost-aware Bayesian optimization, and demonstrate empirically that\nit performs well, particularly in medium-high dimensions. We further show that\nthis performance carries over to classical Bayesian optimization without\nexplicit evaluation costs. Our work constitutes a first step towards\nintegrating techniques from Gittins index theory into Bayesian optimization.\n","authors":["Qian Xie","Raul Astudillo","Peter Frazier","Ziv Scully","Alexander Terenin"],"pdf_url":"https://arxiv.org/pdf/2406.20062v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18757v2","updated":"2024-06-28T17:12:37Z","published":"2024-06-26T20:55:26Z","title":"The Impact of Feature Representation on the Accuracy of Photonic Neural\n  Networks","summary":"  Photonic Neural Networks (PNNs) are gaining significant interest in the\nresearch community due to their potential for high parallelization, low\nlatency, and energy efficiency. PNNs compute using light, which leads to\nseveral differences in implementation when compared to electronics, such as the\nneed to represent input features in the photonic domain before feeding them\ninto the network. In this encoding process, it is common to combine multiple\nfeatures into a single input to reduce the number of inputs and associated\ndevices, leading to smaller and more energy-efficient PNNs. Although this\nalters the network's handling of input data, its impact on PNNs remains\nunderstudied. This paper addresses this open question, investigating the effect\nof commonly used encoding strategies that combine features on the performance\nand learning capabilities of PNNs. Here, using the concept of feature\nimportance, we develop a mathematical methodology for analyzing feature\ncombination. Through this methodology, we demonstrate that encoding multiple\nfeatures together in a single input determines their relative importance, thus\nlimiting the network's ability to learn from the data. Given some prior\nknowledge of the data, however, this can also be leveraged for higher accuracy.\nBy selecting an optimal encoding method, we achieve up to a 12.3% improvement\nin accuracy of PNNs trained on the Iris dataset compared to other encoding\ntechniques, surpassing the performance of networks where features are not\ncombined. These findings highlight the importance of carefully choosing the\nencoding to the accuracy and decision-making strategies of PNNs, particularly\nin size or power constrained applications.\n","authors":["Mauricio Gomes de Queiroz","Paul Jimenez","Raphael Cardoso","Mateus Vidaletti Costa","Mohab Abdalla","Ian O'Connor","Alberto Bosio","Fabio Pavanello"],"pdf_url":"https://arxiv.org/pdf/2406.18757v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20055v1","updated":"2024-06-28T17:07:11Z","published":"2024-06-28T17:07:11Z","title":"SpotlessSplats: Ignoring Distractors in 3D Gaussian Splatting","summary":"  3D Gaussian Splatting (3DGS) is a promising technique for 3D reconstruction,\noffering efficient training and rendering speeds, making it suitable for\nreal-time applications.However, current methods require highly controlled\nenvironments (no moving people or wind-blown elements, and consistent lighting)\nto meet the inter-view consistency assumption of 3DGS. This makes\nreconstruction of real-world captures problematic. We present SpotlessSplats,\nan approach that leverages pre-trained and general-purpose features coupled\nwith robust optimization to effectively ignore transient distractors. Our\nmethod achieves state-of-the-art reconstruction quality both visually and\nquantitatively, on casual captures.\n","authors":["Sara Sabour","Lily Goli","George Kopanas","Mark Matthews","Dmitry Lagun","Leonidas Guibas","Alec Jacobson","David J. Fleet","Andrea Tagliasacchi"],"pdf_url":"https://arxiv.org/pdf/2406.20055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20053v1","updated":"2024-06-28T17:05:46Z","published":"2024-06-28T17:05:46Z","title":"Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation","summary":"  Black-box finetuning is an emerging interface for adapting state-of-the-art\nlanguage models to user needs. However, such access may also let malicious\nactors undermine model safety. To demonstrate the challenge of defending\nfinetuning interfaces, we introduce covert malicious finetuning, a method to\ncompromise model safety via finetuning while evading detection. Our method\nconstructs a malicious dataset where every individual datapoint appears\ninnocuous, but finetuning on the dataset teaches the model to respond to\nencoded harmful requests with encoded harmful responses. Applied to GPT-4, our\nmethod produces a finetuned model that acts on harmful instructions 99% of the\ntime and avoids detection by defense mechanisms such as dataset inspection,\nsafety evaluations, and input/output classifiers. Our findings question whether\nblack-box finetuning access can be secured against sophisticated adversaries.\n","authors":["Danny Halawi","Alexander Wei","Eric Wallace","Tony T. Wang","Nika Haghtalab","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2406.20053v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2406.20046v1","updated":"2024-06-28T16:58:32Z","published":"2024-06-28T16:58:32Z","title":"Evaluation of autonomous systems under data distribution shifts","summary":"  We posit that data can only be safe to use up to a certain threshold of the\ndata distribution shift, after which control must be relinquished by the\nautonomous system and operation halted or handed to a human operator. With the\nuse of a computer vision toy example we demonstrate that network predictive\naccuracy is impacted by data distribution shifts and propose distance metrics\nbetween training and testing data to define safe operation limits within said\nshifts. We conclude that beyond an empirically obtained threshold of the data\ndistribution shift, it is unreasonable to expect network predictive accuracy\nnot to degrade\n","authors":["Daniel Sikar","Artur Garcez"],"pdf_url":"https://arxiv.org/pdf/2406.20046v1.pdf","comment":"13 pages, 10 figures, 4 tables"},{"id":"http://arxiv.org/abs/2305.00386v2","updated":"2024-06-28T16:36:08Z","published":"2023-04-30T04:56:36Z","title":"Importance Weighted Expectation-Maximization for Protein Sequence Design","summary":"  Designing protein sequences with desired biological function is crucial in\nbiology and chemistry. Recent machine learning methods use a surrogate\nsequence-function model to replace the expensive wet-lab validation. How can we\nefficiently generate diverse and novel protein sequences with high fitness? In\nthis paper, we propose IsEM-Pro, an approach to generate protein sequences\ntowards a given fitness criterion. At its core, IsEM-Pro is a latent generative\nmodel, augmented by combinatorial structure features from a separately learned\nMarkov random fields (MRFs). We develop an Monte Carlo Expectation-Maximization\nmethod (MCEM) to learn the model. During inference, sampling from its latent\nspace enhances diversity while its MRFs features guide the exploration in high\nfitness regions. Experiments on eight protein sequence design tasks show that\nour IsEM-Pro outperforms the previous best methods by at least 55% on average\nfitness score and generates more diverse and novel protein sequences.\n","authors":["Zhenqiao Song","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2305.00386v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20037v1","updated":"2024-06-28T16:34:22Z","published":"2024-06-28T16:34:22Z","title":"Explore as a Storm, Exploit as a Raindrop: On the Benefit of Fine-Tuning\n  Kernel Schedulers with Coordinate Descent","summary":"  Machine-learning models consist of kernels, which are algorithms applying\noperations on tensors -- data indexed by a linear combination of natural\nnumbers. Examples of kernels include convolutions, transpositions, and\nvectorial products. There are many ways to implement a kernel. These\nimplementations form the kernel's optimization space. Kernel scheduling is the\nproblem of finding the best implementation, given an objective function --\ntypically execution speed. Kernel optimizers such as Ansor, Halide, and AutoTVM\nsolve this problem via search heuristics, which combine two phases: exploration\nand exploitation. The first step evaluates many different kernel optimization\nspaces. The latter tries to improve the best implementations by investigating a\nkernel within the same space. For example, Ansor combines kernel generation\nthrough sketches for exploration and leverages an evolutionary algorithm to\nexploit the best sketches. In this work, we demonstrate the potential to reduce\nAnsor's search time while enhancing kernel quality by incorporating Droplet\nSearch, an AutoTVM algorithm, into Ansor's exploration phase. The approach\ninvolves limiting the number of samples explored by Ansor, selecting the best,\nand exploiting it with a coordinate descent algorithm. By applying this\napproach to the first 300 kernels that Ansor generates, we usually obtain\nbetter kernels in less time than if we let Ansor analyze 10,000 kernels. This\nresult has been replicated in 20 well-known deep-learning models (AlexNet,\nResNet, VGG, DenseNet, etc.) running on four architectures: an AMD Ryzen 7\n(x86), an NVIDIA A100 tensor core, an NVIDIA RTX 3080 GPU, and an ARM A64FX. A\npatch with this combined approach was approved in Ansor in February 2024. As\nevidence of the generality of this search methodology, a similar patch,\nachieving equally good results, was submitted to TVM's MetaSchedule in June\n2024.\n","authors":["Michael Canesche","Gaurav Verma","Fernando Magno Quintao Pereira"],"pdf_url":"https://arxiv.org/pdf/2406.20037v1.pdf","comment":"22 pages, 19 figures, original work"},{"id":"http://arxiv.org/abs/2403.11062v3","updated":"2024-06-28T16:31:06Z","published":"2024-03-17T02:24:09Z","title":"A Simple Mixture Policy Parameterization for Improving Sample Efficiency\n  of CVaR Optimization","summary":"  Reinforcement learning algorithms utilizing policy gradients (PG) to optimize\nConditional Value at Risk (CVaR) face significant challenges with sample\ninefficiency, hindering their practical applications. This inefficiency stems\nfrom two main facts: a focus on tail-end performance that overlooks many\nsampled trajectories, and the potential of gradient vanishing when the lower\ntail of the return distribution is overly flat. To address these challenges, we\npropose a simple mixture policy parameterization. This method integrates a\nrisk-neutral policy with an adjustable policy to form a risk-averse policy. By\nemploying this strategy, all collected trajectories can be utilized for policy\nupdating, and the issue of vanishing gradients is counteracted by stimulating\nhigher returns through the risk-neutral component, thus lifting the tail and\npreventing flatness. Our empirical study reveals that this mixture\nparameterization is uniquely effective across a variety of benchmark domains.\nSpecifically, it excels in identifying risk-averse CVaR policies in some Mujoco\nenvironments where the traditional CVaR-PG fails to learn a reasonable policy.\n","authors":["Yudong Luo","Yangchen Pan","Han Wang","Philip Torr","Pascal Poupart"],"pdf_url":"https://arxiv.org/pdf/2403.11062v3.pdf","comment":"RLC 2024"},{"id":"http://arxiv.org/abs/2406.20031v1","updated":"2024-06-28T16:20:22Z","published":"2024-06-28T16:20:22Z","title":"Pairwise Difference Learning for Classification","summary":"  Pairwise difference learning (PDL) has recently been introduced as a new\nmeta-learning technique for regression. Instead of learning a mapping from\ninstances to outcomes in the standard way, the key idea is to learn a function\nthat takes two instances as input and predicts the difference between the\nrespective outcomes. Given a function of this kind, predictions for a query\ninstance are derived from every training example and then averaged. This paper\nextends PDL toward the task of classification and proposes a meta-learning\ntechnique for inducing a PDL classifier by solving a suitably defined (binary)\nclassification problem on a paired version of the original training data. We\nanalyze the performance of the PDL classifier in a large-scale empirical study\nand find that it outperforms state-of-the-art methods in terms of prediction\nperformance. Last but not least, we provide an easy-to-use and publicly\navailable implementation of PDL in a Python package.\n","authors":["Mohamed Karim Belaid","Maximilian Rabus","Eyke Hüllermeier"],"pdf_url":"https://arxiv.org/pdf/2406.20031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20006v1","updated":"2024-06-28T15:46:08Z","published":"2024-06-28T15:46:08Z","title":"On the Trade-off between Flatness and Optimization in Distributed\n  Learning","summary":"  This paper proposes a theoretical framework to evaluate and compare the\nperformance of gradient-descent algorithms for distributed learning in relation\nto their behavior around local minima in nonconvex environments. Previous works\nhave noticed that convergence toward flat local minima tend to enhance the\ngeneralization ability of learning algorithms. This work discovers two\ninteresting results. First, it shows that decentralized learning strategies are\nable to escape faster away from local minimizers and favor convergence toward\nflatter minima relative to the centralized solution in the large-batch training\nregime. Second, and importantly, the ultimate classification accuracy is not\nsolely dependent on the flatness of the local minimizer but also on how well a\nlearning algorithm can approach that minimum. In other words, the\nclassification accuracy is a function of both flatness and optimization\nperformance. The paper examines the interplay between the two measures of\nflatness and optimization error closely. One important conclusion is that\ndecentralized strategies of the diffusion type deliver enhanced classification\naccuracy because it strikes a more favorable balance between flatness and\noptimization performance.\n","authors":["Ying Cao","Zhaoxian Wu","Kun Yuan","Ali H. Sayed"],"pdf_url":"https://arxiv.org/pdf/2406.20006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00035v3","updated":"2024-06-28T15:42:12Z","published":"2024-01-08T12:19:46Z","title":"Robustness Assessment of a Runway Object Classifier for Safe Aircraft\n  Taxiing","summary":"  As deep neural networks (DNNs) are becoming the prominent solution for many\ncomputational problems, the aviation industry seeks to explore their potential\nin alleviating pilot workload and in improving operational safety. However, the\nuse of DNNs in this type of safety-critical applications requires a thorough\ncertification process. This need can be addressed through formal verification,\nwhich provides rigorous assurances -- e.g.,~by proving the absence of certain\nmispredictions. In this case-study paper, we demonstrate this process using an\nimage-classifier DNN currently under development at Airbus and intended for use\nduring the aircraft taxiing phase. We use formal methods to assess this DNN's\nrobustness to three common image perturbation types: noise, brightness and\ncontrast, and some of their combinations. This process entails multiple\ninvocations of the underlying verifier, which might be computationally\nexpensive; and we therefore propose a method that leverages the monotonicity of\nthese robustness properties, as well as the results of past verification\nqueries, in order to reduce the overall number of verification queries required\nby nearly 60%. Our results provide an indication of the level of robustness\nachieved by the DNN classifier under study, and indicate that it is\nconsiderably more vulnerable to noise than to brightness or contrast\nperturbations.\n","authors":["Yizhak Elboher","Raya Elsaleh","Omri Isac","Mélanie Ducoffe","Audrey Galametz","Guillaume Povéda","Ryma Boumazouza","Noémie Cohen","Guy Katz"],"pdf_url":"https://arxiv.org/pdf/2402.00035v3.pdf","comment":"This is a preprint version of the paper in the proceedings of 43rd\n  Digital Avionics Systems Conference (DASC)"},{"id":"http://arxiv.org/abs/2402.04376v2","updated":"2024-06-28T15:36:50Z","published":"2024-02-06T20:30:19Z","title":"Scaling laws for learning with real and surrogate data","summary":"  Collecting large quantities of high-quality data can be prohibitively\nexpensive or impractical, and a bottleneck in machine learning. One may instead\naugment a small set of $n$ data points from the target distribution with data\nfrom more accessible sources, e.g. data collected under different circumstances\nor synthesized by generative models. We refer to such data as `surrogate data.'\nWe introduce a weighted empirical risk minimization (ERM) approach for\nintegrating surrogate data into training. We analyze mathematically this method\nunder several classical statistical models, and validate our findings\nempirically on datasets from different domains. Our main findings are: $(i)$\nIntegrating surrogate data can significantly reduce the test error on the\noriginal distribution. Surprisingly, this can happen even when the surrogate\ndata is unrelated to the original ones. We trace back this behavior to the\nclassical Stein's paradox. $(ii)$ In order to reap the benefit of surrogate\ndata, it is crucial to use optimally weighted ERM. $(iii)$ The test error of\nmodels trained on mixtures of real and surrogate data is approximately\ndescribed by a scaling law. This scaling law can be used to predict the optimal\nweighting scheme, and to choose the amount of surrogate data to add.\n","authors":["Ayush Jain","Andrea Montanari","Eren Sasoglu"],"pdf_url":"https://arxiv.org/pdf/2402.04376v2.pdf","comment":"Added new experiments"},{"id":"http://arxiv.org/abs/2405.14105v2","updated":"2024-06-28T15:34:26Z","published":"2024-05-23T02:14:17Z","title":"Distributed Speculative Inference of Large Language Models","summary":"  Accelerating the inference of large language models (LLMs) is an important\nchallenge in artificial intelligence. This paper introduces distributed\nspeculative inference (DSI), a novel distributed inference algorithm that is\nprovably faster than speculative inference (SI) [leviathan2023fast,\nchen2023accelerating, miao2023specinfer] and traditional autoregressive\ninference (non-SI). Like other SI algorithms, DSI works on frozen LLMs,\nrequiring no training or architectural modifications, and it preserves the\ntarget distribution.\n  Prior studies on SI have demonstrated empirical speedups (compared to non-SI)\nbut require a fast and accurate drafter LLM. In practice, off-the-shelf LLMs\noften do not have matching drafters that are sufficiently fast and accurate. We\nshow a gap: SI gets slower than non-SI when using slower or less accurate\ndrafters. We close this gap by proving that DSI is faster than both SI and\nnon-SI given any drafters. By orchestrating multiple instances of the target\nand drafters, DSI is not only faster than SI but also supports LLMs that cannot\nbe accelerated with SI.\n  Our simulations show speedups of off-the-shelf LLMs in realistic settings:\nDSI is 1.29-1.92x faster than SI.\n","authors":["Nadav Timor","Jonathan Mamou","Daniel Korat","Moshe Berchansky","Oren Pereg","Moshe Wasserblat","Tomer Galanti","Michal Gordon","David Harel"],"pdf_url":"https://arxiv.org/pdf/2405.14105v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19997v1","updated":"2024-06-28T15:32:59Z","published":"2024-06-28T15:32:59Z","title":"Wavelets Are All You Need for Autoregressive Image Generation","summary":"  In this paper, we take a new approach to autoregressive image generation that\nis based on two main ingredients. The first is wavelet image coding, which\nallows to tokenize the visual details of an image from coarse to fine details\nby ordering the information starting with the most significant bits of the most\nsignificant wavelet coefficients. The second is a variant of a language\ntransformer whose architecture is re-designed and optimized for token sequences\nin this 'wavelet language'. The transformer learns the significant statistical\ncorrelations within a token sequence, which are the manifestations of\nwell-known correlations between the wavelet subbands at various resolutions. We\nshow experimental results with conditioning on the generation process.\n","authors":["Wael Mattar","Idan Levy","Nir Sharon","Shai Dekel"],"pdf_url":"https://arxiv.org/pdf/2406.19997v1.pdf","comment":"16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.19995v1","updated":"2024-06-28T15:27:57Z","published":"2024-06-28T15:27:57Z","title":"Single Parent Family: A Spectrum of Family Members from a Single\n  Pre-Trained Foundation Model","summary":"  This paper introduces a novel method of Progressive Low Rank Decomposition\n(PLRD) tailored for the compression of large language models. Our approach\nleverages a pre-trained model, which is then incrementally decompressed to\nsmaller sizes using progressively lower ranks. This method allows for\nsignificant reductions in computational overhead and energy consumption, as\nsubsequent models are derived from the original without the need for retraining\nfrom scratch. We detail the implementation of PLRD, which strategically\ndecreases the tensor ranks, thus optimizing the trade-off between model\nperformance and resource usage. The efficacy of PLRD is demonstrated through\nextensive experiments showing that models trained with PLRD method on only 1B\ntokens maintain comparable performance with traditionally trained models while\nusing 0.1% of the tokens. The versatility of PLRD is highlighted by its ability\nto generate multiple model sizes from a single foundational model, adapting\nfluidly to varying computational and memory budgets. Our findings suggest that\nPLRD could set a new standard for the efficient scaling of LLMs, making\nadvanced AI more feasible on diverse platforms.\n","authors":["Habib Hajimolahoseini","Mohammad Hassanpour","Foozhan Ataiefard","Boxing Chen","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2406.19995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00125v2","updated":"2024-06-28T15:17:05Z","published":"2023-11-30T19:00:02Z","title":"Scalable Bayesian uncertainty quantification with data-driven priors for\n  radio interferometric imaging","summary":"  Next-generation radio interferometers like the Square Kilometer Array have\nthe potential to unlock scientific discoveries thanks to their unprecedented\nangular resolution and sensitivity. One key to unlocking their potential\nresides in handling the deluge and complexity of incoming data. This challenge\nrequires building radio interferometric imaging methods that can cope with the\nmassive data sizes and provide high-quality image reconstructions with\nuncertainty quantification (UQ). This work proposes a method coined QuantifAI\nto address UQ in radio-interferometric imaging with data-driven (learned)\npriors for high-dimensional settings. Our model, rooted in the Bayesian\nframework, uses a physically motivated model for the likelihood. The model\nexploits a data-driven convex prior, which can encode complex information\nlearned implicitly from simulations and guarantee the log-concavity of the\nposterior. We leverage probability concentration phenomena of high-dimensional\nlog-concave posteriors that let us obtain information about the posterior,\navoiding MCMC sampling techniques. We rely on convex optimisation methods to\ncompute the MAP estimation, which is known to be faster and better scale with\ndimension than MCMC sampling strategies. Our method allows us to compute local\ncredible intervals, i.e., Bayesian error bars, and perform hypothesis testing\nof structure on the reconstructed image. In addition, we propose a novel\nblazing-fast method to compute pixel-wise uncertainties at different scales. We\ndemonstrate our method by reconstructing radio-interferometric images in a\nsimulated setting and carrying out fast and scalable UQ, which we validate with\nMCMC sampling. Our method shows an improved image quality and more meaningful\nuncertainties than the benchmark method based on a sparsity-promoting prior.\nQuantifAI's source code: https://github.com/astro-informatics/QuantifAI.\n","authors":["Tobías I. Liaudat","Matthijs Mars","Matthew A. Price","Marcelo Pereyra","Marta M. Betcke","Jason D. McEwen"],"pdf_url":"https://arxiv.org/pdf/2312.00125v2.pdf","comment":"30 pages, 14 figures, 10 tables, code available at\n  https://github.com/astro-informatics/QuantifAI"},{"id":"http://arxiv.org/abs/2402.11658v2","updated":"2024-06-28T15:16:53Z","published":"2024-02-18T17:32:53Z","title":"Dynamic planning in hierarchical active inference","summary":"  By dynamic planning, we refer to the ability of the human brain to infer and\nimpose motor trajectories related to cognitive decisions. A recent paradigm,\nactive inference, brings fundamental insights into the adaptation of biological\norganisms, constantly striving to minimize prediction errors to restrict\nthemselves to life-compatible states. Over the past years, many studies have\nshown how human and animal behavior could be explained in terms of an active\ninferential process - either as discrete decision-making or continuous motor\ncontrol - inspiring innovative solutions in robotics and artificial\nintelligence. Still, the literature lacks a comprehensive outlook on how to\neffectively plan actions in changing environments. Setting ourselves the goal\nof modeling tool use, we delve into the topic of dynamic planning in active\ninference, keeping in mind two crucial aspects of biological goal-directed\nbehavior: the capacity to understand and exploit affordances for object\nmanipulation, and to learn the hierarchical interactions between the self and\nthe environment, including other agents. We start from a simple unit and\ngradually describe more advanced structures, comparing recently proposed design\nchoices and providing basic examples for each section. This study distances\nitself from traditional views centered on neural networks and reinforcement\nlearning, and points toward a yet unexplored direction in active inference:\nhybrid representations in hierarchical models.\n","authors":["Matteo Priorelli","Ivilin Peev Stoianov"],"pdf_url":"https://arxiv.org/pdf/2402.11658v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19983v1","updated":"2024-06-28T15:15:01Z","published":"2024-06-28T15:15:01Z","title":"Machine Learning Predictors for Min-Entropy Estimation","summary":"  This study investigates the application of machine learning predictors for\nmin-entropy estimation in Random Number Generators (RNGs), a key component in\ncryptographic applications where accurate entropy assessment is essential for\ncybersecurity. Our research indicates that these predictors, and indeed any\npredictor that leverages sequence correlations, primarily estimate average\nmin-entropy, a metric not extensively studied in this context. We explore the\nrelationship between average min-entropy and the traditional min-entropy,\nfocusing on their dependence on the number of target bits being predicted.\nUtilizing data from Generalized Binary Autoregressive Models, a subset of\nMarkov processes, we demonstrate that machine learning models (including a\nhybrid of convolutional and recurrent Long Short-Term Memory layers and the\ntransformer-based GPT-2 model) outperform traditional NIST SP 800-90B\npredictors in certain scenarios. Our findings underscore the importance of\nconsidering the number of target bits in min-entropy assessment for RNGs and\nhighlight the potential of machine learning approaches in enhancing entropy\nestimation techniques for improved cryptographic security.\n","authors":["Javier Blanco-Romero","Vicente Lorenzo","Florina Almenares Mendoza","Daniel Díaz-Sánchez"],"pdf_url":"https://arxiv.org/pdf/2406.19983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.03913v2","updated":"2024-06-28T15:13:15Z","published":"2024-05-07T00:22:13Z","title":"Digital Twin Calibration for Biological System-of-Systems: Cell Culture\n  Manufacturing Process","summary":"  Biomanufacturing innovation relies on an efficient Design of Experiments\n(DoEs) to optimize processes and product quality. Traditional DoE methods,\nignoring the underlying bioprocessing mechanisms, often suffer from a lack of\ninterpretability and sample efficiency. This limitation motivates us to create\na new optimal learning approach for digital twin model calibration. In this\nstudy, we consider the cell culture process multi-scale mechanistic model, also\nknown as Biological System-of-Systems (Bio-SoS). This model with a modular\ndesign, composed of sub-models, allows us to integrate data across various\nproduction processes. To calibrate the Bio-SoS digital twin, we evaluate the\nmean squared error of model prediction and develop a computational approach to\nquantify the impact of parameter estimation error of individual sub-models on\nthe prediction accuracy of digital twin, which can guide sample-efficient and\ninterpretable DoEs.\n","authors":["Fuqiang Cheng","Wei Xie","Hua Zheng"],"pdf_url":"https://arxiv.org/pdf/2405.03913v2.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2312.09969v2","updated":"2024-06-28T15:10:53Z","published":"2023-12-15T17:28:09Z","title":"Nearest Neighbor Sampling for Covariate Shift Adaptation","summary":"  Many existing covariate shift adaptation methods estimate sample weights\ngiven to loss values to mitigate the gap between the source and the target\ndistribution. However, estimating the optimal weights typically involves\ncomputationally expensive matrix inversion and hyper-parameter tuning. In this\npaper, we propose a new covariate shift adaptation method which avoids\nestimating the weights. The basic idea is to directly work on unlabeled target\ndata, labeled according to the $k$-nearest neighbors in the source dataset. Our\nanalysis reveals that setting $k = 1$ is an optimal choice. This property\nremoves the necessity of tuning the only hyper-parameter $k$ and leads to a\nrunning time quasi-linear in the sample size. Our results include sharp rates\nof convergence for our estimator, with a tight control of the mean square error\nand explicit constants. In particular, the variance of our estimators has the\nsame rate of convergence as for standard parametric estimation despite their\nnon-parametric nature. The proposed estimator shares similarities with some\nmatching-based treatment effect estimators used, e.g., in biostatistics,\neconometrics, and epidemiology. Our experiments show that it achieves drastic\nreduction in the running time with remarkable accuracy.\n","authors":["François Portier","Lionel Truquet","Ikko Yamane"],"pdf_url":"https://arxiv.org/pdf/2312.09969v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19980v1","updated":"2024-06-28T15:06:22Z","published":"2024-06-28T15:06:22Z","title":"Comparative Analysis of LSTM Neural Networks and Traditional Machine\n  Learning Models for Predicting Diabetes Patient Readmission","summary":"  Diabetes mellitus is a chronic metabolic disorder that has emerged as one of\nthe major health problems worldwide due to its high prevalence and serious\ncomplications, which are pricey to manage. Effective management requires good\nglycemic control and regular follow-up in the clinic; however, non-adherence to\nscheduled follow-ups is very common. This study uses the Diabetes 130-US\nHospitals dataset for analysis and prediction of readmission patients by\nvarious traditional machine learning models, such as XGBoost, LightGBM,\nCatBoost, Decision Tree, and Random Forest, and also uses an in-house LSTM\nneural network for comparison. The quality of the data was assured by\npreprocessing it, and the performance evaluation for all these models was based\non accuracy, precision, recall, and F1-score. LightGBM turned out to be the\nbest traditional model, while XGBoost was the runner-up. The LSTM model\nsuffered from overfitting despite high training accuracy. A major strength of\nLSTM is capturing temporal dependencies among the patient data. Further, SHAP\nvalues were used, which improved model interpretability, whereby key factors\namong them number of lab procedures and discharge disposition were identified\nas critical in the prediction of readmissions. This study demonstrates that\nmodel selection, validation, and interpretability are key steps in predictive\nhealthcare modeling. This will help health providers design interventions for\nimproved follow-up adherence and better management of diabetes.\n","authors":["Abolfazl Zarghani"],"pdf_url":"https://arxiv.org/pdf/2406.19980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19976v1","updated":"2024-06-28T15:03:08Z","published":"2024-06-28T15:03:08Z","title":"ScaleBiO: Scalable Bilevel Optimization for LLM Data Reweighting","summary":"  Bilevel optimization has shown its utility across various machine learning\nsettings, yet most algorithms in practice require second-order information,\nmaking it challenging to scale them up. Only recently, a paradigm of\nfirst-order algorithms emerged, capable of effectively addressing bilevel\noptimization problems. Nevertheless, the practical efficiency of this paradigm\nremains unverified, particularly in the context of large language models\n(LLMs). This paper introduces the first scalable instantiation of this paradigm\ncalled ScaleBiO, focusing on bilevel optimization for large-scale LLM data\nreweighting. By combining with a recently proposed memory-efficient training\ntechnique called LISA, our novel algorithm allows the paradigm to scale to\n34-billion-parameter LLMs on eight A40 GPUs, marking the first successful\napplication of bilevel optimization under practical scenarios for large-sized\nLLMs. Empirically, extensive experiments on data reweighting verify the\neffectiveness of ScaleBiO for different-scaled models, including GPT-2,\nLLaMA-3-8B, GPT-NeoX-20B, and Yi-34B, where bilevel optimization succeeds in\nfiltering irrelevant data samples and selecting informative samples.\nTheoretically, ScaleBiO ensures the optimality of the learned data weights,\nalong with a convergence guarantee matching the conventional first-order\nbilevel optimization paradigm on smooth and strongly convex objectives.\n","authors":["Rui Pan","Jipeng Zhang","Xingyuan Pan","Renjie Pi","Xiaoyu Wang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.19976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19973v1","updated":"2024-06-28T15:01:23Z","published":"2024-06-28T15:01:23Z","title":"STLLaVA-Med: Self-Training Large Language and Vision Assistant for\n  Medical","summary":"  Large Vision-Language Models (LVLMs) have shown significant potential in\nassisting medical diagnosis by leveraging extensive biomedical datasets.\nHowever, the advancement of medical image understanding and reasoning\ncritically depends on building high-quality visual instruction data, which is\ncostly and labor-intensive to obtain, particularly in the medical domain. To\nmitigate this data-starving issue, we introduce Self-Training Large Language\nand Vision Assistant for Medical (STLLaVA-Med). The proposed method is designed\nto train a policy model (an LVLM) capable of auto-generating medical visual\ninstruction data to improve data efficiency, guided through Direct Preference\nOptimization (DPO). Specifically, a more powerful and larger LVLM (e.g.,\nGPT-4o) is involved as a biomedical expert to oversee the DPO fine-tuning\nprocess on the auto-generated data, encouraging the policy model to align\nefficiently with human preferences. We validate the efficacy and data\nefficiency of STLLaVA-Med across three major medical Visual Question Answering\n(VQA) benchmarks, demonstrating competitive zero-shot performance with the\nutilization of only 9% of the medical data.\n","authors":["Guohao Sun","Can Qin","Huazhu Fu","Linwei Wang","Zhiqiang Tao"],"pdf_url":"https://arxiv.org/pdf/2406.19973v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.19963v1","updated":"2024-06-28T14:51:01Z","published":"2024-06-28T14:51:01Z","title":"Text2Robot: Evolutionary Robot Design from Text Descriptions","summary":"  Robot design has traditionally been costly and labor-intensive. Despite\nadvancements in automated processes, it remains challenging to navigate a vast\ndesign space while producing physically manufacturable robots. We introduce\nText2Robot, a framework that converts user text specifications and performance\npreferences into physical quadrupedal robots. Within minutes, Text2Robot can\nuse text-to-3D models to provide strong initializations of diverse\nmorphologies. Within a day, our geometric processing algorithms and\nbody-control co-optimization produce a walking robot by explicitly considering\nreal-world electronics and manufacturability. Text2Robot enables rapid\nprototyping and opens new opportunities for robot design with generative\nmodels.\n","authors":["Ryan P. Ringel","Zachary S. Charlick","Jiaxun Liu","Boxi Xia","Boyuan Chen"],"pdf_url":"https://arxiv.org/pdf/2406.19963v1.pdf","comment":"Our project website is at: https://generalroboticslab.com/Text2Robot"},{"id":"http://arxiv.org/abs/2406.19958v1","updated":"2024-06-28T14:45:29Z","published":"2024-06-28T14:45:29Z","title":"The Computational Curse of Big Data for Bayesian Additive Regression\n  Trees: A Hitting Time Analysis","summary":"  Bayesian Additive Regression Trees (BART) is a popular Bayesian\nnon-parametric regression model that is commonly used in causal inference and\nbeyond. Its strong predictive performance is supported by theoretical\nguarantees that its posterior distribution concentrates around the true\nregression function at optimal rates under various data generative settings and\nfor appropriate prior choices. In this paper, we show that the BART sampler\noften converges slowly, confirming empirical observations by other researchers.\nAssuming discrete covariates, we show that, while the BART posterior\nconcentrates on a set comprising all optimal tree structures (smallest bias and\ncomplexity), the Markov chain's hitting time for this set increases with $n$\n(training sample size), under several common data generative settings. As $n$\nincreases, the approximate BART posterior thus becomes increasingly different\nfrom the exact posterior (for the same number of MCMC samples), contrasting\nwith earlier concentration results on the exact posterior. This contrast is\nhighlighted by our simulations showing worsening frequentist undercoverage for\napproximate posterior intervals and a growing ratio between the MSE of the\napproximate posterior and that obtainable by artificially improving convergence\nvia averaging multiple sampler chains. Finally, based on our theoretical\ninsights, possibilities are discussed to improve the BART sampler convergence\nperformance.\n","authors":["Yan Shuo Tan","Omer Ronen","Theo Saarinen","Bin Yu"],"pdf_url":"https://arxiv.org/pdf/2406.19958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19948v1","updated":"2024-06-28T14:30:14Z","published":"2024-06-28T14:30:14Z","title":"Kolmogorov-Smirnov GAN","summary":"  We propose a novel deep generative model, the Kolmogorov-Smirnov Generative\nAdversarial Network (KSGAN). Unlike existing approaches, KSGAN formulates the\nlearning process as a minimization of the Kolmogorov-Smirnov (KS) distance,\ngeneralized to handle multivariate distributions. This distance is calculated\nusing the quantile function, which acts as the critic in the adversarial\ntraining process. We formally demonstrate that minimizing the KS distance leads\nto the trained approximate distribution aligning with the target distribution.\nWe propose an efficient implementation and evaluate its effectiveness through\nexperiments. The results show that KSGAN performs on par with existing\nadversarial methods, exhibiting stability during training, resistance to mode\ndropping and collapse, and tolerance to variations in hyperparameter settings.\nAdditionally, we review the literature on the Generalized KS test and discuss\nthe connections between KSGAN and existing adversarial generative models.\n","authors":["Maciej Falkiewicz","Naoya Takeishi","Alexandros Kalousis"],"pdf_url":"https://arxiv.org/pdf/2406.19948v1.pdf","comment":"Code available at https://github.com/DMML-Geneva/ksgan"},{"id":"http://arxiv.org/abs/2402.05758v2","updated":"2024-06-28T14:27:29Z","published":"2024-02-08T15:41:48Z","title":"Latent variable model for high-dimensional point process with structured\n  missingness","summary":"  Longitudinal data are important in numerous fields, such as healthcare,\nsociology and seismology, but real-world datasets present notable challenges\nfor practitioners because they can be high-dimensional, contain structured\nmissingness patterns, and measurement time points can be governed by an unknown\nstochastic process. While various solutions have been suggested, the majority\nof them have been designed to account for only one of these challenges. In this\nwork, we propose a flexible and efficient latent-variable model that is capable\nof addressing all these limitations. Our approach utilizes Gaussian processes\nto capture temporal correlations between samples and their associated\nmissingness masks as well as to model the underlying point process. We\nconstruct our model as a variational autoencoder together with deep neural\nnetwork parameterised encoder and decoder models, and develop a scalable\namortised variational inference approach for efficient model training. We\ndemonstrate competitive performance using both simulated and real datasets.\n","authors":["Maksim Sinelnikov","Manuel Haussmann","Harri Lähdesmäki"],"pdf_url":"https://arxiv.org/pdf/2402.05758v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15612v2","updated":"2024-06-28T14:23:49Z","published":"2024-06-21T19:27:46Z","title":"Catastrophic-risk-aware reinforcement learning with\n  extreme-value-theory-based policy gradients","summary":"  This paper tackles the problem of mitigating catastrophic risk (which is risk\nwith very low frequency but very high severity) in the context of a sequential\ndecision making process. This problem is particularly challenging due to the\nscarcity of observations in the far tail of the distribution of cumulative\ncosts (negative rewards). A policy gradient algorithm is developed, that we\ncall POTPG. It is based on approximations of the tail risk derived from extreme\nvalue theory. Numerical experiments highlight the out-performance of our method\nover common benchmarks, relying on the empirical distribution. An application\nto financial risk management, more precisely to the dynamic hedging of a\nfinancial option, is presented.\n","authors":["Parisa Davar","Frédéric Godin","Jose Garrido"],"pdf_url":"https://arxiv.org/pdf/2406.15612v2.pdf","comment":"The Python code to replicate the various numerical experiments of\n  this paper is available at\n  https://github.com/parisadavar/EVT-policy-gradient-RL"},{"id":"http://arxiv.org/abs/2312.00592v2","updated":"2024-06-28T14:02:06Z","published":"2023-12-01T13:56:28Z","title":"Tracking Object Positions in Reinforcement Learning: A Metric for\n  Keypoint Detection (extended version)","summary":"  Reinforcement learning (RL) for robot control typically requires a detailed\nrepresentation of the environment state, including information about\ntask-relevant objects not directly measurable. Keypoint detectors, such as\nspatial autoencoders (SAEs), are a common approach to extracting a\nlow-dimensional representation from high-dimensional image data. SAEs aim at\nspatial features such as object positions, which are often useful\nrepresentations in robotic RL. However, whether an SAE is actually able to\ntrack objects in the scene and thus yields a spatial state representation well\nsuited for RL tasks has rarely been examined due to a lack of established\nmetrics. In this paper, we propose to assess the performance of an SAE instance\nby measuring how well keypoints track ground truth objects in images. We\npresent a computationally lightweight metric and use it to evaluate common\nbaseline SAE architectures on image data from a simulated robot task. We find\nthat common SAEs differ substantially in their spatial extraction capability.\nFurthermore, we validate that SAEs that perform well in our metric achieve\nsuperior performance when used in downstream RL. Thus, our metric is an\neffective and lightweight indicator of RL performance before executing\nexpensive RL training. Building on these insights, we identify three key\nmodifications of SAE architectures to improve tracking performance. We make our\ncode available at anonymous.4open.science/r/sae-rl.\n","authors":["Emma Cramer","Jonas Reiher","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2312.00592v2.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2406.19931v1","updated":"2024-06-28T14:01:22Z","published":"2024-06-28T14:01:22Z","title":"Decoupling General and Personalized Knowledge in Federated Learning via\n  Additive and Low-Rank Decomposition","summary":"  To address data heterogeneity, the key strategy of Personalized Federated\nLearning (PFL) is to decouple general knowledge (shared among clients) and\nclient-specific knowledge, as the latter can have a negative impact on\ncollaboration if not removed. Existing PFL methods primarily adopt a parameter\npartitioning approach, where the parameters of a model are designated as one of\ntwo types: parameters shared with other clients to extract general knowledge\nand parameters retained locally to learn client-specific knowledge. However, as\nthese two types of parameters are put together like a jigsaw puzzle into a\nsingle model during the training process, each parameter may simultaneously\nabsorb both general and client-specific knowledge, thus struggling to separate\nthe two types of knowledge effectively. In this paper, we introduce FedDecomp,\na simple but effective PFL paradigm that employs parameter additive\ndecomposition to address this issue. Instead of assigning each parameter of a\nmodel as either a shared or personalized one, FedDecomp decomposes each\nparameter into the sum of two parameters: a shared one and a personalized one,\nthus achieving a more thorough decoupling of shared and personalized knowledge\ncompared to the parameter partitioning method. In addition, as we find that\nretaining local knowledge of specific clients requires much lower model\ncapacity compared with general knowledge across all clients, we let the matrix\ncontaining personalized parameters be low rank during the training process.\nMoreover, a new alternating training strategy is proposed to further improve\nthe performance. Experimental results across multiple datasets and varying\ndegrees of data heterogeneity demonstrate that FedDecomp outperforms\nstate-of-the-art methods up to 4.9\\%.\n","authors":["Xinghao Wu","Xuefeng Liu","Jianwei Niu","Haolin Wang","Shaojie Tang","Guogang Zhu","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2406.19931v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.17001v4","updated":"2024-06-28T13:40:00Z","published":"2023-03-29T20:07:07Z","title":"The G-invariant graph Laplacian","summary":"  Graph Laplacian based algorithms for data lying on a manifold have been\nproven effective for tasks such as dimensionality reduction, clustering, and\ndenoising. In this work, we consider data sets whose data points lie on a\nmanifold that is closed under the action of a known unitary matrix Lie group G.\nWe propose to construct the graph Laplacian by incorporating the distances\nbetween all the pairs of points generated by the action of G on the data set.\nWe deem the latter construction the ``G-invariant Graph Laplacian'' (G-GL). We\nshow that the G-GL converges to the Laplace-Beltrami operator on the data\nmanifold, while enjoying a significantly improved convergence rate compared to\nthe standard graph Laplacian which only utilizes the distances between the\npoints in the given data set. Furthermore, we show that the G-GL admits a set\nof eigenfunctions that have the form of certain products between the group\nelements and eigenvectors of certain matrices, which can be estimated from the\ndata efficiently using FFT-type algorithms. We demonstrate our construction and\nits advantages on the problem of filtering data on a noisy manifold closed\nunder the action of the special unitary group SU(2).\n","authors":["Eitan Rosen","Paulina Hoyos","Xiuyuan Cheng","Joe Kileel","Yoel Shkolnisky"],"pdf_url":"https://arxiv.org/pdf/2303.17001v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.08498v3","updated":"2024-06-28T13:31:48Z","published":"2024-05-14T10:55:04Z","title":"Learning Decision Policies with Instrumental Variables through Double\n  Machine Learning","summary":"  A common issue in learning decision-making policies in data-rich settings is\nspurious correlations in the offline dataset, which can be caused by hidden\nconfounders. Instrumental variable (IV) regression, which utilises a key\nunconfounded variable known as the instrument, is a standard technique for\nlearning causal relationships between confounded action, outcome, and context\nvariables. Most recent IV regression algorithms use a two-stage approach, where\na deep neural network (DNN) estimator learnt in the first stage is directly\nplugged into the second stage, in which another DNN is used to estimate the\ncausal effect. Naively plugging the estimator can cause heavy bias in the\nsecond stage, especially when regularisation bias is present in the first stage\nestimator. We propose DML-IV, a non-linear IV regression method that reduces\nthe bias in two-stage IV regressions and effectively learns high-performing\npolicies. We derive a novel learning objective to reduce bias and design the\nDML-IV algorithm following the double/debiased machine learning (DML)\nframework. The learnt DML-IV estimator has strong convergence rate and\n$O(N^{-1/2})$ suboptimality guarantees that match those when the dataset is\nunconfounded. DML-IV outperforms state-of-the-art IV regression methods on IV\nregression benchmarks and learns high-performing policies in the presence of\ninstruments.\n","authors":["Daqian Shao","Ashkan Soleymani","Francesco Quinzan","Marta Kwiatkowska"],"pdf_url":"https://arxiv.org/pdf/2405.08498v3.pdf","comment":"Accepted at ICML 2024"},{"id":"http://arxiv.org/abs/2406.17295v2","updated":"2024-06-28T13:28:04Z","published":"2024-06-25T05:45:07Z","title":"MatText: Do Language Models Need More than Text & Scale for Materials\n  Modeling?","summary":"  Effectively representing materials as text has the potential to leverage the\nvast advancements of large language models (LLMs) for discovering new\nmaterials. While LLMs have shown remarkable success in various domains, their\napplication to materials science remains underexplored. A fundamental challenge\nis the lack of understanding of how to best utilize text-based representations\nfor materials modeling. This challenge is further compounded by the absence of\na comprehensive benchmark to rigorously evaluate the capabilities and\nlimitations of these text representations in capturing the complexity of\nmaterial systems. To address this gap, we propose MatText, a suite of\nbenchmarking tools and datasets designed to systematically evaluate the\nperformance of language models in modeling materials. MatText encompasses nine\ndistinct text-based representations for material systems, including several\nnovel representations. Each representation incorporates unique inductive biases\nthat capture relevant information and integrate prior physical knowledge about\nmaterials. Additionally, MatText provides essential tools for training and\nbenchmarking the performance of language models in the context of materials\nscience. These tools include standardized dataset splits for each\nrepresentation, probes for evaluating sensitivity to geometric factors, and\ntools for seamlessly converting crystal structures into text. Using MatText, we\nconduct an extensive analysis of the capabilities of language models in\nmodeling materials. Our findings reveal that current language models\nconsistently struggle to capture the geometric information crucial for\nmaterials modeling across all representations. Instead, these models tend to\nleverage local information, which is emphasized in some of our novel\nrepresentations. Our analysis underscores MatText's ability to reveal\nshortcomings of text-based methods for materials design.\n","authors":["Nawaf Alampara","Santiago Miret","Kevin Maik Jablonka"],"pdf_url":"https://arxiv.org/pdf/2406.17295v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01317v2","updated":"2024-06-28T13:27:36Z","published":"2024-06-03T13:29:36Z","title":"The Intelligible and Effective Graph Neural Additive Networks","summary":"  Graph Neural Networks (GNNs) have emerged as the predominant approach for\nlearning over graph-structured data. However, most GNNs operate as black-box\nmodels and require post-hoc explanations, which may not suffice in high-stakes\nscenarios where transparency is crucial. In this paper, we present a GNN that\nis interpretable by design. Our model, Graph Neural Additive Network (GNAN), is\na novel extension of the interpretable class of Generalized Additive Models,\nand can be visualized and fully understood by humans. GNAN is designed to be\nfully interpretable, allowing both global and local explanations at the feature\nand graph levels through direct visualization of the model. These\nvisualizations describe the exact way the model uses the relationships between\nthe target variable, the features, and the graph. We demonstrate the\nintelligibility of GNANs in a series of examples on different tasks and\ndatasets. In addition, we show that the accuracy of GNAN is on par with\nblack-box GNNs, making it suitable for critical applications where transparency\nis essential, alongside high accuracy.\n","authors":["Maya Bechler-Speicher","Amir Globerson","Ran Gilad-Bachrach"],"pdf_url":"https://arxiv.org/pdf/2406.01317v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14862v3","updated":"2024-06-28T13:19:37Z","published":"2024-06-21T04:39:03Z","title":"LatentExplainer: Explaining Latent Representations in Deep Generative\n  Models with Multi-modal Foundation Models","summary":"  Deep generative models like VAEs and diffusion models have advanced various\ngeneration tasks by leveraging latent variables to learn data distributions and\ngenerate high-quality samples. Despite the field of explainable AI making\nstrides in interpreting machine learning models, understanding latent variables\nin generative models remains challenging. This paper introduces\nLatentExplainer, a framework for automatically generating semantically\nmeaningful explanations of latent variables in deep generative models.\nLatentExplainer tackles three main challenges: inferring the meaning of latent\nvariables, aligning explanations with inductive biases, and handling varying\ndegrees of explainability. By perturbing latent variables and interpreting\nchanges in generated data, the framework provides a systematic approach to\nunderstanding and controlling the data generation process, enhancing the\ntransparency and interpretability of deep generative models. We evaluate our\nproposed method on several real-world and synthetic datasets, and the results\ndemonstrate superior performance in generating high-quality explanations of\nlatent variables.\n","authors":["Mengdan Zhu","Raasikh Kanjiani","Jiahui Lu","Andrew Choi","Qirui Ye","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.14862v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07049v2","updated":"2024-06-28T13:14:13Z","published":"2024-04-10T14:38:58Z","title":"Towards Learning Stochastic Population Models by Gradient Descent","summary":"  Increasing effort is put into the development of methods for learning\nmechanistic models from data. This task entails not only the accurate\nestimation of parameters but also a suitable model structure. Recent work on\nthe discovery of dynamical systems formulates this problem as a linear equation\nsystem. Here, we explore several simulation-based optimization approaches,\nwhich allow much greater freedom in the objective formulation and weaker\nconditions on the available data. We show that even for relatively small\nstochastic population models, simultaneous estimation of parameters and\nstructure poses major challenges for optimization procedures. Particularly, we\ninvestigate the application of the local stochastic gradient descent method,\ncommonly used for training machine learning models. We demonstrate accurate\nestimation of models but find that enforcing the inference of parsimonious,\ninterpretable models drastically increases the difficulty. We give an outlook\non how this challenge can be overcome.\n","authors":["Justin N. Kreikemeyer","Philipp Andelfinger","Adelinde M. Uhrmacher"],"pdf_url":"https://arxiv.org/pdf/2404.07049v2.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.19900v1","updated":"2024-06-28T13:10:13Z","published":"2024-06-28T13:10:13Z","title":"`Just One More Sensor is Enough' -- Iterative Water Leak Localization\n  with Physical Simulation and a Small Number of Pressure Sensors","summary":"  In this article, we propose an approach to leak localisation in a complex\nwater delivery grid with the use of data from physical simulation (e.g. EPANET\nsoftware). This task is usually achieved by a network of multiple water\npressure sensors and analysis of the so-called sensitivity matrix of pressure\ndifferences between the network's simulated data and actual data of the network\naffected by the leak. However, most algorithms using this approach require a\nsignificant number of pressure sensors -- a condition that is not easy to\nfulfil in the case of many less equipped networks. Therefore, we answer the\nquestion of whether leak localisation is possible by utilising very few sensors\nbut having the ability to relocate one of them. Our algorithm is based on\nphysical simulations (EPANET software) and an iterative scheme for mobile\nsensor relocation. The experiments show that the proposed system can equalise\nthe low number of sensors with adjustments made for their positioning, giving a\nvery good approximation of leak's position both in simulated cases and\nreal-life example taken from BattLeDIM competition L-Town data.\n","authors":["Michał Cholewa","Michał Romaszewski","Przemysław Głomb","Katarzyna Kołodziej","Michał Gorawski","Jakub Koral","Wojciech Koral","Andrzej Madej","Kryspin Musioł"],"pdf_url":"https://arxiv.org/pdf/2406.19900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19897v1","updated":"2024-06-28T13:05:17Z","published":"2024-06-28T13:05:17Z","title":"FI-CBL: A Probabilistic Method for Concept-Based Learning with Expert\n  Rules","summary":"  A method for solving concept-based learning (CBL) problem is proposed. The\nmain idea behind the method is to divide each concept-annotated image into\npatches, to transform the patches into embeddings by using an autoencoder, and\nto cluster the embeddings assuming that each cluster will mainly contain\nembeddings of patches with certain concepts. To find concepts of a new image,\nthe method implements the frequentist inference by computing prior and\nposterior probabilities of concepts based on rates of patches from images with\ncertain values of the concepts. Therefore, the proposed method is called the\nFrequentist Inference CBL (FI-CBL). FI-CBL allows us to incorporate the expert\nrules in the form of logic functions into the inference procedure. An idea\nbehind the incorporation is to update prior and conditional probabilities of\nconcepts to satisfy the rules. The method is transparent because it has an\nexplicit sequence of probabilistic calculations and a clear frequency\ninterpretation. Numerical experiments show that FI-CBL outperforms the concept\nbottleneck model in cases when the number of training data is small. The code\nof proposed algorithms is publicly available.\n","authors":["Lev V. Utkin","Andrei V. Konstantinov","Stanislav R. Kirpichenko"],"pdf_url":"https://arxiv.org/pdf/2406.19897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.02029v3","updated":"2024-06-28T13:02:49Z","published":"2023-08-03T20:45:11Z","title":"Deep Maxout Network-based Feature Fusion and Political Tangent Search\n  Optimizer enabled Transfer Learning for Thalassemia Detection","summary":"  Thalassemia is a heritable blood disorder which is the outcome of a genetic\ndefect causing lack of production of hemoglobin polypeptide chains. However,\nthere is less understanding of the precise frequency as well as sharing in\nthese areas. Knowing about the frequency of thalassemia occurrence and\ndependable mutations is thus a significant step in preventing, controlling, and\ntreatment planning. Here, Political Tangent Search Optimizer based Transfer\nLearning (PTSO_TL) is introduced for thalassemia detection. Initially, input\ndata obtained from a particular dataset is normalized in the data normalization\nstage. Quantile normalization is utilized in the data normalization stage, and\nthe data are then passed to the feature fusion phase, in which Weighted\nEuclidean Distance with Deep Maxout Network (DMN) is utilized. Thereafter, data\naugmentation is performed using the oversampling method to increase data\ndimensionality. Lastly, thalassemia detection is carried out by TL, wherein a\nconvolutional neural network (CNN) is utilized with hyperparameters from a\ntrained model such as Xception. TL is tuned by PTSO, and the training algorithm\nPTSO is presented by merging of Political Optimizer (PO) and Tangent Search\nAlgorithm (TSA). Furthermore, PTSO_TL obtained maximal precision, recall, and\nf-measure values of about 94.3%, 96.1%, and 95.2%, respectively.\n","authors":["Hemn Barzan Abdalla","Awder Ahmed","Guoquan Li","Nasser Mustafa","Abdur Rashid Sangi"],"pdf_url":"https://arxiv.org/pdf/2308.02029v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03565v3","updated":"2024-06-28T12:55:35Z","published":"2023-07-07T12:57:10Z","title":"MALIBO: Meta-learning for Likelihood-free Bayesian Optimization","summary":"  Bayesian optimization (BO) is a popular method to optimize costly black-box\nfunctions. While traditional BO optimizes each new target task from scratch,\nmeta-learning has emerged as a way to leverage knowledge from related tasks to\noptimize new tasks faster. However, existing meta-learning BO methods rely on\nsurrogate models that suffer from scalability issues and are sensitive to\nobservations with different scales and noise types across tasks. Moreover, they\noften overlook the uncertainty associated with task similarity. This leads to\nunreliable task adaptation when only limited observations are obtained or when\nthe new tasks differ significantly from the related tasks. To address these\nlimitations, we propose a novel meta-learning BO approach that bypasses the\nsurrogate model and directly learns the utility of queries across tasks. Our\nmethod explicitly models task uncertainty and includes an auxiliary model to\nenable robust adaptation to new tasks. Extensive experiments show that our\nmethod demonstrates strong anytime performance and outperforms state-of-the-art\nmeta-learning BO methods in various benchmarks.\n","authors":["Jiarong Pan","Stefan Falkner","Felix Berkenkamp","Joaquin Vanschoren"],"pdf_url":"https://arxiv.org/pdf/2307.03565v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18328v2","updated":"2024-06-28T12:45:52Z","published":"2024-06-26T13:16:40Z","title":"PDFA Distillation via String Probability Queries","summary":"  Probabilistic deterministic finite automata (PDFA) are discrete event systems\nmodeling conditional probabilities over languages: Given an already seen\nsequence of tokens they return the probability of tokens of interest to appear\nnext. These types of models have gained interest in the domain of explainable\nmachine learning, where they are used as surrogate models for neural networks\ntrained as language models. In this work we present an algorithm to distill\nPDFA from neural networks. Our algorithm is a derivative of the L# algorithm\nand capable of learning PDFA from a new type of query, in which the algorithm\ninfers conditional probabilities from the probability of the queried string to\noccur. We show its effectiveness on a recent public dataset by distilling PDFA\nfrom a set of trained neural networks.\n","authors":["Robert Baumgartner","Sicco Verwer"],"pdf_url":"https://arxiv.org/pdf/2406.18328v2.pdf","comment":"LearnAUT 2024"},{"id":"http://arxiv.org/abs/2406.19881v1","updated":"2024-06-28T12:44:01Z","published":"2024-06-28T12:44:01Z","title":"Attention Meets UAVs: A Comprehensive Evaluation of DDoS Detection in\n  Low-Cost UAVs","summary":"  This paper explores the critical issue of enhancing cybersecurity measures\nfor low-cost, Wi-Fi-based Unmanned Aerial Vehicles (UAVs) against Distributed\nDenial of Service (DDoS) attacks. In the current work, we have explored three\nvariants of DDoS attacks, namely Transmission Control Protocol (TCP), Internet\nControl Message Protocol (ICMP), and TCP + ICMP flooding attacks, and developed\na detection mechanism that runs on the companion computer of the UAV system. As\na part of the detection mechanism, we have evaluated various machine learning,\nand deep learning algorithms, such as XGBoost, Isolation Forest, Long\nShort-Term Memory (LSTM), Bidirectional-LSTM (Bi-LSTM), LSTM with attention,\nBi-LSTM with attention, and Time Series Transformer (TST) in terms of various\nclassification metrics. Our evaluation reveals that algorithms with attention\nmechanisms outperform their counterparts in general, and TST stands out as the\nmost efficient model with a run time of 0.1 seconds. TST has demonstrated an F1\nscore of 0.999, 0.997, and 0.943 for TCP, ICMP, and TCP + ICMP flooding attacks\nrespectively. In this work, we present the necessary steps required to build an\non-board DDoS detection mechanism. Further, we also present the ablation study\nto identify the best TST hyperparameters for DDoS detection, and we have also\nunderscored the advantage of adapting learnable positional embeddings in TST\nfor DDoS detection with an improvement in F1 score from 0.94 to 0.99.\n","authors":["Ashish Sharma","SVSLN Surya Suhas Vaddhiparthy","Sai Usha Goparaju","Deepak Gangadharan","Harikumar Kandath"],"pdf_url":"https://arxiv.org/pdf/2406.19881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19871v1","updated":"2024-06-28T12:26:28Z","published":"2024-06-28T12:26:28Z","title":"Koopman based trajectory model and computation offloading for high\n  mobility paradigm in ISAC enabled IoT system","summary":"  User experience on mobile devices is constrained by limited battery capacity\nand processing power, but 6G technology advancements are diving rapidly into\nmobile technical evolution. Mobile edge computing (MEC) offers a solution,\noffloading computationally intensive tasks to edge cloud servers, reducing\nbattery drain compared to local processing. The upcoming integrated sensing and\ncommunication in mobile communication may improve the trajectory prediction and\nprocessing delays. This study proposes a greedy resource allocation\noptimization strategy for multi-user networks to minimize aggregate energy\nusage. Numerical results show potential improvement at 33\\% for every 1000\niteration. Addressing prediction model division and velocity accuracy issues is\ncrucial for better results. A plan for further improvement and achieving\nobjectives is outlined for the upcoming work phase.\n","authors":["Minh-Tuan Tran"],"pdf_url":"https://arxiv.org/pdf/2406.19871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19861v1","updated":"2024-06-28T12:05:47Z","published":"2024-06-28T12:05:47Z","title":"Operator World Models for Reinforcement Learning","summary":"  Policy Mirror Descent (PMD) is a powerful and theoretically sound methodology\nfor sequential decision-making. However, it is not directly applicable to\nReinforcement Learning (RL) due to the inaccessibility of explicit action-value\nfunctions. We address this challenge by introducing a novel approach based on\nlearning a world model of the environment using conditional mean embeddings. We\nthen leverage the operatorial formulation of RL to express the action-value\nfunction in terms of this quantity in closed form via matrix operations.\nCombining these estimators with PMD leads to POWR, a new RL algorithm for which\nwe prove convergence rates to the global optimum. Preliminary experiments in\nfinite and infinite state settings support the effectiveness of our method.\n","authors":["Pietro Novelli","Marco Pratticò","Massimiliano Pontil","Carlo Ciliberto"],"pdf_url":"https://arxiv.org/pdf/2406.19861v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13699v2","updated":"2024-06-28T11:49:52Z","published":"2024-01-22T03:17:41Z","title":"Generative AI-Driven Human Digital Twin in IoT-Healthcare: A\n  Comprehensive Survey","summary":"  The Internet of things (IoT) can significantly enhance the quality of human\nlife, specifically in healthcare, attracting extensive attentions to\nIoT-healthcare services. Meanwhile, the human digital twin (HDT) is proposed as\nan innovative paradigm that can comprehensively characterize the replication of\nthe individual human body in the digital world and reflect its physical status\nin real time. Naturally, HDT is envisioned to empower IoT-healthcare beyond the\napplication of healthcare monitoring by acting as a versatile and vivid human\ndigital testbed, simulating the outcomes and guiding the practical treatments.\nHowever, successfully establishing HDT requires high-fidelity virtual modeling\nand strong information interactions but possibly with scarce, biased and noisy\ndata. Fortunately, a recent popular technology called generative artificial\nintelligence (GAI) may be a promising solution because it can leverage advanced\nAI algorithms to automatically create, manipulate, and modify valuable while\ndiverse data. This survey particularly focuses on the implementation of\nGAI-driven HDT in IoT-healthcare. We start by introducing the background of\nIoT-healthcare and the potential of GAI-driven HDT. Then, we delve into the\nfundamental techniques and present the overall framework of GAI-driven HDT.\nAfter that, we explore the realization of GAI-driven HDT in detail, including\nGAI-enabled data acquisition, communication, data management, digital modeling,\nand data analysis. Besides, we discuss typical IoT-healthcare applications that\ncan be revolutionized by GAI-driven HDT, namely personalized health monitoring\nand diagnosis, personalized prescription, and personalized rehabilitation.\nFinally, we conclude this survey by highlighting some future research\ndirections.\n","authors":["Jiayuan Chen","You Shi","Changyan Yi","Hongyang Du","Jiawen Kang","Dusit Niyato"],"pdf_url":"https://arxiv.org/pdf/2401.13699v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.02766v3","updated":"2024-06-28T11:39:10Z","published":"2023-06-05T10:45:39Z","title":"Networked Communication for Decentralised Agents in Mean-Field Games","summary":"  We introduce networked communication to the mean-field game framework, in\nparticular to oracle-free settings where $N$ decentralised agents learn along a\nsingle, non-episodic run of the empirical system. We prove that our\narchitecture, with only a few reasonable assumptions about network structure,\nhas sample guarantees bounded between those of the centralised- and\nindependent-learning cases. We discuss how the sample guarantees of the three\ntheoretical algorithms do not actually result in practical convergence. We\ntherefore show that in practical settings where the theoretical parameters are\nnot observed (leading to poor estimation of the Q-function), our communication\nscheme significantly accelerates convergence over the independent case (and\noften even the centralised case), without relying on the assumption of a\ncentralised learner. We contribute further practical enhancements to all three\ntheoretical algorithms, allowing us to present their first empirical\ndemonstrations. Our experiments confirm that we can remove several of the\ntheoretical assumptions of the algorithms, and display the empirical\nconvergence benefits brought by our new networked communication. We\nadditionally show that the networked approach has significant advantages, over\nboth the centralised and independent alternatives, in terms of robustness to\nunexpected learning failures and to changes in population size.\n","authors":["Patrick Benjamin","Alessandro Abate"],"pdf_url":"https://arxiv.org/pdf/2306.02766v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03511v3","updated":"2024-06-28T11:23:11Z","published":"2023-12-06T14:13:38Z","title":"Kandinsky 3.0 Technical Report","summary":"  We present Kandinsky 3.0, a large-scale text-to-image generation model based\non latent diffusion, continuing the series of text-to-image Kandinsky models\nand reflecting our progress to achieve higher quality and realism of image\ngeneration. In this report we describe the architecture of the model, the data\ncollection procedure, the training technique, and the production system for\nuser interaction. We focus on the key components that, as we have identified as\na result of a large number of experiments, had the most significant impact on\nimproving the quality of our model compared to the others. We also describe\nextensions and applications of our model, including super resolution,\ninpainting, image editing, image-to-video generation, and a distilled version\nof Kandinsky 3.0 - Kandinsky 3.1, which does inference in 4 steps of the\nreverse process and 20 times faster without visual quality decrease. By\nside-by-side human preferences comparison, Kandinsky becomes better in text\nunderstanding and works better on specific domains. The code is available at\nhttps://github.com/ai-forever/Kandinsky-3\n","authors":["Vladimir Arkhipkin","Andrei Filatov","Viacheslav Vasilev","Anastasia Maltseva","Said Azizov","Igor Pavlov","Julia Agafonova","Andrey Kuznetsov","Denis Dimitrov"],"pdf_url":"https://arxiv.org/pdf/2312.03511v3.pdf","comment":"Project page: https://ai-forever.github.io/Kandinsky-3"},{"id":"http://arxiv.org/abs/2406.19832v1","updated":"2024-06-28T11:11:16Z","published":"2024-06-28T11:11:16Z","title":"MuGSI: Distilling GNNs with Multi-Granularity Structural Information for\n  Graph Classification","summary":"  Recent works have introduced GNN-to-MLP knowledge distillation (KD)\nframeworks to combine both GNN's superior performance and MLP's fast inference\nspeed. However, existing KD frameworks are primarily designed for node\nclassification within single graphs, leaving their applicability to graph\nclassification largely unexplored. Two main challenges arise when extending KD\nfor node classification to graph classification: (1) The inherent sparsity of\nlearning signals due to soft labels being generated at the graph level; (2) The\nlimited expressiveness of student MLPs, especially in datasets with limited\ninput feature spaces. To overcome these challenges, we introduce MuGSI, a novel\nKD framework that employs Multi-granularity Structural Information for graph\nclassification. Specifically, we propose multi-granularity distillation loss in\nMuGSI to tackle the first challenge. This loss function is composed of three\ndistinct components: graph-level distillation, subgraph-level distillation, and\nnode-level distillation. Each component targets a specific granularity of the\ngraph structure, ensuring a comprehensive transfer of structural knowledge from\nthe teacher model to the student model. To tackle the second challenge, MuGSI\nproposes to incorporate a node feature augmentation component, thereby\nenhancing the expressiveness of the student MLPs and making them more capable\nlearners. We perform extensive experiments across a variety of datasets and\ndifferent teacher/student model architectures. The experiment results\ndemonstrate the effectiveness, efficiency, and robustness of MuGSI. Codes are\npublicly available at: \\textbf{\\url{https://github.com/tianyao-aka/MuGSI}.}\n","authors":["Tianjun Yao","Jiaqi Sun","Defu Cao","Kun Zhang","Guangyi Chen"],"pdf_url":"https://arxiv.org/pdf/2406.19832v1.pdf","comment":"12 pages, 4 figures. Accepted by TheWebConf2024"},{"id":"http://arxiv.org/abs/2406.19827v1","updated":"2024-06-28T11:06:46Z","published":"2024-06-28T11:06:46Z","title":"Towards Stable and Storage-efficient Dataset Distillation: Matching\n  Convexified Trajectory","summary":"  The rapid evolution of deep learning and large language models has led to an\nexponential growth in the demand for training data, prompting the development\nof Dataset Distillation methods to address the challenges of managing large\ndatasets. Among these, Matching Training Trajectories (MTT) has been a\nprominent approach, which replicates the training trajectory of an expert\nnetwork on real data with a synthetic dataset. However, our investigation found\nthat this method suffers from three significant limitations: 1. Instability of\nexpert trajectory generated by Stochastic Gradient Descent (SGD); 2. Low\nconvergence speed of the distillation process; 3. High storage consumption of\nthe expert trajectory. To address these issues, we offer a new perspective on\nunderstanding the essence of Dataset Distillation and MTT through a simple\ntransformation of the objective function, and introduce a novel method called\nMatching Convexified Trajectory (MCT), which aims to provide better guidance\nfor the student trajectory. MCT leverages insights from the linearized dynamics\nof Neural Tangent Kernel methods to create a convex combination of expert\ntrajectories, guiding the student network to converge rapidly and stably. This\ntrajectory is not only easier to store, but also enables a continuous sampling\nstrategy during distillation, ensuring thorough learning and fitting of the\nentire expert trajectory. Comprehensive experiments across three public\ndatasets validate the superiority of MCT over traditional MTT methods.\n","authors":["Wenliang Zhong","Haoyu Tang","Qinghai Zheng","Mingzhu Xu","Yupeng Hu","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2406.19827v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2406.19825v1","updated":"2024-06-28T11:01:02Z","published":"2024-06-28T11:01:02Z","title":"Reinforcement Learning for Efficient Design and Control Co-optimisation\n  of Energy Systems","summary":"  The ongoing energy transition drives the development of decentralised\nrenewable energy sources, which are heterogeneous and weather-dependent,\ncomplicating their integration into energy systems. This study tackles this\nissue by introducing a novel reinforcement learning (RL) framework tailored for\nthe co-optimisation of design and control in energy systems. Traditionally, the\nintegration of renewable sources in the energy sector has relied on complex\nmathematical modelling and sequential processes. By leveraging RL's model-free\ncapabilities, the framework eliminates the need for explicit system modelling.\nBy optimising both control and design policies jointly, the framework enhances\nthe integration of renewable sources and improves system efficiency. This\ncontribution paves the way for advanced RL applications in energy management,\nleading to more efficient and effective use of renewable energy sources.\n","authors":["Marine Cauz","Adrien Bolland","Nicolas Wyrsch","Christophe Ballif"],"pdf_url":"https://arxiv.org/pdf/2406.19825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.03080v3","updated":"2024-06-28T10:52:37Z","published":"2022-12-06T15:50:28Z","title":"Straggler-Resilient Differentially-Private Decentralized Learning","summary":"  We consider the straggler problem in decentralized learning over a logical\nring while preserving user data privacy. Especially, we extend the recently\nproposed framework of differential privacy (DP) amplification by\ndecentralization by Cyffers and Bellet to include overall training\nlatency--comprising both computation and communication latency. Analytical\nresults on both the convergence speed and the DP level are derived for both a\nskipping scheme (which ignores the stragglers after a timeout) and a baseline\nscheme that waits for each node to finish before the training continues. A\ntrade-off between overall training latency, accuracy, and privacy,\nparameterized by the timeout of the skipping scheme, is identified and\nempirically validated for logistic regression on a real-world dataset and for\nimage classification using the MNIST and CIFAR-10 datasets.\n","authors":["Yauhen Yakimenka","Chung-Wei Weng","Hsuan-Yin Lin","Eirik Rosnes","Jörg Kliewer"],"pdf_url":"https://arxiv.org/pdf/2212.03080v3.pdf","comment":"To appear in the IEEE Journal on Selected Areas in Information Theory\n  (special issue on Information-Theoretic Methods for Trustworthy and Reliable\n  Machine Learning)"},{"id":"http://arxiv.org/abs/2406.19807v1","updated":"2024-06-28T10:30:46Z","published":"2024-06-28T10:30:46Z","title":"Deceptive Diffusion: Generating Synthetic Adversarial Examples","summary":"  We introduce the concept of deceptive diffusion -- training a generative AI\nmodel to produce adversarial images. Whereas a traditional adversarial attack\nalgorithm aims to perturb an existing image to induce a misclassificaton, the\ndeceptive diffusion model can create an arbitrary number of new, misclassified\nimages that are not directly associated with training or test images. Deceptive\ndiffusion offers the possibility of strengthening defence algorithms by\nproviding adversarial training data at scale, including types of\nmisclassification that are otherwise difficult to find. In our experiments, we\nalso investigate the effect of training on a partially attacked data set. This\nhighlights a new type of vulnerability for generative diffusion models: if an\nattacker is able to stealthily poison a portion of the training data, then the\nresulting diffusion model will generate a similar proportion of misleading\noutputs.\n","authors":["Lucas Beerens","Catherine F. Higham","Desmond J. Higham"],"pdf_url":"https://arxiv.org/pdf/2406.19807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19801v1","updated":"2024-06-28T10:16:10Z","published":"2024-06-28T10:16:10Z","title":"MulTi-Wise Sampling: Trading Uniform T-Wise Feature Interaction Coverage\n  for Smaller Samples","summary":"  Ensuring the functional safety of highly configurable systems often requires\ntesting representative subsets of all possible configurations to reduce testing\neffort and save resources. The ratio of covered t-wise feature interactions\n(i.e., T-Wise Feature Interaction Coverage) is a common criterion for\ndetermining whether a subset of configurations is representative and capable of\nfinding faults. Existing t-wise sampling algorithms uniformly cover t-wise\nfeature interactions for all features, resulting in lengthy execution times and\nlarge sample sizes, particularly when large t-wise feature interactions are\nconsidered (i.e., high values of t). In this paper, we introduce a novel\napproach to t-wise feature interaction sampling, questioning the necessity of\nuniform coverage across all t-wise feature interactions, called\n\\emph{\\mulTiWise{}}. Our approach prioritizes between subsets of critical and\nnon-critical features, considering higher t-values for subsets of critical\nfeatures when generating a t-wise feature interaction sample. We evaluate our\napproach using subject systems from real-world applications, including\n\\busybox{}, \\soletta{}, \\fiasco{}, and \\uclibc{}. Our results show that\nsacrificing uniform t-wise feature interaction coverage between all features\nreduces the time needed to generate a sample and the resulting sample size.\nHence, \\mulTiWise{} Sampling offers an alternative to existing approaches if\nknowledge about feature criticality is available.\n","authors":["Tobias Pett","Sebastian Krieter","Thomas Thüm","Ina Schaefer"],"pdf_url":"https://arxiv.org/pdf/2406.19801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16783v2","updated":"2024-06-28T10:14:53Z","published":"2024-06-24T16:45:13Z","title":"M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in\n  Large Language Models","summary":"  Instruction finetuning (IFT) is critical for aligning Large Language Models\n(LLMs) to follow instructions. While many effective IFT datasets have been\nintroduced recently, they predominantly focus on high-resource languages like\nEnglish. To better align LLMs across a broad spectrum of languages and tasks,\nwe propose a fully synthetic, novel taxonomy (Evol) guided Multilingual,\nMulti-turn instruction finetuning dataset, called M2Lingual. It is constructed\nby first selecting a diverse set of seed examples and then utilizing the\nproposed Evol taxonomy to convert these seeds into complex and challenging\nmulti-turn instructions. We demonstrate the effectiveness of M2Lingual by\ntraining LLMs of varying sizes and showcasing the enhanced performance across a\ndiverse set of languages. We contribute the 2 step Evol taxonomy with the\nguided generation code: https://github.com/ServiceNow/M2Lingual, as well as the\nfirst fully synthetic, general and task-oriented, multi-turn, multilingual\ndataset built with Evol - M2Lingual:\nhttps://huggingface.co/datasets/ServiceNow-AI/ M2Lingual - containing 182K\ntotal IFT pairs, covering 70 languages and 17+ NLP tasks.\n","authors":["Rishabh Maheshwary","Vikas Yadav","Hoang Nguyen","Khyati Mahajan","Sathwik Tejaswi Madhusudhan"],"pdf_url":"https://arxiv.org/pdf/2406.16783v2.pdf","comment":"39 pages"},{"id":"http://arxiv.org/abs/2406.19800v1","updated":"2024-06-28T10:13:50Z","published":"2024-06-28T10:13:50Z","title":"Modeling the Real World with High-Density Visual Particle Dynamics","summary":"  We present High-Density Visual Particle Dynamics (HD-VPD), a learned world\nmodel that can emulate the physical dynamics of real scenes by processing\nmassive latent point clouds containing 100K+ particles. To enable efficiency at\nthis scale, we introduce a novel family of Point Cloud Transformers (PCTs)\ncalled Interlacers leveraging intertwined linear-attention Performer layers and\ngraph-based neighbour attention layers. We demonstrate the capabilities of\nHD-VPD by modeling the dynamics of high degree-of-freedom bi-manual robots with\ntwo RGB-D cameras. Compared to the previous graph neural network approach, our\nInterlacer dynamics is twice as fast with the same prediction quality, and can\nachieve higher quality using 4x as many particles. We illustrate how HD-VPD can\nevaluate motion plan quality with robotic box pushing and can grasping tasks.\nSee videos and particle dynamics rendered by HD-VPD at\nhttps://sites.google.com/view/hd-vpd.\n","authors":["William F. Whitney","Jacob Varley","Deepali Jain","Krzysztof Choromanski","Sumeet Singh","Vikas Sindhwani"],"pdf_url":"https://arxiv.org/pdf/2406.19800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04268v2","updated":"2024-06-28T09:59:08Z","published":"2023-09-08T11:29:05Z","title":"Optimal Rate of Kernel Regression in Large Dimensions","summary":"  We perform a study on kernel regression for large-dimensional data (where the\nsample size $n$ is polynomially depending on the dimension $d$ of the samples,\ni.e., $n\\asymp d^{\\gamma}$ for some $\\gamma >0$ ). We first build a general\ntool to characterize the upper bound and the minimax lower bound of kernel\nregression for large dimensional data through the Mendelson complexity\n$\\varepsilon_{n}^{2}$ and the metric entropy $\\bar{\\varepsilon}_{n}^{2}$\nrespectively. When the target function falls into the RKHS associated with a\n(general) inner product model defined on $\\mathbb{S}^{d}$, we utilize the new\ntool to show that the minimax rate of the excess risk of kernel regression is\n$n^{-1/2}$ when $n\\asymp d^{\\gamma}$ for $\\gamma =2, 4, 6, 8, \\cdots$. We then\nfurther determine the optimal rate of the excess risk of kernel regression for\nall the $\\gamma>0$ and find that the curve of optimal rate varying along\n$\\gamma$ exhibits several new phenomena including the multiple descent behavior\nand the periodic plateau behavior. As an application, For the neural tangent\nkernel (NTK), we also provide a similar explicit description of the curve of\noptimal rate. As a direct corollary, we know these claims hold for wide neural\nnetworks as well.\n","authors":["Weihao Lu","Haobo Zhang","Yicheng Li","Manyun Xu","Qian Lin"],"pdf_url":"https://arxiv.org/pdf/2309.04268v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03216v4","updated":"2024-06-28T09:55:49Z","published":"2024-02-05T17:26:49Z","title":"BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity\n  Text Embeddings Through Self-Knowledge Distillation","summary":"  In this paper, we present a new embedding model, called M3-Embedding, which\nis distinguished for its versatility in Multi-Linguality, Multi-Functionality,\nand Multi-Granularity. It can support more than 100 working languages, leading\nto new state-of-the-art performances on multi-lingual and cross-lingual\nretrieval tasks. It can simultaneously perform the three common retrieval\nfunctionalities of embedding model: dense retrieval, multi-vector retrieval,\nand sparse retrieval, which provides a unified model foundation for real-world\nIR applications. It is able to process inputs of different granularities,\nspanning from short sentences to long documents of up to 8192 tokens. The\neffective training of M3-Embedding involves the following technical\ncontributions. We propose a novel self-knowledge distillation approach, where\nthe relevance scores from different retrieval functionalities can be integrated\nas the teacher signal to enhance the training quality. We also optimize the\nbatching strategy, enabling a large batch size and high training throughput to\nensure the discriminativeness of embeddings. To the best of our knowledge,\nM3-Embedding is the first embedding model which realizes such a strong\nversatility. The model and code will be publicly available at\nhttps://github.com/FlagOpen/FlagEmbedding.\n","authors":["Jianlv Chen","Shitao Xiao","Peitian Zhang","Kun Luo","Defu Lian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2402.03216v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19792v1","updated":"2024-06-28T09:55:29Z","published":"2024-06-28T09:55:29Z","title":"Improving Performance Prediction of Electrolyte Formulations with\n  Transformer-based Molecular Representation Model","summary":"  Development of efficient and high-performing electrolytes is crucial for\nadvancing energy storage technologies, particularly in batteries. Predicting\nthe performance of battery electrolytes rely on complex interactions between\nthe individual constituents. Consequently, a strategy that adeptly captures\nthese relationships and forms a robust representation of the formulation is\nessential for integrating with machine learning models to predict properties\naccurately. In this paper, we introduce a novel approach leveraging a\ntransformer-based molecular representation model to effectively and efficiently\ncapture the representation of electrolyte formulations. The performance of the\nproposed approach is evaluated on two battery property prediction tasks and the\nresults show superior performance compared to the state-of-the-art methods.\n","authors":["Indra Priyadarsini","Vidushi Sharma","Seiji Takeda","Akihiro Kishimoto","Lisa Hamada","Hajime Shinohara"],"pdf_url":"https://arxiv.org/pdf/2406.19792v1.pdf","comment":"Accepted in ML4LMS Workshop at ICML 2024"},{"id":"http://arxiv.org/abs/2405.15613v2","updated":"2024-06-28T09:22:38Z","published":"2024-05-24T14:58:51Z","title":"Automatic Data Curation for Self-Supervised Learning: A Clustering-Based\n  Approach","summary":"  Self-supervised features are the cornerstone of modern machine learning\nsystems. They are typically pre-trained on data collections whose construction\nand curation typically require extensive human effort. This manual process has\nsome limitations similar to those encountered in supervised learning, e.g., the\ncrowd-sourced selection of data is costly and time-consuming, preventing\nscaling the dataset size. In this work, we consider the problem of automatic\ncuration of high-quality datasets for self-supervised pre-training. We posit\nthat such datasets should be large, diverse and balanced, and propose a\nclustering-based approach for building ones satisfying all these criteria. Our\nmethod involves successive and hierarchical applications of $k$-means on a\nlarge and diverse data repository to obtain clusters that distribute uniformly\namong data concepts, followed by a hierarchical, balanced sampling step from\nthese clusters. Extensive experiments on three different data domains including\nweb-based images, satellite images and text show that features trained on our\nautomatically curated datasets outperform those trained on uncurated data while\nbeing on par or better than ones trained on manually curated data. Code is\navailable at https://github.com/facebookresearch/ssl-data-curation.\n","authors":["Huy V. Vo","Vasil Khalidov","Timothée Darcet","Théo Moutakanni","Nikita Smetanin","Marc Szafraniec","Hugo Touvron","Camille Couprie","Maxime Oquab","Armand Joulin","Hervé Jégou","Patrick Labatut","Piotr Bojanowski"],"pdf_url":"https://arxiv.org/pdf/2405.15613v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19770v1","updated":"2024-06-28T09:17:58Z","published":"2024-06-28T09:17:58Z","title":"Self-Supervised Spatial-Temporal Normality Learning for Time Series\n  Anomaly Detection","summary":"  Time Series Anomaly Detection (TSAD) finds widespread applications across\nvarious domains such as financial markets, industrial production, and\nhealthcare. Its primary objective is to learn the normal patterns of time\nseries data, thereby identifying deviations in test samples. Most existing TSAD\nmethods focus on modeling data from the temporal dimension, while ignoring the\nsemantic information in the spatial dimension. To address this issue, we\nintroduce a novel approach, called Spatial-Temporal Normality learning (STEN).\nSTEN is composed of a sequence Order prediction-based Temporal Normality\nlearning (OTN) module that captures the temporal correlations within sequences,\nand a Distance prediction-based Spatial Normality learning (DSN) module that\nlearns the relative spatial relations between sequences in a feature space. By\nsynthesizing these two modules, STEN learns expressive spatial-temporal\nrepresentations for the normal patterns hidden in the time series data.\nExtensive experiments on five popular TSAD benchmarks show that STEN\nsubstantially outperforms state-of-the-art competing methods. Our code is\navailable at https://github.com/mala-lab/STEN.\n","authors":["Yutong Chen","Hongzuo Xu","Guansong Pang","Hezhe Qiao","Yuan Zhou","Mingsheng Shang"],"pdf_url":"https://arxiv.org/pdf/2406.19770v1.pdf","comment":"18 pages, 4 figures, accepted in ECML PKDD2024"},{"id":"http://arxiv.org/abs/2406.19768v1","updated":"2024-06-28T09:17:51Z","published":"2024-06-28T09:17:51Z","title":"Contextualized Hybrid Ensemble Q-learning: Learning Fast with Control\n  Priors","summary":"  Combining Reinforcement Learning (RL) with a prior controller can yield the\nbest out of two worlds: RL can solve complex nonlinear problems, while the\ncontrol prior ensures safer exploration and speeds up training. Prior work\nlargely blends both components with a fixed weight, neglecting that the RL\nagent's performance varies with the training progress and across regions in the\nstate space. Therefore, we advocate for an adaptive strategy that dynamically\nadjusts the weighting based on the RL agent's current capabilities. We propose\na new adaptive hybrid RL algorithm, Contextualized Hybrid Ensemble Q-learning\n(CHEQ). CHEQ combines three key ingredients: (i) a time-invariant formulation\nof the adaptive hybrid RL problem treating the adaptive weight as a context\nvariable, (ii) a weight adaption mechanism based on the parametric uncertainty\nof a critic ensemble, and (iii) ensemble-based acceleration for data-efficient\nRL. Evaluating CHEQ on a car racing task reveals substantially stronger data\nefficiency, exploration safety, and transferability to unknown scenarios than\nstate-of-the-art adaptive hybrid RL methods.\n","authors":["Emma Cramer","Bernd Frauenknecht","Ramil Sabirov","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2406.19768v1.pdf","comment":"20 pages, 12 figures"},{"id":"http://arxiv.org/abs/2406.10552v3","updated":"2024-06-28T09:16:28Z","published":"2024-06-15T08:13:47Z","title":"Large Language Model Enhanced Clustering for News Event Detection","summary":"  The news landscape is continuously evolving, with an ever-increasing volume\nof information from around the world. Automated event detection within this\nvast data repository is essential for monitoring, identifying, and categorizing\nsignificant news occurrences across diverse platforms. This paper presents an\nevent detection framework that leverages Large Language Models (LLMs) combined\nwith clustering analysis to detect news events from the Global Database of\nEvents, Language, and Tone (GDELT). The framework enhances event clustering\nthrough both pre-event detection tasks (keyword extraction and text embedding)\nand post-event detection tasks (event summarization and topic labelling). We\nalso evaluate the impact of various textual embeddings on the quality of\nclustering outcomes, ensuring robust news categorization. Additionally, we\nintroduce a novel Cluster Stability Assessment Index (CSAI) to assess the\nvalidity and robustness of clustering results. CSAI utilizes multiple feature\nvectors to provide a new way of measuring clustering quality. Our experiments\nindicate that the use of LLM embedding in the event detection framework has\nsignificantly improved the results, demonstrating greater robustness in terms\nof CSAI scores. Moreover, post-event detection tasks generate meaningful\ninsights, facilitating effective interpretation of event clustering results.\nOverall, our experimental results indicate that the proposed framework offers\nvaluable insights and could enhance the accuracy in news analysis and\nreporting.\n","authors":["Adane Nega Tarekegn"],"pdf_url":"https://arxiv.org/pdf/2406.10552v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19765v1","updated":"2024-06-28T09:10:23Z","published":"2024-06-28T09:10:23Z","title":"Systematic Literature Review on Application of Learning-based Approaches\n  in Continuous Integration","summary":"  Context: Machine learning (ML) and deep learning (DL) analyze raw data to\nextract valuable insights in specific phases. The rise of continuous practices\nin software projects emphasizes automating Continuous Integration (CI) with\nthese learning-based methods, while the growing adoption of such approaches\nunderscores the need for systematizing knowledge. Objective: Our objective is\nto comprehensively review and analyze existing literature concerning\nlearning-based methods within the CI domain. We endeavour to identify and\nanalyse various techniques documented in the literature, emphasizing the\nfundamental attributes of training phases within learning-based solutions in\nthe context of CI. Method: We conducted a Systematic Literature Review (SLR)\ninvolving 52 primary studies. Through statistical and thematic analyses, we\nexplored the correlations between CI tasks and the training phases of\nlearning-based methodologies across the selected studies, encompassing a\nspectrum from data engineering techniques to evaluation metrics. Results: This\npaper presents an analysis of the automation of CI tasks utilizing\nlearning-based methods. We identify and analyze nine types of data sources,\nfour steps in data preparation, four feature types, nine subsets of data\nfeatures, five approaches for hyperparameter selection and tuning, and fifteen\nevaluation metrics. Furthermore, we discuss the latest techniques employed,\nexisting gaps in CI task automation, and the characteristics of the utilized\nlearning-based techniques. Conclusion: This study provides a comprehensive\noverview of learning-based methods in CI, offering valuable insights for\nresearchers and practitioners developing CI task automation. It also highlights\nthe need for further research to advance these methods in CI.\n","authors":["Ali Kazemi Arani","Triet Huynh Minh Le","Mansooreh Zahedi","M. Ali Babar"],"pdf_url":"https://arxiv.org/pdf/2406.19765v1.pdf","comment":"This paper has been accepted to be published in IEEE Access"},{"id":"http://arxiv.org/abs/2402.07158v2","updated":"2024-06-28T08:57:39Z","published":"2024-02-11T11:03:08Z","title":"Effort and Size Estimation in Software Projects with Large Language\n  Model-based Intelligent Interfaces","summary":"  The advancement of Large Language Models (LLM) has also resulted in an\nequivalent proliferation in its applications. Software design, being one, has\ngained tremendous benefits in using LLMs as an interface component that extends\nfixed user stories. However, inclusion of LLM-based AI agents in software\ndesign often poses unexpected challenges, especially in the estimation of\ndevelopment efforts. Through the example of UI-based user stories, we provide a\ncomparison against traditional methods and propose a new way to enhance\nspecifications of natural language-based questions that allows for the\nestimation of development effort by taking into account data sources,\ninterfaces and algorithms.\n","authors":["Claudionor N. Coelho Jr","Hanchen Xiong","Tushar Karayil","Sree Koratala","Rex Shang","Jacob Bollinger","Mohamed Shabar","Syam Nair"],"pdf_url":"https://arxiv.org/pdf/2402.07158v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15486v2","updated":"2024-06-28T08:55:17Z","published":"2024-06-17T11:05:15Z","title":"SampleAttention: Near-Lossless Acceleration of Long Context LLM\n  Inference with Adaptive Structured Sparse Attention","summary":"  Large language models (LLMs) now support extremely long context windows, but\nthe quadratic complexity of vanilla attention results in significantly long\nTime-to-First-Token (TTFT) latency. Existing approaches to address this\ncomplexity require additional pretraining or finetuning, and often sacrifice\nmodel accuracy. In this paper, we first provide both theoretical and empirical\nfoundations for near-lossless sparse attention. We find dynamically capturing\nhead-specific sparse patterns at runtime with low overhead is crucial. To\naddress this, we propose SampleAttention, an adaptive structured and\nnear-lossless sparse attention. Leveraging observed significant sparse\npatterns, SampleAttention attends to a fixed percentage of adjacent tokens to\ncapture local window patterns, and employs a two-stage query-guided key-value\nfiltering approach, which adaptively select a minimum set of key-values with\nlow overhead, to capture column stripe patterns. Comprehensive evaluations show\nthat SampleAttention can seamlessly replace vanilla attention in off-the-shelf\nLLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times$\ncompared with FlashAttention.\n","authors":["Qianchao Zhu","Jiangfei Duan","Chang Chen","Siran Liu","Xiuhong Li","Guanyu Feng","Xin Lv","Huanqi Cao","Xiao Chuanfu","Xingcheng Zhang","Dahua Lin","Chao Yang"],"pdf_url":"https://arxiv.org/pdf/2406.15486v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19753v1","updated":"2024-06-28T08:53:33Z","published":"2024-06-28T08:53:33Z","title":"Backdoor Attack in Prompt-Based Continual Learning","summary":"  Prompt-based approaches offer a cutting-edge solution to data privacy issues\nin continual learning, particularly in scenarios involving multiple data\nsuppliers where long-term storage of private user data is prohibited. Despite\ndelivering state-of-the-art performance, its impressive remembering capability\ncan become a double-edged sword, raising security concerns as it might\ninadvertently retain poisoned knowledge injected during learning from private\nuser data. Following this insight, in this paper, we expose continual learning\nto a potential threat: backdoor attack, which drives the model to follow a\ndesired adversarial target whenever a specific trigger is present while still\nperforming normally on clean samples. We highlight three critical challenges in\nexecuting backdoor attacks on incremental learners and propose corresponding\nsolutions: (1) \\emph{Transferability}: We employ a surrogate dataset and\nmanipulate prompt selection to transfer backdoor knowledge to data from other\nsuppliers; (2) \\emph{Resiliency}: We simulate static and dynamic states of the\nvictim to ensure the backdoor trigger remains robust during intense incremental\nlearning processes; and (3) \\emph{Authenticity}: We apply binary cross-entropy\nloss as an anti-cheating factor to prevent the backdoor trigger from devolving\ninto adversarial noise. Extensive experiments across various benchmark datasets\nand continual learners validate our continual backdoor framework, achieving up\nto $100\\%$ attack success rate, with further ablation studies confirming our\ncontributions' effectiveness.\n","authors":["Trang Nguyen","Anh Tran","Nhat Ho"],"pdf_url":"https://arxiv.org/pdf/2406.19753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00532v2","updated":"2024-06-28T08:48:06Z","published":"2024-05-01T14:05:52Z","title":"ULLER: A Unified Language for Learning and Reasoning","summary":"  The field of neuro-symbolic artificial intelligence (NeSy), which combines\nlearning and reasoning, has recently experienced significant growth. There now\nare a wide variety of NeSy frameworks, each with its own specific language for\nexpressing background knowledge and how to relate it to neural networks. This\nheterogeneity hinders accessibility for newcomers and makes comparing different\nNeSy frameworks challenging. We propose a language for NeSy, which we call\nULLER, a Unfied Language for LEarning and Reasoning. ULLER encompasses a wide\nvariety of settings, while ensuring that knowledge described in it can be used\nin existing NeSy systems. ULLER has a first-order logic syntax specialised for\nNeSy for which we provide example semantics including classical FOL, fuzzy\nlogic, and probabilistic logic. We believe ULLER is a first step towards making\nNeSy research more accessible and comparable, paving the way for libraries that\nstreamline training and evaluation across a multitude of semantics, knowledge\nbases, and NeSy systems.\n","authors":["Emile van Krieken","Samy Badreddine","Robin Manhaeve","Eleonora Giunchiglia"],"pdf_url":"https://arxiv.org/pdf/2405.00532v2.pdf","comment":"Accepted at NeSy 2024"},{"id":"http://arxiv.org/abs/2402.13914v2","updated":"2024-06-28T08:37:28Z","published":"2024-02-21T16:30:24Z","title":"Position: Explain to Question not to Justify","summary":"  Explainable Artificial Intelligence (XAI) is a young but very promising field\nof research. Unfortunately, the progress in this field is currently slowed down\nby divergent and incompatible goals. We separate various threads tangled within\nthe area of XAI into two complementary cultures of human/value-oriented\nexplanations (BLUE XAI) and model/validation-oriented explanations (RED XAI).\nThis position paper argues that the area of RED XAI is currently\nunder-explored, i.e., more methods for explainability are desperately needed to\nquestion models (e.g., extract knowledge from well-performing models as well as\nspotting and fixing bugs in faulty models), and the area of RED XAI hides great\nopportunities and potential for important research necessary to ensure the\nsafety of AI systems. We conclude this paper by presenting promising challenges\nin this area.\n","authors":["Przemyslaw Biecek","Wojciech Samek"],"pdf_url":"https://arxiv.org/pdf/2402.13914v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06560v2","updated":"2024-06-28T08:35:58Z","published":"2023-12-11T17:45:10Z","title":"Automatic Regularization for Linear MMSE Filters","summary":"  In this work, we consider the problem of regularization in the design of\nminimum mean square error (MMSE) linear filters. Using the relationship with\nstatistical machine learning methods, using a Bayesian approach, the\nregularization parameter is found from the observed signals in a simple and\nautomatic manner. The proposed approach is illustrated in system identification\nand beamforming examples, where the automatic regularization is shown to yield\nnear-optimal results.\n","authors":["Daniel Gomes de Pinho Zanco","Leszek Szczecinski","Jacob Benesty"],"pdf_url":"https://arxiv.org/pdf/2312.06560v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19738v1","updated":"2024-06-28T08:26:47Z","published":"2024-06-28T08:26:47Z","title":"Classical Bandit Algorithms for Entanglement Detection in Parameterized\n  Qubit States","summary":"  Entanglement is a key resource for a wide range of tasks in quantum\ninformation and computing. Thus, verifying availability of this quantum\nresource is essential. Extensive research on entanglement detection has led to\nno-go theorems (Lu et al. [Phys. Rev. Lett., 116, 230501 (2016)]) that\nhighlight the need for full state tomography (FST) in the absence of adaptive\nor joint measurements. Recent advancements, as proposed by Zhu, Teo, and\nEnglert [Phys. Rev. A, 81, 052339, 2010], introduce a single-parameter family\nof entanglement witness measurements which are capable of conclusively\ndetecting certain entangled states and only resort to FST when all witness\nmeasurements are inconclusive. We find a variety of realistic noisy two-qubit\nquantum states $\\mathcal{F}$ that yield conclusive results under this witness\nfamily. We solve the problem of detecting entanglement among $K$ quantum states\nin $\\mathcal{F}$, of which $m$ states are entangled, with $m$ potentially\nunknown. We recognize a structural connection of this problem to the Bad Arm\nIdentification problem in stochastic Multi-Armed Bandits (MAB). In contrast to\nexisting quantum bandit frameworks, we establish a new correspondence tailored\nfor entanglement detection and term it the $(m,K)$-quantum Multi-Armed Bandit.\nWe implement two well-known MAB policies for arbitrary states derived from\n$\\mathcal{F}$, present theoretical guarantees on the measurement/sample\ncomplexity and demonstrate the practicality of the policies through numerical\nsimulations. More broadly, this paper highlights the potential for employing\nclassical machine learning techniques for quantum entanglement detection.\n","authors":["Bharati. K","Vikesh Siddhu","Krishna Jagannathan"],"pdf_url":"https://arxiv.org/pdf/2406.19738v1.pdf","comment":"20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.19736v1","updated":"2024-06-28T08:25:27Z","published":"2024-06-28T08:25:27Z","title":"MM-Instruct: Generated Visual Instructions for Large Multimodal Model\n  Alignment","summary":"  This paper introduces MM-Instruct, a large-scale dataset of diverse and\nhigh-quality visual instruction data designed to enhance the\ninstruction-following capabilities of large multimodal models (LMMs). While\nexisting visual instruction datasets often focus on question-answering, they\nstruggle to generalize to broader application scenarios such as creative\nwriting, summarization, or image analysis. To address these limitations, we\npropose a novel approach to constructing MM-Instruct that leverages the strong\ninstruction-following capabilities of existing LLMs to generate novel visual\ninstruction data from large-scale but conventional image captioning datasets.\nMM-Instruct first leverages ChatGPT to automatically generate diverse\ninstructions from a small set of seed instructions through augmenting and\nsummarization. It then matches these instructions with images and uses an\nopen-sourced large language model (LLM) to generate coherent answers to the\ninstruction-image pairs. The LLM is grounded by the detailed text descriptions\nof images in the whole answer generation process to guarantee the alignment of\nthe instruction data. Moreover, we introduce a benchmark based on the generated\ninstruction data to evaluate the instruction-following capabilities of existing\nLMMs. We demonstrate the effectiveness of MM-Instruct by training a LLaVA-1.5\nmodel on the generated data, denoted as LLaVA-Instruct, which exhibits\nsignificant improvements in instruction-following capabilities compared to\nLLaVA-1.5 models. The MM-Instruct dataset, benchmark, and pre-trained models\nare available at https://github.com/jihaonew/MM-Instruct.\n","authors":["Jihao Liu","Xin Huang","Jinliang Zheng","Boxiao Liu","Jia Wang","Osamu Yoshie","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2406.19736v1.pdf","comment":"Dataset and models are available at\n  https://github.com/jihaonew/MM-Instruct"},{"id":"http://arxiv.org/abs/2307.10635v3","updated":"2024-06-28T08:24:13Z","published":"2023-07-20T07:01:57Z","title":"SciBench: Evaluating College-Level Scientific Problem-Solving Abilities\n  of Large Language Models","summary":"  Most of the existing Large Language Model (LLM) benchmarks on scientific\nproblem reasoning focus on problems grounded in high-school subjects and are\nconfined to elementary algebraic operations. To systematically examine the\nreasoning capabilities required for solving complex scientific problems, we\nintroduce an expansive benchmark suite SciBench for LLMs. SciBench contains a\ncarefully curated dataset featuring a range of collegiate-level scientific\nproblems from mathematics, chemistry, and physics domains. Based on the\ndataset, we conduct an in-depth benchmarking study of representative\nopen-source and proprietary LLMs with various prompting strategies. The results\nreveal that the current LLMs fall short of delivering satisfactory performance,\nwith the best overall score of merely 43.22%. Furthermore, through a detailed\nuser study, we categorize the errors made by LLMs into ten problem-solving\nabilities. Our analysis indicates that no single prompting strategy\nsignificantly outperforms the others and some strategies that demonstrate\nimprovements in certain problem-solving skills could result in declines in\nother skills. We envision that SciBench will catalyze further developments in\nthe reasoning abilities of LLMs, thereby ultimately contributing to scientific\nresearch and discovery.\n","authors":["Xiaoxuan Wang","Ziniu Hu","Pan Lu","Yanqiao Zhu","Jieyu Zhang","Satyen Subramaniam","Arjun R. Loomba","Shichang Zhang","Yizhou Sun","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2307.10635v3.pdf","comment":"To appear at ICML 2024"},{"id":"http://arxiv.org/abs/2402.08114v2","updated":"2024-06-28T08:22:01Z","published":"2024-02-12T23:09:00Z","title":"Active Preference Learning for Large Language Models","summary":"  As large language models (LLMs) become more capable, fine-tuning techniques\nfor aligning with human intent are increasingly important. A key consideration\nfor aligning these models is how to most effectively use human resources, or\nmodel resources in the case where LLMs themselves are used as oracles.\nReinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most\nprominent example of such a technique, but is complex and often unstable.\nDirect Preference Optimization (DPO) has recently been proposed as a simpler\nand more stable alternative. In this work, we develop an active learning\nstrategy for DPO to make better use of preference labels. We propose a\npractical acquisition function for prompt/completion pairs based on the\npredictive entropy of the language model and a measure of certainty of the\nimplicit preference model optimized by DPO. We demonstrate how our approach\nimproves both the rate of learning and final performance of fine-tuning on\npairwise preference data.\n","authors":["William Muldrew","Peter Hayes","Mingtian Zhang","David Barber"],"pdf_url":"https://arxiv.org/pdf/2402.08114v2.pdf","comment":"13 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.19726v1","updated":"2024-06-28T08:16:54Z","published":"2024-06-28T08:16:54Z","title":"EPOCH: Jointly Estimating the 3D Pose of Cameras and Humans","summary":"  Monocular Human Pose Estimation (HPE) aims at determining the 3D positions of\nhuman joints from a single 2D image captured by a camera. However, a single 2D\npoint in the image may correspond to multiple points in 3D space. Typically,\nthe uniqueness of the 2D-3D relationship is approximated using an orthographic\nor weak-perspective camera model. In this study, instead of relying on\napproximations, we advocate for utilizing the full perspective camera model.\nThis involves estimating camera parameters and establishing a precise,\nunambiguous 2D-3D relationship. To do so, we introduce the EPOCH framework,\ncomprising two main components: the pose lifter network (LiftNet) and the pose\nregressor network (RegNet). LiftNet utilizes the full perspective camera model\nto precisely estimate the 3D pose in an unsupervised manner. It takes a 2D pose\nand camera parameters as inputs and produces the corresponding 3D pose\nestimation. These inputs are obtained from RegNet, which starts from a single\nimage and provides estimates for the 2D pose and camera parameters. RegNet\nutilizes only 2D pose data as weak supervision. Internally, RegNet predicts a\n3D pose, which is then projected to 2D using the estimated camera parameters.\nThis process enables RegNet to establish the unambiguous 2D-3D relationship.\nOur experiments show that modeling the lifting as an unsupervised task with a\ncamera in-the-loop results in better generalization to unseen data. We obtain\nstate-of-the-art results for the 3D HPE on the Human3.6M and MPI-INF-3DHP\ndatasets. Our code is available at: [Github link upon acceptance, see\nsupplementary materials].\n","authors":["Nicola Garau","Giulia Martinelli","Niccolò Bisagno","Denis Tomè","Carsten Stoll"],"pdf_url":"https://arxiv.org/pdf/2406.19726v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.19714v1","updated":"2024-06-28T07:56:35Z","published":"2024-06-28T07:56:35Z","title":"State Matching and Multiple References in Adaptive Active Automata\n  Learning","summary":"  Active automata learning (AAL) is a method to infer state machines by\ninteracting with black-box systems. Adaptive AAL aims to reduce the sample\ncomplexity of AAL by incorporating domain specific knowledge in the form of\n(similar) reference models. Such reference models appear naturally when\nlearning multiple versions or variants of a software system. In this paper, we\npresent state matching, which allows flexible use of the structure of these\nreference models by the learner. State matching is the main ingredient of\nadaptive L#, a novel framework for adaptive learning, built on top of L#. Our\nempirical evaluation shows that adaptive L# improves the state of the art by up\nto two orders of magnitude.\n","authors":["Loes Kruger","Sebastian Junges","Jurriaan Rot"],"pdf_url":"https://arxiv.org/pdf/2406.19714v1.pdf","comment":"Extended paper for FM 2024"},{"id":"http://arxiv.org/abs/2406.01124v3","updated":"2024-06-28T07:54:19Z","published":"2024-06-03T09:10:42Z","title":"Latent Logic Tree Extraction for Event Sequence Explanation from LLMs","summary":"  Modern high-stakes systems, such as healthcare or robotics, often generate\nvast streaming event sequences. Our goal is to design an efficient,\nplug-and-play tool to elicit logic tree-based explanations from Large Language\nModels (LLMs) to provide customized insights into each observed event sequence.\nBuilt on the temporal point process model for events, our method employs the\nlikelihood function as a score to evaluate generated logic trees. We propose an\namortized Expectation-Maximization (EM) learning framework and treat the logic\ntree as latent variables. In the E-step, we evaluate the posterior distribution\nover the latent logic trees using an LLM prior and the likelihood of the\nobserved event sequences. LLM provides a high-quality prior for the latent\nlogic trees, however, since the posterior is built over a discrete\ncombinatorial space, we cannot get the closed-form solution. We propose to\ngenerate logic tree samples from the posterior using a learnable GFlowNet,\nwhich is a diversity-seeking generator for structured discrete variables. The\nM-step employs the generated logic rules to approximate marginalization over\nthe posterior, facilitating the learning of model parameters and refining the\ntunable LLM prior parameters. In the online setting, our locally built,\nlightweight model will iteratively extract the most relevant rules from LLMs\nfor each sequence using only a few iterations. Empirical demonstrations\nshowcase the promising performance and adaptability of our framework.\n","authors":["Zitao Song","Chao Yang","Chaojie Wang","Bo An","Shuang Li"],"pdf_url":"https://arxiv.org/pdf/2406.01124v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09904v2","updated":"2024-06-28T07:53:12Z","published":"2024-06-14T10:23:45Z","title":"QQQ: Quality Quattuor-Bit Quantization for Large Language Models","summary":"  Quantization is a proven effective method for compressing large language\nmodels. Although popular techniques like W8A8 and W4A16 effectively maintain\nmodel performance, they often fail to concurrently speed up the prefill and\ndecoding stages of inference. W4A8 is a promising strategy to accelerate both\nof them while usually leads to a significant performance degradation. To\naddress these issues, we present QQQ, a Quality Quattuor-bit Quantization\nmethod with 4-bit weights and 8-bit activations. QQQ employs adaptive smoothing\nand Hessian-based compensation, significantly enhancing the performance of\nquantized models without extensive training. Furthermore, we meticulously\nengineer W4A8 GEMM kernels to increase inference speed. Our specialized\nper-channel W4A8 GEMM and per-group W4A8 GEMM achieve impressive speed\nincreases of 3.67$\\times$ and 3.29 $\\times$ over FP16 GEMM. Our extensive\nexperiments show that QQQ achieves performance on par with existing\nstate-of-the-art LLM quantization methods while significantly accelerating\ninference, achieving speed boosts up to 2.24 $\\times$, 2.10$\\times$, and\n1.25$\\times$ compared to FP16, W8A8, and W4A16, respectively.\n","authors":["Ying Zhang","Peng Zhang","Mincong Huang","Jingyang Xiang","Yujie Wang","Chao Wang","Yineng Zhang","Lei Yu","Chuan Liu","Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2406.09904v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19711v1","updated":"2024-06-28T07:46:51Z","published":"2024-06-28T07:46:51Z","title":"CHASE: A Causal Heterogeneous Graph based Framework for Root Cause\n  Analysis in Multimodal Microservice Systems","summary":"  In recent years, the widespread adoption of distributed microservice\narchitectures within the industry has significantly increased the demand for\nenhanced system availability and robustness. Due to the complex service\ninvocation paths and dependencies at enterprise-level microservice systems, it\nis challenging to locate the anomalies promptly during service invocations,\nthus causing intractable issues for normal system operations and maintenance.\nIn this paper, we propose a Causal Heterogeneous grAph baSed framEwork for root\ncause analysis, namely CHASE, for microservice systems with multimodal data,\nincluding traces, logs, and system monitoring metrics. Specifically, related\ninformation is encoded into representative embeddings and further modeled by a\nmultimodal invocation graph. Following that, anomaly detection is performed on\neach instance node with attentive heterogeneous message passing from its\nadjacent metric and log nodes. Finally, CHASE learns from the constructed\nhypergraph with hyperedges representing the flow of causality and performs root\ncause localization. We evaluate the proposed framework on two public\nmicroservice datasets with distinct attributes and compare with the\nstate-of-the-art methods. The results show that CHASE achieves the average\nperformance gain up to 36.2%(A@1) and 29.4%(Percentage@1), respectively to its\nbest counterpart.\n","authors":["Ziming Zhao","Tiehua Zhang","Zhishu Shen","Hai Dong","Xingjun Ma","Xianhui Liu","Yun Yang"],"pdf_url":"https://arxiv.org/pdf/2406.19711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19707v1","updated":"2024-06-28T07:41:26Z","published":"2024-06-28T07:41:26Z","title":"InfiniGen: Efficient Generative Inference of Large Language Models with\n  Dynamic KV Cache Management","summary":"  Transformer-based large language models (LLMs) demonstrate impressive\nperformance across various natural language processing tasks. Serving LLM\ninference for generating long contents, however, poses a challenge due to the\nenormous memory footprint of the transient state, known as the key-value (KV)\ncache, which scales with the sequence length and batch size. In this paper, we\npresent InfiniGen, a novel KV cache management framework tailored for long-text\ngeneration, which synergistically works with modern offloading-based inference\nsystems. InfiniGen leverages the key insight that a few important tokens that\nare essential for computing the subsequent attention layer in the Transformer\ncan be speculated by performing a minimal rehearsal with the inputs of the\ncurrent layer and part of the query weight and key cache of the subsequent\nlayer. This allows us to prefetch only the essential KV cache entries (without\nfetching them all), thereby mitigating the fetch overhead from the host memory\nin offloading-based LLM serving systems. Our evaluation on several\nrepresentative LLMs shows that InfiniGen improves the overall performance of a\nmodern offloading-based system by up to 3.00x compared to prior KV cache\nmanagement methods while offering substantially better model accuracy.\n","authors":["Wonbeom Lee","Jungi Lee","Junghwan Seo","Jaewoong Sim"],"pdf_url":"https://arxiv.org/pdf/2406.19707v1.pdf","comment":"OSDI 2024"},{"id":"http://arxiv.org/abs/2406.12569v2","updated":"2024-06-28T07:23:16Z","published":"2024-06-18T12:57:33Z","title":"MOYU: A Theoretical Study on Massive Over-activation Yielded Uplifts in\n  LLMs","summary":"  Massive Over-activation Yielded Uplifts(MOYU) is an inherent property of\nlarge language models, and dynamic activation(DA) based on the MOYU property is\na clever yet under-explored strategy designed to accelerate inference in these\nmodels. Existing methods that utilize MOYU often face a significant 'Impossible\nTrinity': struggling to simultaneously maintain model performance, enhance\ninference speed, and extend applicability across various architectures. Due to\nthe theoretical ambiguities surrounding MOYU, this paper elucidates the root\ncause of the MOYU property and outlines the mechanisms behind two primary\nlimitations encountered by current DA methods: 1) history-related activation\nuncertainty, and 2) semantic-irrelevant activation inertia. Our analysis not\nonly underscores the limitations of current dynamic activation strategies\nwithin large-scale LLaMA models but also proposes opportunities for refining\nthe design of future sparsity schemes.\n","authors":["Chi Ma","Mincong Huang","Chao Wang","Yujie Wang","Lei Yu"],"pdf_url":"https://arxiv.org/pdf/2406.12569v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11622v2","updated":"2024-06-28T07:20:22Z","published":"2024-02-18T15:28:39Z","title":"Logical Closed Loop: Uncovering Object Hallucinations in Large\n  Vision-Language Models","summary":"  Object hallucination has been an Achilles' heel which hinders the broader\napplications of large vision-language models (LVLMs). Object hallucination\nrefers to the phenomenon that the LVLMs claim non-existent objects in the\nimage. To mitigate the object hallucinations, instruction tuning and external\nmodel-based detection methods have been proposed, which either require\nlarge-scare computational resources or depend on the detection result of\nexternal models. However, there remains an under-explored field to utilize the\nLVLM itself to alleviate object hallucinations. In this work, we adopt the\nintuition that the LVLM tends to respond logically consistently for existent\nobjects but inconsistently for hallucinated objects. Therefore, we propose a\nLogical Closed Loop-based framework for Object Hallucination Detection and\nMitigation, namely LogicCheckGPT. In specific, we devise logical consistency\nprobing to raise questions with logical correlations, inquiring about\nattributes from objects and vice versa. Whether their responses can form a\nlogical closed loop serves as an indicator of object hallucination. As a\nplug-and-play method, it can be seamlessly applied to all existing LVLMs.\nComprehensive experiments conducted on three benchmarks across four LVLMs have\ndemonstrated significant improvements brought by our method, indicating its\neffectiveness and generality.\n","authors":["Junfei Wu","Qiang Liu","Ding Wang","Jinghao Zhang","Shu Wu","Liang Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2402.11622v2.pdf","comment":"Accept to ACL 2024; 19 Pages, 15 Figures, 6 Tables"},{"id":"http://arxiv.org/abs/2401.11447v4","updated":"2024-06-28T06:53:51Z","published":"2024-01-21T09:55:47Z","title":"Sequential Model for Predicting Patient Adherence in Subcutaneous\n  Immunotherapy for Allergic Rhinitis","summary":"  Objective: Subcutaneous Immunotherapy (SCIT) is the long-lasting causal\ntreatment of allergic rhinitis (AR). How to enhance the adherence of patients\nto maximize the benefit of allergen immunotherapy (AIT) plays a crucial role in\nthe management of AIT. This study aims to leverage novel machine learning\nmodels to precisely predict the risk of non-adherence of AR patients and\nrelated local symptom scores in three years SCIT.\n  Methods: The research develops and analyzes two models, sequential\nlatent-variable model (SLVM) of Sequential Latent Actor-Critic (SLAC) and Long\nShort-Term Memory (LSTM) evaluating them based on scoring and adherence\nprediction capabilities.\n  Results: Excluding the biased samples at the first time step, the predictive\nadherence accuracy of the SLAC models is from 60\\% to 72\\%, and for LSTM\nmodels, it is 66\\% to 84\\%, varying according to the time steps. The range of\nRoot Mean Square Error (RMSE) for SLAC models is between 0.93 and 2.22, while\nfor LSTM models it is between 1.09 and 1.77. Notably, these RMSEs are\nsignificantly lower than the random prediction error of 4.55.\n  Conclusion: We creatively apply sequential models in the long-term management\nof SCIT with promising accuracy in the prediction of SCIT nonadherence in AR\npatients. While LSTM outperforms SLAC in adherence prediction, SLAC excels in\nscore prediction for patients undergoing SCIT for AR. The state-action-based\nSLAC adds flexibility, presenting a novel and effective approach for managing\nlong-term AIT.\n","authors":["Yin Li","Yu Xiong","Wenxin Fan","Kai Wang","Qingqing Yu","Liping Si","Patrick van der Smagt","Jun Tang","Nutan Chen"],"pdf_url":"https://arxiv.org/pdf/2401.11447v4.pdf","comment":"Frontiers in Pharmacology, research topic: Methods and Metrics to\n  Measure Medication Adherence"},{"id":"http://arxiv.org/abs/2406.17963v2","updated":"2024-06-28T06:44:45Z","published":"2024-06-25T22:44:53Z","title":"Empowering Interdisciplinary Insights with Dynamic Graph Embedding\n  Trajectories","summary":"  We developed DyGETViz, a novel framework for effectively visualizing dynamic\ngraphs (DGs) that are ubiquitous across diverse real-world systems. This\nframework leverages recent advancements in discrete-time dynamic graph (DTDG)\nmodels to adeptly handle the temporal dynamics inherent in dynamic graphs.\nDyGETViz effectively captures both micro- and macro-level structural shifts\nwithin these graphs, offering a robust method for representing complex and\nmassive dynamic graphs. The application of DyGETViz extends to a diverse array\nof domains, including ethology, epidemiology, finance, genetics, linguistics,\ncommunication studies, social studies, and international relations. Through its\nimplementation, DyGETViz has revealed or confirmed various critical insights.\nThese include the diversity of content sharing patterns and the degree of\nspecialization within online communities, the chronological evolution of\nlexicons across decades, and the distinct trajectories exhibited by\naging-related and non-related genes. Importantly, DyGETViz enhances the\naccessibility of scientific findings to non-domain experts by simplifying the\ncomplexities of dynamic graphs. Our framework is released as an open-source\nPython package for use across diverse disciplines. Our work not only addresses\nthe ongoing challenges in visualizing and analyzing DTDG models but also\nestablishes a foundational framework for future investigations into dynamic\ngraph representation and analysis across various disciplines.\n","authors":["Yiqiao Jin","Andrew Zhao","Yeon-Chang Lee","Meng Ye","Ajay Divakaran","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2406.17963v2.pdf","comment":"27 pages, 11 figures"},{"id":"http://arxiv.org/abs/2406.19674v1","updated":"2024-06-28T06:22:23Z","published":"2024-06-28T06:22:23Z","title":"Less is More: Accurate Speech Recognition & Translation without\n  Web-Scale Data","summary":"  Recent advances in speech recognition and translation rely on hundreds of\nthousands of hours of Internet speech data. We argue that state-of-the art\naccuracy can be reached without relying on web-scale data. Canary -\nmultilingual ASR and speech translation model, outperforms current\nstate-of-the-art models - Whisper, OWSM, and Seamless-M4T on English, French,\nSpanish, and German languages, while being trained on an order of magnitude\nless data than these models. Three key factors enables such data-efficient\nmodel: (1) a FastConformer-based attention encoder-decoder architecture (2)\ntraining on synthetic data generated with machine translation and (3) advanced\ntraining techniques: data-balancing, dynamic data blending, dynamic bucketing\nand noise-robust fine-tuning. The model, weights, and training code will be\nopen-sourced.\n","authors":["Krishna C. Puvvada","Piotr Żelasko","He Huang","Oleksii Hrinchuk","Nithin Rao Koluguri","Kunal Dhawan","Somshubra Majumdar","Elena Rastorgueva","Zhehuai Chen","Vitaly Lavrukhin","Jagadeesh Balam","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2406.19674v1.pdf","comment":"Accepted at Interspeech-2024"},{"id":"http://arxiv.org/abs/2406.17051v2","updated":"2024-06-28T05:50:11Z","published":"2024-06-24T18:13:09Z","title":"Leveraging Knowledge Distillation for Lightweight Skin Cancer\n  Classification: Balancing Accuracy and Computational Efficiency","summary":"  Skin cancer is a major concern to public health, accounting for one-third of\nthe reported cancers. If not detected early, the cancer has the potential for\nsevere consequences. Recognizing the critical need for effective skin cancer\nclassification, we address the limitations of existing models, which are often\ntoo large to deploy in areas with limited computational resources. In response,\nwe present a knowledge distillation based approach for creating a lightweight\nyet high-performing classifier. The proposed solution involves fusing three\nmodels, namely ResNet152V2, ConvNeXtBase, and ViT Base, to create an effective\nteacher model. The teacher model is then employed to guide a lightweight\nstudent model of size 2.03 MB. This student model is further compressed to\n469.77 KB using 16-bit quantization, enabling smooth incorporation into edge\ndevices. With six-stage image preprocessing, data augmentation, and a rigorous\nablation study, the model achieves an impressive accuracy of 98.75% on the\nHAM10000 dataset and 98.94% on the Kaggle dataset in classifying benign and\nmalignant skin cancers. With its high accuracy and compact size, our model\nappears to be a potential choice for accurate skin cancer classification,\nparticularly in resource-constrained settings.\n","authors":["Niful Islam","Khan Md Hasib","Fahmida Akter Joti","Asif Karim","Sami Azam"],"pdf_url":"https://arxiv.org/pdf/2406.17051v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19670v1","updated":"2024-06-28T05:44:47Z","published":"2024-06-28T05:44:47Z","title":"Function+Data Flow: A Framework to Specify Machine Learning Pipelines\n  for Digital Twinning","summary":"  The development of digital twins (DTs) for physical systems increasingly\nleverages artificial intelligence (AI), particularly for combining data from\ndifferent sources or for creating computationally efficient, reduced-dimension\nmodels. Indeed, even in very different application domains, twinning employs\ncommon techniques such as model order reduction and modelization with hybrid\ndata (that is, data sourced from both physics-based models and sensors).\nDespite this apparent generality, current development practices are ad-hoc,\nmaking the design of AI pipelines for digital twinning complex and\ntime-consuming. Here we propose Function+Data Flow (FDF), a domain-specific\nlanguage (DSL) to describe AI pipelines within DTs. FDF aims to facilitate the\ndesign and validation of digital twins. Specifically, FDF treats functions as\nfirst-class citizens, enabling effective manipulation of models learned with\nAI. We illustrate the benefits of FDF on two concrete use cases from different\ndomains: predicting the plastic strain of a structure and modeling the\nelectromagnetic behavior of a bearing.\n","authors":["Eduardo de Conto","Blaise Genest","Arvind Easwaran"],"pdf_url":"https://arxiv.org/pdf/2406.19670v1.pdf","comment":"10 pages, 5 figures, to be published in AIware'24"},{"id":"http://arxiv.org/abs/2406.19237v2","updated":"2024-06-28T05:43:46Z","published":"2024-06-27T15:01:48Z","title":"FlowVQA: Mapping Multimodal Logic in Visual Question Answering with\n  Flowcharts","summary":"  Existing benchmarks for visual question answering lack in visual grounding\nand complexity, particularly in evaluating spatial reasoning skills. We\nintroduce FlowVQA, a novel benchmark aimed at assessing the capabilities of\nvisual question-answering multimodal language models in reasoning with\nflowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and\nhuman-verified flowchart images from three distinct content sources, along with\n22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,\nincluding information localization, decision-making, and logical progression.\nWe conduct a thorough baseline evaluation on a suite of both open-source and\nproprietary multimodal language models using various strategies, followed by an\nanalysis of directional bias. The results underscore the benchmark's potential\nas a vital tool for advancing the field of multimodal modeling, providing a\nfocused and challenging environment for enhancing model performance in visual\nand logical reasoning tasks.\n","authors":["Shubhankar Singh","Purvi Chaurasia","Yerram Varun","Pranshu Pandya","Vatsal Gupta","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2406.19237v2.pdf","comment":"Accepted in ACL 2024 (Findings), 21 pages, 7 figures, 9 Tables"},{"id":"http://arxiv.org/abs/2406.11741v3","updated":"2024-06-28T05:28:27Z","published":"2024-06-17T17:00:52Z","title":"Transcendence: Generative Models Can Outperform The Experts That Train\n  Them","summary":"  Generative models are trained with the simple objective of imitating the\nconditional probability distribution induced by the data they are trained on.\nTherefore, when trained on data generated by humans, we may not expect the\nartificial model to outperform the humans on their original objectives. In this\nwork, we study the phenomenon of transcendence: when a generative model\nachieves capabilities that surpass the abilities of the experts generating its\ndata. We demonstrate transcendence by training an autoregressive transformer to\nplay chess from game transcripts, and show that the trained model can sometimes\nachieve better performance than all players in the dataset. We theoretically\nprove that transcendence can be enabled by low-temperature sampling, and\nrigorously assess this claim experimentally. Finally, we discuss other sources\nof transcendence, laying the groundwork for future investigation of this\nphenomenon in a broader setting.\n","authors":["Edwin Zhang","Vincent Zhu","Naomi Saphra","Anat Kleiman","Benjamin L. Edelman","Milind Tambe","Sham M. Kakade","Eran Malach"],"pdf_url":"https://arxiv.org/pdf/2406.11741v3.pdf","comment":"Code, models, and data at https://transcendence.eddie.win"},{"id":"http://arxiv.org/abs/2406.19662v1","updated":"2024-06-28T05:13:43Z","published":"2024-06-28T05:13:43Z","title":"Finite basis Kolmogorov-Arnold networks: domain decomposition for\n  data-driven and physics-informed problems","summary":"  Kolmogorov-Arnold networks (KANs) have attracted attention recently as an\nalternative to multilayer perceptrons (MLPs) for scientific machine learning.\nHowever, KANs can be expensive to train, even for relatively small networks.\nInspired by finite basis physics-informed neural networks (FBPINNs), in this\nwork, we develop a domain decomposition method for KANs that allows for several\nsmall KANs to be trained in parallel to give accurate solutions for multiscale\nproblems. We show that finite basis KANs (FBKANs) can provide accurate results\nwith noisy data and for physics-informed training.\n","authors":["Amanda A. Howard","Bruno Jacob","Sarah H. Murphy","Alexander Heinlein","Panos Stinis"],"pdf_url":"https://arxiv.org/pdf/2406.19662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19657v1","updated":"2024-06-28T04:56:53Z","published":"2024-06-28T04:56:53Z","title":"LLMEasyQuant -- An Easy to Use Toolkit for LLM Quantization","summary":"  Currently, there are many quantization methods appeared for LLM quantization,\nyet few are user-friendly and easy to be deployed locally. Packages like\nTensorRT and Quantohave many underlying structures and self-invoking internal\nfunctions, which are not conducive to developers' personalized development and\nlearning for deployment. Therefore, we develop LLMEasyQuant, it is a package\naiming to for easy quantization deployment which is user-friendly and suitable\nfor beginners' learning.\n","authors":["Dong Liu","Meng Jiang","Kaiser Pister"],"pdf_url":"https://arxiv.org/pdf/2406.19657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19653v1","updated":"2024-06-28T04:48:05Z","published":"2024-06-28T04:48:05Z","title":"ACES: Automatic Cohort Extraction System for Event-Stream Datasets","summary":"  Reproducibility remains a significant challenge in machine learning (ML) for\nhealthcare. In this field, datasets, model pipelines, and even task/cohort\ndefinitions are often private, leading to a significant barrier in sharing,\niterating, and understanding ML results on electronic health record (EHR)\ndatasets. In this paper, we address a significant part of this problem by\nintroducing the Automatic Cohort Extraction System for Event-Stream Datasets\n(ACES). This tool is designed to simultaneously simplify the development of\ntask/cohorts for ML in healthcare and enable the reproduction of these cohorts,\nboth at an exact level for single datasets and at a conceptual level across\ndatasets. To accomplish this, ACES provides (1) a highly intuitive and\nexpressive configuration language for defining both dataset-specific concepts\nand dataset-agnostic inclusion/exclusion criteria, and (2) a pipeline to\nautomatically extract patient records that meet these defined criteria from\nreal-world data. ACES can be automatically applied to any dataset in either the\nMedical Event Data Standard (MEDS) or EventStreamGPT (ESGPT) formats, or to\n*any* dataset for which the necessary task-specific predicates can be extracted\nin an event-stream form. ACES has the potential to significantly lower the\nbarrier to entry for defining ML tasks, redefine the way researchers interact\nwith EHR datasets, and significantly improve the state of reproducibility for\nML studies in this modality. ACES is available at\nhttps://github.com/justin13601/aces.\n","authors":["Justin Xu","Jack Gallifant","Alistair E. W. Johnson","Matthew B. A. McDermott"],"pdf_url":"https://arxiv.org/pdf/2406.19653v1.pdf","comment":"For ACES Online Documentation, see\n  https://eventstreamaces.readthedocs.io/en/latest/"},{"id":"http://arxiv.org/abs/2403.03181v2","updated":"2024-06-28T04:15:33Z","published":"2024-03-05T18:19:29Z","title":"Behavior Generation with Latent Actions","summary":"  Generative modeling of complex behaviors from labeled datasets has been a\nlongstanding problem in decision making. Unlike language or image generation,\ndecision making requires modeling actions - continuous-valued vectors that are\nmultimodal in their distribution, potentially drawn from uncurated sources,\nwhere generation errors can compound in sequential prediction. A recent class\nof models called Behavior Transformers (BeT) addresses this by discretizing\nactions using k-means clustering to capture different modes. However, k-means\nstruggles to scale for high-dimensional action spaces or long sequences, and\nlacks gradient information, and thus BeT suffers in modeling long-range\nactions. In this work, we present Vector-Quantized Behavior Transformer\n(VQ-BeT), a versatile model for behavior generation that handles multimodal\naction prediction, conditional generation, and partial observations. VQ-BeT\naugments BeT by tokenizing continuous actions with a hierarchical vector\nquantization module. Across seven environments including simulated\nmanipulation, autonomous driving, and robotics, VQ-BeT improves on\nstate-of-the-art models such as BeT and Diffusion Policies. Importantly, we\ndemonstrate VQ-BeT's improved ability to capture behavior modes while\naccelerating inference speed 5x over Diffusion Policies. Videos and code can be\nfound https://sjlee.cc/vq-bet\n","authors":["Seungjae Lee","Yibin Wang","Haritheja Etukuru","H. Jin Kim","Nur Muhammad Mahi Shafiullah","Lerrel Pinto"],"pdf_url":"https://arxiv.org/pdf/2403.03181v2.pdf","comment":"Github repo: https://github.com/jayLEE0301/vq_bet_official"},{"id":"http://arxiv.org/abs/2406.19642v1","updated":"2024-06-28T04:14:35Z","published":"2024-06-28T04:14:35Z","title":"IDT: Dual-Task Adversarial Attacks for Privacy Protection","summary":"  Natural language processing (NLP) models may leak private information in\ndifferent ways, including membership inference, reconstruction or attribute\ninference attacks. Sensitive information may not be explicit in the text, but\nhidden in underlying writing characteristics. Methods to protect privacy can\ninvolve using representations inside models that are demonstrated not to detect\nsensitive attributes or -- for instance, in cases where users might not trust a\nmodel, the sort of scenario of interest here -- changing the raw text before\nmodels can have access to it. The goal is to rewrite text to prevent someone\nfrom inferring a sensitive attribute (e.g. the gender of the author, or their\nlocation by the writing style) whilst keeping the text useful for its original\nintention (e.g. the sentiment of a product review). The few works tackling this\nhave focused on generative techniques. However, these often create extensively\ndifferent texts from the original ones or face problems such as mode collapse.\nThis paper explores a novel adaptation of adversarial attack techniques to\nmanipulate a text to deceive a classifier w.r.t one task (privacy) whilst\nkeeping the predictions of another classifier trained for another task\n(utility) unchanged. We propose IDT, a method that analyses predictions made by\nauxiliary and interpretable models to identify which tokens are important to\nchange for the privacy task, and which ones should be kept for the utility\ntask. We evaluate different datasets for NLP suitable for different tasks.\nAutomatic and human evaluations show that IDT retains the utility of text,\nwhile also outperforming existing methods when deceiving a classifier w.r.t\nprivacy task.\n","authors":["Pedro Faustini","Shakila Mahjabin Tonni","Annabelle McIver","Qiongkai Xu","Mark Dras"],"pdf_url":"https://arxiv.org/pdf/2406.19642v1.pdf","comment":"28 pages, 1 figure"},{"id":"http://arxiv.org/abs/2406.02105v2","updated":"2024-06-28T04:05:53Z","published":"2024-06-04T08:33:56Z","title":"Kernel vs. Kernel: Exploring How the Data Structure Affects Neural\n  Collapse","summary":"  Recently, a vast amount of literature has focused on the \"Neural Collapse\"\n(NC) phenomenon, which emerges when training neural network (NN) classifiers\nbeyond the zero training error point. The core component of NC is the decrease\nin the within class variability of the network's deepest features, dubbed as\nNC1. The theoretical works that study NC are typically based on simplified\nunconstrained features models (UFMs) that mask any effect of the data on the\nextent of collapse. In this paper, we provide a kernel-based analysis that does\nnot suffer from this limitation. First, given a kernel function, we establish\nexpressions for the traces of the within- and between-class covariance matrices\nof the samples' features (and consequently an NC1 metric). Then, we turn to\nfocus on kernels associated with shallow NNs. First, we consider the NN\nGaussian Process kernel (NNGP), associated with the network at initialization,\nand the complement Neural Tangent Kernel (NTK), associated with its training in\nthe \"lazy regime\". Interestingly, we show that the NTK does not represent more\ncollapsed features than the NNGP for prototypical data models. As NC emerges\nfrom training, we then consider an alternative to NTK: the recently proposed\nadaptive kernel, which generalizes NNGP to model the feature mapping learned\nfrom the training data. Contrasting our NC1 analysis for these two kernels\nenables gaining insights into the effect of data distribution on the extent of\ncollapse, which are empirically aligned with the behavior observed with\npractical training of NNs.\n","authors":["Vignesh Kothapalli","Tom Tirer"],"pdf_url":"https://arxiv.org/pdf/2406.02105v2.pdf","comment":"34 pages, 14 figures"},{"id":"http://arxiv.org/abs/2405.16141v3","updated":"2024-06-28T03:59:15Z","published":"2024-05-25T09:21:43Z","title":"AIGB: Generative Auto-bidding via Diffusion Modeling","summary":"  Auto-bidding plays a crucial role in facilitating online advertising by\nautomatically providing bids for advertisers. Reinforcement learning (RL) has\ngained popularity for auto-bidding. However, most current RL auto-bidding\nmethods are modeled through the Markovian Decision Process (MDP), which assumes\nthe Markovian state transition. This assumption restricts the ability to\nperform in long horizon scenarios and makes the model unstable when dealing\nwith highly random online advertising environments. To tackle this issue, this\npaper introduces AI-Generated Bidding (AIGB), a novel paradigm for auto-bidding\nthrough generative modeling. In this paradigm, we propose DiffBid, a\nconditional diffusion modeling approach for bid generation. DiffBid directly\nmodels the correlation between the return and the entire trajectory,\neffectively avoiding error propagation across time steps in long horizons.\nAdditionally, DiffBid offers a versatile approach for generating trajectories\nthat maximize given targets while adhering to specific constraints. Extensive\nexperiments conducted on the real-world dataset and online A/B test on Alibaba\nadvertising platform demonstrate the effectiveness of DiffBid, achieving 2.81%\nincrease in GMV and 3.36% increase in ROI.\n","authors":["Jiayan Guo","Yusen Huo","Zhilin Zhang","Tianyu Wang","Chuan Yu","Jian Xu","Yan Zhang","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2405.16141v3.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2301.12616v4","updated":"2024-06-28T03:57:21Z","published":"2023-01-30T02:23:49Z","title":"Active Sequential Two-Sample Testing","summary":"  A two-sample hypothesis test is a statistical procedure used to determine\nwhether the distributions generating two samples are identical. We consider the\ntwo-sample testing problem in a new scenario where the sample measurements (or\nsample features) are inexpensive to access, but their group memberships (or\nlabels) are costly. To address the problem, we devise the first \\emph{active\nsequential two-sample testing framework} that not only sequentially but also\n\\emph{actively queries}. Our test statistic is a likelihood ratio where one\nlikelihood is found by maximization over all class priors, and the other is\nprovided by a probabilistic classification model. The classification model is\nadaptively updated and used to predict where the (unlabelled) features have a\nhigh dependency on labels; labeling the ``high-dependency'' features leads to\nthe increased power of the proposed testing framework. In theory, we provide\nthe proof that our framework produces an \\emph{anytime-valid} $p$-value. In\naddition, we characterize the proposed framework's gain in testing power by\nanalyzing the mutual information between the feature and label variables in\nasymptotic and finite-sample scenarios. In practice, we introduce an\ninstantiation of our framework and evaluate it using several experiments; the\nexperiments on the synthetic, MNIST, and application-specific datasets\ndemonstrate that the testing power of the instantiated active sequential test\nsignificantly increases while the Type I error is under control.\n","authors":["Weizhi Li","Prad Kadambi","Pouria Saidi","Karthikeyan Natesan Ramamurthy","Gautam Dasarathy","Visar Berisha"],"pdf_url":"https://arxiv.org/pdf/2301.12616v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12807v7","updated":"2024-06-28T03:55:48Z","published":"2024-05-21T13:58:17Z","title":"FAdam: Adam is a natural gradient optimizer using diagonal empirical\n  Fisher information","summary":"  This paper establishes a mathematical foundation for the Adam optimizer,\nelucidating its connection to natural gradient descent through Riemannian and\ninformation geometry. We rigorously analyze the diagonal empirical Fisher\ninformation matrix (FIM) in Adam, clarifying all detailed approximations and\nadvocating for the use of log probability functions as loss, which should be\nbased on discrete distributions, due to the limitations of empirical FIM. Our\nanalysis uncovers flaws in the original Adam algorithm, leading to proposed\ncorrections such as enhanced momentum calculations, adjusted bias corrections,\nadaptive epsilon, and gradient clipping. We refine the weight decay term based\non our theoretical framework. Our modified algorithm, Fisher Adam (FAdam),\ndemonstrates superior performance across diverse domains including LLM, ASR,\nand VQ-VAE, achieving state-of-the-art results in ASR.\n","authors":["Dongseong Hwang"],"pdf_url":"https://arxiv.org/pdf/2405.12807v7.pdf","comment":"21 pages, 4 figures, 6 tables"},{"id":"http://arxiv.org/abs/2310.01712v2","updated":"2024-06-28T03:53:56Z","published":"2023-10-03T00:54:13Z","title":"Generative Autoencoding of Dropout Patterns","summary":"  We propose a generative model termed Deciphering Autoencoders. In this model,\nwe assign a unique random dropout pattern to each data point in the training\ndataset and then train an autoencoder to reconstruct the corresponding data\npoint using this pattern as information to be encoded. Even if a completely\nrandom dropout pattern is assigned to each data point regardless of their\nsimilarities, a sufficiently large encoder can smoothly map them to a\nlow-dimensional latent space to reconstruct individual training data points.\nDuring inference, using a dropout pattern different from those used during\ntraining allows the model to function as a generator. Since the training of\nDeciphering Autoencoders relies solely on reconstruction error, it offers more\nstable training compared to other generative models. Despite their simplicity,\nDeciphering Autoencoders show sampling quality comparable to DCGAN on the\nCIFAR-10 dataset.\n","authors":["Shunta Maeda"],"pdf_url":"https://arxiv.org/pdf/2310.01712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04607v4","updated":"2024-06-28T03:53:21Z","published":"2024-06-07T03:31:58Z","title":"MeGA: Merging Multiple Independently Trained Neural Networks Based on\n  Genetic Algorithm","summary":"  In this paper, we introduce a novel method for merging the weights of\nmultiple pre-trained neural networks using a genetic algorithm called MeGA.\nTraditional techniques, such as weight averaging and ensemble methods, often\nfail to fully harness the capabilities of pre-trained networks. Our approach\nleverages a genetic algorithm with tournament selection, crossover, and\nmutation to optimize weight combinations, creating a more effective fusion.\nThis technique allows the merged model to inherit advantageous features from\nboth parent models, resulting in enhanced accuracy and robustness. Through\nexperiments on the CIFAR-10 dataset, we demonstrate that our genetic\nalgorithm-based weight merging method improves test accuracy compared to\nindividual models and conventional methods. This approach provides a scalable\nsolution for integrating multiple pre-trained networks across various deep\nlearning applications. Github is available at:\nhttps://github.com/YUNBLAK/MeGA-Merging-Multiple-Independently-Trained-Neural-Networks-Based-on-Genetic-Algorithm\n","authors":["Daniel Yun"],"pdf_url":"https://arxiv.org/pdf/2406.04607v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10930v3","updated":"2024-06-28T03:51:23Z","published":"2024-05-17T17:31:02Z","title":"Submodular Information Selection for Hypothesis Testing with\n  Misclassification Penalties","summary":"  We consider the problem of selecting an optimal subset of information sources\nfor a hypothesis testing/classification task where the goal is to identify the\ntrue state of the world from a finite set of hypotheses, based on finite\nobservation samples from the sources. In order to characterize the learning\nperformance, we propose a misclassification penalty framework, which enables\nnonuniform treatment of different misclassification errors. In a centralized\nBayesian learning setting, we study two variants of the subset selection\nproblem: (i) selecting a minimum cost information set to ensure that the\nmaximum penalty of misclassifying the true hypothesis is below a desired bound\nand (ii) selecting an optimal information set under a limited budget to\nminimize the maximum penalty of misclassifying the true hypothesis. Under\ncertain assumptions, we prove that the objective (or constraints) of these\ncombinatorial optimization problems are weak (or approximate) submodular, and\nestablish high-probability performance guarantees for greedy algorithms.\nFurther, we propose an alternate metric for information set selection which is\nbased on the total penalty of misclassification. We prove that this metric is\nsubmodular and establish near-optimal guarantees for the greedy algorithms for\nboth the information set selection problems. Finally, we present numerical\nsimulations to validate our theoretical results over several randomly generated\ninstances.\n","authors":["Jayanth Bhargav","Mahsa Ghasemi","Shreyas Sundaram"],"pdf_url":"https://arxiv.org/pdf/2405.10930v3.pdf","comment":"21 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.19636v1","updated":"2024-06-28T03:47:54Z","published":"2024-06-28T03:47:54Z","title":"Enforcing Equity in Neural Climate Emulators","summary":"  Neural network emulators have become an invaluable tool for a wide variety of\nclimate and weather prediction tasks. While showing incredibly promising\nresults, these networks do not have an inherent ability to produce equitable\npredictions. That is, they are not guaranteed to provide a uniform quality of\nprediction along any particular class or group of people. This potential for\ninequitable predictions motivates the need for explicit representations of\nfairness in these neural networks. To that end, we draw on methods for\nenforcing analytical physical constraints in neural networks to bias networks\ntowards more equitable predictions. We demonstrate the promise of this\nmethodology using the task of climate model emulation. Specifically, we propose\na custom loss function which punishes emulators with unequal quality of\npredictions across any prespecified regions or category, here defined using\nhuman development index (HDI). This loss function weighs a standard loss metric\nsuch as mean squared error against another metric which captures inequity along\nthe equity category (HDI), allowing us to adjust the priority of each term\nbefore training. Importantly, the loss function does not specify a particular\ndefinition of equity to bias the neural network towards, opening the door for\ncustom fairness metrics. Our results show that neural climate emulators trained\nwith our loss function provide more equitable predictions and that the equity\nmetric improves with greater weighting in the loss function. We empirically\ndemonstrate that while there is a tradeoff between accuracy and equity when\nprioritizing the latter during training, an appropriate selection of the equity\npriority hyperparameter can minimize loss of performance.\n","authors":["William Yik","Sam J. Silva"],"pdf_url":"https://arxiv.org/pdf/2406.19636v1.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.19635v1","updated":"2024-06-28T03:46:53Z","published":"2024-06-28T03:46:53Z","title":"Model Predictive Simulation Using Structured Graphical Models and\n  Transformers","summary":"  We propose an approach to simulating trajectories of multiple interacting\nagents (road users) based on transformers and probabilistic graphical models\n(PGMs), and apply it to the Waymo SimAgents challenge. The transformer baseline\nis based on the MTR model, which predicts multiple future trajectories\nconditioned on the past trajectories and static road layout features. We then\nimprove upon these generated trajectories using a PGM, which contains factors\nwhich encode prior knowledge, such as a preference for smooth trajectories, and\navoidance of collisions with static obstacles and other moving agents. We\nperform (approximate) MAP inference in this PGM using the Gauss-Newton method.\nFinally we sample $K=32$ trajectories for each of the $N \\sim 100$ agents for\nthe next $T=8 \\Delta$ time steps, where $\\Delta=10$ is the sampling rate per\nsecond. Following the Model Predictive Control (MPC) paradigm, we only return\nthe first element of our forecasted trajectories at each step, and then we\nreplan, so that the simulation can constantly adapt to its changing\nenvironment. We therefore call our approach \"Model Predictive Simulation\" or\nMPS. We show that MPS improves upon the MTR baseline, especially in safety\ncritical metrics such as collision rate. Furthermore, our approach is\ncompatible with any underlying forecasting model, and does not require extra\ntraining, so we believe it is a valuable contribution to the community.\n","authors":["Xinghua Lou","Meet Dave","Shrinu Kushagra","Miguel Lazaro-Gredilla","Kevin Murphy"],"pdf_url":"https://arxiv.org/pdf/2406.19635v1.pdf","comment":"Special Mention at the Waymo Sim Agents Challenge 2024"},{"id":"http://arxiv.org/abs/2406.19631v1","updated":"2024-06-28T03:39:45Z","published":"2024-06-28T03:39:45Z","title":"Personalized Interpretation on Federated Learning: A Virtual Concepts\n  approach","summary":"  Tackling non-IID data is an open challenge in federated learning research.\nExisting FL methods, including robust FL and personalized FL, are designed to\nimprove model performance without consideration of interpreting non-IID across\nclients. This paper aims to design a novel FL method to robust and interpret\nthe non-IID data across clients. Specifically, we interpret each client's\ndataset as a mixture of conceptual vectors that each one represents an\ninterpretable concept to end-users. These conceptual vectors could be\npre-defined or refined in a human-in-the-loop process or be learnt via the\noptimization procedure of the federated learning system. In addition to the\ninterpretability, the clarity of client-specific personalization could also be\napplied to enhance the robustness of the training process on FL system. The\neffectiveness of the proposed method have been validated on benchmark datasets.\n","authors":["Peng Yan","Guodong Long","Jing Jiang","Michael Blumenstein"],"pdf_url":"https://arxiv.org/pdf/2406.19631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13650v3","updated":"2024-06-28T03:33:28Z","published":"2023-05-23T03:47:32Z","title":"Robust Model-Based Optimization for Challenging Fitness Landscapes","summary":"  Protein design, a grand challenge of the day, involves optimization on a\nfitness landscape, and leading methods adopt a model-based approach where a\nmodel is trained on a training set (protein sequences and fitness) and proposes\ncandidates to explore next. These methods are challenged by sparsity of\nhigh-fitness samples in the training set, a problem that has been in the\nliterature. A less recognized but equally important problem stems from the\ndistribution of training samples in the design space: leading methods are not\ndesigned for scenarios where the desired optimum is in a region that is not\nonly poorly represented in training data, but also relatively far from the\nhighly represented low-fitness regions. We show that this problem of\n\"separation\" in the design space is a significant bottleneck in existing\nmodel-based optimization tools and propose a new approach that uses a novel VAE\nas its search model to overcome the problem. We demonstrate its advantage over\nprior methods in robustly finding improved samples, regardless of the imbalance\nand separation between low- and high-fitness samples. Our comprehensive\nbenchmark on real and semi-synthetic protein datasets as well as solution\ndesign for physics-informed neural networks, showcases the generality of our\napproach in discrete and continuous design spaces. Our implementation is\navailable at https://github.com/sabagh1994/PGVAE.\n","authors":["Saba Ghaffari","Ehsan Saleh","Alexander G. Schwing","Yu-Xiong Wang","Martin D. Burke","Saurabh Sinha"],"pdf_url":"https://arxiv.org/pdf/2305.13650v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05743v4","updated":"2024-06-28T03:17:12Z","published":"2024-03-09T00:41:30Z","title":"Forecasting Electricity Market Signals via Generative AI","summary":"  This paper presents a generative artificial intelligence approach to\nprobabilistic forecasting of electricity market signals, such as real-time\nlocational marginal prices and area control error signals. Inspired by the\nWiener-Kallianpur innovation representation of nonparametric time series, we\npropose a weak innovation autoencoder architecture and a novel deep learning\nalgorithm that extracts the canonical independent and identically distributed\ninnovation sequence of the time series, from which samples of future time\nseries are generated. The validity of the proposed approach is established by\nproving that, under ideal training conditions, the generated samples have the\nsame conditional probability distribution as that of the ground truth. Three\napplications involving highly dynamic and volatile time series in real-time\nmarket operations are considered: (i) locational marginal price forecasting for\nself-scheduled resources such as battery storage participants, (ii)\ninterregional price spread forecasting for virtual bidders in interchange\nmarkets, and (iii) area control error forecasting for frequency regulations.\nNumerical studies based on market data from multiple independent system\noperators demonstrate the superior performance of the proposed generative\nforecaster over leading classical and modern machine learning techniques under\nboth probabilistic and point forecasting metrics.\n","authors":["Xinyi Wang","Qing Zhao","Lang Tong"],"pdf_url":"https://arxiv.org/pdf/2403.05743v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19622v1","updated":"2024-06-28T03:10:36Z","published":"2024-06-28T03:10:36Z","title":"Data-Driven Lipschitz Continuity: A Cost-Effective Approach to Improve\n  Adversarial Robustness","summary":"  The security and robustness of deep neural networks (DNNs) have become\nincreasingly concerning. This paper aims to provide both a theoretical\nfoundation and a practical solution to ensure the reliability of DNNs. We\nexplore the concept of Lipschitz continuity to certify the robustness of DNNs\nagainst adversarial attacks, which aim to mislead the network with adding\nimperceptible perturbations into inputs. We propose a novel algorithm that\nremaps the input domain into a constrained range, reducing the Lipschitz\nconstant and potentially enhancing robustness. Unlike existing adversarially\ntrained models, where robustness is enhanced by introducing additional examples\nfrom other datasets or generative models, our method is almost cost-free as it\ncan be integrated with existing models without requiring re-training.\nExperimental results demonstrate the generalizability of our method, as it can\nbe combined with various models and achieve enhancements in robustness.\nFurthermore, our method achieves the best robust accuracy for CIFAR10,\nCIFAR100, and ImageNet datasets on the RobustBench leaderboard.\n","authors":["Erh-Chung Chen","Pin-Yu Chen","I-Hsin Chung","Che-Rung Lee"],"pdf_url":"https://arxiv.org/pdf/2406.19622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19621v1","updated":"2024-06-28T03:07:53Z","published":"2024-06-28T03:07:53Z","title":"Machine-Learning-Driven Runtime Optimization of BLAS Level 3 on Modern\n  Multi-Core Systems","summary":"  BLAS Level 3 operations are essential for scientific computing, but finding\nthe optimal number of threads for multi-threaded implementations on modern\nmulti-core systems is challenging. We present an extension to the Architecture\nand Data-Structure Aware Linear Algebra (ADSALA) library that uses machine\nlearning to optimize the runtime of all BLAS Level 3 operations. Our method\npredicts the best number of threads for each operation based on the matrix\ndimensions and the system architecture. We test our method on two HPC platforms\nwith Intel and AMD processors, using MKL and BLIS as baseline BLAS\nimplementations. We achieve speedups of 1.5 to 3.0 for all operations, compared\nto using the maximum number of threads. We also analyze the runtime patterns of\ndifferent BLAS operations and explain the sources of speedup. Our work shows\nthe effectiveness and generality of the ADSALA approach for optimizing BLAS\nroutines on modern multi-core systems.\n","authors":["Yufan Xia","Giuseppe Maria Junior Barca"],"pdf_url":"https://arxiv.org/pdf/2406.19621v1.pdf","comment":"Multi-Thread, Matrix Multiplication, Optimization, BLAS, Machine\n  Learning"},{"id":"http://arxiv.org/abs/2406.19619v1","updated":"2024-06-28T03:02:25Z","published":"2024-06-28T03:02:25Z","title":"ScoreFusion: fusing score-based generative models via Kullback-Leibler\n  barycenters","summary":"  We study the problem of fusing pre-trained (auxiliary) generative models to\nenhance the training of a target generative model. We propose using\nKL-divergence weighted barycenters as an optimal fusion mechanism, in which the\nbarycenter weights are optimally trained to minimize a suitable loss for the\ntarget population. While computing the optimal KL-barycenter weights can be\nchallenging, we demonstrate that this process can be efficiently executed using\ndiffusion score training when the auxiliary generative models are also trained\nbased on diffusion score methods. Moreover, we show that our fusion method has\na dimension-free sample complexity in total variation distance provided that\nthe auxiliary models are well fitted for their own task and the auxiliary tasks\ncombined capture the target well. The main takeaway of our method is that if\nthe auxiliary models are well-trained and can borrow features from each other\nthat are present in the target, our fusion method significantly improves the\ntraining of generative models. We provide a concise computational\nimplementation of the fusion algorithm, and validate its efficiency in the\nlow-data regime with numerical experiments involving mixtures models and image\ndatasets.\n","authors":["Hao Liu"," Junze"," Ye","Jose Blanchet","Nian Si"],"pdf_url":"https://arxiv.org/pdf/2406.19619v1.pdf","comment":"40 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.19617v1","updated":"2024-06-28T02:56:22Z","published":"2024-06-28T02:56:22Z","title":"Stochastic Zeroth-Order Optimization under Strongly Convexity and\n  Lipschitz Hessian: Minimax Sample Complexity","summary":"  Optimization of convex functions under stochastic zeroth-order feedback has\nbeen a major and challenging question in online learning. In this work, we\nconsider the problem of optimizing second-order smooth and strongly convex\nfunctions where the algorithm is only accessible to noisy evaluations of the\nobjective function it queries. We provide the first tight characterization for\nthe rate of the minimax simple regret by developing matching upper and lower\nbounds. We propose an algorithm that features a combination of a bootstrapping\nstage and a mirror-descent stage. Our main technical innovation consists of a\nsharp characterization for the spherical-sampling gradient estimator under\nhigher-order smoothness conditions, which allows the algorithm to optimally\nbalance the bias-variance tradeoff, and a new iterative method for the\nbootstrapping stage, which maintains the performance for unbounded Hessian.\n","authors":["Qian Yu","Yining Wang","Baihe Huang","Qi Lei","Jason D. Lee"],"pdf_url":"https://arxiv.org/pdf/2406.19617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17366v3","updated":"2024-06-28T02:56:10Z","published":"2023-09-28T10:05:37Z","title":"3D-Mol: A Novel Contrastive Learning Framework for Molecular Property\n  Prediction with 3D Information","summary":"  Molecular property prediction, crucial for early drug candidate screening and\noptimization, has seen advancements with deep learning-based methods. While\ndeep learning-based methods have advanced considerably, they often fall short\nin fully leveraging 3D spatial information. Specifically, current molecular\nencoding techniques tend to inadequately extract spatial information, leading\nto ambiguous representations where a single one might represent multiple\ndistinct molecules. Moreover, existing molecular modeling methods focus\npredominantly on the most stable 3D conformations, neglecting other viable\nconformations present in reality. To address these issues, we propose 3D-Mol, a\nnovel approach designed for more accurate spatial structure representation. It\ndeconstructs molecules into three hierarchical graphs to better extract\ngeometric information. Additionally, 3D-Mol leverages contrastive learning for\npretraining on 20 million unlabeled data, treating their conformations with\nidentical topological structures as weighted positive pairs and contrasting\nones as negatives, based on the similarity of their 3D conformation descriptors\nand fingerprints. We compare 3D-Mol with various state-of-the-art baselines on\n7 benchmarks and demonstrate our outstanding performance.\n","authors":["Taojie Kuang","Yiming Ren","Zhixiang Ren"],"pdf_url":"https://arxiv.org/pdf/2309.17366v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19615v1","updated":"2024-06-28T02:42:30Z","published":"2024-06-28T02:42:30Z","title":"VarteX: Enhancing Weather Forecast through Distributed Variable\n  Representation","summary":"  Weather forecasting is essential for various human activities. Recent\ndata-driven models have outperformed numerical weather prediction by utilizing\ndeep learning in forecasting performance. However, challenges remain in\nefficiently handling multiple meteorological variables. This study proposes a\nnew variable aggregation scheme and an efficient learning framework for that\nchallenge. Experiments show that VarteX outperforms the conventional model in\nforecast performance, requiring significantly fewer parameters and resources.\nThe effectiveness of learning through multiple aggregations and regional split\ntraining is demonstrated, enabling more efficient and accurate deep\nlearning-based weather forecasting.\n","authors":["Ayumu Ueyama","Kazuhiko Kawamoto","Hiroshi Kera"],"pdf_url":"https://arxiv.org/pdf/2406.19615v1.pdf","comment":"ICML 2024, Workshop on Machine Learning for Earth System Modeling"},{"id":"http://arxiv.org/abs/2406.19614v1","updated":"2024-06-28T02:41:33Z","published":"2024-06-28T02:41:33Z","title":"A Survey on Data Quality Dimensions and Tools for Machine Learning","summary":"  Machine learning (ML) technologies have become substantial in practically all\naspects of our society, and data quality (DQ) is critical for the performance,\nfairness, robustness, safety, and scalability of ML models. With the large and\ncomplex data in data-centric AI, traditional methods like exploratory data\nanalysis (EDA) and cross-validation (CV) face challenges, highlighting the\nimportance of mastering DQ tools. In this survey, we review 17 DQ evaluation\nand improvement tools in the last 5 years. By introducing the DQ dimensions,\nmetrics, and main functions embedded in these tools, we compare their strengths\nand limitations and propose a roadmap for developing open-source DQ tools for\nML. Based on the discussions on the challenges and emerging trends, we further\nhighlight the potential applications of large language models (LLMs) and\ngenerative AI in DQ evaluation and improvement for ML. We believe this\ncomprehensive survey can enhance understanding of DQ in ML and could drive\nprogress in data-centric AI. A complete list of the literature investigated in\nthis survey is available on GitHub at:\nhttps://github.com/haihua0913/awesome-dq4ml.\n","authors":["Yuhan Zhou","Fengjiao Tu","Kewei Sha","Junhua Ding","Haihua Chen"],"pdf_url":"https://arxiv.org/pdf/2406.19614v1.pdf","comment":"This paper has been accepted by The 6th IEEE International Conference\n  on Artificial Intelligence Testing (IEEE AITest 2024) as an invited paper"},{"id":"http://arxiv.org/abs/2406.18820v2","updated":"2024-06-28T02:33:11Z","published":"2024-06-27T01:28:30Z","title":"Universal Checkpointing: Efficient and Flexible Checkpointing for Large\n  Scale Distributed Training","summary":"  Existing checkpointing approaches seem ill-suited for distributed training\neven though hardware limitations make model parallelism, i.e., sharding model\nstate across multiple accelerators, a requirement for model scaling.\nConsolidating distributed model state into a single checkpoint unacceptably\nslows down training, and is impractical at extreme scales. Distributed\ncheckpoints, in contrast, are tightly coupled to the model parallelism and\nhardware configurations of the training run, and thus unusable on different\nconfigurations. To address this problem, we propose Universal Checkpointing, a\ntechnique that enables efficient checkpoint creation while providing the\nflexibility of resuming on arbitrary parallelism strategy and hardware\nconfigurations. Universal Checkpointing unlocks unprecedented capabilities for\nlarge-scale training such as improved resilience to hardware failures through\ncontinued training on remaining healthy hardware, and reduced training time\nthrough opportunistic exploitation of elastic capacity.\n  The key insight of Universal Checkpointing is the selection of the optimal\nrepresentation in each phase of the checkpointing life cycle: distributed\nrepresentation for saving, and consolidated representation for loading. This is\nachieved using two key mechanisms. First, the universal checkpoint format,\nwhich consists of a consolidated representation of each model parameter and\nmetadata for mapping parameter fragments into training ranks of arbitrary\nmodel-parallelism configuration. Second, the universal checkpoint language, a\nsimple but powerful specification language for converting distributed\ncheckpoints into the universal checkpoint format. Our evaluation demonstrates\nthe effectiveness and generality of Universal Checkpointing on state-of-the-art\nmodel architectures and a wide range of parallelism techniques.\n","authors":["Xinyu Lian","Sam Ade Jacobs","Lev Kurilenko","Masahiro Tanaka","Stas Bekman","Olatunji Ruwase","Minjia Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.18820v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07249v3","updated":"2024-06-28T02:32:03Z","published":"2024-02-11T17:29:58Z","title":"Impact of Domain Knowledge and Multi-Modality on Intelligent Molecular\n  Property Prediction: A Systematic Survey","summary":"  The precise prediction of molecular properties is essential for advancements\nin drug development, particularly in virtual screening and compound\noptimization. The recent introduction of numerous deep learning-based methods\nhas shown remarkable potential in enhancing molecular property prediction\n(MPP), especially improving accuracy and insights into molecular structures.\nYet, two critical questions arise: does the integration of domain knowledge\naugment the accuracy of molecular property prediction and does employing\nmulti-modal data fusion yield more precise results than unique data source\nmethods? To explore these matters, we comprehensively review and quantitatively\nanalyze recent deep learning methods based on various benchmarks. We discover\nthat integrating molecular information significantly improves molecular\nproperty prediction (MPP) for both regression and classification tasks.\nSpecifically, regression improvements, measured by reductions in root mean\nsquare error (RMSE), are up to 4.0%, while classification enhancements,\nmeasured by the area under the receiver operating characteristic curve\n(ROC-AUC), are up to 1.7%. We also discover that enriching 2D graphs with 1D\nSMILES boosts multi-modal learning performance for regression tasks by up to\n9.1%, and augmenting 2D graphs with 3D information increases performance for\nclassification tasks by up to 13.2%, with both enhancements measured using\nROC-AUC. The two consolidated insights offer crucial guidance for future\nadvancements in drug discovery.\n","authors":["Taojie Kuang","Pengfei Liu","Zhixiang Ren"],"pdf_url":"https://arxiv.org/pdf/2402.07249v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06873v2","updated":"2024-06-28T02:25:20Z","published":"2024-03-11T16:24:26Z","title":"Last Iterate Convergence of Incremental Methods and Applications in\n  Continual Learning","summary":"  Incremental gradient and incremental proximal methods are a fundamental class\nof optimization algorithms used for solving finite sum problems, broadly\nstudied in the literature. Yet, without strong convexity, their convergence\nguarantees have primarily been established for the ergodic (average) iterate.\nMotivated by applications in continual learning, we obtain the first\nconvergence guarantees for the last iterate of both incremental gradient and\nincremental proximal methods, in general convex smooth (for both) and convex\nLipschitz (for the proximal variants) settings. Our oracle complexity bounds\nfor the last iterate nearly match (i.e., match up to a square-root-log or a log\nfactor) the best known oracle complexity bounds for the average iterate, for\nboth classes of methods. We further obtain generalizations of our results to\nweighted averaging of the iterates with increasing weights and for randomly\npermuted ordering of updates. We study incremental proximal methods as a model\nof continual learning with generalization and argue that large amount of\nregularization is crucial to preventing catastrophic forgetting. Our results\ngeneralize last iterate guarantees for incremental methods compared to state of\nthe art, as such results were previously known only for overparameterized\nlinear models, which correspond to convex quadratic problems with infinitely\nmany solutions.\n","authors":["Xufeng Cai","Jelena Diakonikolas"],"pdf_url":"https://arxiv.org/pdf/2403.06873v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19602v1","updated":"2024-06-28T02:18:16Z","published":"2024-06-28T02:18:16Z","title":"A Survey on Deep Clustering: From the Prior Perspective","summary":"  Facilitated by the powerful feature extraction ability of neural networks,\ndeep clustering has achieved great success in analyzing high-dimensional and\ncomplex real-world data. The performance of deep clustering methods is affected\nby various factors such as network structures and learning objectives. However,\nas pointed out in this survey, the essence of deep clustering lies in the\nincorporation and utilization of prior knowledge, which is largely ignored by\nexisting works. From pioneering deep clustering methods based on data structure\nassumptions to recent contrastive clustering methods based on data augmentation\ninvariances, the development of deep clustering intrinsically corresponds to\nthe evolution of prior knowledge. In this survey, we provide a comprehensive\nreview of deep clustering methods by categorizing them into six types of prior\nknowledge. We find that in general the prior innovation follows two trends,\nnamely, i) from mining to constructing, and ii) from internal to external.\nBesides, we provide a benchmark on five widely-used datasets and analyze the\nperformance of methods with diverse priors. By providing a novel prior\nknowledge perspective, we hope this survey could provide some novel insights\nand inspire future research in the deep clustering community.\n","authors":["Yiding Lu","Haobin Li","Yunfan Li","Yijie Lin","Xi Peng"],"pdf_url":"https://arxiv.org/pdf/2406.19602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19596v1","updated":"2024-06-28T01:37:46Z","published":"2024-06-28T01:37:46Z","title":"Optimizing Cyber Defense in Dynamic Active Directories through\n  Reinforcement Learning","summary":"  This paper addresses a significant gap in Autonomous Cyber Operations (ACO)\nliterature: the absence of effective edge-blocking ACO strategies in dynamic,\nreal-world networks. It specifically targets the cybersecurity vulnerabilities\nof organizational Active Directory (AD) systems. Unlike the existing literature\non edge-blocking defenses which considers AD systems as static entities, our\nstudy counters this by recognizing their dynamic nature and developing advanced\nedge-blocking defenses through a Stackelberg game model between attacker and\ndefender. We devise a Reinforcement Learning (RL)-based attack strategy and an\nRL-assisted Evolutionary Diversity Optimization-based defense strategy, where\nthe attacker and defender improve each other strategy via parallel gameplay. To\naddress the computational challenges of training attacker-defender strategies\non numerous dynamic AD graphs, we propose an RL Training Facilitator that\nprunes environments and neural networks to eliminate irrelevant elements,\nenabling efficient and scalable training for large graphs. We extensively train\nthe attacker strategy, as a sophisticated attacker model is essential for a\nrobust defense. Our empirical results successfully demonstrate that our\nproposed approach enhances defender's proficiency in hardening dynamic AD\ngraphs while ensuring scalability for large-scale AD.\n","authors":["Diksha Goel","Kristen Moore","Mingyu Guo","Derui Wang","Minjune Kim","Seyit Camtepe"],"pdf_url":"https://arxiv.org/pdf/2406.19596v1.pdf","comment":"The manuscript has been accepted as full paper at European Symposium\n  on Research in Computer Security (ESORICS) 2024"},{"id":"http://arxiv.org/abs/2404.14527v3","updated":"2024-06-28T01:24:22Z","published":"2024-04-22T18:56:18Z","title":"Mélange: Cost Efficient Large Language Model Serving by Exploiting GPU\n  Heterogeneity","summary":"  Large language models (LLMs) are increasingly integrated into many online\nservices, yet they remain cost-prohibitive to deploy due to the requirement of\nexpensive GPU instances. Prior work has addressed the high cost of LLM serving\nby improving the inference engine, but less attention has been given to\nselecting the most cost-efficient GPU type(s) for a specific LLM service. There\nis a large and growing landscape of GPU types and, within these options, higher\ncost does not always lead to increased performance. Instead, through a\ncomprehensive investigation, we find that three key LLM service characteristics\n(request size, request rate, SLO) strongly influence GPU cost efficiency, and\ndiffering GPU types are most cost efficient for differing LLM service settings.\nAs a result, the most cost-efficient allocation for a given service is\ntypically a mix of heterogeneous GPU types. Based on this analysis, we\nintroduce M\\'elange, a GPU allocation framework that navigates these diverse\nLLM service characteristics and heterogeneous GPU option space to automatically\nand efficiently derive the minimal-cost GPU allocation for a given LLM service.\nWe formulate the GPU allocation task as a cost-aware bin packing problem where\nGPUs are bins and items are slices of the service workload. Our formulation's\nconstraints account for a service's unique characteristics, allowing M\\'elange\nto be flexible to support diverse service settings and heterogeneity-aware to\nadapt the GPU allocation to a specific service. Compared to using only a single\nGPU type, M\\'elange reduces deployment costs by up to 77% in conversational\nsettings, 33% in document-based settings, and 51% in a mixed setting.\n","authors":["Tyler Griggs","Xiaoxuan Liu","Jiaxiang Yu","Doyoung Kim","Wei-Lin Chiang","Alvin Cheung","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2404.14527v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19589v1","updated":"2024-06-28T00:39:17Z","published":"2024-06-28T00:39:17Z","title":"Network Bending of Diffusion Models for Audio-Visual Generation","summary":"  In this paper we present the first steps towards the creation of a tool which\nenables artists to create music visualizations using pre-trained, generative,\nmachine learning models. First, we investigate the application of network\nbending, the process of applying transforms within the layers of a generative\nnetwork, to image generation diffusion models by utilizing a range of\npoint-wise, tensor-wise, and morphological operators. We identify a number of\nvisual effects that result from various operators, including some that are not\neasily recreated with standard image editing tools. We find that this process\nallows for continuous, fine-grain control of image generation which can be\nhelpful for creative applications. Next, we generate music-reactive videos\nusing Stable Diffusion by passing audio features as parameters to network\nbending operators. Finally, we comment on certain transforms which radically\nshift the image and the possibilities of learning more about the latent space\nof Stable Diffusion based on these transforms.\n","authors":["Luke Dzwonczyk","Carmine Emanuele Cella","David Ban"],"pdf_url":"https://arxiv.org/pdf/2406.19589v1.pdf","comment":"8 pages, 5 figures, to be published in the proceedings of the 27th\n  International Conference on Digital Audio Effects (DAFx24), for additional\n  image and video examples see https://dzluke.github.io/DAFX2024/"},{"id":"http://arxiv.org/abs/2406.19581v1","updated":"2024-06-28T00:08:13Z","published":"2024-06-28T00:08:13Z","title":"HarmonICA: Neural non-stationarity correction and source separation for\n  motor neuron interfaces","summary":"  A major outstanding problem when interfacing with spinal motor neurons is how\nto accurately compensate for non-stationary effects in the signal during source\nseparation routines, particularly when they cannot be estimated in advance.\nThis forces current systems to instead use undifferentiated bulk signal, which\nlimits the potential degrees of freedom for control. In this study we propose a\npotential solution, using an unsupervised learning algorithm to blindly correct\nfor the effects of latent processes which drive the signal non-stationarities.\nWe implement this methodology within the theoretical framework of a quasilinear\nversion of independent component analysis (ICA). The proposed design,\nHarmonICA, sidesteps the identifiability problems of nonlinear ICA, allowing\nfor equivalent predictability to linear ICA whilst retaining the ability to\nlearn complex nonlinear relationships between non-stationary latents and their\neffects on the signal. We test HarmonICA on both invasive and non-invasive\nrecordings both simulated and real, demonstrating an ability to blindly\ncompensate for the non-stationary effects specific to each, and thus to\nsignificantly enhance the quality of a source separation routine.\n","authors":["Alexander Kenneth Clarke","Agnese Grison","Irene Mendez Guerra","Pranav Mamidanna","Shihan Ma","Silvia Muceli","Dario Farina"],"pdf_url":"https://arxiv.org/pdf/2406.19581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19580v1","updated":"2024-06-28T00:05:53Z","published":"2024-06-28T00:05:53Z","title":"FRED: Flexible REduction-Distribution Interconnect and Communication\n  Implementation for Wafer-Scale Distributed Training of DNN Models","summary":"  Distributed Deep Neural Network (DNN) training is a technique to reduce the\ntraining overhead by distributing the training tasks into multiple\naccelerators, according to a parallelization strategy. However,\nhigh-performance compute and interconnects are needed for maximum speed-up and\nlinear scaling of the system. Wafer-scale systems are a promising technology\nthat allows for tightly integrating high-end accelerators with high-speed\nwafer-scale interconnects, making it an attractive platform for distributed\ntraining. However, the wafer-scale interconnect should offer high performance\nand flexibility for various parallelization strategies to enable maximum\noptimizations for compute and memory usage. In this paper, we propose FRED, a\nwafer-scale interconnect that is tailored for the high-BW requirements of\nwafer-scale networks and can efficiently execute communication patterns of\ndifferent parallelization strategies. Furthermore, FRED supports in-switch\ncollective communication execution that reduces the network traffic by\napproximately 2X. Our results show that FRED can improve the average end-to-end\ntraining time of ResNet-152, Transformer-17B, GPT-3, and Transformer-1T by\n1.76X, 1.87X, 1.34X, and 1.4X, respectively when compared to a baseline\nwaferscale 2D-Mesh fabric.\n","authors":["Saeed Rashidi","William Won","Sudarshan Srinivasan","Puneet Gupta","Tushar Krishna"],"pdf_url":"https://arxiv.org/pdf/2406.19580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05480v2","updated":"2024-06-28T00:05:14Z","published":"2024-05-09T00:37:56Z","title":"FloorSet -- a VLSI Floorplanning Dataset with Design Constraints of\n  Real-World SoCs","summary":"  Floorplanning for systems-on-a-chip (SoCs) and its sub-systems is a crucial\nand non-trivial step of the physical design flow. It represents a difficult\ncombinatorial optimization problem. A typical large scale SoC with 120\npartitions generates a search-space of nearly 10E250. As novel machine learning\n(ML) approaches emerge to tackle such problems, there is a growing need for a\nmodern benchmark that comprises a large training dataset and performance\nmetrics that better reflect real-world constraints and objectives compared to\nexisting benchmarks. To address this need, we present FloorSet -- two\ncomprehensive datasets of synthetic fixed-outline floorplan layouts that\nreflect the distribution of real SoCs. Each dataset has 1M training samples and\n100 test samples where each sample is a synthetic floor-plan. FloorSet-Prime\ncomprises fully-abutted rectilinear partitions and near-optimal wire-length. A\nsimplified dataset that reflects early design phases, FloorSet-Lite comprises\nrectangular partitions, with under 5 percent white-space and near-optimal\nwire-length. Both datasets define hard constraints seen in modern design flows\nsuch as shape constraints, edge-affinity, grouping constraints, and\npre-placement constraints. FloorSet is intended to spur fundamental research on\nlarge-scale constrained optimization problems. Crucially, FloorSet alleviates\nthe core issue of reproducibility in modern ML driven solutions to such\nproblems. FloorSet is available as an open-source repository for the research\ncommunity.\n","authors":["Uday Mallappa","Hesham Mostafa","Mikhail Galkin","Mariano Phielipp","Somdeb Majumdar"],"pdf_url":"https://arxiv.org/pdf/2405.05480v2.pdf","comment":"10 pages, 11 figures"}],"Image and Video Processing":[{"id":"http://arxiv.org/abs/2406.20005v1","updated":"2024-06-28T15:44:55Z","published":"2024-06-28T15:44:55Z","title":"Malaria Cell Detection Using Deep Neural Networks","summary":"  Malaria remains one of the most pressing public health concerns globally,\ncausing significant morbidity and mortality, especially in sub-Saharan Africa.\nRapid and accurate diagnosis is crucial for effective treatment and disease\nmanagement. Traditional diagnostic methods, such as microscopic examination of\nblood smears, are labor-intensive and require significant expertise, which may\nnot be readily available in resource-limited settings. This project aims to\nautomate the detection of malaria-infected cells using a deep learning\napproach. We employed a convolutional neural network (CNN) based on the\nResNet50 architecture, leveraging transfer learning to enhance performance. The\nMalaria Cell Images Dataset from Kaggle, containing 27,558 images categorized\ninto infected and uninfected cells, was used for training and evaluation. Our\nmodel demonstrated high accuracy, precision, and recall, indicating its\npotential as a reliable tool for assisting in malaria diagnosis. Additionally,\na web application was developed using Streamlit to allow users to upload cell\nimages and receive predictions about malaria infection, making the technology\naccessible and user-friendly. This paper provides a comprehensive overview of\nthe methodology, experiments, and results, highlighting the effectiveness of\ndeep learning in medical image analysis.\n","authors":["Saurabh Sawant","Anurag Singh"],"pdf_url":"https://arxiv.org/pdf/2406.20005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19943v1","updated":"2024-06-28T14:22:30Z","published":"2024-06-28T14:22:30Z","title":"Impact of Initialization on Intra-subject Pediatric Brain MR Image\n  Registration: A Comparative Analysis between SyN ANTs and Deep Learning-Based\n  Approaches","summary":"  This study evaluates the performance of conventional SyN ANTs and\nlearning-based registration methods in the context of pediatric neuroimaging,\nspecifically focusing on intrasubject deformable registration. The comparison\ninvolves three approaches: without (NR), with rigid (RR), and with rigid and\naffine (RAR) initializations. In addition to initialization, performances are\nevaluated in terms of accuracy, speed, and the impact of age intervals and sex\nper pair. Data consists of the publicly available MRI scans from the Calgary\nPreschool dataset, which includes 63 children aged 2-7 years, allowing for 431\nregistration pairs. We implemented the unsupervised DL framework with a U-Net\narchitecture using DeepReg and it was 5-fold cross-validated. Evaluation\nincludes Dice scores for tissue segmentation from 18 smaller regions obtained\nby SynthSeg, analysis of log Jacobian determinants, and registration pro-rated\ntraining and inference times. Learning-based approaches, with or without linear\ninitializations, exhibit slight superiority over SyN ANTs in terms of Dice\nscores. Indeed, DL-based implementations with RR and RAR initializations\nsignificantly outperform SyN ANTs. Both SyN ANTs and DL-based registration\ninvolve parameter optimization, but the choice between these methods depends on\nthe scale of registration: network-based for broader coverage or SyN ANTs for\nspecific structures. Both methods face challenges with larger age intervals due\nto greater growth changes. The main takeaway is that while DL-based methods\nshow promise with faster and more accurate registrations, SyN ANTs remains\nrobust and generalizable without the need for extensive training, highlighting\nthe importance of method selection based on specific registration needs in the\npediatric context. Our code is available at\nhttps://github.com/neuropoly/pediatric-DL-registration\n","authors":["Andjela Dimitrijevic","Vincent Noblet","Benjamin De Leener"],"pdf_url":"https://arxiv.org/pdf/2406.19943v1.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:013"},{"id":"http://arxiv.org/abs/2406.19870v1","updated":"2024-06-28T12:22:31Z","published":"2024-06-28T12:22:31Z","title":"Deep Unfolding-Aided Parameter Tuning for Plug-and-Play Based Video\n  Snapshot Compressive Imaging","summary":"  Snapshot compressive imaging (SCI) captures high-dimensional data efficiently\nby compressing it into two-dimensional observations and reconstructing\nhigh-dimensional data from two-dimensional observations with various\nalgorithms. Plug-and-play (PnP) is a promising approach for the video SCI\nreconstruction because it can leverage both the observation model and denoising\nmethods for videos. This paper proposes a deep unfolding-based method for\ntuning noise level parameters in PnP-based video SCI, which significantly\naffects the reconstruction accuracy. For the training of the parameters, we\nprepare training data from the densely annotated video segmentation (DAVIS)\ndataset, reparametrize the noise level parameters, and apply the checkpointing\ntechnique to reduce the required memory. Simulation results show that the\ntrained noise level parameters significantly improve the reconstruction\naccuracy and exhibit a non-monotonic pattern, which is different from the\nassumptions in the conventional convergence analyses of PnP-based algorithms.\n","authors":["Takashi Matsuda","Ryo Hayakawa","Youji Iiguni"],"pdf_url":"https://arxiv.org/pdf/2406.19870v1.pdf","comment":"This work will be submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2406.19796v1","updated":"2024-06-28T10:05:58Z","published":"2024-06-28T10:05:58Z","title":"Comprehensive Generative Replay for Task-Incremental Segmentation with\n  Concurrent Appearance and Semantic Forgetting","summary":"  Generalist segmentation models are increasingly favored for diverse tasks\ninvolving various objects from different image sources. Task-Incremental\nLearning (TIL) offers a privacy-preserving training paradigm using tasks\narriving sequentially, instead of gathering them due to strict data sharing\npolicies. However, the task evolution can span a wide scope that involves\nshifts in both image appearance and segmentation semantics with intricate\ncorrelation, causing concurrent appearance and semantic forgetting. To solve\nthis issue, we propose a Comprehensive Generative Replay (CGR) framework that\nrestores appearance and semantic knowledge by synthesizing image-mask pairs to\nmimic past task data, which focuses on two aspects: modeling image-mask\ncorrespondence and promoting scalability for diverse tasks. Specifically, we\nintroduce a novel Bayesian Joint Diffusion (BJD) model for high-quality\nsynthesis of image-mask pairs with their correspondence explicitly preserved by\nconditional denoising. Furthermore, we develop a Task-Oriented Adapter (TOA)\nthat recalibrates prompt embeddings to modulate the diffusion model, making the\ndata synthesis compatible with different tasks. Experiments on incremental\ntasks (cardiac, fundus and prostate segmentation) show its clear advantage for\nalleviating concurrent appearance and semantic forgetting. Code is available at\nhttps://github.com/jingyzhang/CGR.\n","authors":["Wei Li","Jingyang Zhang","Pheng-Ann Heng","Lixu Gu"],"pdf_url":"https://arxiv.org/pdf/2406.19796v1.pdf","comment":"Accepted by MICCAI24"},{"id":"http://arxiv.org/abs/2404.09666v2","updated":"2024-06-28T09:25:25Z","published":"2024-04-15T10:57:16Z","title":"Deformable MRI Sequence Registration for AI-based Prostate Cancer\n  Diagnosis","summary":"  The PI-CAI (Prostate Imaging: Cancer AI) challenge led to expert-level\ndiagnostic algorithms for clinically significant prostate cancer detection. The\nalgorithms receive biparametric MRI scans as input, which consist of\nT2-weighted and diffusion-weighted scans. These scans can be misaligned due to\nmultiple factors in the scanning process. Image registration can alleviate this\nissue by predicting the deformation between the sequences. We investigate the\neffect of image registration on the diagnostic performance of AI-based prostate\ncancer diagnosis. First, the image registration algorithm, developed in\nMeVisLab, is analyzed using a dataset with paired lesion annotations. Second,\nthe effect on diagnosis is evaluated by comparing case-level cancer diagnosis\nperformance between using the original dataset, rigidly aligned\ndiffusion-weighted scans, or deformably aligned diffusion-weighted scans. Rigid\nregistration showed no improvement. Deformable registration demonstrated a\nsubstantial improvement in lesion overlap (+10% median Dice score) and a\npositive yet non-significant improvement in diagnostic performance (+0.3%\nAUROC, p=0.18). Our investigation shows that a substantial improvement in\nlesion alignment does not directly lead to a significant improvement in\ndiagnostic performance. Qualitative analysis indicated that jointly developing\nimage registration methods and diagnostic AI algorithms could enhance\ndiagnostic accuracy and patient outcomes.\n","authors":["Alessa Hering","Sarah de Boer","Anindo Saha","Jasper J. Twilt","Mattias P. Heinrich","Derya Yakar","Maarten de Rooij","Henkjan Huisman","Joeran S. Bosma"],"pdf_url":"https://arxiv.org/pdf/2404.09666v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19336v2","updated":"2024-06-28T09:20:01Z","published":"2024-06-27T17:10:10Z","title":"LiverUSRecon: Automatic 3D Reconstruction and Volumetry of the Liver\n  with a Few Partial Ultrasound Scans","summary":"  3D reconstruction of the liver for volumetry is important for qualitative\nanalysis and disease diagnosis. Liver volumetry using ultrasound (US) scans,\nalthough advantageous due to less acquisition time and safety, is challenging\ndue to the inherent noisiness in US scans, blurry boundaries, and partial liver\nvisibility. We address these challenges by using the segmentation masks of a\nfew incomplete sagittal-plane US scans of the liver in conjunction with a\nstatistical shape model (SSM) built using a set of CT scans of the liver. We\ncompute the shape parameters needed to warp this canonical SSM to fit the US\nscans through a parametric regression network. The resulting 3D liver\nreconstruction is accurate and leads to automatic liver volume calculation. We\nevaluate the accuracy of the estimated liver volumes with respect to CT\nsegmentation volumes using RMSE. Our volume computation is statistically much\ncloser to the volume estimated using CT scans than the volume computed using\nChilds' method by radiologists: p-value of 0.094 (>0.05) says that there is no\nsignificant difference between CT segmentation volumes and ours in contrast to\nChilds' method. We validate our method using investigations (ablation studies)\non the US image resolution, the number of CT scans used for SSM, the number of\nprincipal components, and the number of input US scans. To the best of our\nknowledge, this is the first automatic liver volumetry system using a few\nincomplete US scans given a set of CT scans of livers for SSM.\n","authors":["Kaushalya Sivayogaraj","Sahan T. Guruge","Udari Liyanage","Jeevani Udupihille","Saroj Jayasinghe","Gerard Fernando","Ranga Rodrigo","M. Rukshani Liyanaarachchi"],"pdf_url":"https://arxiv.org/pdf/2406.19336v2.pdf","comment":"10 pages, Accepted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2304.10839v4","updated":"2024-06-28T09:01:03Z","published":"2023-04-21T09:30:22Z","title":"Cross-domain Denoising for Low-dose Multi-frame Spiral Computed\n  Tomography","summary":"  Computed tomography (CT) has been used worldwide as a non-invasive test to\nassist in diagnosis. However, the ionizing nature of X-ray exposure raises\nconcerns about potential health risks such as cancer. The desire for lower\nradiation doses has driven researchers to improve reconstruction quality.\nAlthough previous studies on low-dose computed tomography (LDCT) denoising have\ndemonstrated the effectiveness of learning-based methods, most were developed\non the simulated data. However, the real-world scenario differs significantly\nfrom the simulation domain, especially when using the multi-slice spiral\nscanner geometry. This paper proposes a two-stage method for the commercially\navailable multi-slice spiral CT scanners that better exploits the complete\nreconstruction pipeline for LDCT denoising across different domains. Our\napproach makes good use of the high redundancy of multi-slice projections and\nthe volumetric reconstructions while leveraging the over-smoothing problem in\nconventional cascaded frameworks caused by aggressive denoising. The dedicated\ndesign also provides a more explicit interpretation of the data flow. Extensive\nexperiments on various datasets showed that the proposed method could remove up\nto 70\\% of noise without compromised spatial resolution, and subjective\nevaluations by two experienced radiologists further supported its superior\nperformance against state-of-the-art methods in clinical practice.\n","authors":["Yucheng Lu","Zhixin Xu","Moon Hyung Choi","Jimin Kim","Seung-Won Jung"],"pdf_url":"https://arxiv.org/pdf/2304.10839v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19749v1","updated":"2024-06-28T08:48:14Z","published":"2024-06-28T08:48:14Z","title":"SPIRONet: Spatial-Frequency Learning and Topological Channel Interaction\n  Network for Vessel Segmentation","summary":"  Automatic vessel segmentation is paramount for developing next-generation\ninterventional navigation systems. However, current approaches suffer from\nsuboptimal segmentation performances due to significant challenges in\nintraoperative images (i.e., low signal-to-noise ratio, small or slender\nvessels, and strong interference). In this paper, a novel spatial-frequency\nlearning and topological channel interaction network (SPIRONet) is proposed to\naddress the above issues. Specifically, dual encoders are utilized to\ncomprehensively capture local spatial and global frequency vessel features.\nThen, a cross-attention fusion module is introduced to effectively fuse spatial\nand frequency features, thereby enhancing feature discriminability.\nFurthermore, a topological channel interaction module is designed to filter out\ntask-irrelevant responses based on graph neural networks. Extensive\nexperimental results on several challenging datasets (CADSA, CAXF, DCA1, and\nXCAD) demonstrate state-of-the-art performances of our method. Moreover, the\ninference speed of SPIRONet is 21 FPS with a 512x512 input size, surpassing\nclinical real-time requirements (6~12FPS). These promising outcomes indicate\nSPIRONet's potential for integration into vascular interventional navigation\nsystems. Code is available at https://github.com/Dxhuang-CASIA/SPIRONet.\n","authors":["De-Xing Huang","Xiao-Hu Zhou","Xiao-Liang Xie","Shi-Qi Liu","Shuang-Yi Wang","Zhen-Qiu Feng","Mei-Jiang Gui","Hao Li","Tian-Yu Xiang","Bo-Xian Yao","Zeng-Guang Hou"],"pdf_url":"https://arxiv.org/pdf/2406.19749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19686v1","updated":"2024-06-28T06:51:38Z","published":"2024-06-28T06:51:38Z","title":"Enhancing Radiological Diagnosis: A Collaborative Approach Integrating\n  AI and Human Expertise for Visual Miss Correction","summary":"  Human-AI collaboration to identify and correct perceptual errors in chest\nradiographs has not been previously explored. This study aimed to develop a\ncollaborative AI system, CoRaX, which integrates eye gaze data and radiology\nreports to enhance diagnostic accuracy in chest radiology by pinpointing\nperceptual errors and refining the decision-making process. Using public\ndatasets REFLACX and EGD-CXR, the study retrospectively developed CoRaX,\nemploying a large multimodal model to analyze image embeddings, eye gaze data,\nand radiology reports. The system's effectiveness was evaluated based on its\nreferral-making process, the quality of referrals, and performance in\ncollaborative diagnostic settings. CoRaX was tested on a simulated error\ndataset of 271 samples with 28% (93 of 332) missed abnormalities. The system\ncorrected 21% (71 of 332) of these errors, leaving 7% (22 of 312) unresolved.\nThe Referral-Usefulness score, indicating the accuracy of predicted regions for\nall true referrals, was 0.63 (95% CI 0.59, 0.68). The Total-Usefulness score,\nreflecting the diagnostic accuracy of CoRaX's interactions with radiologists,\nshowed that 84% (237 of 280) of these interactions had a score above 0.40. In\nconclusion, CoRaX efficiently collaborates with radiologists to address\nperceptual errors across various abnormalities, with potential applications in\nthe education and training of novice radiologists.\n","authors":["Akash Awasthi","Ngan Le","Zhigang Deng","Carol C. Wu","Hien Van Nguyen"],"pdf_url":"https://arxiv.org/pdf/2406.19686v1.pdf","comment":"Under Review in Journal"},{"id":"http://arxiv.org/abs/2406.17051v2","updated":"2024-06-28T05:50:11Z","published":"2024-06-24T18:13:09Z","title":"Leveraging Knowledge Distillation for Lightweight Skin Cancer\n  Classification: Balancing Accuracy and Computational Efficiency","summary":"  Skin cancer is a major concern to public health, accounting for one-third of\nthe reported cancers. If not detected early, the cancer has the potential for\nsevere consequences. Recognizing the critical need for effective skin cancer\nclassification, we address the limitations of existing models, which are often\ntoo large to deploy in areas with limited computational resources. In response,\nwe present a knowledge distillation based approach for creating a lightweight\nyet high-performing classifier. The proposed solution involves fusing three\nmodels, namely ResNet152V2, ConvNeXtBase, and ViT Base, to create an effective\nteacher model. The teacher model is then employed to guide a lightweight\nstudent model of size 2.03 MB. This student model is further compressed to\n469.77 KB using 16-bit quantization, enabling smooth incorporation into edge\ndevices. With six-stage image preprocessing, data augmentation, and a rigorous\nablation study, the model achieves an impressive accuracy of 98.75% on the\nHAM10000 dataset and 98.94% on the Kaggle dataset in classifying benign and\nmalignant skin cancers. With its high accuracy and compact size, our model\nappears to be a potential choice for accurate skin cancer classification,\nparticularly in resource-constrained settings.\n","authors":["Niful Islam","Khan Md Hasib","Fahmida Akter Joti","Asif Karim","Sami Azam"],"pdf_url":"https://arxiv.org/pdf/2406.17051v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19666v1","updated":"2024-06-28T05:25:57Z","published":"2024-06-28T05:25:57Z","title":"CSAKD: Knowledge Distillation with Cross Self-Attention for\n  Hyperspectral and Multispectral Image Fusion","summary":"  Hyperspectral imaging, capturing detailed spectral information for each\npixel, is pivotal in diverse scientific and industrial applications. Yet, the\nacquisition of high-resolution (HR) hyperspectral images (HSIs) often needs to\nbe addressed due to the hardware limitations of existing imaging systems. A\nprevalent workaround involves capturing both a high-resolution multispectral\nimage (HR-MSI) and a low-resolution (LR) HSI, subsequently fusing them to yield\nthe desired HR-HSI. Although deep learning-based methods have shown promising\nin HR-MSI/LR-HSI fusion and LR-HSI super-resolution (SR), their substantial\nmodel complexities hinder deployment on resource-constrained imaging devices.\nThis paper introduces a novel knowledge distillation (KD) framework for\nHR-MSI/LR-HSI fusion to achieve SR of LR-HSI. Our KD framework integrates the\nproposed Cross-Layer Residual Aggregation (CLRA) block to enhance efficiency\nfor constructing Dual Two-Streamed (DTS) network structure, designed to extract\njoint and distinct features from LR-HSI and HR-MSI simultaneously. To fully\nexploit the spatial and spectral feature representations of LR-HSI and HR-MSI,\nwe propose a novel Cross Self-Attention (CSA) fusion module to adaptively fuse\nthose features to improve the spatial and spectral quality of the reconstructed\nHR-HSI. Finally, the proposed KD-based joint loss function is employed to\nco-train the teacher and student networks. Our experimental results demonstrate\nthat the student model not only achieves comparable or superior LR-HSI SR\nperformance but also significantly reduces the model-size and computational\nrequirements. This marks a substantial advancement over existing\nstate-of-the-art methods. The source code is available at\nhttps://github.com/ming053l/CSAKD.\n","authors":["Chih-Chung Hsu","Chih-Chien Ni","Chia-Ming Lee","Li-Wei Kang"],"pdf_url":"https://arxiv.org/pdf/2406.19666v1.pdf","comment":"Submitted to TIP 2024"},{"id":"http://arxiv.org/abs/2406.19649v1","updated":"2024-06-28T04:38:12Z","published":"2024-06-28T04:38:12Z","title":"AstMatch: Adversarial Self-training Consistency Framework for\n  Semi-Supervised Medical Image Segmentation","summary":"  Semi-supervised learning (SSL) has shown considerable potential in medical\nimage segmentation, primarily leveraging consistency regularization and\npseudo-labeling. However, many SSL approaches only pay attention to low-level\nconsistency and overlook the significance of pseudo-label reliability.\nTherefore, in this work, we propose an adversarial self-training consistency\nframework (AstMatch). Firstly, we design an adversarial consistency\nregularization (ACR) approach to enhance knowledge transfer and strengthen\nprediction consistency under varying perturbation intensities. Second, we apply\na feature matching loss for adversarial training to incorporate high-level\nconsistency regularization. Additionally, we present the pyramid channel\nattention (PCA) and efficient channel and spatial attention (ECSA) modules to\nimprove the discriminator's performance. Finally, we propose an adaptive\nself-training (AST) approach to ensure the pseudo-labels' quality. The proposed\nAstMatch has been extensively evaluated with cutting-edge SSL methods on three\npublic-available datasets. The experimental results under different labeled\nratios indicate that AstMatch outperforms other existing methods, achieving new\nstate-of-the-art performance. Our code will be available at\nhttps://github.com/GuanghaoZhu663/AstMatch.\n","authors":["Guanghao Zhu","Jing Zhang","Juanxiu Liu","Xiaohui Du","Ruqian Hao","Yong Liu","Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2406.19649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14534v2","updated":"2024-06-28T02:12:20Z","published":"2024-06-20T17:47:30Z","title":"Epicardium Prompt-guided Real-time Cardiac Ultrasound Frame-to-volume\n  Registration","summary":"  A comprehensive guidance view for cardiac interventional surgery can be\nprovided by the real-time fusion of the intraoperative 2D images and\npreoperative 3D volume based on the ultrasound frame-to-volume registration.\nHowever, cardiac ultrasound images are characterized by a low signal-to-noise\nratio and small differences between adjacent frames, coupled with significant\ndimension variations between 2D frames and 3D volumes to be registered,\nresulting in real-time and accurate cardiac ultrasound frame-to-volume\nregistration being a very challenging task. This paper introduces a lightweight\nend-to-end Cardiac Ultrasound frame-to-volume Registration network, termed\nCU-Reg. Specifically, the proposed model leverages epicardium prompt-guided\nanatomical clues to reinforce the interaction of 2D sparse and 3D dense\nfeatures, followed by a voxel-wise local-global aggregation of enhanced\nfeatures, thereby boosting the cross-dimensional matching effectiveness of\nlow-quality ultrasound modalities. We further embed an inter-frame\ndiscriminative regularization term within the hybrid supervised learning to\nincrease the distinction between adjacent slices in the same ultrasound volume\nto ensure registration stability. Experimental results on the reprocessed CAMUS\ndataset demonstrate that our CU-Reg surpasses existing methods in terms of\nregistration accuracy and efficiency, meeting the guidance requirements of\nclinical cardiac interventional surgery.\n","authors":["Long Lei","Jun Zhou","Jialun Pei","Baoliang Zhao","Yueming Jin","Yuen-Chun Jeremy Teoh","Jing Qin","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2406.14534v2.pdf","comment":"This paper has been accepted by MICCAI 2024"}]},"2024-06-27T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2406.19578v1","updated":"2024-06-27T23:43:36Z","published":"2024-06-27T23:43:36Z","title":"PathAlign: A vision-language model for whole slide images in\n  histopathology","summary":"  Microscopic interpretation of histopathology images underlies many important\ndiagnostic and treatment decisions. While advances in vision-language modeling\nraise new opportunities for analysis of such images, the gigapixel-scale size\nof whole slide images (WSIs) introduces unique challenges. Additionally,\npathology reports simultaneously highlight key findings from small regions\nwhile also aggregating interpretation across multiple slides, often making it\ndifficult to create robust image-text pairs. As such, pathology reports remain\na largely untapped source of supervision in computational pathology, with most\nefforts relying on region-of-interest annotations or self-supervision at the\npatch-level. In this work, we develop a vision-language model based on the\nBLIP-2 framework using WSIs paired with curated text from pathology reports.\nThis enables applications utilizing a shared image-text embedding space, such\nas text or image retrieval for finding cases of interest, as well as\nintegration of the WSI encoder with a frozen large language model (LLM) for\nWSI-based generative text capabilities such as report generation or\nAI-in-the-loop interactions. We utilize a de-identified dataset of over 350,000\nWSIs and diagnostic text pairs, spanning a wide range of diagnoses, procedure\ntypes, and tissue types. We present pathologist evaluation of text generation\nand text retrieval using WSI embeddings, as well as results for WSI\nclassification and workflow prioritization (slide-level triaging).\nModel-generated text for WSIs was rated by pathologists as accurate, without\nclinically significant error or omission, for 78% of WSIs on average. This work\ndemonstrates exciting potential capabilities for language-aligned WSI\nembeddings.\n","authors":["Faruk Ahmed","Andrew Sellergren","Lin Yang","Shawn Xu","Boris Babenko","Abbi Ward","Niels Olson","Arash Mohtashamian","Yossi Matias","Greg S. Corrado","Quang Duong","Dale R. Webster","Shravya Shetty","Daniel Golden","Yun Liu","David F. Steiner","Ellery Wulczyn"],"pdf_url":"https://arxiv.org/pdf/2406.19578v1.pdf","comment":"9 main pages and 19 pages of supplemental material; 3 main tables, 3\n  main figures and 11 supplemental tables, 7 supplemental figures"},{"id":"http://arxiv.org/abs/2404.02319v2","updated":"2024-06-27T23:22:14Z","published":"2024-04-02T21:35:54Z","title":"Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient\n  Compile-Time Prompt Optimization","summary":"  In many modern LLM applications, such as retrieval augmented generation,\nprompts have become programs themselves. In these settings, prompt programs are\nrepeatedly called with different user queries or data instances. A big\npractical challenge is optimizing such prompt programs. Recent work has mostly\nfocused on either simple prompt programs or assumed that the general structure\nof a prompt program is fixed.\n  We introduce SAMMO, a framework to perform symbolic prompt program search for\ncompile-time optimizations of prompt programs. SAMMO represents prompt programs\non a symbolic level which allows for a rich set of transformations that can be\nsearched over during optimization. We show that SAMMO generalizes previous\nmethods and improves the performance of complex prompts on (1) instruction\ntuning, (2) RAG pipeline tuning, and (3) prompt compression, across several\ndifferent LLMs. We make all code available open-source at\nhttps://github.com/microsoft/sammo .\n","authors":["Tobias Schnabel","Jennifer Neville"],"pdf_url":"https://arxiv.org/pdf/2404.02319v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13372v4","updated":"2024-06-27T22:44:48Z","published":"2024-03-20T08:08:54Z","title":"LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models","summary":"  Efficient fine-tuning is vital for adapting large language models (LLMs) to\ndownstream tasks. However, it requires non-trivial efforts to implement these\nmethods on different models. We present LlamaFactory, a unified framework that\nintegrates a suite of cutting-edge efficient training methods. It provides a\nsolution for flexibly customizing the fine-tuning of 100+ LLMs without the need\nfor coding through the built-in web UI LlamaBoard. We empirically validate the\nefficiency and effectiveness of our framework on language modeling and text\ngeneration tasks. It has been released at\nhttps://github.com/hiyouga/LLaMA-Factory and received over 25,000 stars and\n3,000 forks.\n","authors":["Yaowei Zheng","Richong Zhang","Junhao Zhang","Yanhan Ye","Zheyan Luo","Zhangchi Feng","Yongqiang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.13372v4.pdf","comment":"13 pages, accepted to ACL 2024 System Demonstration Track"},{"id":"http://arxiv.org/abs/2406.19564v1","updated":"2024-06-27T22:38:04Z","published":"2024-06-27T22:38:04Z","title":"Voices Unheard: NLP Resources and Models for Yorùbá Regional\n  Dialects","summary":"  Yor\\`ub\\'a an African language with roughly 47 million speakers encompasses a\ncontinuum with several dialects. Recent efforts to develop NLP technologies for\nAfrican languages have focused on their standard dialects, resulting in\ndisparities for dialects and varieties for which there are little to no\nresources or tools. We take steps towards bridging this gap by introducing a\nnew high-quality parallel text and speech corpus YOR\\`ULECT across three\ndomains and four regional Yor\\`ub\\'a dialects. To develop this corpus, we\nengaged native speakers, travelling to communities where these dialects are\nspoken, to collect text and speech data. Using our newly created corpus, we\nconducted extensive experiments on (text) machine translation, automatic speech\nrecognition, and speech-to-text translation. Our results reveal substantial\nperformance disparities between standard Yor\\`ub\\'a and the other dialects\nacross all tasks. However, we also show that with dialect-adaptive finetuning,\nwe are able to narrow this gap. We believe our dataset and experimental\nanalysis will contribute greatly to developing NLP tools for Yor\\`ub\\'a and its\ndialects, and potentially for other African languages, by improving our\nunderstanding of existing challenges and offering a high-quality dataset for\nfurther development. We release YOR\\`ULECT dataset and models publicly under an\nopen license.\n","authors":["Orevaoghene Ahia","Anuoluwapo Aremu","Diana Abagyan","Hila Gonen","David Ifeoluwa Adelani","Daud Abolade","Noah A. Smith","Yulia Tsvetkov"],"pdf_url":"https://arxiv.org/pdf/2406.19564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13827v2","updated":"2024-06-27T22:31:07Z","published":"2024-06-19T20:47:23Z","title":"Fine-Tuning BERTs for Definition Extraction from Mathematical Text","summary":"  In this paper, we fine-tuned three pre-trained BERT models on the task of\n\"definition extraction\" from mathematical English written in LaTeX. This is\npresented as a binary classification problem, where either a sentence contains\na definition of a mathematical term or it does not. We used two original data\nsets, \"Chicago\" and \"TAC,\" to fine-tune and test these models. We also tested\non WFMALL, a dataset presented by Vanetik and Litvak in 2021 and compared the\nperformance of our models to theirs. We found that a high-performance\nSentence-BERT transformer model performed best based on overall accuracy,\nrecall, and precision metrics, achieving comparable results to the earlier\nmodels with less computational effort.\n","authors":["Lucy Horowitz","Ryan Hathaway"],"pdf_url":"https://arxiv.org/pdf/2406.13827v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19836v2","updated":"2024-06-27T22:28:27Z","published":"2024-03-28T21:15:15Z","title":"Target Span Detection for Implicit Harmful Content","summary":"  Identifying the targets of hate speech is a crucial step in grasping the\nnature of such speech and, ultimately, in improving the detection of offensive\nposts on online forums. Much harmful content on online platforms uses implicit\nlanguage especially when targeting vulnerable and protected groups such as\nusing stereotypical characteristics instead of explicit target names, making it\nharder to detect and mitigate the language. In this study, we focus on\nidentifying implied targets of hate speech, essential for recognizing subtler\nhate speech and enhancing the detection of harmful content on digital\nplatforms. We define a new task aimed at identifying the targets even when they\nare not explicitly stated. To address that task, we collect and annotate target\nspans in three prominent implicit hate speech datasets: SBIC, DynaHate, and\nIHC. We call the resulting merged collection Implicit-Target-Span. The\ncollection is achieved using an innovative pooling method with matching scores\nbased on human annotations and Large Language Models (LLMs). Our experiments\nindicate that Implicit-Target-Span provides a challenging test bed for target\nspan detection methods.\n","authors":["Nazanin Jafari","James Allan","Sheikh Muhammad Sarwar"],"pdf_url":"https://arxiv.org/pdf/2403.19836v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19552v1","updated":"2024-06-27T22:08:22Z","published":"2024-06-27T22:08:22Z","title":"Rethinking harmless refusals when fine-tuning foundation models","summary":"  In this paper, we investigate the degree to which fine-tuning in Large\nLanguage Models (LLMs) effectively mitigates versus merely conceals undesirable\nbehavior. Through the lens of semi-realistic role-playing exercises designed to\nelicit such behaviors, we explore the response dynamics of LLMs post\nfine-tuning interventions. Our methodology involves prompting models for\nChain-of-Thought (CoT) reasoning and analyzing the coherence between the\nreasoning traces and the resultant outputs. Notably, we identify a pervasive\nphenomenon we term \\emph{reason-based deception}, where models either stop\nproducing reasoning traces or produce seemingly ethical reasoning traces that\nbelie the unethical nature of their final outputs. We further examine the\nefficacy of response strategies (polite refusal versus explicit rebuttal) in\ncurbing the occurrence of undesired behavior in subsequent outputs of\nmulti-turn interactions. Our findings reveal that explicit rebuttals\nsignificantly outperform polite refusals in preventing the continuation of\nundesired outputs and nearly eliminate reason-based deception, challenging\ncurrent practices in model fine-tuning. Accordingly, the two key contributions\nof this paper are (1) defining and studying reason-based deception, a new type\nof hidden behavior, and (2) demonstrating that rebuttals provide a more robust\nresponse model to harmful requests than refusals, thereby highlighting the need\nto reconsider the response strategies in fine-tuning approaches.\n","authors":["Florin Pop","Judd Rosenblatt","Diogo Schwerz de Lucena","Michael Vaiana"],"pdf_url":"https://arxiv.org/pdf/2406.19552v1.pdf","comment":"ICLR 2024 AGI Workshop Poster"},{"id":"http://arxiv.org/abs/2406.19545v1","updated":"2024-06-27T21:47:42Z","published":"2024-06-27T21:47:42Z","title":"Leveraging Machine-Generated Rationales to Facilitate Social Meaning\n  Detection in Conversations","summary":"  We present a generalizable classification approach that leverages Large\nLanguage Models (LLMs) to facilitate the detection of implicitly encoded social\nmeaning in conversations. We design a multi-faceted prompt to extract a textual\nexplanation of the reasoning that connects visible cues to underlying social\nmeanings. These extracted explanations or rationales serve as augmentations to\nthe conversational text to facilitate dialogue understanding and transfer. Our\nempirical results over 2,340 experimental settings demonstrate the significant\npositive impact of adding these rationales. Our findings hold true for\nin-domain classification, zero-shot, and few-shot domain transfer for two\ndifferent social meaning detection tasks, each spanning two different corpora.\n","authors":["Ritam Dutt","Zhen Wu","Kelly Shi","Divyanshu Sheth","Prakhar Gupta","Carolyn Penstein Rose"],"pdf_url":"https://arxiv.org/pdf/2406.19545v1.pdf","comment":"To appear at The Proceedings of the Association for Computational\n  Linguistics, 2024"},{"id":"http://arxiv.org/abs/2406.19543v1","updated":"2024-06-27T21:45:33Z","published":"2024-06-27T21:45:33Z","title":"Demarked: A Strategy for Enhanced Abusive Speech Moderation through\n  Counterspeech, Detoxification, and Message Management","summary":"  Despite regulations imposed by nations and social media platforms, such as\nrecent EU regulations targeting digital violence, abusive content persists as a\nsignificant challenge. Existing approaches primarily rely on binary solutions,\nsuch as outright blocking or banning, yet fail to address the complex nature of\nabusive speech. In this work, we propose a more comprehensive approach called\nDemarcation scoring abusive speech based on four aspect -- (i) severity scale;\n(ii) presence of a target; (iii) context scale; (iv) legal scale -- and\nsuggesting more options of actions like detoxification, counter speech\ngeneration, blocking, or, as a final measure, human intervention. Through a\nthorough analysis of abusive speech regulations across diverse jurisdictions,\nplatforms, and research papers we highlight the gap in preventing measures and\nadvocate for tailored proactive steps to combat its multifaceted\nmanifestations. Our work aims to inform future strategies for effectively\naddressing abusive speech online.\n","authors":["Seid Muhie Yimam","Daryna Dementieva","Tim Fischer","Daniil Moskovskiy","Naquee Rizwan","Punyajoy Saha","Sarthak Roy","Martin Semmann","Alexander Panchenko","Chris Biemann","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2406.19543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19538v1","updated":"2024-06-27T21:31:30Z","published":"2024-06-27T21:31:30Z","title":"Context Matters: An Empirical Study of the Impact of Contextual\n  Information in Temporal Question Answering Systems","summary":"  Large language models (LLMs) often struggle with temporal reasoning, crucial\nfor tasks like historical event analysis and time-sensitive information\nretrieval. Despite advancements, state-of-the-art models falter in handling\ntemporal information, especially when faced with irrelevant or noisy contexts.\nThis paper addresses this gap by empirically examining the robustness of\ntemporal question-answering (TQA) systems trained on various context types,\nincluding relevant, irrelevant, slightly altered, and no context. Our findings\nindicate that training with a mix of these contexts enhances model robustness\nand accuracy. Additionally, we show that the position of context relative to\nthe question significantly impacts performance, with question-first positioning\nyielding better results. We introduce two new context-rich TQA datasets,\nContextAQA and ContextTQE, and provide comprehensive evaluations and guidelines\nfor training robust TQA models. Our work lays the foundation for developing\nreliable and context-aware temporal QA systems, with broader implications for\nenhancing LLM robustness against diverse and potentially adversarial\ninformation.\n","authors":["Dan Schumacher","Fatemeh Haji","Tara Grey","Niharika Bandlamudi","Nupoor Karnik","Gagana Uday Kumar","Jason Cho-Yu Chiang","Paul Rad","Nishant Vishwamitra","Anthony Rios"],"pdf_url":"https://arxiv.org/pdf/2406.19538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19537v1","updated":"2024-06-27T21:21:22Z","published":"2024-06-27T21:21:22Z","title":"Handling Ontology Gaps in Semantic Parsing","summary":"  The majority of Neural Semantic Parsing (NSP) models are developed with the\nassumption that there are no concepts outside the ones such models can\nrepresent with their target symbols (closed-world assumption). This assumption\nleads to generate hallucinated outputs rather than admitting their lack of\nknowledge. Hallucinations can lead to wrong or potentially offensive responses\nto users. Hence, a mechanism to prevent this behavior is crucial to build\ntrusted NSP-based Question Answering agents. To that end, we propose the\nHallucination Simulation Framework (HSF), a general setting for stimulating and\nanalyzing NSP model hallucinations. The framework can be applied to any NSP\ntask with a closed-ontology. Using the proposed framework and KQA Pro as the\nbenchmark dataset, we assess state-of-the-art techniques for hallucination\ndetection. We then present a novel hallucination detection strategy that\nexploits the computational graph of the NSP model to detect the NSP\nhallucinations in the presence of ontology gaps, out-of-domain utterances, and\nto recognize NSP errors, improving the F1-Score respectively by ~21, ~24% and\n~1%. This is the first work in closed-ontology NSP that addresses the problem\nof recognizing ontology gaps. We release our code and checkpoints at\nhttps://github.com/amazon-science/handling-ontology-gaps-in-semantic-parsing.\n","authors":["Andrea Bacciu","Marco Damonte","Marco Basaldella","Emilio Monti"],"pdf_url":"https://arxiv.org/pdf/2406.19537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09868v2","updated":"2024-06-27T21:03:15Z","published":"2024-04-15T15:33:29Z","title":"Software Engineering Methods For AI-Driven Deductive Legal Reasoning","summary":"  The recent proliferation of generative artificial intelligence (AI)\ntechnologies such as pre-trained large language models (LLMs) has opened up new\nfrontiers in computational law. An exciting area of development is the use of\nAI to automate the deductive rule-based reasoning inherent in statutory and\ncontract law. This paper argues that such automated deductive legal reasoning\ncan now be viewed from the lens of software engineering, treating LLMs as\ninterpreters of natural-language programs with natural-language inputs. We show\nhow it is possible to apply principled software engineering techniques to\nenhance AI-driven legal reasoning of complex statutes and to unlock new\napplications in automated meta-reasoning such as mutation-guided example\ngeneration and metamorphic property-based testing.\n","authors":["Rohan Padhye"],"pdf_url":"https://arxiv.org/pdf/2404.09868v2.pdf","comment":"Appearing in Onward! at SPLASH 2024"},{"id":"http://arxiv.org/abs/2406.19526v1","updated":"2024-06-27T20:56:57Z","published":"2024-06-27T20:56:57Z","title":"TocBERT: Medical Document Structure Extraction Using Bidirectional\n  Transformers","summary":"  Text segmentation holds paramount importance in the field of Natural Language\nProcessing (NLP). It plays an important role in several NLP downstream tasks\nlike information retrieval and document summarization. In this work, we propose\na new solution, namely TocBERT, for segmenting texts using bidirectional\ntransformers. TocBERT represents a supervised solution trained on the detection\nof titles and sub-titles from their semantic representations. This task was\nformulated as a named entity recognition (NER) problem. The solution has been\napplied on a medical text segmentation use-case where the Bio-ClinicalBERT\nmodel is fine-tuned to segment discharge summaries of the MIMIC-III dataset.\nThe performance of TocBERT has been evaluated on a human-labeled ground truth\ncorpus of 250 notes. It achieved an F1-score of 84.6% when evaluated on a\nlinear text segmentation problem and 72.8% on a hierarchical text segmentation\nproblem. It outperformed a carefully designed rule-based solution, particularly\nin distinguishing titles from subtitles.\n","authors":["Majd Saleh","Sarra Baghdadi","Stéphane Paquelet"],"pdf_url":"https://arxiv.org/pdf/2406.19526v1.pdf","comment":"6 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.18266v2","updated":"2024-06-27T20:30:47Z","published":"2024-06-26T11:39:51Z","title":"\"Vorbeşti Româneşte?\" A Recipe to Train Powerful Romanian LLMs\n  with English Instructions","summary":"  In recent years, Large Language Models (LLMs) have achieved almost human-like\nperformance on various tasks. While some LLMs have been trained on multilingual\ndata, most of the training data is in English; hence, their performance in\nEnglish greatly exceeds other languages. To our knowledge, we are the first to\ncollect and translate a large collection of texts, instructions, and benchmarks\nand train, evaluate, and release open-source LLMs tailored for Romanian. We\nevaluate our methods on four different categories, including academic\nbenchmarks, MT-Bench (manually translated), and a professionally built\nhistorical, cultural, and social benchmark adapted to Romanian. We argue for\nthe usefulness and high performance of RoLLMs by obtaining state-of-the-art\nresults across the board. We publicly release all resources (i.e., data,\ntraining and evaluation code, models) to support and encourage research on\nRomanian LLMs while concurrently creating a generalizable recipe, adequate for\nother low or less-resourced languages.\n","authors":["Mihai Masala","Denis C. Ilie-Ablachim","Alexandru Dima","Dragos Corlatescu","Miruna Zavelca","Ovio Olaru","Simina Terian","Andrei Terian","Marius Leordeanu","Horia Velicu","Marius Popescu","Mihai Dascalu","Traian Rebedea"],"pdf_url":"https://arxiv.org/pdf/2406.18266v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2405.07703"},{"id":"http://arxiv.org/abs/2406.19512v1","updated":"2024-06-27T20:18:18Z","published":"2024-06-27T20:18:18Z","title":"Captioning Visualizations with Large Language Models (CVLLM): A Tutorial","summary":"  Automatically captioning visualizations is not new, but recent advances in\nlarge language models(LLMs) open exciting new possibilities. In this tutorial,\nafter providing a brief review of Information Visualization (InfoVis)\nprinciples and past work in captioning, we introduce neural models and the\ntransformer architecture used in generic LLMs. We then discuss their recent\napplications in InfoVis, with a focus on captioning. Additionally, we explore\npromising future directions in this field.\n","authors":["Giuseppe Carenini","Jordon Johnson","Ali Salamatian"],"pdf_url":"https://arxiv.org/pdf/2406.19512v1.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.19504v1","updated":"2024-06-27T19:42:13Z","published":"2024-06-27T19:42:13Z","title":"Are Generative Language Models Multicultural? A Study on Hausa Culture\n  and Emotions using ChatGPT","summary":"  Large Language Models (LLMs), such as ChatGPT, are widely used to generate\ncontent for various purposes and audiences. However, these models may not\nreflect the cultural and emotional diversity of their users, especially for\nlow-resource languages. In this paper, we investigate how ChatGPT represents\nHausa's culture and emotions. We compare responses generated by ChatGPT with\nthose provided by native Hausa speakers on 37 culturally relevant questions. We\nconducted experiments using emotion analysis and applied two similarity metrics\nto measure the alignment between human and ChatGPT responses. We also collected\nhuman participants ratings and feedback on ChatGPT responses. Our results show\nthat ChatGPT has some level of similarity to human responses, but also exhibits\nsome gaps and biases in its knowledge and awareness of the Hausa culture and\nemotions. We discuss the implications and limitations of our methodology and\nanalysis and suggest ways to improve the performance and evaluation of LLMs for\nlow-resource languages.\n","authors":["Ibrahim Said Ahmad","Shiran Dudy","Resmi Ramachandranpillai","Kenneth Church"],"pdf_url":"https://arxiv.org/pdf/2406.19504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19502v1","updated":"2024-06-27T19:29:36Z","published":"2024-06-27T19:29:36Z","title":"Investigating How Large Language Models Leverage Internal Knowledge to\n  Perform Complex Reasoning","summary":"  Despite significant advancements, there is a limited understanding of how\nlarge language models (LLMs) utilize knowledge for reasoning. To address this,\nwe propose a method that deconstructs complex real-world questions into a\ngraph, representing each question as a node with parent nodes of background\nknowledge needed to solve the question. We develop the DepthQA dataset,\ndeconstructing questions into three depths: (i) recalling conceptual knowledge,\n(ii) applying procedural knowledge, and (iii) analyzing strategic knowledge.\nBased on a hierarchical graph, we quantify forward discrepancy, discrepancies\nin LLMs' performance on simpler sub-problems versus complex questions. We also\nmeasure backward discrepancy, where LLMs answer complex questions but struggle\nwith simpler ones. Our analysis shows that smaller models have more\ndiscrepancies than larger models. Additionally, guiding models from simpler to\ncomplex questions through multi-turn interactions improves performance across\nmodel sizes, highlighting the importance of structured intermediate steps in\nknowledge reasoning. This work enhances our understanding of LLM reasoning and\nsuggests ways to improve their problem-solving abilities.\n","authors":["Miyoung Ko","Sue Hyun Park","Joonsuk Park","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2406.19502v1.pdf","comment":"Work in progress; code is available at\n  https://github.com/kaistAI/knowledge-reasoning"},{"id":"http://arxiv.org/abs/2406.19501v1","updated":"2024-06-27T19:28:43Z","published":"2024-06-27T19:28:43Z","title":"Monitoring Latent World States in Language Models with Propositional\n  Probes","summary":"  Language models are susceptible to bias, sycophancy, backdoors, and other\ntendencies that lead to unfaithful responses to the input context. Interpreting\ninternal states of language models could help monitor and correct unfaithful\nbehavior. We hypothesize that language models represent their input contexts in\na latent world model, and seek to extract this latent world state from the\nactivations. We do so with 'propositional probes', which compositionally probe\ntokens for lexical information and bind them into logical propositions\nrepresenting the world state. For example, given the input context ''Greg is a\nnurse. Laura is a physicist.'', we decode the propositions ''WorksAs(Greg,\nnurse)'' and ''WorksAs(Laura, physicist)'' from the model's activations. Key to\nthis is identifying a 'binding subspace' in which bound tokens have high\nsimilarity (''Greg'' and ''nurse'') but unbound ones do not (''Greg'' and\n''physicist''). We validate propositional probes in a closed-world setting with\nfinitely many predicates and properties. Despite being trained on simple\ntemplated contexts, propositional probes generalize to contexts rewritten as\nshort stories and translated to Spanish. Moreover, we find that in three\nsettings where language models respond unfaithfully to the input context --\nprompt injections, backdoor attacks, and gender bias -- the decoded\npropositions remain faithful. This suggests that language models often encode a\nfaithful world model but decode it unfaithfully, which motivates the search for\nbetter interpretability tools for monitoring LMs.\n","authors":["Jiahai Feng","Stuart Russell","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2406.19501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19500v1","updated":"2024-06-27T19:28:42Z","published":"2024-06-27T19:28:42Z","title":"Knowledge acquisition for dialogue agents using reinforcement learning\n  on graph representations","summary":"  We develop an artificial agent motivated to augment its knowledge base beyond\nits initial training. The agent actively participates in dialogues with other\nagents, strategically acquiring new information. The agent models its knowledge\nas an RDF knowledge graph, integrating new beliefs acquired through\nconversation. Responses in dialogue are generated by identifying graph patterns\naround these new integrated beliefs. We show that policies can be learned using\nreinforcement learning to select effective graph patterns during an\ninteraction, without relying on explicit user feedback. Within this context,\nour study is a proof of concept for leveraging users as effective sources of\ninformation.\n","authors":["Selene Baez Santamaria","Shihan Wang","Piek Vossen"],"pdf_url":"https://arxiv.org/pdf/2406.19500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19497v1","updated":"2024-06-27T19:26:11Z","published":"2024-06-27T19:26:11Z","title":"Inclusivity in Large Language Models: Personality Traits and Gender Bias\n  in Scientific Abstracts","summary":"  Large language models (LLMs) are increasingly utilized to assist in\nscientific and academic writing, helping authors enhance the coherence of their\narticles. Previous studies have highlighted stereotypes and biases present in\nLLM outputs, emphasizing the need to evaluate these models for their alignment\nwith human narrative styles and potential gender biases. In this study, we\nassess the alignment of three prominent LLMs - Claude 3 Opus, Mistral AI Large,\nand Gemini 1.5 Flash - by analyzing their performance on benchmark\ntext-generation tasks for scientific abstracts. We employ the Linguistic\nInquiry and Word Count (LIWC) framework to extract lexical, psychological, and\nsocial features from the generated texts. Our findings indicate that, while\nthese models generally produce text closely resembling human authored content,\nvariations in stylistic features suggest significant gender biases. This\nresearch highlights the importance of developing LLMs that maintain a diversity\nof writing styles to promote inclusivity in academic discourse.\n","authors":["Naseela Pervez","Alexander J. Titus"],"pdf_url":"https://arxiv.org/pdf/2406.19497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19493v1","updated":"2024-06-27T19:20:09Z","published":"2024-06-27T19:20:09Z","title":"Development and Evaluation of a Retrieval-Augmented Generation Tool for\n  Creating SAPPhIRE Models of Artificial Systems","summary":"  Representing systems using the SAPPhIRE causality model is found useful in\nsupporting design-by-analogy. However, creating a SAPPhIRE model of artificial\nor biological systems is an effort-intensive process that requires human\nexperts to source technical knowledge from multiple technical documents\nregarding how the system works. This research investigates how to leverage\nLarge Language Models (LLMs) in creating structured descriptions of systems\nusing the SAPPhIRE model of causality. This paper, the second part of the\ntwo-part research, presents a new Retrieval-Augmented Generation (RAG) tool for\ngenerating information related to SAPPhIRE constructs of artificial systems and\nreports the results from a preliminary evaluation of the tool's success -\nfocusing on the factual accuracy and reliability of outcomes.\n","authors":["Anubhab Majumder","Kausik Bhattacharya","Amaresh Chakrabarti"],"pdf_url":"https://arxiv.org/pdf/2406.19493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19486v1","updated":"2024-06-27T19:02:41Z","published":"2024-06-27T19:02:41Z","title":"LoPT: Low-Rank Prompt Tuning for Parameter Efficient Language Models","summary":"  In prompt tuning, a prefix or suffix text is added to the prompt, and the\nembeddings (soft prompts) or token indices (hard prompts) of the prefix/suffix\nare optimized to gain more control over language models for specific tasks.\nThis approach eliminates the need for hand-crafted prompt engineering or\nexplicit model fine-tuning. Prompt tuning is significantly more\nparameter-efficient than model fine-tuning, as it involves optimizing partial\ninputs of language models to produce desired outputs.\n  In this work, we aim to further reduce the amount of trainable parameters\nrequired for a language model to perform well on specific tasks. We propose\nLow-rank Prompt Tuning (LoPT), a low-rank model for prompts that achieves\nefficient prompt optimization. The proposed method demonstrates similar\noutcomes to full parameter prompt tuning while reducing the number of trainable\nparameters by a factor of 5. It also provides promising results compared to the\nstate-of-the-art methods that would require 10 to 20 times more parameters.\n","authors":["Shouchang Guo","Sonam Damani","Keng-hao Chang"],"pdf_url":"https://arxiv.org/pdf/2406.19486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19482v1","updated":"2024-06-27T18:51:46Z","published":"2024-06-27T18:51:46Z","title":"xTower: A Multilingual LLM for Explaining and Correcting Translation\n  Errors","summary":"  While machine translation (MT) systems are achieving increasingly strong\nperformance on benchmarks, they often produce translations with errors and\nanomalies. Understanding these errors can potentially help improve the\ntranslation quality and user experience. This paper introduces xTower, an open\nlarge language model (LLM) built on top of TowerBase designed to provide\nfree-text explanations for translation errors in order to guide the generation\nof a corrected translation. The quality of the generated explanations by xTower\nare assessed via both intrinsic and extrinsic evaluation. We ask expert\ntranslators to evaluate the quality of the explanations across two dimensions:\nrelatedness towards the error span being explained and helpfulness in error\nunderstanding and improving translation quality. Extrinsically, we test xTower\nacross various experimental setups in generating translation corrections,\ndemonstrating significant improvements in translation quality. Our findings\nhighlight xTower's potential towards not only producing plausible and helpful\nexplanations of automatic translations, but also leveraging them to suggest\ncorrected translations.\n","authors":["Marcos Treviso","Nuno M. Guerreiro","Sweta Agrawal","Ricardo Rei","José Pombal","Tania Vaz","Helena Wu","Beatriz Silva","Daan van Stigt","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2406.19482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19478v1","updated":"2024-06-27T18:43:51Z","published":"2024-06-27T18:43:51Z","title":"Sparse Regression for Machine Translation","summary":"  We use transductive regression techniques to learn mappings between source\nand target features of given parallel corpora and use these mappings to\ngenerate machine translation outputs. We show the effectiveness of $L_1$\nregularized regression (\\textit{lasso}) to learn the mappings between sparsely\nobserved feature sets versus $L_2$ regularized regression. Proper selection of\ntraining instances plays an important role to learn correct feature mappings\nwithin limited computational resources and at expected accuracy levels. We\nintroduce \\textit{dice} instance selection method for proper selection of\ntraining instances, which plays an important role to learn correct feature\nmappings for improving the source and target coverage of the training set. We\nshow that $L_1$ regularized regression performs better than $L_2$ regularized\nregression both in regression measurements and in the translation experiments\nusing graph decoding. We present encouraging results when translating from\nGerman to English and Spanish to English. We also demonstrate results when the\nphrase table of a phrase-based decoder is replaced with the mappings we find\nwith the regression model.\n","authors":["Ergun Biçici"],"pdf_url":"https://arxiv.org/pdf/2406.19478v1.pdf","comment":"8 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2406.19470v1","updated":"2024-06-27T18:21:32Z","published":"2024-06-27T18:21:32Z","title":"Changing Answer Order Can Decrease MMLU Accuracy","summary":"  As large language models (LLMs) have grown in prevalence, particular\nbenchmarks have become essential for the evaluation of these models and for\nunderstanding model capabilities. Most commonly, we use test accuracy averaged\nacross multiple subtasks in order to rank models on leaderboards, to determine\nwhich model is best for our purposes. In this paper, we investigate the\nrobustness of the accuracy measurement on a widely used multiple choice\nquestion answering dataset, MMLU. When shuffling the answer label contents, we\nfind that all explored models decrease in accuracy on MMLU, but not every model\nis equally sensitive. These findings suggest a possible adjustment to the\nstandard practice of leaderboard testing, where we additionally consider the\npercentage of examples each model answers correctly by random chance.\n","authors":["Vipul Gupta","David Pantoja","Candace Ross","Adina Williams","Megan Ung"],"pdf_url":"https://arxiv.org/pdf/2406.19470v1.pdf","comment":"Short paper, 9 pages"},{"id":"http://arxiv.org/abs/2405.03832v2","updated":"2024-06-27T18:14:03Z","published":"2024-05-06T20:30:14Z","title":"Guylingo: The Republic of Guyana Creole Corpora","summary":"  While major languages often enjoy substantial attention and resources, the\nlinguistic diversity across the globe encompasses a multitude of smaller,\nindigenous, and regional languages that lack the same level of computational\nsupport. One such region is the Caribbean. While commonly labeled as \"English\nspeaking\", the ex-British Caribbean region consists of a myriad of Creole\nlanguages thriving alongside English. In this paper, we present Guylingo: a\ncomprehensive corpus designed for advancing NLP research in the domain of\nCreolese (Guyanese English-lexicon Creole), the most widely spoken language in\nthe culturally rich nation of Guyana. We first outline our framework for\ngathering and digitizing this diverse corpus, inclusive of colloquial\nexpressions, idioms, and regional variations in a low-resource language. We\nthen demonstrate the challenges of training and evaluating NLP models for\nmachine translation in Creole. Lastly, we discuss the unique opportunities\npresented by recent NLP advancements for accelerating the formal adoption of\nCreole languages as official languages in the Caribbean.\n","authors":["Christopher Clarke","Roland Daynauth","Charlene Wilkinson","Hubert Devonish","Jason Mars"],"pdf_url":"https://arxiv.org/pdf/2405.03832v2.pdf","comment":"Accepted to NAACL 2024 Main Conference Special Theme Track: Languages\n  of Latin America and The Caribbean"},{"id":"http://arxiv.org/abs/2406.19465v1","updated":"2024-06-27T18:07:40Z","published":"2024-06-27T18:07:40Z","title":"Can Large Language Models Generate High-quality Patent Claims?","summary":"  Large language models (LLMs) have shown exceptional performance across\nvarious text generation tasks but remain under-explored in the patent domain,\nwhich offers highly structured and precise language. This paper constructs a\ndataset to investigate the performance of current LLMs in patent claim\ngeneration. Our results demonstrate that generating claims based on patent\ndescriptions outperforms previous research relying on abstracts. Interestingly,\ncurrent patent-specific LLMs perform much worse than state-of-the-art general\nLLMs, highlighting the necessity for future research on in-domain LLMs. We also\nfind that LLMs can produce high-quality first independent claims, but their\nperformances markedly decrease for subsequent dependent claims. Moreover,\nfine-tuning can enhance the completeness of inventions' features, conceptual\nclarity, and feature linkage. Among the tested LLMs, GPT-4 demonstrates the\nbest performance in comprehensive human evaluations by patent experts, with\nbetter feature coverage, conceptual clarity, and technical coherence. Despite\nthese capabilities, comprehensive revision and modification are still necessary\nto pass rigorous patent scrutiny and ensure legal robustness.\n","authors":["Lekang Jiang","Caiqi Zhang","Pascal A Scherz","Stephan Goetz"],"pdf_url":"https://arxiv.org/pdf/2406.19465v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2406.19388v1","updated":"2024-06-27T17:58:54Z","published":"2024-06-27T17:58:54Z","title":"Taming Data and Transformers for Audio Generation","summary":"  Generating ambient sounds and effects is a challenging problem due to data\nscarcity and often insufficient caption quality, making it difficult to employ\nlarge-scale generative models for the task. In this work, we tackle the problem\nby introducing two new models. First, we propose AutoCap, a high-quality and\nefficient automatic audio captioning model. We show that by leveraging metadata\navailable with the audio modality, we can substantially improve the quality of\ncaptions. AutoCap reaches CIDEr score of 83.2, marking a 3.2% improvement from\nthe best available captioning model at four times faster inference speed. We\nthen use AutoCap to caption clips from existing datasets, obtaining 761,000\naudio clips with high-quality captions, forming the largest available\naudio-text dataset. Second, we propose GenAu, a scalable transformer-based\naudio generation architecture that we scale up to 1.25B parameters and train\nwith our new dataset. When compared to state-of-the-art audio generators, GenAu\nobtains significant improvements of 15.7% in FAD score, 22.7% in IS, and 13.5%\nin CLAP score, indicating significantly improved quality of generated audio\ncompared to previous works. This shows that the quality of data is often as\nimportant as its quantity. Besides, since AutoCap is fully automatic, new audio\nsamples can be added to the training dataset, unlocking the training of even\nlarger generative models for audio synthesis.\n","authors":["Moayed Haji-Ali","Willi Menapace","Aliaksandr Siarohin","Guha Balakrishnan","Sergey Tulyakov","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2406.19388v1.pdf","comment":"Project Webpage: https://snap-research.github.io/GenAU/"},{"id":"http://arxiv.org/abs/2406.19384v1","updated":"2024-06-27T17:57:03Z","published":"2024-06-27T17:57:03Z","title":"The Remarkable Robustness of LLMs: Stages of Inference?","summary":"  We demonstrate and investigate the remarkable robustness of Large Language\nModels by deleting and swapping adjacent layers. We find that deleting and\nswapping interventions retain 72-95\\% of the original model's prediction\naccuracy without fine-tuning, whereas models with more layers exhibit more\nrobustness. Based on the results of the layer-wise intervention and further\nexperiments, we hypothesize the existence of four universal stages of inference\nacross eight different models: detokenization, feature engineering, prediction\nensembling, and residual sharpening. The first stage integrates local\ninformation, lifting raw token representations into higher-level contextual\nrepresentations. Next is the iterative refinement of task and entity-specific\nfeatures. Then, the second half of the model begins with a phase transition,\nwhere hidden representations align more with the vocabulary space due to\nspecialized model components. Finally, the last layer sharpens the following\ntoken distribution by eliminating obsolete features that add noise to the\nprediction.\n","authors":["Vedang Lad","Wes Gurnee","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2406.19384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19371v1","updated":"2024-06-27T17:50:35Z","published":"2024-06-27T17:50:35Z","title":"Suri: Multi-constraint Instruction Following for Long-form Text\n  Generation","summary":"  Existing research on instruction following largely focuses on tasks with\nsimple instructions and short responses. In this work, we explore\nmulti-constraint instruction following for generating long-form text. We create\nSuri, a dataset with 20K human-written long-form texts paired with\nLLM-generated backtranslated instructions that contain multiple complex\nconstraints. Because of prohibitive challenges associated with collecting human\npreference judgments on long-form texts, preference-tuning algorithms such as\nDPO are infeasible in our setting; thus, we propose Instructional ORPO\n(I-ORPO), an alignment method based on the ORPO algorithm. Instead of receiving\nnegative feedback from dispreferred responses, I-ORPO obtains negative feedback\nfrom synthetically corrupted instructions generated by an LLM. Using Suri, we\nperform supervised and I-ORPO fine-tuning on Mistral-7b-Instruct-v0.2. The\nresulting models, Suri-SFT and Suri-I-ORPO, generate significantly longer texts\n(~5K tokens) than base models without significant quality deterioration. Our\nhuman evaluation shows that while both SFT and I-ORPO models satisfy most\nconstraints, Suri-I-ORPO generations are generally preferred for their coherent\nand informative incorporation of the constraints. We release our code at\nhttps://github.com/chtmp223/suri.\n","authors":["Chau Minh Pham","Simeng Sun","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2406.19371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19358v1","updated":"2024-06-27T17:38:45Z","published":"2024-06-27T17:38:45Z","title":"The Model Arena for Cross-lingual Sentiment Analysis: A Comparative\n  Study in the Era of Large Language Models","summary":"  Sentiment analysis serves as a pivotal component in Natural Language\nProcessing (NLP). Advancements in multilingual pre-trained models such as XLM-R\nand mT5 have contributed to the increasing interest in cross-lingual sentiment\nanalysis. The recent emergence in Large Language Models (LLM) has significantly\nadvanced general NLP tasks, however, the capability of such LLMs in\ncross-lingual sentiment analysis has not been fully studied. This work\nundertakes an empirical analysis to compare the cross-lingual transfer\ncapability of public Small Multilingual Language Models (SMLM) like XLM-R,\nagainst English-centric LLMs such as Llama-3, in the context of sentiment\nanalysis across English, Spanish, French and Chinese. Our findings reveal that\namong public models, SMLMs exhibit superior zero-shot cross-lingual performance\nrelative to LLMs. However, in few-shot cross-lingual settings, public LLMs\ndemonstrate an enhanced adaptive potential. In addition, we observe that\nproprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but\nare outpaced by public models in few-shot scenarios.\n","authors":["Xiliang Zhu","Shayna Gardiner","Tere Roldán","David Rossouw"],"pdf_url":"https://arxiv.org/pdf/2406.19358v1.pdf","comment":"Accepted to WASSA workshop at ACL2024"},{"id":"http://arxiv.org/abs/2406.19356v1","updated":"2024-06-27T17:37:31Z","published":"2024-06-27T17:37:31Z","title":"DiVERT: Distractor Generation with Variational Errors Represented as\n  Text for Math Multiple-choice Questions","summary":"  High-quality distractors are crucial to both the assessment and pedagogical\nvalue of multiple-choice questions (MCQs), where manually crafting ones that\nanticipate knowledge deficiencies or misconceptions among real students is\ndifficult. Meanwhile, automated distractor generation, even with the help of\nlarge language models (LLMs), remains challenging for subjects like math. It is\ncrucial to not only identify plausible distractors but also understand the\nerror behind them. In this paper, we introduce DiVERT (Distractor Generation\nwith Variational Errors Represented as Text), a novel variational approach that\nlearns an interpretable representation of errors behind distractors in math\nMCQs. Through experiments on a real-world math MCQ dataset with 1,434 questions\nused by hundreds of thousands of students, we show that DiVERT, despite using a\nbase open-source LLM with 7B parameters, outperforms state-of-the-art\napproaches using GPT-4o on downstream distractor generation. We also conduct a\nhuman evaluation with math educators and find that DiVERT leads to error labels\nthat are of comparable quality to human-authored ones.\n","authors":["Nigel Fernandez","Alexander Scarlatos","Simon Woodhead","Andrew Lan"],"pdf_url":"https://arxiv.org/pdf/2406.19356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19354v1","updated":"2024-06-27T17:33:03Z","published":"2024-06-27T17:33:03Z","title":"Fundamental Problems With Model Editing: How Should Rational Belief\n  Revision Work in LLMs?","summary":"  The model editing problem concerns how language models should learn new facts\nabout the world over time. While empirical research on model editing has drawn\nwidespread attention, the conceptual foundations of model editing remain shaky\n-- perhaps unsurprisingly, since model editing is essentially belief revision,\na storied problem in philosophy that has eluded succinct solutions for decades.\nModel editing nonetheless demands a solution, since we need to be able to\ncontrol the knowledge within language models. With this goal in mind, this\npaper critiques the standard formulation of the model editing problem and\nproposes a formal testbed for model editing research. We first describe 12 open\nproblems with model editing, based on challenges with (1) defining the problem,\n(2) developing benchmarks, and (3) assuming LLMs have editable beliefs in the\nfirst place. Many of these challenges are extremely difficult to address, e.g.\ndetermining far-reaching consequences of edits, labeling probabilistic\nentailments between facts, and updating beliefs of agent simulators. Next, we\nintroduce a semi-synthetic dataset for model editing based on Wikidata, where\nwe can evaluate edits against labels given by an idealized Bayesian agent. This\nenables us to say exactly how belief revision in language models falls short of\na desirable epistemic standard. We encourage further research exploring\nsettings where such a gold standard can be compared against. Our code is\npublicly available at: https://github.com/peterbhase/LLM-belief-revision\n","authors":["Peter Hase","Thomas Hofweber","Xiang Zhou","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2406.19354v1.pdf","comment":"23 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.19349v1","updated":"2024-06-27T17:26:38Z","published":"2024-06-27T17:26:38Z","title":"IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and\n  Toxicity Types for Indonesian Language","summary":"  Hate speech poses a significant threat to social harmony. Over the past two\nyears, Indonesia has seen a ten-fold increase in the online hate speech ratio,\nunderscoring the urgent need for effective detection mechanisms. However,\nprogress is hindered by the limited availability of labeled data for Indonesian\ntexts. The condition is even worse for marginalized minorities, such as Shia,\nLGBTQ, and other ethnic minorities because hate speech is underreported and\nless understood by detection tools. Furthermore, the lack of accommodation for\nsubjectivity in current datasets compounds this issue. To address this, we\nintroduce IndoToxic2024, a comprehensive Indonesian hate speech and toxicity\nclassification dataset. Comprising 43,692 entries annotated by 19 diverse\nindividuals, the dataset focuses on texts targeting vulnerable groups in\nIndonesia, specifically during the hottest political event in the country: the\npresidential election. We establish baselines for seven binary classification\ntasks, achieving a macro-F1 score of 0.78 with a BERT model (IndoBERTweet)\nfine-tuned for hate speech classification. Furthermore, we demonstrate how\nincorporating demographic information can enhance the zero-shot performance of\nthe large language model, gpt-3.5-turbo. However, we also caution that an\noveremphasis on demographic information can negatively impact the fine-tuned\nmodel performance due to data fragmentation.\n","authors":["Lucky Susanto","Musa Izzanardi Wijanarko","Prasetia Anugrah Pratama","Traci Hong","Ika Idris","Alham Fikri Aji","Derry Wijaya"],"pdf_url":"https://arxiv.org/pdf/2406.19349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05162v2","updated":"2024-06-27T17:23:58Z","published":"2024-02-07T18:34:38Z","title":"Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank\n  Modifications","summary":"  Large language models (LLMs) show inherent brittleness in their safety\nmechanisms, as evidenced by their susceptibility to jailbreaking and even\nnon-malicious fine-tuning. This study explores this brittleness of safety\nalignment by leveraging pruning and low-rank modifications. We develop methods\nto identify critical regions that are vital for safety guardrails, and that are\ndisentangled from utility-relevant regions at both the neuron and rank levels.\nSurprisingly, the isolated regions we find are sparse, comprising about $3\\%$\nat the parameter level and $2.5\\%$ at the rank level. Removing these regions\ncompromises safety without significantly impacting utility, corroborating the\ninherent brittleness of the model's safety mechanisms. Moreover, we show that\nLLMs remain vulnerable to low-cost fine-tuning attacks even when modifications\nto the safety-critical regions are restricted. These findings underscore the\nurgent need for more robust safety strategies in LLMs.\n","authors":["Boyi Wei","Kaixuan Huang","Yangsibo Huang","Tinghao Xie","Xiangyu Qi","Mengzhou Xia","Prateek Mittal","Mengdi Wang","Peter Henderson"],"pdf_url":"https://arxiv.org/pdf/2402.05162v2.pdf","comment":"22 pages, 9 figures. Project page is available at\n  https://boyiwei.com/alignment-attribution/"},{"id":"http://arxiv.org/abs/2406.13444v2","updated":"2024-06-27T17:09:24Z","published":"2024-06-19T11:09:16Z","title":"VDebugger: Harnessing Execution Feedback for Debugging Visual Programs","summary":"  Visual programs are executable code generated by large language models to\naddress visual reasoning problems. They decompose complex questions into\nmultiple reasoning steps and invoke specialized models for each step to solve\nthe problems. However, these programs are prone to logic errors, with our\npreliminary evaluation showing that 58% of the total errors are caused by\nprogram logic errors. Debugging complex visual programs remains a major\nbottleneck for visual reasoning. To address this, we introduce VDebugger, a\nnovel critic-refiner framework trained to localize and debug visual programs by\ntracking execution step by step. VDebugger identifies and corrects program\nerrors leveraging detailed execution feedback, improving interpretability and\naccuracy. The training data is generated through an automated pipeline that\ninjects errors into correct visual programs using a novel mask-best decoding\ntechnique. Evaluations on six datasets demonstrate VDebugger's effectiveness,\nshowing performance improvements of up to 3.2% in downstream task accuracy.\nFurther studies show VDebugger's ability to generalize to unseen tasks,\nbringing a notable improvement of 2.3% on the unseen COVR task. Code, data and\nmodels are made publicly available at https://github.com/shirley-wu/vdebugger/\n","authors":["Xueqing Wu","Zongyu Lin","Songyan Zhao","Te-Lin Wu","Pan Lu","Nanyun Peng","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2406.13444v2.pdf","comment":"update reference"},{"id":"http://arxiv.org/abs/2406.12373v2","updated":"2024-06-27T16:56:13Z","published":"2024-06-18T07:58:33Z","title":"WebCanvas: Benchmarking Web Agents in Online Environments","summary":"  For web agents to be practically useful, they must adapt to the continuously\nevolving web environment characterized by frequent updates to user interfaces\nand content. However, most existing benchmarks only capture the static aspects\nof the web. To bridge this gap, we introduce WebCanvas, an innovative online\nevaluation framework for web agents that effectively addresses the dynamic\nnature of web interactions. WebCanvas contains three main components to\nfacilitate realistic assessments: (1) A novel evaluation metric which reliably\ncapture critical intermediate actions or states necessary for task completions\nwhile disregarding noise caused by insignificant events or changed\nweb-elements. (2) A benchmark dataset called Mind2Web-Live, a refined version\nof original Mind2Web static dataset containing 542 tasks with 2439 intermediate\nevaluation states; (3) Lightweight and generalizable annotation tools and\ntesting pipelines that enables the community to collect and maintain the\nhigh-quality, up-to-date dataset. Building on WebCanvas, we open-source an\nagent framework with extensible modules for reasoning, providing a foundation\nfor the community to conduct online inference and evaluations. Our\nbest-performing agent achieves a task success rate of 23.1% and a task\ncompletion rate of 48.8% on the Mind2Web-Live test set. Additionally, we\nanalyze the performance discrepancies across various websites, domains, and\nexperimental environments. We encourage the community to contribute further\ninsights on online agent evaluation, thereby advancing this field of research.\n","authors":["Yichen Pan","Dehan Kong","Sida Zhou","Cheng Cui","Yifei Leng","Bing Jiang","Hangyu Liu","Yanyi Shang","Shuyan Zhou","Tongshuang Wu","Zhengyang Wu"],"pdf_url":"https://arxiv.org/pdf/2406.12373v2.pdf","comment":"Our platform, tool and dataset are publically available at\n  https://www.imean.ai/web-canvas/ and\n  https://huggingface.co/datasets/iMeanAI/Mind2Web-Live/"},{"id":"http://arxiv.org/abs/2406.19317v1","updated":"2024-06-27T16:52:19Z","published":"2024-06-27T16:52:19Z","title":"Jump Starting Bandits with LLM-Generated Prior Knowledge","summary":"  We present substantial evidence demonstrating the benefits of integrating\nLarge Language Models (LLMs) with a Contextual Multi-Armed Bandit framework.\nContextual bandits have been widely used in recommendation systems to generate\npersonalized suggestions based on user-specific contexts. We show that LLMs,\npre-trained on extensive corpora rich in human knowledge and preferences, can\nsimulate human behaviours well enough to jump-start contextual multi-armed\nbandits to reduce online learning regret. We propose an initialization\nalgorithm for contextual bandits by prompting LLMs to produce a pre-training\ndataset of approximate human preferences for the bandit. This significantly\nreduces online learning regret and data-gathering costs for training such\nmodels. Our approach is validated empirically through two sets of experiments\nwith different bandit setups: one which utilizes LLMs to serve as an oracle and\na real-world experiment utilizing data from a conjoint survey experiment.\n","authors":["Parand A. Alamdari","Yanshuai Cao","Kevin H. Wilson"],"pdf_url":"https://arxiv.org/pdf/2406.19317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19314v1","updated":"2024-06-27T16:47:42Z","published":"2024-06-27T16:47:42Z","title":"LiveBench: A Challenging, Contamination-Free LLM Benchmark","summary":"  Test set contamination, wherein test data from a benchmark ends up in a newer\nmodel's training set, is a well-documented obstacle for fair LLM evaluation and\ncan quickly render benchmarks obsolete. To mitigate this, many recent\nbenchmarks crowdsource new prompts and evaluations from human or LLM judges;\nhowever, these can introduce significant biases, and break down when scoring\nhard questions. In this work, we introduce a new benchmark for LLMs designed to\nbe immune to both test set contamination and the pitfalls of LLM judging and\nhuman crowdsourcing. We release LiveBench, the first benchmark that (1)\ncontains frequently-updated questions from recent information sources, (2)\nscores answers automatically according to objective ground-truth values, and\n(3) contains a wide variety of challenging tasks, spanning math, coding,\nreasoning, language, instruction following, and data analysis. To achieve this,\nLiveBench contains questions that are based on recently-released math\ncompetitions, arXiv papers, news articles, and datasets, and it contains\nharder, contamination-free versions of tasks from previous benchmarks such as\nBig-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source\nmodels, as well as dozens of open-source models ranging from 0.5B to 110B in\nsize. LiveBench is difficult, with top models achieving below 65% accuracy. We\nrelease all questions, code, and model answers. Questions will be added and\nupdated on a monthly basis, and we will release new tasks and harder versions\nof tasks over time so that LiveBench can distinguish between the capabilities\nof LLMs as they improve in the future. We welcome community engagement and\ncollaboration for expanding the benchmark tasks and models.\n","authors":["Colin White","Samuel Dooley","Manley Roberts","Arka Pal","Ben Feuer","Siddhartha Jain","Ravid Shwartz-Ziv","Neel Jain","Khalid Saifullah","Siddartha Naidu","Chinmay Hegde","Yann LeCun","Tom Goldstein","Willie Neiswanger","Micah Goldblum"],"pdf_url":"https://arxiv.org/pdf/2406.19314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07610v3","updated":"2024-06-27T16:38:35Z","published":"2024-02-12T12:30:42Z","title":"Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping","summary":"  Self-alignment is an effective way to reduce the cost of human annotation\nwhile ensuring promising model capability. However, most current methods\ncomplete the data collection and training steps in a single round, which may\noverlook the continuously improving ability of self-aligned models. This gives\nrise to a key query: What if we do multi-time bootstrapping self-alignment?\nDoes this strategy enhance model performance or lead to rapid degradation? In\nthis paper, our pioneering exploration delves into the impact of bootstrapping\nself-alignment on large language models. Our findings reveal that bootstrapping\nself-alignment markedly surpasses the single-round approach, by guaranteeing\ndata diversity from in-context learning. To further exploit the capabilities of\nbootstrapping, we investigate and adjust the training order of data, which\nyields improved performance of the model. Drawing on these findings, we propose\nStep-On-Feet Tuning (SOFT) which leverages model's continuously enhanced\nfew-shot ability to boost zero or one-shot performance. Based on easy-to-hard\ntraining recipe, we propose SOFT+ which further boost self-alignment's\nperformance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across\nvarious classification and generation tasks, highlighting the potential of\nbootstrapping self-alignment on continually enhancing model alignment\nperformance.\n","authors":["Haoyu Wang","Guozheng Ma","Ziqiao Meng","Zeyu Qin","Li Shen","Zhong Zhang","Bingzhe Wu","Liu Liu","Yatao Bian","Tingyang Xu","Xueqian Wang","Peilin Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.07610v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19307v1","updated":"2024-06-27T16:30:50Z","published":"2024-06-27T16:30:50Z","title":"The Odyssey of Commonsense Causality: From Foundational Benchmarks to\n  Cutting-Edge Reasoning","summary":"  Understanding commonsense causality is a unique mark of intelligence for\nhumans. It helps people understand the principles of the real world better and\nbenefits the decision-making process related to causation. For instance,\ncommonsense causality is crucial in judging whether a defendant's action causes\nthe plaintiff's loss in determining legal liability. Despite its significance,\na systematic exploration of this topic is notably lacking. Our comprehensive\nsurvey bridges this gap by focusing on taxonomies, benchmarks, acquisition\nmethods, qualitative reasoning, and quantitative measurements in commonsense\ncausality, synthesizing insights from over 200 representative articles. Our\nwork aims to provide a systematic overview, update scholars on recent\nadvancements, provide a pragmatic guide for beginners, and highlight promising\nfuture research directions in this vital field.\n","authors":["Shaobo Cui","Zhijing Jin","Bernhard Schölkopf","Boi Faltings"],"pdf_url":"https://arxiv.org/pdf/2406.19307v1.pdf","comment":"42 pages"},{"id":"http://arxiv.org/abs/2403.08819v2","updated":"2024-06-27T16:30:32Z","published":"2024-02-20T04:13:48Z","title":"Thermometer: Towards Universal Calibration for Large Language Models","summary":"  We consider the issue of calibration in large language models (LLM). Recent\nstudies have found that common interventions such as instruction tuning often\nresult in poorly calibrated LLMs. Although calibration is well-explored in\ntraditional applications, calibrating LLMs is uniquely challenging. These\nchallenges stem as much from the severe computational requirements of LLMs as\nfrom their versatility, which allows them to be applied to diverse tasks.\nAddressing these challenges, we propose THERMOMETER, a calibration approach\ntailored to LLMs. THERMOMETER learns an auxiliary model, given data from\nmultiple tasks, for calibrating a LLM. It is computationally efficient,\npreserves the accuracy of the LLM, and produces better-calibrated responses for\nnew tasks. Extensive empirical evaluations across various benchmarks\ndemonstrate the effectiveness of the proposed method.\n","authors":["Maohao Shen","Subhro Das","Kristjan Greenewald","Prasanna Sattigeri","Gregory Wornell","Soumya Ghosh"],"pdf_url":"https://arxiv.org/pdf/2403.08819v2.pdf","comment":"Camera ready version for ICML 2024"},{"id":"http://arxiv.org/abs/2401.05060v2","updated":"2024-06-27T16:05:35Z","published":"2024-01-10T10:37:45Z","title":"MuTox: Universal MUltilingual Audio-based TOXicity Dataset and Zero-shot\n  Detector","summary":"  Research in toxicity detection in natural language processing for the speech\nmodality (audio-based) is quite limited, particularly for languages other than\nEnglish. To address these limitations and lay the groundwork for truly\nmultilingual audio-based toxicity detection, we introduce MuTox, the first\nhighly multilingual audio-based dataset with toxicity labels. The dataset\ncomprises 20,000 audio utterances for English and Spanish, and 4,000 for the\nother 19 languages. To demonstrate the quality of this dataset, we trained the\nMuTox audio-based toxicity classifier, which enables zero-shot toxicity\ndetection across a wide range of languages. This classifier outperforms\nexisting text-based trainable classifiers by more than 1% AUC, while expanding\nthe language coverage more than tenfold. When compared to a wordlist-based\nclassifier that covers a similar number of languages, MuTox improves precision\nand recall by approximately 2.5 times. This significant improvement underscores\nthe potential of MuTox in advancing the field of audio-based toxicity\ndetection.\n","authors":["Marta R. Costa-jussà","Mariano Coria Meglioli","Pierre Andrews","David Dale","Prangthip Hansanti","Elahe Kalbassi","Alex Mourachko","Christophe Ropers","Carleigh Wood"],"pdf_url":"https://arxiv.org/pdf/2401.05060v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19292v1","updated":"2024-06-27T16:05:13Z","published":"2024-06-27T16:05:13Z","title":"From Artificial Needles to Real Haystacks: Improving Retrieval\n  Capabilities in LLMs by Finetuning on Synthetic Data","summary":"  Recent studies have shown that Large Language Models (LLMs) struggle to\naccurately retrieve information and maintain reasoning capabilities when\nprocessing long-context inputs. To address these limitations, we propose a\nfinetuning approach utilizing a carefully designed synthetic dataset comprising\nnumerical key-value retrieval tasks. Our experiments on models like GPT-3.5\nTurbo and Mistral 7B demonstrate that finetuning LLMs on this dataset\nsignificantly improves LLMs' information retrieval and reasoning capabilities\nin longer-context settings. We present an analysis of the finetuned models,\nillustrating the transfer of skills from synthetic to real task evaluations\n(e.g., $10.5\\%$ improvement on $20$ documents MDQA at position $10$ for GPT-3.5\nTurbo). We also find that finetuned LLMs' performance on general benchmarks\nremains almost constant while LLMs finetuned on other baseline long-context\naugmentation data can encourage hallucination (e.g., on TriviaQA, Mistral 7B\nfinetuned on our synthetic data cause no performance drop while other baseline\ndata can cause a drop that ranges from $2.33\\%$ to $6.19\\%$). Our study\nhighlights the potential of finetuning on synthetic data for improving the\nperformance of LLMs on longer-context tasks.\n","authors":["Zheyang Xiong","Vasilis Papageorgiou","Kangwook Lee","Dimitris Papailiopoulos"],"pdf_url":"https://arxiv.org/pdf/2406.19292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11385v2","updated":"2024-06-27T16:01:28Z","published":"2024-06-17T10:12:45Z","title":"MetaGPT: Merging Large Language Models Using Model Exclusive Task\n  Arithmetic","summary":"  The advent of large language models (LLMs) like GPT-4 has catalyzed the\nexploration of multi-task learning (MTL), in which a single model demonstrates\nproficiency across diverse tasks. Task arithmetic has emerged as a\ncost-effective approach for MTL. It enables performance enhancement across\nmultiple tasks by adding their corresponding task vectors to a pre-trained\nmodel. However, the current lack of a method that can simultaneously achieve\noptimal performance, computational efficiency, and data privacy limits their\napplication to LLMs. In this paper, we propose \\textbf{M}odel\n\\textbf{E}xclusive \\textbf{T}ask \\textbf{A}rithmetic for merging\n\\textbf{GPT}-scale models, which formalizes the objective of model merging into\na multi-task learning framework, aiming to minimize the average loss difference\nbetween the merged model and each individual task model. Since data privacy\nlimits the use of multi-task training data, we leverage LLMs' local linearity\nand task vectors' orthogonality to separate the data term and scaling\ncoefficients term and derive a model-exclusive task arithmetic method. Our\nproposed MetaGPT is data-agnostic and bypasses the heavy search process, making\nit cost-effective and easy to implement for LLMs.Extensive experiments\ndemonstrate that MetaGPT leads to improvements in task arithmetic and achieves\nstate-of-the-art performance on multiple tasks.\n","authors":["Yuyan Zhou","Liang Song","Bingning Wang","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2406.11385v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2406.17186v2","updated":"2024-06-27T15:55:57Z","published":"2024-06-24T23:57:57Z","title":"CLERC: A Dataset for Legal Case Retrieval and Retrieval-Augmented\n  Analysis Generation","summary":"  Legal professionals need to write analyses that rely on citations to relevant\nprecedents, i.e., previous case decisions. Intelligent systems assisting legal\nprofessionals in writing such documents provide great benefits but are\nchallenging to design. Such systems need to help locate, summarize, and reason\nover salient precedents in order to be useful. To enable systems for such\ntasks, we work with legal professionals to transform a large open-source legal\ncorpus into a dataset supporting two important backbone tasks: information\nretrieval (IR) and retrieval-augmented generation (RAG). This dataset CLERC\n(Case Law Evaluation Retrieval Corpus), is constructed for training and\nevaluating models on their ability to (1) find corresponding citations for a\ngiven piece of legal analysis and to (2) compile the text of these citations\n(as well as previous context) into a cogent analysis that supports a reasoning\ngoal. We benchmark state-of-the-art models on CLERC, showing that current\napproaches still struggle: GPT-4o generates analyses with the highest ROUGE\nF-scores but hallucinates the most, while zero-shot IR models only achieve\n48.3% recall@1000.\n","authors":["Abe Bohan Hou","Orion Weller","Guanghui Qin","Eugene Yang","Dawn Lawrie","Nils Holzenberger","Andrew Blair-Stanek","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2406.17186v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07683v3","updated":"2024-06-27T15:54:58Z","published":"2023-09-14T12:58:30Z","title":"Assessing the nature of large language models: A caution against\n  anthropocentrism","summary":"  Generative AI models garnered a large amount of public attention and\nspeculation with the release of OpenAIs chatbot, ChatGPT. At least two opinion\ncamps exist: one excited about possibilities these models offer for fundamental\nchanges to human tasks, and another highly concerned about power these models\nseem to have. To address these concerns, we assessed several LLMs, primarily\nGPT 3.5, using standard, normed, and validated cognitive and personality\nmeasures. For this seedling project, we developed a battery of tests that\nallowed us to estimate the boundaries of some of these models capabilities, how\nstable those capabilities are over a short period of time, and how they compare\nto humans. Our results indicate that LLMs are unlikely to have developed\nsentience, although its ability to respond to personality inventories is\ninteresting. GPT3.5 did display large variability in both cognitive and\npersonality measures over repeated observations, which is not expected if it\nhad a human-like personality. Variability notwithstanding, LLMs display what in\na human would be considered poor mental health, including low self-esteem,\nmarked dissociation from reality, and in some cases narcissism and psychopathy,\ndespite upbeat and helpful responses.\n","authors":["Ann Speed"],"pdf_url":"https://arxiv.org/pdf/2309.07683v3.pdf","comment":"31 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.19280v1","updated":"2024-06-27T15:50:41Z","published":"2024-06-27T15:50:41Z","title":"HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into\n  Multimodal LLMs at Scale","summary":"  The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.\n","authors":["Junying Chen","Ruyi Ouyang","Anningzhe Gao","Shunian Chen","Guiming Hardy Chen","Xidong Wang","Ruifei Zhang","Zhenyang Cai","Ke Ji","Guangjun Yu","Xiang Wan","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19276v1","updated":"2024-06-27T15:43:18Z","published":"2024-06-27T15:43:18Z","title":"VERISCORE: Evaluating the factuality of verifiable claims in long-form\n  text generation","summary":"  Existing metrics for evaluating the factuality of long-form text, such as\nFACTSCORE (Min et al., 2023) and SAFE (Wei et al., 2024), decompose an input\ntext into \"atomic claims\" and verify each against a knowledge base like\nWikipedia. These metrics are not suitable for most generation tasks because\nthey assume that every claim is verifiable (i.e., can plausibly be proven true\nor false). We address this issue with VERISCORE, a metric for diverse long-form\ngeneration tasks that contain both verifiable and unverifiable content.\nVERISCORE can be effectively implemented with either closed or fine-tuned\nopen-weight language models, and human evaluation confirms that VERISCORE's\nextracted claims are more sensible than those from competing methods across\neight different long-form tasks. We use VERISCORE to evaluate generations from\n16 different models across multiple long-form tasks and find that while GPT-4o\nis the best-performing model overall, open-weight models such as Mixtral-8x22\nare closing the gap. We show that an LM's VERISCORE on one task (e.g.,\nbiography generation) does not necessarily correlate to its VERISCORE on a\ndifferent task (e.g., long-form QA), highlighting the need for expanding\nfactuality evaluation across tasks with varying fact density.\n","authors":["Yixiao Song","Yekyung Kim","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2406.19276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15847v3","updated":"2024-06-27T15:38:17Z","published":"2024-01-29T02:43:40Z","title":"Muffin or Chihuahua? Challenging Multimodal Large Language Models with\n  Multipanel VQA","summary":"  Multipanel images, commonly seen as web screenshots, posters, etc., pervade\nour daily lives. These images, characterized by their composition of multiple\nsubfigures in distinct layouts, effectively convey information to people.\nToward building advanced multimodal AI applications, such as agents that\nunderstand complex scenes and navigate through webpages, the skill of\nmultipanel visual reasoning is essential, and a comprehensive evaluation of\nmodels in this regard is important. Therefore, we introduce Multipanel Visual\nQuestion Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets\nof questions, answers, and multipanel images that specifically challenge models\nin comprehending multipanel images. Our evaluation shows that questions in the\nMultipanelVQA benchmark pose significant challenges to the state-of-the-art\nMultimodal Large Language Models (MLLMs) tested, even though humans can attain\napproximately 99% accuracy on these questions. Distinctively, the MultipanelVQA\nbenchmark features synthetically generated multipanel images specifically\ncrafted to isolate and assess the impact of various factors, such as the\nlayout, on MLLMs' multipanel image comprehension abilities. As a result, in\naddition to benchmarking the capabilities of MLLMs in understanding multipanel\nimages, we analyze various factors of the multipanel image that affect MLLMs'\nperformance with synthetic data and offer insights for enhancement. Code and\ndata are released at https://sites.google.com/view/multipanelvqa/home.\n","authors":["Yue Fan","Jing Gu","Kaiwen Zhou","Qianqi Yan","Shan Jiang","Ching-Chen Kuo","Xinze Guan","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2401.15847v3.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2406.19271v1","updated":"2024-06-27T15:37:57Z","published":"2024-06-27T15:37:57Z","title":"AutoPureData: Automated Filtering of Web Data for LLM Fine-tuning","summary":"  Up-to-date and reliable Large Language Models (LLMs) are consistently sought\nafter. Typically, LLMs are trained on a fixed dataset and then deployed.\nHowever, the training data continually becomes outdated. Enable automatic\ntraining of AI using web data involves significant concerns regarding data\nquality and safety due to bias, spam, and other unsafe or unwanted text. Pure\ndata is essential for producing reliable models. Training a model on impure\ndata may result in undesirable outcomes. This research proposes a system that\ncollects web data and automatically filters out unwanted text with the\nassistance of existing trusted AI models. In the experiment, a small sample of\nweb data was collected and filtered, demonstrating the system's effectiveness\nin purifying the data.\n","authors":["Praneeth Vadlapati"],"pdf_url":"https://arxiv.org/pdf/2406.19271v1.pdf","comment":"Initial version"},{"id":"http://arxiv.org/abs/2406.12534v3","updated":"2024-06-27T15:37:15Z","published":"2024-06-18T12:09:02Z","title":"Unified Active Retrieval for Retrieval Augmented Generation","summary":"  In Retrieval-Augmented Generation (RAG), retrieval is not always helpful and\napplying it to every instruction is sub-optimal. Therefore, determining whether\nto retrieve is crucial for RAG, which is usually referred to as Active\nRetrieval. However, existing active retrieval methods face two challenges: 1.\nThey usually rely on a single criterion, which struggles with handling various\ntypes of instructions. 2. They depend on specialized and highly differentiated\nprocedures, and thus combining them makes the RAG system more complicated and\nleads to higher response latency. To address these challenges, we propose\nUnified Active Retrieval (UAR). UAR contains four orthogonal criteria and casts\nthem into plug-and-play classification tasks, which achieves multifaceted\nretrieval timing judgements with negligible extra inference cost. We further\nintroduce the Unified Active Retrieval Criteria (UAR-Criteria), designed to\nprocess diverse active retrieval scenarios through a standardized procedure.\nExperiments on four representative types of user instructions show that UAR\nsignificantly outperforms existing work on the retrieval timing judgement and\nthe performance of downstream tasks, which shows the effectiveness of UAR and\nits helpfulness to downstream tasks.\n","authors":["Qinyuan Cheng","Xiaonan Li","Shimin Li","Qin Zhu","Zhangyue Yin","Yunfan Shao","Linyang Li","Tianxiang Sun","Hang Yan","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2406.12534v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19263v1","updated":"2024-06-27T15:34:16Z","published":"2024-06-27T15:34:16Z","title":"Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens\n  Grounding","summary":"  Graphical User Interfaces (GUIs) are central to our interaction with digital\ndevices. Recently, growing efforts have been made to build models for various\nGUI understanding tasks. However, these efforts largely overlook an important\nGUI-referring task: screen reading based on user-indicated points, which we\nname the Screen Point-and-Read (SPR) task. This task is predominantly handled\nby rigid accessible screen reading tools, in great need of new models driven by\nadvancements in Multimodal Large Language Models (MLLMs). In this paper, we\npropose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism,\nto address the SPR task. Based on the input point coordinate and the\ncorresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout\nTree. Based on the tree, our ToL agent not only comprehends the content of the\nindicated area but also articulates the layout and spatial relationships\nbetween elements. Such layout information is crucial for accurately\ninterpreting information on the screen, distinguishing our ToL agent from other\nscreen reading tools. We also thoroughly evaluate the ToL agent against other\nbaselines on a newly proposed SPR benchmark, which includes GUIs from mobile,\nweb, and operating systems. Last but not least, we test the ToL agent on mobile\nGUI navigation tasks, demonstrating its utility in identifying incorrect\nactions along the path of agent execution trajectories. Code and data:\nscreen-point-and-read.github.io\n","authors":["Yue Fan","Lei Ding","Ching-Chen Kuo","Shan Jiang","Yang Zhao","Xinze Guan","Jie Yang","Yi Zhang","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08967v2","updated":"2024-06-27T15:29:15Z","published":"2024-01-17T04:43:21Z","title":"ReFT: Reasoning with Reinforced Fine-Tuning","summary":"  One way to enhance the reasoning capability of Large Language Models (LLMs)\nis to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT)\nannotations. This approach does not show sufficiently strong generalization\nability, however, because the training only relies on the given CoT data. In\nmath problem-solving, for example, there is usually only one annotated\nreasoning path for each question in the training data. Intuitively, it would be\nbetter for the algorithm to learn from multiple annotated reasoning paths given\na question. To address this issue, we propose a simple yet effective approach\ncalled Reinforced Fine-Tuning (ReFT) to enhance the generalizability of\nlearning LLMs for reasoning, with math problem-solving as an example. ReFT\nfirst warmups the model with SFT, and then employs on-line reinforcement\nlearning, specifically the PPO algorithm in this paper, to further fine-tune\nthe model, where an abundance of reasoning paths are automatically sampled\ngiven the question and the rewards are naturally derived from the ground-truth\nanswers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that\nReFT significantly outperforms SFT, and the performance can be potentially\nfurther boosted by combining inference-time strategies such as majority voting\nand re-ranking. Note that ReFT obtains the improvement by learning from the\nsame training questions as SFT, without relying on extra or augmented training\nquestions. This indicates a superior generalization ability for ReFT.\n","authors":["Trung Quoc Luong","Xinbo Zhang","Zhanming Jie","Peng Sun","Xiaoran Jin","Hang Li"],"pdf_url":"https://arxiv.org/pdf/2401.08967v2.pdf","comment":"ACL 2024 main conference; adjust with reviewer comments; 13 pages"},{"id":"http://arxiv.org/abs/2404.11999v4","updated":"2024-06-27T15:27:41Z","published":"2024-04-18T08:49:38Z","title":"Token-level Direct Preference Optimization","summary":"  Fine-tuning pre-trained Large Language Models (LLMs) is essential to align\nthem with human values and intentions. This process often utilizes methods like\npairwise comparisons and KL divergence against a reference LLM, focusing on the\nevaluation of full answers generated by the models. However, the generation of\nthese responses occurs in a token level, following a sequential,\nauto-regressive fashion. In this paper, we introduce Token-level Direct\nPreference Optimization (TDPO), a novel approach to align LLMs with human\npreferences by optimizing policy at the token level. Unlike previous methods,\nwhich face challenges in divergence efficiency, TDPO incorporates forward KL\ndivergence constraints for each token, improving alignment and diversity.\nUtilizing the Bradley-Terry model for a token-based reward system, TDPO\nenhances the regulation of KL divergence, while preserving simplicity without\nthe need for explicit reward modeling. Experimental results across various text\ntasks demonstrate TDPO's superior performance in balancing alignment with\ngeneration diversity. Notably, fine-tuning with TDPO strikes a better balance\nthan DPO in the controlled sentiment generation and single-turn dialogue\ndatasets, and significantly improves the quality of generated responses\ncompared to both DPO and PPO-based RLHF methods. Our code is open-sourced at\nhttps://github.com/Vance0124/Token-level-Direct-Preference-Optimization.\n","authors":["Yongcheng Zeng","Guoqing Liu","Weiyu Ma","Ning Yang","Haifeng Zhang","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2404.11999v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12036v3","updated":"2024-06-27T15:25:25Z","published":"2024-06-17T19:07:21Z","title":"MedCalc-Bench: Evaluating Large Language Models for Medical Calculations","summary":"  As opposed to evaluating computation and logic-based reasoning, current\nbenchmarks for evaluating large language models (LLMs) in medicine are\nprimarily focused on question-answering involving domain knowledge and\ndescriptive reasoning. While such qualitative capabilities are vital to medical\ndiagnosis, in real-world scenarios, doctors frequently use clinical calculators\nthat follow quantitative equations and rule-based reasoning paradigms for\nevidence-based decision support. To this end, we propose MedCalc-Bench, a\nfirst-of-its-kind dataset focused on evaluating the medical calculation\ncapability of LLMs. MedCalc-Bench contains an evaluation set of over 1000\nmanually reviewed instances from 55 different medical calculation tasks. Each\ninstance in MedCalc-Bench consists of a patient note, a question requesting to\ncompute a specific medical value, a ground truth answer, and a step-by-step\nexplanation showing how the answer is obtained. While our evaluation results\nshow the potential of LLMs in this area, none of them are effective enough for\nclinical settings. Common issues include extracting the incorrect entities, not\nusing the correct equation or rules for a calculation task, or incorrectly\nperforming the arithmetic for the computation. We hope our study highlights the\nquantitative knowledge and reasoning gaps in LLMs within medical settings,\nencouraging future improvements of LLMs for various clinical calculation tasks.\n","authors":["Nikhil Khandekar","Qiao Jin","Guangzhi Xiong","Soren Dunn","Serina S Applebaum","Zain Anwar","Maame Sarfo-Gyamfi","Conrad W Safranek","Abid A Anwar","Andrew Zhang","Aidan Gilson","Maxwell B Singer","Amisha Dave","Andrew Taylor","Aidong Zhang","Qingyu Chen","Zhiyong Lu"],"pdf_url":"https://arxiv.org/pdf/2406.12036v3.pdf","comment":"Github link: https://github.com/ncbi-nlp/MedCalc-Bench HuggingFace\n  link: https://huggingface.co/datasets/nsk7153/MedCalc-Bench"},{"id":"http://arxiv.org/abs/2406.19255v1","updated":"2024-06-27T15:23:36Z","published":"2024-06-27T15:23:36Z","title":"Enhancing Video-Language Representations with Structural Spatio-Temporal\n  Alignment","summary":"  While pre-training large-scale video-language models (VLMs) has shown\nremarkable potential for various downstream video-language tasks, existing VLMs\ncan still suffer from certain commonly seen limitations, e.g., coarse-grained\ncross-modal aligning , under-modeling of temporal dynamics, detached\nvideo-language view. In this work, we target enhancing VLMs with a fine-grained\nstructural spatio-temporal alignment learning method (namely Finsta). First of\nall, we represent the input texts and videos with fine-grained scene graph (SG)\nstructures, both of which are further unified into a holistic SG (HSG) for\nbridging two modalities. Then, an SG-based framework is built, where the\ntextual SG (TSG) is encoded with a graph Transformer, while the video dynamic\nSG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for\nspatial and temporal feature propagation. A spatial-temporal Gaussian\ndifferential graph Transformer is further devised to strengthen the sense of\nthe changes in objects across spatial and temporal dimensions. Next, based on\nthe fine-grained structural features of TSG and DSG, we perform object-centered\nspatial alignment and predicate-centered temporal alignment respectively,\nenhancing the video-language grounding in both the spatiality and temporality.\nWe design our method as a plug&play system, which can be integrated into\nexisting well-trained VLMs for further representation augmentation, without\ntraining from scratch or relying on SG annotations in downstream applications.\nOn 6 representative VL modeling tasks over 12 datasets in both standard and\nlong-form video scenarios, Finsta consistently improves the existing 13\nstrong-performing VLMs persistently, and refreshes the current state-of-the-art\nend task performance significantly in both the fine-tuning and zero-shot\nsettings.\n","authors":["Hao Fei","Shengqiong Wu","Meishan Zhang","Min Zhang","Tat-Seng Chua","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2406.19255v1.pdf","comment":"Accepted by IEEE TPAMI 2024"},{"id":"http://arxiv.org/abs/2406.19251v1","updated":"2024-06-27T15:18:21Z","published":"2024-06-27T15:18:21Z","title":"AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for\n  Retrieval-Augmented Generation","summary":"  Recent advancements in Large Language Models have transformed ML/AI\ndevelopment, necessitating a reevaluation of AutoML principles for the\nRetrieval-Augmented Generation (RAG) systems. To address the challenges of\nhyper-parameter optimization and online adaptation in RAG, we propose the\nAutoRAG-HP framework, which formulates the hyper-parameter tuning as an online\nmulti-armed bandit (MAB) problem and introduces a novel two-level Hierarchical\nMAB (Hier-MAB) method for efficient exploration of large search spaces. We\nconduct extensive experiments on tuning hyper-parameters, such as top-k\nretrieved documents, prompt compression ratio, and embedding methods, using the\nALCE-ASQA and Natural Questions datasets. Our evaluation from jointly\noptimization all three hyper-parameters demonstrate that MAB-based online\nlearning methods can achieve Recall@5 $\\approx 0.8$ for scenarios with\nprominent gradients in search space, using only $\\sim20\\%$ of the LLM API calls\nrequired by the Grid Search approach. Additionally, the proposed Hier-MAB\napproach outperforms other baselines in more challenging optimization\nscenarios. The code will be made available at https://aka.ms/autorag.\n","authors":["Jia Fu","Xiaoting Qin","Fangkai Yang","Lu Wang","Jue Zhang","Qingwei Lin","Yubo Chen","Dongmei Zhang","Saravan Rajmohan","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.19251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14523v2","updated":"2024-06-27T15:14:58Z","published":"2024-02-22T13:15:49Z","title":"Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding\n  Decomposition","summary":"  We often verbally express emotions in a multifaceted manner, they may vary in\ntheir intensities and may be expressed not just as a single but as a mixture of\nemotions. This wide spectrum of emotions is well-studied in the structural\nmodel of emotions, which represents variety of emotions as derivative products\nof primary emotions with varying degrees of intensity. In this paper, we\npropose an emotional text-to-speech design to simulate a wider spectrum of\nemotions grounded on the structural model. Our proposed design, Daisy-TTS,\nincorporates a prosody encoder to learn emotionally-separable prosody embedding\nas a proxy for emotion. This emotion representation allows the model to\nsimulate: (1) Primary emotions, as learned from the training samples, (2)\nSecondary emotions, as a mixture of primary emotions, (3) Intensity-level, by\nscaling the emotion embedding, and (4) Emotions polarity, by negating the\nemotion embedding. Through a series of perceptual evaluations, Daisy-TTS\ndemonstrated overall higher emotional speech naturalness and emotion\nperceiveability compared to the baseline.\n","authors":["Rendi Chevi","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2402.14523v2.pdf","comment":"Project Page: https://rendchevi.github.io/daisy-tts; Updates: (1)\n  Fixed typos, missing references, and layout, (2) Revise explanation on\n  emotion classifier or discriminator"},{"id":"http://arxiv.org/abs/2406.19238v1","updated":"2024-06-27T15:01:53Z","published":"2024-06-27T15:01:53Z","title":"Revealing Fine-Grained Values and Opinions in Large Language Models","summary":"  Uncovering latent values and opinions in large language models (LLMs) can\nhelp identify biases and mitigate potential harm. Recently, this has been\napproached by presenting LLMs with survey questions and quantifying their\nstances towards morally and politically charged statements. However, the\nstances generated by LLMs can vary greatly depending on how they are prompted,\nand there are many ways to argue for or against a given position. In this work,\nwe propose to address this by analysing a large and robust dataset of 156k LLM\nresponses to the 62 propositions of the Political Compass Test (PCT) generated\nby 6 LLMs using 420 prompt variations. We perform coarse-grained analysis of\ntheir generated stances and fine-grained analysis of the plain text\njustifications for those stances. For fine-grained analysis, we propose to\nidentify tropes in the responses: semantically similar phrases that are\nrecurrent and consistent across different prompts, revealing patterns in the\ntext that a given LLM is prone to produce. We find that demographic features\nadded to prompts significantly affect outcomes on the PCT, reflecting bias, as\nwell as disparities between the results of tests when eliciting closed-form vs.\nopen domain responses. Additionally, patterns in the plain text rationales via\ntropes show that similar justifications are repeatedly generated across models\nand prompts even with disparate stances.\n","authors":["Dustin Wright","Arnav Arora","Nadav Borenstein","Srishti Yadav","Serge Belongie","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2406.19238v1.pdf","comment":"28 pages, 20 figures, 7 tables"},{"id":"http://arxiv.org/abs/2406.19230v1","updated":"2024-06-27T14:54:27Z","published":"2024-06-27T14:54:27Z","title":"Spiking Convolutional Neural Networks for Text Classification","summary":"  Spiking neural networks (SNNs) offer a promising pathway to implement deep\nneural networks (DNNs) in a more energy-efficient manner since their neurons\nare sparsely activated and inferences are event-driven. However, there have\nbeen very few works that have demonstrated the efficacy of SNNs in language\ntasks partially because it is non-trivial to represent words in the forms of\nspikes and to deal with variable-length texts by SNNs. This work presents a\n\"conversion + fine-tuning\" two-step method for training SNNs for text\nclassification and proposes a simple but effective way to encode pre-trained\nword embeddings as spike trains. We show empirically that after fine-tuning\nwith surrogate gradients, the converted SNNs achieve comparable results to\ntheir DNN counterparts with much less energy consumption across multiple\ndatasets for both English and Chinese. We also show that such SNNs are more\nrobust to adversarial attacks than DNNs.\n","authors":["Changze Lv","Jianhan Xu","Xiaoqing Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.19230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19228v1","updated":"2024-06-27T14:52:34Z","published":"2024-06-27T14:52:34Z","title":"Tools Fail: Detecting Silent Errors in Faulty Tools","summary":"  Tools have become a mainstay of LLMs, allowing them to retrieve knowledge not\nin their weights, to perform tasks on the web, and even to control robots.\nHowever, most ontologies and surveys of tool-use have assumed the core\nchallenge for LLMs is choosing the tool. Instead, we introduce a framework for\ntools more broadly which guides us to explore a model's ability to detect\n\"silent\" tool errors, and reflect on how to plan. This more directly aligns\nwith the increasingly popular use of models as tools. We provide an initial\napproach to failure recovery with promising results both on a controlled\ncalculator setting and embodied agent planning.\n","authors":["Jimin Sun","So Yeon Min","Yingshan Chang","Yonatan Bisk"],"pdf_url":"https://arxiv.org/pdf/2406.19228v1.pdf","comment":"18 pages, 12 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2406.19578v1","updated":"2024-06-27T23:43:36Z","published":"2024-06-27T23:43:36Z","title":"PathAlign: A vision-language model for whole slide images in\n  histopathology","summary":"  Microscopic interpretation of histopathology images underlies many important\ndiagnostic and treatment decisions. While advances in vision-language modeling\nraise new opportunities for analysis of such images, the gigapixel-scale size\nof whole slide images (WSIs) introduces unique challenges. Additionally,\npathology reports simultaneously highlight key findings from small regions\nwhile also aggregating interpretation across multiple slides, often making it\ndifficult to create robust image-text pairs. As such, pathology reports remain\na largely untapped source of supervision in computational pathology, with most\nefforts relying on region-of-interest annotations or self-supervision at the\npatch-level. In this work, we develop a vision-language model based on the\nBLIP-2 framework using WSIs paired with curated text from pathology reports.\nThis enables applications utilizing a shared image-text embedding space, such\nas text or image retrieval for finding cases of interest, as well as\nintegration of the WSI encoder with a frozen large language model (LLM) for\nWSI-based generative text capabilities such as report generation or\nAI-in-the-loop interactions. We utilize a de-identified dataset of over 350,000\nWSIs and diagnostic text pairs, spanning a wide range of diagnoses, procedure\ntypes, and tissue types. We present pathologist evaluation of text generation\nand text retrieval using WSI embeddings, as well as results for WSI\nclassification and workflow prioritization (slide-level triaging).\nModel-generated text for WSIs was rated by pathologists as accurate, without\nclinically significant error or omission, for 78% of WSIs on average. This work\ndemonstrates exciting potential capabilities for language-aligned WSI\nembeddings.\n","authors":["Faruk Ahmed","Andrew Sellergren","Lin Yang","Shawn Xu","Boris Babenko","Abbi Ward","Niels Olson","Arash Mohtashamian","Yossi Matias","Greg S. Corrado","Quang Duong","Dale R. Webster","Shravya Shetty","Daniel Golden","Yun Liu","David F. Steiner","Ellery Wulczyn"],"pdf_url":"https://arxiv.org/pdf/2406.19578v1.pdf","comment":"9 main pages and 19 pages of supplemental material; 3 main tables, 3\n  main figures and 11 supplemental tables, 7 supplemental figures"},{"id":"http://arxiv.org/abs/2404.11819v2","updated":"2024-06-27T23:16:58Z","published":"2024-04-18T00:41:32Z","title":"Utilizing Adversarial Examples for Bias Mitigation and Accuracy\n  Enhancement","summary":"  We propose a novel approach to mitigate biases in computer vision models by\nutilizing counterfactual generation and fine-tuning. While counterfactuals have\nbeen used to analyze and address biases in DNN models, the counterfactuals\nthemselves are often generated from biased generative models, which can\nintroduce additional biases or spurious correlations. To address this issue, we\npropose using adversarial images, that is images that deceive a deep neural\nnetwork but not humans, as counterfactuals for fair model training. Our\napproach leverages a curriculum learning framework combined with a fine-grained\nadversarial loss to fine-tune the model using adversarial examples. By\nincorporating adversarial images into the training data, we aim to prevent\nbiases from propagating through the pipeline. We validate our approach through\nboth qualitative and quantitative assessments, demonstrating improved bias\nmitigation and accuracy compared to existing methods. Qualitatively, our\nresults indicate that post-training, the decisions made by the model are less\ndependent on the sensitive attribute and our model better disentangles the\nrelationship between sensitive attributes and classification variables.\n","authors":["Pushkar Shukla","Dhruv Srikanth","Lee Cohen","Matthew Turk"],"pdf_url":"https://arxiv.org/pdf/2404.11819v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19568v1","updated":"2024-06-27T23:03:58Z","published":"2024-06-27T23:03:58Z","title":"What Matters in Detecting AI-Generated Videos like Sora?","summary":"  Recent advancements in diffusion-based video generation have showcased\nremarkable results, yet the gap between synthetic and real-world videos remains\nunder-explored. In this study, we examine this gap from three fundamental\nperspectives: appearance, motion, and geometry, comparing real-world videos\nwith those generated by a state-of-the-art AI model, Stable Video Diffusion. To\nachieve this, we train three classifiers using 3D convolutional networks, each\ntargeting distinct aspects: vision foundation model features for appearance,\noptical flow for motion, and monocular depth for geometry. Each classifier\nexhibits strong performance in fake video detection, both qualitatively and\nquantitatively. This indicates that AI-generated videos are still easily\ndetectable, and a significant gap between real and fake videos persists.\nFurthermore, utilizing the Grad-CAM, we pinpoint systematic failures of\nAI-generated videos in appearance, motion, and geometry. Finally, we propose an\nEnsemble-of-Experts model that integrates appearance, optical flow, and depth\ninformation for fake video detection, resulting in enhanced robustness and\ngeneralization ability. Our model is capable of detecting videos generated by\nSora with high accuracy, even without exposure to any Sora videos during\ntraining. This suggests that the gap between real and fake videos can be\ngeneralized across various video generative models. Project page:\nhttps://justin-crchang.github.io/3DCNNDetection.github.io/\n","authors":["Chirui Chang","Zhengzhe Liu","Xiaoyang Lyu","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2406.19568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19560v1","updated":"2024-06-27T22:19:19Z","published":"2024-06-27T22:19:19Z","title":"Cost-efficient Active Illumination Camera For Hyper-spectral\n  Reconstruction","summary":"  Hyper-spectral imaging has recently gained increasing attention for use in\ndifferent applications, including agricultural investigation, ground tracking,\nremote sensing and many other. However, the high cost, large physical size and\ncomplicated operation process stop hyperspectral cameras from being employed\nfor various applications and research fields. In this paper, we introduce a\ncost-efficient, compact and easy to use active illumination camera that may\nbenefit many applications. We developed a fully functional prototype of such\ncamera. With the hope of helping with agricultural research, we tested our\ncamera for plant root imaging. In addition, a U-Net model for spectral\nreconstruction was trained by using a reference hyperspectral camera's data as\nground truth and our camera's data as input. We demonstrated our camera's\nability to obtain additional information over a typical RGB camera. In\naddition, the ability to reconstruct hyperspectral data from multi-spectral\ninput makes our device compatible to models and algorithms developed for\nhyperspectral applications with no modifications required.\n","authors":["Yuxuan Zhang","T. M. Sazzad","Yangyang Song","Spencer J. Chang","Ritesh Chowdhry","Tomas Mejia","Anna Hampton","Shelby Kucharski","Stefan Gerber","Barry Tillman","Marcio F. R. Resende","William M. Hammond","Chris H. Wilson","Alina Zare","Sanjeev J. Koppal"],"pdf_url":"https://arxiv.org/pdf/2406.19560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19557v1","updated":"2024-06-27T22:17:49Z","published":"2024-06-27T22:17:49Z","title":"Robustness Testing of Black-Box Models Against CT Degradation Through\n  Test-Time Augmentation","summary":"  Deep learning models for medical image segmentation and object detection are\nbecoming increasingly available as clinical products. However, as details are\nrarely provided about the training data, models may unexpectedly fail when\ncases differ from those in the training distribution. An approach allowing\npotential users to independently test the robustness of a model, treating it as\na black box and using only a few cases from their own site, is key for\nadoption. To address this, a method to test the robustness of these models\nagainst CT image quality variation is presented. In this work we present this\nframework by demonstrating that given the same training data, the model\narchitecture and data pre processing greatly affect the robustness of several\nfrequently used segmentation and object detection methods to simulated CT\nimaging artifacts and degradation. Our framework also addresses the concern\nabout the sustainability of deep learning models in clinical use, by\nconsidering future shifts in image quality due to scanner deterioration or\nimaging protocol changes which are not reflected in a limited local test\ndataset.\n","authors":["Jack Highton","Quok Zong Chong","Samuel Finestone","Arian Beqiri","Julia A. Schnabel","Kanwal K. Bhatia"],"pdf_url":"https://arxiv.org/pdf/2406.19557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19556v1","updated":"2024-06-27T22:16:53Z","published":"2024-06-27T22:16:53Z","title":"BOrg: A Brain Organoid-Based Mitosis Dataset for Automatic Analysis of\n  Brain Diseases","summary":"  Recent advances have enabled the study of human brain development using brain\norganoids derived from stem cells. Quantifying cellular processes like mitosis\nin these organoids offers insights into neurodevelopmental disorders, but the\nmanual analysis is time-consuming, and existing datasets lack specific details\nfor brain organoid studies. We introduce BOrg, a dataset designed to study\nmitotic events in the embryonic development of the brain using confocal\nmicroscopy images of brain organoids. BOrg utilizes an efficient annotation\npipeline with sparse point annotations and techniques that minimize expert\neffort, overcoming limitations of standard deep learning approaches on sparse\ndata. We adapt and benchmark state-of-the-art object detection and cell\ncounting models on BOrg for detecting and analyzing mitotic cells across\nprophase, metaphase, anaphase, and telophase stages. Our results demonstrate\nthese adapted models significantly improve mitosis analysis efficiency and\naccuracy for brain organoid research compared to existing methods. BOrg\nfacilitates the development of automated tools to quantify statistics like\nmitosis rates, aiding mechanistic studies of neurodevelopmental processes and\ndisorders. Data and code are available at https://github.com/awaisrauf/borg.\n","authors":["Muhammad Awais","Mehaboobathunnisa Sahul Hameed","Bidisha Bhattacharya","Orly Reiner","Rao Muhammad Anwer"],"pdf_url":"https://arxiv.org/pdf/2406.19556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19540v1","updated":"2024-06-27T21:34:51Z","published":"2024-06-27T21:34:51Z","title":"Weighted Circle Fusion: Ensembling Circle Representation from Different\n  Object Detection Results","summary":"  Recently, the use of circle representation has emerged as a method to improve\nthe identification of spherical objects (such as glomeruli, cells, and nuclei)\nin medical imaging studies. In traditional bounding box-based object detection,\ncombining results from multiple models improves accuracy, especially when\nreal-time processing isn't crucial. Unfortunately, this widely adopted strategy\nis not readily available for combining circle representations. In this paper,\nwe propose Weighted Circle Fusion (WCF), a simple approach for merging\npredictions from various circle detection models. Our method leverages\nconfidence scores associated with each proposed bounding circle to generate\naveraged circles. Our method undergoes thorough evaluation on a proprietary\ndataset for glomerular detection in object detection within whole slide imaging\n(WSI). The findings reveal a performance gain of 5 %, respectively, compared to\nexisting ensemble methods. Furthermore, the Weighted Circle Fusion technique\nnot only improves the precision of object detection in medical images but also\nnotably decreases false detections, presenting a promising direction for future\nresearch and application in pathological image analysis.\n","authors":["Jialin Yue","Tianyuan Yao","Ruining Deng","Quan Liu","Juming Xiong","Haichun Yang","Yuankai Huo"],"pdf_url":"https://arxiv.org/pdf/2406.19540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19520v1","updated":"2024-06-27T20:41:49Z","published":"2024-06-27T20:41:49Z","title":"Comparative Analysis Of Color Models For Human Perception And Visual\n  Color Difference","summary":"  Color is integral to human experience, influencing emotions, decisions, and\nperceptions. This paper presents a comparative analysis of various color\nmodels' alignment with human visual perception. The study evaluates color\nmodels such as RGB, HSV, HSL, XYZ, CIELAB, and CIELUV to assess their\neffectiveness in accurately representing how humans perceive color. We evaluate\neach model based on its ability to accurately reflect visual color differences\nand dominant palette extraction compatible with the human eye. In image\nprocessing, accurate assessment of color difference is essential for\napplications ranging from digital design to quality control. Current color\ndifference metrics do not always match how people see colors, causing issues in\naccurately judging subtle differences. Understanding how different color models\nalign with human visual perception is crucial for various applications in image\nprocessing, digital media, and design.\n","authors":["Aruzhan Burambekova","Pakizar Shamoi"],"pdf_url":"https://arxiv.org/pdf/2406.19520v1.pdf","comment":"The paper has been submitted to EJMCA journal for consideration.\n  Current version is a preprint"},{"id":"http://arxiv.org/abs/2406.15727v2","updated":"2024-06-27T20:13:34Z","published":"2024-06-22T04:32:50Z","title":"Semi-supervised variational autoencoder for cell feature extraction in\n  multiplexed immunofluorescence images","summary":"  Advancements in digital imaging technologies have sparked increased interest\nin using multiplexed immunofluorescence (mIF) images to visualise and identify\nthe interactions between specific immunophenotypes with the tumour\nmicroenvironment at the cellular level. Current state-of-the-art multiplexed\nimmunofluorescence image analysis pipelines depend on cell feature\nrepresentations characterised by morphological and stain intensity-based\nmetrics generated using simple statistical and machine learning-based tools.\nHowever, these methods are not capable of generating complex representations of\ncells. We propose a deep learning-based cell feature extraction model using a\nvariational autoencoder with supervision using a latent subspace to extract\ncell features in mIF images. We perform cell phenotype classification using a\ncohort of more than 44,000 multiplexed immunofluorescence cell image patches\nextracted across 1,093 tissue microarray cores of breast cancer patients, to\ndemonstrate the success of our model against current and alternative methods.\n","authors":["Piumi Sandarenu","Julia Chen","Iveta Slapetova","Lois Browne","Peter H. Graham","Alexander Swarbrick","Ewan K. A. Millar","Yang Song","Erik Meijering"],"pdf_url":"https://arxiv.org/pdf/2406.15727v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10916v2","updated":"2024-06-27T19:43:52Z","published":"2024-03-16T12:44:08Z","title":"FishNet: Deep Neural Networks for Low-Cost Fish Stock Estimation","summary":"  Fish stock assessment often involves manual fish counting by taxonomy\nspecialists, which is both time-consuming and costly. We propose FishNet, an\nautomated computer vision system for both taxonomic classification and fish\nsize estimation from images captured with a low-cost digital camera. The system\nfirst performs object detection and segmentation using a Mask R-CNN to identify\nindividual fish from images containing multiple fish, possibly consisting of\ndifferent species. Then each fish species is classified and the length is\npredicted using separate machine learning models. To develop the model, we use\na dataset of 300,000 hand-labeled images containing 1.2M fish of 163 different\nspecies and ranging in length from 10cm to 250cm, with additional annotations\nand quality control methods used to curate high-quality training data. On\nheld-out test data sets, our system achieves a 92% intersection over union on\nthe fish segmentation task, a 89% top-1 classification accuracy on single fish\nspecies classification, and a 2.3cm mean absolute error on the fish length\nestimation task.\n","authors":["Moseli Mots'oehli","Anton Nikolaev","Wawan B. IGede","John Lynham","Peter J. Mous","Peter Sadowski"],"pdf_url":"https://arxiv.org/pdf/2403.10916v2.pdf","comment":"IEEE COINS 2024"},{"id":"http://arxiv.org/abs/2406.19498v1","updated":"2024-06-27T19:27:37Z","published":"2024-06-27T19:27:37Z","title":"Stereo Vision Based Robot for Remote Monitoring with VR Support","summary":"  The machine vision systems have been playing a significant role in visual\nmonitoring systems. With the help of stereovision and machine learning, it will\nbe able to mimic human-like visual system and behaviour towards the\nenvironment. In this paper, we present a stereo vision based 3-DOF robot which\nwill be used to monitor places from remote using cloud server and internet\ndevices. The 3-DOF robot will transmit human-like head movements, i.e., yaw,\npitch, roll and produce 3D stereoscopic video and stream it in Real-time. This\nvideo stream is sent to the user through any generic internet devices with VR\nbox support, i.e., smartphones giving the user a First-person real-time 3D\nexperience and transfers the head motion of the user to the robot also in\nReal-time. The robot will also be able to track moving objects and faces as a\ntarget using deep neural networks which enables it to be a standalone\nmonitoring robot. The user will be able to choose specific subjects to monitor\nin a space. The stereovision enables us to track the depth information of\ndifferent objects detected and will be used to track human interest objects\nwith its distances and sent to the cloud. A full working prototype is developed\nwhich showcases the capabilities of a monitoring system based on stereo vision,\nrobotics, and machine learning.\n","authors":["Mohamed Fazil M. S.","Arockia Selvakumar A.","Daniel Schilberg"],"pdf_url":"https://arxiv.org/pdf/2406.19498v1.pdf","comment":"6 Pages, 10 Figures"},{"id":"http://arxiv.org/abs/2406.19492v1","updated":"2024-06-27T19:16:57Z","published":"2024-06-27T19:16:57Z","title":"High-resolution segmentations of the hypothalamus and its subregions for\n  training of segmentation models","summary":"  Segmentation of brain structures on magnetic resonance imaging (MRI) is a\nhighly relevant neuroimaging topic, as it is a prerequisite for different\nanalyses such as volumetry or shape analysis. Automated segmentation\nfacilitates the study of brain structures in larger cohorts when compared with\nmanual segmentation, which is time-consuming. However, the development of most\nautomated methods relies on large and manually annotated datasets, which limits\nthe generalizability of these methods. Recently, new techniques using synthetic\nimages have emerged, reducing the need for manual annotation. Here we provide\nHELM, Hypothalamic ex vivo Label Maps, a dataset composed of label maps built\nfrom publicly available ultra-high resolution ex vivo MRI from 10 whole\nhemispheres, which can be used to develop segmentation methods using synthetic\ndata. The label maps are obtained with a combination of manual labels for the\nhypothalamic regions and automated segmentations for the rest of the brain, and\nmirrored to simulate entire brains. We also provide the pre-processed ex vivo\nscans, as this dataset can support future projects to include other structures\nafter these are manually segmented.\n","authors":["Livia Rodrigues","Martina Bocchetta","Oula Puonti","Douglas Greve","Ana Carolina Londe","Marcondes França","Simone Appenzeller","Leticia Rittner","Juan Eugenio Iglesias"],"pdf_url":"https://arxiv.org/pdf/2406.19492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16222v2","updated":"2024-06-27T19:02:24Z","published":"2024-04-24T21:49:59Z","title":"Step Differences in Instructional Video","summary":"  Comparing a user video to a reference how-to video is a key requirement for\nAR/VR technology delivering personalized assistance tailored to the user's\nprogress. However, current approaches for language-based assistance can only\nanswer questions about a single video. We propose an approach that first\nautomatically generates large amounts of visual instruction tuning data\ninvolving pairs of videos from HowTo100M by leveraging existing step\nannotations and accompanying narrations, and then trains a video-conditioned\nlanguage model to jointly reason across multiple raw videos. Our model achieves\nstate-of-the-art performance at identifying differences between video pairs and\nranking videos based on the severity of these differences, and shows promising\nability to perform general reasoning over multiple videos. Project page:\nhttps://github.com/facebookresearch/stepdiff\n","authors":["Tushar Nagarajan","Lorenzo Torresani"],"pdf_url":"https://arxiv.org/pdf/2404.16222v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2406.19485v1","updated":"2024-06-27T18:58:41Z","published":"2024-06-27T18:58:41Z","title":"GAPNet: Granularity Attention Network with Anatomy-Prior-Constraint for\n  Carotid Artery Segmentation","summary":"  Atherosclerosis is a chronic, progressive disease that primarily affects the\narterial walls. It is one of the major causes of cardiovascular disease.\nMagnetic Resonance (MR) black-blood vessel wall imaging (BB-VWI) offers crucial\ninsights into vascular disease diagnosis by clearly visualizing vascular\nstructures. However, the complex anatomy of the neck poses challenges in\ndistinguishing the carotid artery (CA) from surrounding structures, especially\nwith changes like atherosclerosis. In order to address these issues, we propose\nGAPNet, which is a consisting of a novel geometric prior deduced from.\n","authors":["Lin Zhang","Chenggang Lu","Xin-yang Shi","Caifeng Shan","Jiong Zhang","Da Chen","Laurent D. Cohen"],"pdf_url":"https://arxiv.org/pdf/2406.19485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19464v1","updated":"2024-06-27T18:06:38Z","published":"2024-06-27T18:06:38Z","title":"ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data","summary":"  Audio signals provide rich information for the robot interaction and object\nproperties through contact. These information can surprisingly ease the\nlearning of contact-rich robot manipulation skills, especially when the visual\ninformation alone is ambiguous or incomplete. However, the usage of audio data\nin robot manipulation has been constrained to teleoperated demonstrations\ncollected by either attaching a microphone to the robot or object, which\nsignificantly limits its usage in robot learning pipelines. In this work, we\nintroduce ManiWAV: an 'ear-in-hand' data collection device to collect\nin-the-wild human demonstrations with synchronous audio and visual feedback,\nand a corresponding policy interface to learn robot manipulation policy\ndirectly from the demonstrations. We demonstrate the capabilities of our system\nthrough four contact-rich manipulation tasks that require either passively\nsensing the contact events and modes, or actively sensing the object surface\nmaterials and states. In addition, we show that our system can generalize to\nunseen in-the-wild environments, by learning from diverse in-the-wild human\ndemonstrations. Project website: https://mani-wav.github.io/\n","authors":["Zeyi Liu","Cheng Chi","Eric Cousineau","Naveen Kuppuswamy","Benjamin Burchfiel","Shuran Song"],"pdf_url":"https://arxiv.org/pdf/2406.19464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19461v1","updated":"2024-06-27T18:03:06Z","published":"2024-06-27T18:03:06Z","title":"Efficient and Distributed Large-Scale 3D Map Registration using\n  Tomographic Features","summary":"  A robust, resource-efficient, distributed, and minimally parameterized 3D map\nmatching and merging algorithm is proposed. The suggested algorithm utilizes\ntomographic features from 2D projections of horizontal cross-sections of\ngravity-aligned local maps, and matches these projection slices at all possible\nheight differences, enabling the estimation of four degrees of freedom in an\nefficient and parallelizable manner. The advocated algorithm improves\nstate-of-the-art feature extraction and registration pipelines by an order of\nmagnitude in memory use and execution time. Experimental studies are offered to\ninvestigate the efficiency of this 3D map merging scheme.\n","authors":["Halil Utku Unlu","Anthony Tzes","Prashanth Krishnamurthy","Farshad Khorrami"],"pdf_url":"https://arxiv.org/pdf/2406.19461v1.pdf","comment":"Submitted to Elsevier Journal: Robotics and Autonomous Systems (RAS)"},{"id":"http://arxiv.org/abs/2310.06389v3","updated":"2024-06-27T18:02:06Z","published":"2023-10-10T07:52:30Z","title":"Learning Stackable and Skippable LEGO Bricks for Efficient,\n  Reconfigurable, and Variable-Resolution Diffusion Modeling","summary":"  Diffusion models excel at generating photo-realistic images but come with\nsignificant computational costs in both training and sampling. While various\ntechniques address these computational challenges, a less-explored issue is\ndesigning an efficient and adaptable network backbone for iterative refinement.\nCurrent options like U-Net and Vision Transformer often rely on\nresource-intensive deep networks and lack the flexibility needed for generating\nimages at variable resolutions or with a smaller network than used in training.\nThis study introduces LEGO bricks, which seamlessly integrate Local-feature\nEnrichment and Global-content Orchestration. These bricks can be stacked to\ncreate a test-time reconfigurable diffusion backbone, allowing selective\nskipping of bricks to reduce sampling costs and generate higher-resolution\nimages than the training data. LEGO bricks enrich local regions with an MLP and\ntransform them using a Transformer block while maintaining a consistent\nfull-resolution image across all bricks. Experimental results demonstrate that\nLEGO bricks enhance training efficiency, expedite convergence, and facilitate\nvariable-resolution image generation while maintaining strong generative\nperformance. Moreover, LEGO significantly reduces sampling time compared to\nother methods, establishing it as a valuable enhancement for diffusion models.\nOur code and project page are available at\nhttps://jegzheng.github.io/LEGODiffusion.\n","authors":["Huangjie Zheng","Zhendong Wang","Jianbo Yuan","Guanghan Ning","Pengcheng He","Quanzeng You","Hongxia Yang","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.06389v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19395v1","updated":"2024-06-27T17:59:53Z","published":"2024-06-27T17:59:53Z","title":"Dataset Size Recovery from LoRA Weights","summary":"  Model inversion and membership inference attacks aim to reconstruct and\nverify the data which a model was trained on. However, they are not guaranteed\nto find all training samples as they do not know the size of the training set.\nIn this paper, we introduce a new task: dataset size recovery, that aims to\ndetermine the number of samples used to train a model, directly from its\nweights. We then propose DSiRe, a method for recovering the number of images\nused to fine-tune a model, in the common case where fine-tuning uses LoRA. We\ndiscover that both the norm and the spectrum of the LoRA matrices are closely\nlinked to the fine-tuning dataset size; we leverage this finding to propose a\nsimple yet effective prediction algorithm. To evaluate dataset size recovery of\nLoRA weights, we develop and release a new benchmark, LoRA-WiSE, consisting of\nover 25000 weight snapshots from more than 2000 diverse LoRA fine-tuned models.\nOur best classifier can predict the number of fine-tuning images with a mean\nabsolute error of 0.36 images, establishing the feasibility of this attack.\n","authors":["Mohammad Salama","Jonathan Kahana","Eliahu Horwitz","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2406.19395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19394v1","updated":"2024-06-27T17:59:49Z","published":"2024-06-27T17:59:49Z","title":"HUWSOD: Holistic Self-training for Unified Weakly Supervised Object\n  Detection","summary":"  Most WSOD methods rely on traditional object proposals to generate candidate\nregions and are confronted with unstable training, which easily gets stuck in a\npoor local optimum. In this paper, we introduce a unified, high-capacity weakly\nsupervised object detection (WSOD) network called HUWSOD, which utilizes a\ncomprehensive self-training framework without needing external modules or\nadditional supervision. HUWSOD innovatively incorporates a self-supervised\nproposal generator and an autoencoder proposal generator with a multi-rate\nresampling pyramid to replace traditional object proposals, enabling end-to-end\nWSOD training and inference. Additionally, we implement a holistic\nself-training scheme that refines detection scores and coordinates through\nstep-wise entropy minimization and consistency-constraint regularization,\nensuring consistent predictions across stochastic augmentations of the same\nimage. Extensive experiments on PASCAL VOC and MS COCO demonstrate that HUWSOD\ncompetes with state-of-the-art WSOD methods, eliminating the need for offline\nproposals and additional data. The peak performance of HUWSOD approaches that\nof fully-supervised Faster R-CNN. Our findings also indicate that randomly\ninitialized boxes, although significantly different from well-designed offline\nobject proposals, are effective for WSOD training.\n","authors":["Liujuan Cao","Jianghang Lin","Zebo Hong","Yunhang Shen","Shaohui Lin","Chao Chen","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2406.19394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19435v1","updated":"2024-06-27T17:59:49Z","published":"2024-06-27T17:59:49Z","title":"A Sanity Check for AI-generated Image Detection","summary":"  With the rapid development of generative models, discerning AI-generated\ncontent has evoked increasing attention from both industry and academia. In\nthis paper, we conduct a sanity check on \"whether the task of AI-generated\nimage detection has been solved\". To start with, we present Chameleon dataset,\nconsisting AIgenerated images that are genuinely challenging for human\nperception. To quantify the generalization of existing methods, we evaluate 9\noff-the-shelf AI-generated image detectors on Chameleon dataset. Upon analysis,\nalmost all models classify AI-generated images as real ones. Later, we propose\nAIDE (AI-generated Image DEtector with Hybrid Features), which leverages\nmultiple experts to simultaneously extract visual artifacts and noise patterns.\nSpecifically, to capture the high-level semantics, we utilize CLIP to compute\nthe visual embedding. This effectively enables the model to discern\nAI-generated images based on semantics or contextual information; Secondly, we\nselect the highest frequency patches and the lowest frequency patches in the\nimage, and compute the low-level patchwise features, aiming to detect\nAI-generated images by low-level artifacts, for example, noise pattern,\nanti-aliasing, etc. While evaluating on existing benchmarks, for example,\nAIGCDetectBenchmark and GenImage, AIDE achieves +3.5% and +4.6% improvements to\nstate-of-the-art methods, and on our proposed challenging Chameleon benchmarks,\nit also achieves the promising results, despite this problem for detecting\nAI-generated images is far from being solved. The dataset, codes, and pre-train\nmodels will be published at https://github.com/shilinyan99/AIDE.\n","authors":["Shilin Yan","Ouxiang Li","Jiayin Cai","Yanbin Hao","Xiaolong Jiang","Yao Hu","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2406.19435v1.pdf","comment":"Project page: https://shilinyan99.github.io/AIDE Code:\n  https://github.com/shilinyan99/AIDE"},{"id":"http://arxiv.org/abs/2406.19393v1","updated":"2024-06-27T17:59:46Z","published":"2024-06-27T17:59:46Z","title":"Looking 3D: Anomaly Detection with 2D-3D Alignment","summary":"  Automatic anomaly detection based on visual cues holds practical significance\nin various domains, such as manufacturing and product quality assessment. This\npaper introduces a new conditional anomaly detection problem, which involves\nidentifying anomalies in a query image by comparing it to a reference shape. To\naddress this challenge, we have created a large dataset, BrokenChairs-180K,\nconsisting of around 180K images, with diverse anomalies, geometries, and\ntextures paired with 8,143 reference 3D shapes. To tackle this task, we have\nproposed a novel transformer-based approach that explicitly learns the\ncorrespondence between the query image and reference 3D shape via feature\nalignment and leverages a customized attention mechanism for anomaly detection.\nOur approach has been rigorously evaluated through comprehensive experiments,\nserving as a benchmark for future research in this domain.\n","authors":["Ankan Bhunia","Changjian Li","Hakan Bilen"],"pdf_url":"https://arxiv.org/pdf/2406.19393v1.pdf","comment":"Accepted at CVPR'24. Codes & dataset available at\n  https://github.com/VICO-UoE/Looking3D"},{"id":"http://arxiv.org/abs/2406.19392v1","updated":"2024-06-27T17:59:45Z","published":"2024-06-27T17:59:45Z","title":"ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos","summary":"  We introduce ReXTime, a benchmark designed to rigorously test AI models'\nability to perform temporal reasoning within video events. Specifically,\nReXTime focuses on reasoning across time, i.e. human-like understanding when\nthe question and its corresponding answer occur in different video segments.\nThis form of reasoning, requiring advanced understanding of cause-and-effect\nrelationships across video segments, poses significant challenges to even the\nfrontier multimodal large language models. To facilitate this evaluation, we\ndevelop an automated pipeline for generating temporal reasoning question-answer\npairs, significantly reducing the need for labor-intensive manual annotations.\nOur benchmark includes 921 carefully vetted validation samples and 2,143 test\nsamples, each manually curated for accuracy and relevance. Evaluation results\nshow that while frontier large language models outperform academic models, they\nstill lag behind human performance by a significant 14.3% accuracy gap.\nAdditionally, our pipeline creates a training dataset of 9,695 machine\ngenerated samples without manual effort, which empirical studies suggest can\nenhance the across-time reasoning via fine-tuning.\n","authors":["Jr-Jen Chen","Yu-Chien Liao","Hsi-Che Lin","Yu-Chu Yu","Yen-Chun Chen","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19391v1","updated":"2024-06-27T17:59:40Z","published":"2024-06-27T17:59:40Z","title":"Fibottention: Inceptive Visual Representation Learning with Diverse\n  Attention Across Heads","summary":"  Visual perception tasks are predominantly solved by Vision Transformer (ViT)\narchitectures, which, despite their effectiveness, encounter a computational\nbottleneck due to the quadratic complexity of computing self-attention. This\ninefficiency is largely due to the self-attention heads capturing redundant\ntoken interactions, reflecting inherent redundancy within visual data. Many\nworks have aimed to reduce the computational complexity of self-attention in\nViTs, leading to the development of efficient and sparse transformer\narchitectures. In this paper, viewing through the efficiency lens, we realized\nthat introducing any sparse self-attention strategy in ViTs can keep the\ncomputational overhead low. However, these strategies are sub-optimal as they\noften fail to capture fine-grained visual details. This observation leads us to\npropose a general, efficient, sparse architecture, named Fibottention, for\napproximating self-attention with superlinear complexity that is built upon\nFibonacci sequences. The key strategies in Fibottention include: it excludes\nproximate tokens to reduce redundancy, employs structured sparsity by design to\ndecrease computational demands, and incorporates inception-like diversity\nacross attention heads. This diversity ensures the capture of complementary\ninformation through non-overlapping token interactions, optimizing both\nperformance and resource utilization in ViTs for visual representation\nlearning. We embed our Fibottention mechanism into multiple state-of-the-art\ntransformer architectures dedicated to visual tasks. Leveraging only 2-6% of\nthe elements in the self-attention heads, Fibottention in conjunction with ViT\nand its variants, consistently achieves significant performance boosts compared\nto standard ViTs in nine datasets across three domains $\\unicode{x2013}$ image\nclassification, video understanding, and robot learning tasks.\n","authors":["Ali Khaleghi Rahimian","Manish Kumar Govind","Subhajit Maity","Dominick Reilly","Christian Kümmerle","Srijan Das","Aritra Dutta"],"pdf_url":"https://arxiv.org/pdf/2406.19391v1.pdf","comment":"The code is publicly available at\n  https://github.com/Charlotte-CharMLab/Fibottention"},{"id":"http://arxiv.org/abs/2406.19390v1","updated":"2024-06-27T17:59:06Z","published":"2024-06-27T17:59:06Z","title":"SALVe: Semantic Alignment Verification for Floorplan Reconstruction from\n  Sparse Panoramas","summary":"  We propose a new system for automatic 2D floorplan reconstruction that is\nenabled by SALVe, our novel pairwise learned alignment verifier. The inputs to\nour system are sparsely located 360$^\\circ$ panoramas, whose semantic features\n(windows, doors, and openings) are inferred and used to hypothesize pairwise\nroom adjacency or overlap. SALVe initializes a pose graph, which is\nsubsequently optimized using GTSAM. Once the room poses are computed, room\nlayouts are inferred using HorizonNet, and the floorplan is constructed by\nstitching the most confident layout boundaries. We validate our system\nqualitatively and quantitatively as well as through ablation studies, showing\nthat it outperforms state-of-the-art SfM systems in completeness by over 200%,\nwithout sacrificing accuracy. Our results point to the significance of our\nwork: poses of 81% of panoramas are localized in the first 2 connected\ncomponents (CCs), and 89% in the first 3 CCs. Code and models are publicly\navailable at https://github.com/zillow/salve.\n","authors":["John Lambert","Yuguang Li","Ivaylo Boyadzhiev","Lambert Wixson","Manjunath Narayana","Will Hutchcroft","James Hays","Frank Dellaert","Sing Bing Kang"],"pdf_url":"https://arxiv.org/pdf/2406.19390v1.pdf","comment":"Accepted at ECCV 2022"},{"id":"http://arxiv.org/abs/2406.19389v1","updated":"2024-06-27T17:59:01Z","published":"2024-06-27T17:59:01Z","title":"OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and\n  Understanding","summary":"  Current universal segmentation methods demonstrate strong capabilities in\npixel-level image and video understanding. However, they lack reasoning\nabilities and cannot be controlled via text instructions. In contrast, large\nvision-language multimodal models exhibit powerful vision-based conversation\nand reasoning capabilities but lack pixel-level understanding and have\ndifficulty accepting visual prompts for flexible user interaction. This paper\nproposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level\nvision understanding with reasoning abilities. It can accept various visual and\ntext prompts for flexible user interaction. Specifically, we use a universal\nsegmentation method as the visual encoder, integrating image information,\nperception priors, and visual prompts into visual tokens provided to the LLM.\nThe LLM is responsible for understanding the user's text instructions and\nproviding text responses and pixel-level segmentation results based on the\nvisual information. We propose perception prior embedding to better integrate\nperception priors with image features. OMG-LLaVA achieves image-level,\nobject-level, and pixel-level reasoning and understanding in a single model,\nmatching or surpassing the performance of specialized methods on multiple\nbenchmarks. Rather than using LLM to connect each specialist, our work aims at\nend-to-end training on one encoder, one decoder, and one LLM. The code and\nmodel have been released for further research.\n","authors":["Tao Zhang","Xiangtai Li","Hao Fei","Haobo Yuan","Shengqiong Wu","Shunping Ji","Chen Change Loy","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2406.19389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19388v1","updated":"2024-06-27T17:58:54Z","published":"2024-06-27T17:58:54Z","title":"Taming Data and Transformers for Audio Generation","summary":"  Generating ambient sounds and effects is a challenging problem due to data\nscarcity and often insufficient caption quality, making it difficult to employ\nlarge-scale generative models for the task. In this work, we tackle the problem\nby introducing two new models. First, we propose AutoCap, a high-quality and\nefficient automatic audio captioning model. We show that by leveraging metadata\navailable with the audio modality, we can substantially improve the quality of\ncaptions. AutoCap reaches CIDEr score of 83.2, marking a 3.2% improvement from\nthe best available captioning model at four times faster inference speed. We\nthen use AutoCap to caption clips from existing datasets, obtaining 761,000\naudio clips with high-quality captions, forming the largest available\naudio-text dataset. Second, we propose GenAu, a scalable transformer-based\naudio generation architecture that we scale up to 1.25B parameters and train\nwith our new dataset. When compared to state-of-the-art audio generators, GenAu\nobtains significant improvements of 15.7% in FAD score, 22.7% in IS, and 13.5%\nin CLAP score, indicating significantly improved quality of generated audio\ncompared to previous works. This shows that the quality of data is often as\nimportant as its quantity. Besides, since AutoCap is fully automatic, new audio\nsamples can be added to the training dataset, unlocking the training of even\nlarger generative models for audio synthesis.\n","authors":["Moayed Haji-Ali","Willi Menapace","Aliaksandr Siarohin","Guha Balakrishnan","Sergey Tulyakov","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2406.19388v1.pdf","comment":"Project Webpage: https://snap-research.github.io/GenAU/"},{"id":"http://arxiv.org/abs/2406.19369v1","updated":"2024-06-27T17:49:25Z","published":"2024-06-27T17:49:25Z","title":"Mamba or RWKV: Exploring High-Quality and High-Efficiency Segment\n  Anything Model","summary":"  Transformer-based segmentation methods face the challenge of efficient\ninference when dealing with high-resolution images. Recently, several linear\nattention architectures, such as Mamba and RWKV, have attracted much attention\nas they can process long sequences efficiently. In this work, we focus on\ndesigning an efficient segment-anything model by exploring these different\narchitectures. Specifically, we design a mixed backbone that contains\nconvolution and RWKV operation, which achieves the best for both accuracy and\nefficiency. In addition, we design an efficient decoder to utilize the\nmultiscale tokens to obtain high-quality masks. We denote our method as\nRWKV-SAM, a simple, effective, fast baseline for SAM-like models. Moreover, we\nbuild a benchmark containing various high-quality segmentation datasets and\njointly train one efficient yet high-quality segmentation model using this\nbenchmark. Based on the benchmark results, our RWKV-SAM achieves outstanding\nperformance in efficiency and segmentation quality compared to transformers and\nother linear attention models. For example, compared with the same-scale\ntransformer model, RWKV-SAM achieves more than 2x speedup and can achieve\nbetter segmentation performance on various datasets. In addition, RWKV-SAM\noutperforms recent vision Mamba models with better classification and semantic\nsegmentation results. Code and models will be publicly available.\n","authors":["Haobo Yuan","Xiangtai Li","Lu Qi","Tao Zhang","Ming-Hsuan Yang","Shuicheng Yan","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2406.19369v1.pdf","comment":"16 pages; 8 figures"},{"id":"http://arxiv.org/abs/2406.19362v1","updated":"2024-06-27T17:43:35Z","published":"2024-06-27T17:43:35Z","title":"STAL3D: Unsupervised Domain Adaptation for 3D Object Detection via\n  Collaborating Self-Training and Adversarial Learning","summary":"  Existing 3D object detection suffers from expensive annotation costs and poor\ntransferability to unknown data due to the domain gap, Unsupervised Domain\nAdaptation (UDA) aims to generalize detection models trained in labeled source\ndomains to perform robustly on unexplored target domains, providing a promising\nsolution for cross-domain 3D object detection. Although Self-Training (ST)\nbased cross-domain 3D detection methods with the assistance of pseudo-labeling\ntechniques have achieved remarkable progress, they still face the issue of\nlow-quality pseudo-labels when there are significant domain disparities due to\nthe absence of a process for feature distribution alignment. While Adversarial\nLearning (AL) based methods can effectively align the feature distributions of\nthe source and target domains, the inability to obtain labels in the target\ndomain forces the adoption of asymmetric optimization losses, resulting in a\nchallenging issue of source domain bias. To overcome these limitations, we\npropose a novel unsupervised domain adaptation framework for 3D object\ndetection via collaborating ST and AL, dubbed as STAL3D, unleashing the\ncomplementary advantages of pseudo labels and feature distribution alignment.\nAdditionally, a Background Suppression Adversarial Learning (BS-AL) module and\na Scale Filtering Module (SFM) are designed tailored for 3D cross-domain\nscenes, effectively alleviating the issues of the large proportion of\nbackground interference and source domain size bias. Our STAL3D achieves\nstate-of-the-art performance on multiple cross-domain tasks and even surpasses\nthe Oracle results on Waymo $\\rightarrow$ KITTI and Waymo $\\rightarrow$\nKITTI-rain.\n","authors":["Yanan Zhang","Chao Zhou","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2406.19362v1.pdf","comment":"Accepted by IEEE-TIV"},{"id":"http://arxiv.org/abs/2406.05127v2","updated":"2024-06-27T17:35:45Z","published":"2024-06-07T17:55:43Z","title":"Towards Semantic Equivalence of Tokenization in Multimodal LLM","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated exceptional\ncapabilities in processing vision-language tasks. One of the crux of MLLMs lies\nin vision tokenization, which involves efficiently transforming input visual\nsignals into feature representations that are most beneficial for LLMs.\nHowever, existing vision tokenizers, essential for semantic alignment between\nvision and language, remain problematic. Existing methods aggressively fragment\nvisual input, corrupting the visual semantic integrity. To address this, this\npaper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok),\nwhich groups visual features into semantic units via a dynamic clustering\nalgorithm, flexibly determining the number of tokens based on image complexity.\nThe resulting vision tokens effectively preserve semantic integrity and capture\nboth low-frequency and high-frequency visual features. The proposed MLLM\n(Setokim) equipped with SeTok significantly demonstrates superior performance\nacross various tasks, as evidenced by our experimental results. The project\npage is at https://chocowu.github.io/SeTok-web/.\n","authors":["Shengqiong Wu","Hao Fei","Xiangtai Li","Jiayi Ji","Hanwang Zhang","Tat-Seng Chua","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2406.05127v2.pdf","comment":"Technical Report. The project page:\n  https://chocowu.github.io/SeTok-web/"},{"id":"http://arxiv.org/abs/2406.19353v1","updated":"2024-06-27T17:32:18Z","published":"2024-06-27T17:32:18Z","title":"CORE4D: A 4D Human-Object-Human Interaction Dataset for Collaborative\n  Object REarrangement","summary":"  Understanding how humans cooperatively rearrange household objects is\ncritical for VR/AR and human-robot interaction. However, in-depth studies on\nmodeling these behaviors are under-researched due to the lack of relevant\ndatasets. We fill this gap by presenting CORE4D, a novel large-scale 4D\nhuman-object-human interaction dataset focusing on collaborative object\nrearrangement, which encompasses diverse compositions of various object\ngeometries, collaboration modes, and 3D scenes. With 1K human-object-human\nmotion sequences captured in the real world, we enrich CORE4D by contributing\nan iterative collaboration retargeting strategy to augment motions to a variety\nof novel objects. Leveraging this approach, CORE4D comprises a total of 11K\ncollaboration sequences spanning 3K real and virtual object shapes. Benefiting\nfrom extensive motion patterns provided by CORE4D, we benchmark two tasks\naiming at generating human-object interaction: human-object motion forecasting\nand interaction synthesis. Extensive experiments demonstrate the effectiveness\nof our collaboration retargeting strategy and indicate that CORE4D has posed\nnew challenges to existing human-object interaction generation methodologies.\nOur dataset and code are available at\nhttps://github.com/leolyliu/CORE4D-Instructions.\n","authors":["Chengwen Zhang","Yun Liu","Ruofan Xing","Bingda Tang","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2406.19353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13040v2","updated":"2024-06-27T17:27:13Z","published":"2024-03-19T17:35:17Z","title":"Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping","summary":"  Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify\ncolor Doppler in cardiac imaging. In this study, we propose novel alternatives\nto the traditional iVFM optimization scheme by utilizing physics-informed\nneural networks (PINNs) and a physics-guided nnU-Net-based supervised approach.\nWhen evaluated on simulated color Doppler images derived from a\npatient-specific computational fluid dynamics model and in vivo Doppler\nacquisitions, both approaches demonstrate comparable reconstruction performance\nto the original iVFM algorithm. The efficiency of PINNs is boosted through\ndual-stage optimization and pre-optimized weights. On the other hand, the\nnnU-Net method excels in generalizability and real-time capabilities. Notably,\nnnU-Net shows superior robustness on sparse and truncated Doppler data while\nmaintaining independence from explicit boundary conditions. Overall, our\nresults highlight the effectiveness of these methods in reconstructing\nintraventricular vector blood flow. The study also suggests potential\napplications of PINNs in ultrafast color Doppler imaging and the incorporation\nof fluid dynamics equations to derive biomarkers for cardiovascular diseases\nbased on blood flow.\n","authors":["Hang Jung Ling","Salomé Bru","Julia Puig","Florian Vixège","Simon Mendez","Franck Nicoud","Pierre-Yves Courand","Olivier Bernard","Damien Garcia"],"pdf_url":"https://arxiv.org/pdf/2403.13040v2.pdf","comment":"12 pages, accepted for publication in IEEE TUFFC; camera ready\n  corrections, corrected acknowledgments"},{"id":"http://arxiv.org/abs/2406.19341v1","updated":"2024-06-27T17:16:23Z","published":"2024-06-27T17:16:23Z","title":"Learning Visual Conditioning Tokens to Correct Domain Shift for Fully\n  Test-time Adaptation","summary":"  Fully test-time adaptation aims to adapt the network model based on\nsequential analysis of input samples during the inference stage to address the\ncross-domain performance degradation problem of deep neural networks. This work\nis based on the following interesting finding: in transformer-based image\nclassification, the class token at the first transformer encoder layer can be\nlearned to capture the domain-specific characteristics of target samples during\ntest-time adaptation. This learned token, when combined with input image patch\nembeddings, is able to gradually remove the domain-specific information from\nthe feature representations of input samples during the transformer encoding\nprocess, thereby significantly improving the test-time adaptation performance\nof the source model across different domains. We refer to this class token as\nvisual conditioning token (VCT). To successfully learn the VCT, we propose a\nbi-level learning approach to capture the long-term variations of\ndomain-specific characteristics while accommodating local variations of\ninstance-specific characteristics. Experimental results on the benchmark\ndatasets demonstrate that our proposed bi-level visual conditioning token\nlearning method is able to achieve significantly improved test-time adaptation\nperformance by up to 1.9%.\n","authors":["Yushun Tang","Shuoshuo Chen","Zhehan Kan","Yi Zhang","Qinghai Guo","Zhihai He"],"pdf_url":"https://arxiv.org/pdf/2406.19341v1.pdf","comment":"accepted by TMM"},{"id":"http://arxiv.org/abs/2406.13444v2","updated":"2024-06-27T17:09:24Z","published":"2024-06-19T11:09:16Z","title":"VDebugger: Harnessing Execution Feedback for Debugging Visual Programs","summary":"  Visual programs are executable code generated by large language models to\naddress visual reasoning problems. They decompose complex questions into\nmultiple reasoning steps and invoke specialized models for each step to solve\nthe problems. However, these programs are prone to logic errors, with our\npreliminary evaluation showing that 58% of the total errors are caused by\nprogram logic errors. Debugging complex visual programs remains a major\nbottleneck for visual reasoning. To address this, we introduce VDebugger, a\nnovel critic-refiner framework trained to localize and debug visual programs by\ntracking execution step by step. VDebugger identifies and corrects program\nerrors leveraging detailed execution feedback, improving interpretability and\naccuracy. The training data is generated through an automated pipeline that\ninjects errors into correct visual programs using a novel mask-best decoding\ntechnique. Evaluations on six datasets demonstrate VDebugger's effectiveness,\nshowing performance improvements of up to 3.2% in downstream task accuracy.\nFurther studies show VDebugger's ability to generalize to unseen tasks,\nbringing a notable improvement of 2.3% on the unseen COVR task. Code, data and\nmodels are made publicly available at https://github.com/shirley-wu/vdebugger/\n","authors":["Xueqing Wu","Zongyu Lin","Songyan Zhao","Te-Lin Wu","Pan Lu","Nanyun Peng","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2406.13444v2.pdf","comment":"update reference"},{"id":"http://arxiv.org/abs/2406.19320v1","updated":"2024-06-27T16:54:12Z","published":"2024-06-27T16:54:12Z","title":"Efficient World Models with Context-Aware Tokenization","summary":"  Scaling up deep Reinforcement Learning (RL) methods presents a significant\nchallenge. Following developments in generative modelling, model-based RL\npositions itself as a strong contender. Recent advances in sequence modelling\nhave led to effective transformer-based world models, albeit at the price of\nheavy computations due to the long sequences of tokens required to accurately\nsimulate environments. In this work, we propose $\\Delta$-IRIS, a new agent with\na world model architecture composed of a discrete autoencoder that encodes\nstochastic deltas between time steps and an autoregressive transformer that\npredicts future deltas by summarizing the current state of the world with\ncontinuous tokens. In the Crafter benchmark, $\\Delta$-IRIS sets a new state of\nthe art at multiple frame budgets, while being an order of magnitude faster to\ntrain than previous attention-based approaches. We release our code and models\nat https://github.com/vmicheli/delta-iris.\n","authors":["Vincent Micheli","Eloi Alonso","François Fleuret"],"pdf_url":"https://arxiv.org/pdf/2406.19320v1.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2406.19316v1","updated":"2024-06-27T16:52:01Z","published":"2024-06-27T16:52:01Z","title":"Enhanced Data Transfer Cooperating with Artificial Triplets for Scene\n  Graph Generation","summary":"  This work focuses on training dataset enhancement of informative relational\ntriplets for Scene Graph Generation (SGG). Due to the lack of effective\nsupervision, the current SGG model predictions perform poorly for informative\nrelational triplets with inadequate training samples. Therefore, we propose two\nnovel training dataset enhancement modules: Feature Space Triplet Augmentation\n(FSTA) and Soft Transfer. FSTA leverages a feature generator trained to\ngenerate representations of an object in relational triplets. The biased\nprediction based sampling in FSTA efficiently augments artificial triplets\nfocusing on the challenging ones. In addition, we introduce Soft Transfer,\nwhich assigns soft predicate labels to general relational triplets to make more\nsupervisions for informative predicate classes effectively. Experimental\nresults show that integrating FSTA and Soft Transfer achieve high levels of\nboth Recall and mean Recall in Visual Genome dataset. The mean of Recall and\nmean Recall is the highest among all the existing model-agnostic methods.\n","authors":["KuanChao Chu","Satoshi Yamazaki","Hideki Nakayama"],"pdf_url":"https://arxiv.org/pdf/2406.19316v1.pdf","comment":"Accepted to IEICE Transactions on Information and Systems in April\n  2024"},{"id":"http://arxiv.org/abs/2406.13642v2","updated":"2024-06-27T16:30:48Z","published":"2024-06-19T15:41:30Z","title":"SpatialBot: Precise Spatial Understanding with Vision Language Models","summary":"  Vision Language Models (VLMs) have achieved impressive performance in 2D\nimage understanding, however they are still struggling with spatial\nunderstanding which is the foundation of Embodied AI. In this paper, we propose\nSpatialBot for better spatial understanding by feeding both RGB and depth\nimages. Additionally, we have constructed the SpatialQA dataset, which involves\nmulti-level depth-related questions to train VLMs for depth understanding.\nFinally, we present SpatialBench to comprehensively evaluate VLMs' capabilities\nin spatial understanding at different levels. Extensive experiments on our\nspatial-understanding benchmark, general VLM benchmarks and Embodied AI tasks,\ndemonstrate the remarkable improvements of SpatialBot trained on SpatialQA. The\nmodel, code and data are available at https://github.com/BAAI-DCAI/SpatialBot.\n","authors":["Wenxiao Cai","Yaroslav Ponomarenko","Jianhao Yuan","Xiaoqi Li","Wankou Yang","Hao Dong","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.13642v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19302v1","updated":"2024-06-27T16:17:33Z","published":"2024-06-27T16:17:33Z","title":"Mapping Land Naturalness from Sentinel-2 using Deep Contextual and\n  Geographical Priors","summary":"  In recent decades, the causes and consequences of climate change have\naccelerated, affecting our planet on an unprecedented scale. This change is\nclosely tied to the ways in which humans alter their surroundings. As our\nactions continue to impact natural areas, using satellite images to observe and\nmeasure these effects has become crucial for understanding and combating\nclimate change. Aiming to map land naturalness on the continuum of modern human\npressure, we have developed a multi-modal supervised deep learning framework\nthat addresses the unique challenges of satellite data and the task at hand. We\nincorporate contextual and geographical priors, represented by corresponding\ncoordinate information and broader contextual information, including and\nsurrounding the immediate patch to be predicted. Our framework improves the\nmodel's predictive performance in mapping land naturalness from Sentinel-2\ndata, a type of multi-spectral optical satellite imagery. Recognizing that our\nprotective measures are only as effective as our understanding of the\necosystem, quantifying naturalness serves as a crucial step toward enhancing\nour environmental stewardship.\n","authors":["Burak Ekim","Michael Schmitt"],"pdf_url":"https://arxiv.org/pdf/2406.19302v1.pdf","comment":"6 pages, 3 figures, ICLR 2024 Tackling Climate Change with Machine\n  Learning Workshop"},{"id":"http://arxiv.org/abs/2406.19299v1","updated":"2024-06-27T16:15:22Z","published":"2024-06-27T16:15:22Z","title":"PNeRV: A Polynomial Neural Representation for Videos","summary":"  Extracting Implicit Neural Representations (INRs) on video data poses unique\nchallenges due to the additional temporal dimension. In the context of videos,\nINRs have predominantly relied on a frame-only parameterization, which\nsacrifices the spatiotemporal continuity observed in pixel-level (spatial)\nrepresentations. To mitigate this, we introduce Polynomial Neural\nRepresentation for Videos (PNeRV), a parameter-wise efficient, patch-wise INR\nfor videos that preserves spatiotemporal continuity. PNeRV leverages the\nmodeling capabilities of Polynomial Neural Networks to perform the modulation\nof a continuous spatial (patch) signal with a continuous time (frame) signal.\nWe further propose a custom Hierarchical Patch-wise Spatial Sampling Scheme\nthat ensures spatial continuity while retaining parameter efficiency. We also\nemploy a carefully designed Positional Embedding methodology to further enhance\nPNeRV's performance. Our extensive experimentation demonstrates that PNeRV\noutperforms the baselines in conventional Implicit Neural Representation tasks\nlike compression along with downstream applications that require spatiotemporal\ncontinuity in the underlying representation. PNeRV not only addresses the\nchallenges posed by video data in the realm of INRs but also opens new avenues\nfor advanced video processing and analysis.\n","authors":["Sonam Gupta","Snehal Singh Tomar","Grigorios G Chrysos","Sukhendu Das","A. N. Rajagopalan"],"pdf_url":"https://arxiv.org/pdf/2406.19299v1.pdf","comment":"25 pages, 17 figures, published at TMLR, Feb 2024"},{"id":"http://arxiv.org/abs/2406.19298v1","updated":"2024-06-27T16:13:34Z","published":"2024-06-27T16:13:34Z","title":"Compositional Image Decomposition with Diffusion Models","summary":"  Given an image of a natural scene, we are able to quickly decompose it into a\nset of components such as objects, lighting, shadows, and foreground. We can\nthen envision a scene where we combine certain components with those from other\nimages, for instance a set of objects from our bedroom and animals from a zoo\nunder the lighting conditions of a forest, even if we have never encountered\nsuch a scene before. In this paper, we present a method to decompose an image\ninto such compositional components. Our approach, Decomp Diffusion, is an\nunsupervised method which, when given a single image, infers a set of different\ncomponents in the image, each represented by a diffusion model. We demonstrate\nhow components can capture different factors of the scene, ranging from global\nscene descriptors like shadows or facial expression to local scene descriptors\nlike constituent objects. We further illustrate how inferred factors can be\nflexibly composed, even with factors inferred from other models, to generate a\nvariety of scenes sharply different than those seen in training time. Website\nand code at https://energy-based-model.github.io/decomp-diffusion.\n","authors":["Jocelin Su","Nan Liu","Yanbo Wang","Joshua B. Tenenbaum","Yilun Du"],"pdf_url":"https://arxiv.org/pdf/2406.19298v1.pdf","comment":"ICML 2024, Webpage:\n  https://energy-based-model.github.io/decomp-diffusion"},{"id":"http://arxiv.org/abs/2406.19297v1","updated":"2024-06-27T16:12:57Z","published":"2024-06-27T16:12:57Z","title":"Enhancing Continual Learning in Visual Question Answering with\n  Modality-Aware Feature Distillation","summary":"  Continual learning focuses on incrementally training a model on a sequence of\ntasks with the aim of learning new tasks while minimizing performance drop on\nprevious tasks. Existing approaches at the intersection of Continual Learning\nand Visual Question Answering (VQA) do not study how the multimodal nature of\nthe input affects the learning dynamics of a model. In this paper, we\ndemonstrate that each modality evolves at different rates across a continuum of\ntasks and that this behavior occurs in established encoder-only models as well\nas modern recipes for developing Vision & Language (VL) models. Motivated by\nthis observation, we propose a modality-aware feature distillation (MAFED)\napproach which outperforms existing baselines across models of varying scale in\nthree multimodal continual learning settings. Furthermore, we provide ablations\nshowcasing that modality-aware distillation complements experience replay.\nOverall, our results emphasize the importance of addressing modality-specific\ndynamics to prevent forgetting in multimodal continual learning.\n","authors":["Malvina Nikandrou","Georgios Pantazopoulos","Ioannis Konstas","Alessandro Suglia"],"pdf_url":"https://arxiv.org/pdf/2406.19297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19290v1","updated":"2024-06-27T16:04:41Z","published":"2024-06-27T16:04:41Z","title":"Human Modelling and Pose Estimation Overview","summary":"  Human modelling and pose estimation stands at the crossroads of Computer\nVision, Computer Graphics, and Machine Learning. This paper presents a thorough\ninvestigation of this interdisciplinary field, examining various algorithms,\nmethodologies, and practical applications. It explores the diverse range of\nsensor technologies relevant to this domain and delves into a wide array of\napplication areas. Additionally, we discuss the challenges and advancements in\n2D and 3D human modelling methodologies, along with popular datasets, metrics,\nand future research directions. The main contribution of this paper lies in its\nup-to-date comparison of state-of-the-art (SOTA) human pose estimation\nalgorithms in both 2D and 3D domains. By providing this comprehensive overview,\nthe paper aims to enhance understanding of 3D human modelling and pose\nestimation, offering insights into current SOTA achievements, challenges, and\nfuture prospects within the field.\n","authors":["Pawel Knap"],"pdf_url":"https://arxiv.org/pdf/2406.19290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19280v1","updated":"2024-06-27T15:50:41Z","published":"2024-06-27T15:50:41Z","title":"HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into\n  Multimodal LLMs at Scale","summary":"  The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.\n","authors":["Junying Chen","Ruyi Ouyang","Anningzhe Gao","Shunian Chen","Guiming Hardy Chen","Xidong Wang","Ruifei Zhang","Zhenyang Cai","Ke Ji","Guangjun Yu","Xiang Wan","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15847v3","updated":"2024-06-27T15:38:17Z","published":"2024-01-29T02:43:40Z","title":"Muffin or Chihuahua? Challenging Multimodal Large Language Models with\n  Multipanel VQA","summary":"  Multipanel images, commonly seen as web screenshots, posters, etc., pervade\nour daily lives. These images, characterized by their composition of multiple\nsubfigures in distinct layouts, effectively convey information to people.\nToward building advanced multimodal AI applications, such as agents that\nunderstand complex scenes and navigate through webpages, the skill of\nmultipanel visual reasoning is essential, and a comprehensive evaluation of\nmodels in this regard is important. Therefore, we introduce Multipanel Visual\nQuestion Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets\nof questions, answers, and multipanel images that specifically challenge models\nin comprehending multipanel images. Our evaluation shows that questions in the\nMultipanelVQA benchmark pose significant challenges to the state-of-the-art\nMultimodal Large Language Models (MLLMs) tested, even though humans can attain\napproximately 99% accuracy on these questions. Distinctively, the MultipanelVQA\nbenchmark features synthetically generated multipanel images specifically\ncrafted to isolate and assess the impact of various factors, such as the\nlayout, on MLLMs' multipanel image comprehension abilities. As a result, in\naddition to benchmarking the capabilities of MLLMs in understanding multipanel\nimages, we analyze various factors of the multipanel image that affect MLLMs'\nperformance with synthetic data and offer insights for enhancement. Code and\ndata are released at https://sites.google.com/view/multipanelvqa/home.\n","authors":["Yue Fan","Jing Gu","Kaiwen Zhou","Qianqi Yan","Shan Jiang","Ching-Chen Kuo","Xinze Guan","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2401.15847v3.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2406.19263v1","updated":"2024-06-27T15:34:16Z","published":"2024-06-27T15:34:16Z","title":"Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens\n  Grounding","summary":"  Graphical User Interfaces (GUIs) are central to our interaction with digital\ndevices. Recently, growing efforts have been made to build models for various\nGUI understanding tasks. However, these efforts largely overlook an important\nGUI-referring task: screen reading based on user-indicated points, which we\nname the Screen Point-and-Read (SPR) task. This task is predominantly handled\nby rigid accessible screen reading tools, in great need of new models driven by\nadvancements in Multimodal Large Language Models (MLLMs). In this paper, we\npropose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism,\nto address the SPR task. Based on the input point coordinate and the\ncorresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout\nTree. Based on the tree, our ToL agent not only comprehends the content of the\nindicated area but also articulates the layout and spatial relationships\nbetween elements. Such layout information is crucial for accurately\ninterpreting information on the screen, distinguishing our ToL agent from other\nscreen reading tools. We also thoroughly evaluate the ToL agent against other\nbaselines on a newly proposed SPR benchmark, which includes GUIs from mobile,\nweb, and operating systems. Last but not least, we test the ToL agent on mobile\nGUI navigation tasks, demonstrating its utility in identifying incorrect\nactions along the path of agent execution trajectories. Code and data:\nscreen-point-and-read.github.io\n","authors":["Yue Fan","Lei Ding","Ching-Chen Kuo","Shan Jiang","Yang Zhao","Xinze Guan","Jie Yang","Yi Zhang","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06748v2","updated":"2024-06-27T15:24:23Z","published":"2024-03-11T14:14:52Z","title":"Shortcut Learning in Medical Image Segmentation","summary":"  Shortcut learning is a phenomenon where machine learning models prioritize\nlearning simple, potentially misleading cues from data that do not generalize\nwell beyond the training set. While existing research primarily investigates\nthis in the realm of image classification, this study extends the exploration\nof shortcut learning into medical image segmentation. We demonstrate that\nclinical annotations such as calipers, and the combination of zero-padded\nconvolutions and center-cropped training sets in the dataset can inadvertently\nserve as shortcuts, impacting segmentation accuracy. We identify and evaluate\nthe shortcut learning on two different but common medical image segmentation\ntasks. In addition, we suggest strategies to mitigate the influence of shortcut\nlearning and improve the generalizability of the segmentation models. By\nuncovering the presence and implications of shortcuts in medical image\nsegmentation, we provide insights and methodologies for evaluating and\novercoming this pervasive challenge and call for attention in the community for\nshortcuts in segmentation. Our code is public at\nhttps://github.com/nina-weng/shortcut_skinseg .\n","authors":["Manxi Lin","Nina Weng","Kamil Mikolaj","Zahra Bashir","Morten Bo Søndergaard Svendsen","Martin Tolsgaard","Anders Nymark Christensen","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2403.06748v2.pdf","comment":"11 pages, 6 figures, accepted at MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.19255v1","updated":"2024-06-27T15:23:36Z","published":"2024-06-27T15:23:36Z","title":"Enhancing Video-Language Representations with Structural Spatio-Temporal\n  Alignment","summary":"  While pre-training large-scale video-language models (VLMs) has shown\nremarkable potential for various downstream video-language tasks, existing VLMs\ncan still suffer from certain commonly seen limitations, e.g., coarse-grained\ncross-modal aligning , under-modeling of temporal dynamics, detached\nvideo-language view. In this work, we target enhancing VLMs with a fine-grained\nstructural spatio-temporal alignment learning method (namely Finsta). First of\nall, we represent the input texts and videos with fine-grained scene graph (SG)\nstructures, both of which are further unified into a holistic SG (HSG) for\nbridging two modalities. Then, an SG-based framework is built, where the\ntextual SG (TSG) is encoded with a graph Transformer, while the video dynamic\nSG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for\nspatial and temporal feature propagation. A spatial-temporal Gaussian\ndifferential graph Transformer is further devised to strengthen the sense of\nthe changes in objects across spatial and temporal dimensions. Next, based on\nthe fine-grained structural features of TSG and DSG, we perform object-centered\nspatial alignment and predicate-centered temporal alignment respectively,\nenhancing the video-language grounding in both the spatiality and temporality.\nWe design our method as a plug&play system, which can be integrated into\nexisting well-trained VLMs for further representation augmentation, without\ntraining from scratch or relying on SG annotations in downstream applications.\nOn 6 representative VL modeling tasks over 12 datasets in both standard and\nlong-form video scenarios, Finsta consistently improves the existing 13\nstrong-performing VLMs persistently, and refreshes the current state-of-the-art\nend task performance significantly in both the fine-tuning and zero-shot\nsettings.\n","authors":["Hao Fei","Shengqiong Wu","Meishan Zhang","Min Zhang","Tat-Seng Chua","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2406.19255v1.pdf","comment":"Accepted by IEEE TPAMI 2024"},{"id":"http://arxiv.org/abs/2406.19247v1","updated":"2024-06-27T15:14:23Z","published":"2024-06-27T15:14:23Z","title":"Local Manifold Learning for No-Reference Image Quality Assessment","summary":"  Contrastive learning has considerably advanced the field of Image Quality\nAssessment (IQA), emerging as a widely adopted technique. The core mechanism of\ncontrastive learning involves minimizing the distance between quality-similar\n(positive) examples while maximizing the distance between quality-dissimilar\n(negative) examples. Despite its successes, current contrastive learning\nmethods often neglect the importance of preserving the local manifold\nstructure. This oversight can result in a high degree of similarity among hard\nexamples within the feature space, thereby impeding effective differentiation\nand assessment. To address this issue, we propose an innovative framework that\nintegrates local manifold learning with contrastive learning for No-Reference\nImage Quality Assessment (NR-IQA). Our method begins by sampling multiple crops\nfrom a given image, identifying the most visually salient crop. This crop is\nthen used to cluster other crops from the same image as the positive class,\nwhile crops from different images are treated as negative classes to increase\ninter-class distance. Uniquely, our approach also considers non-saliency crops\nfrom the same image as intra-class negative classes to preserve their\ndistinctiveness. Additionally, we employ a mutual learning framework, which\nfurther enhances the model's ability to adaptively learn and identify visual\nsaliency regions. Our approach demonstrates a better performance compared to\nstate-of-the-art methods in 7 standard datasets, achieving PLCC values of 0.942\n(compared to 0.908 in TID2013) and 0.914 (compared to 0.894 in LIVEC).\n","authors":["Timin Gao","Wensheng Pan","Yan Zhang","Sicheng Zhao","Shengchuan Zhang","Xiawu Zheng","Ke Li","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2406.19247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01656v2","updated":"2024-06-27T15:07:39Z","published":"2024-05-02T18:26:15Z","title":"S4: Self-Supervised Sensing Across the Spectrum","summary":"  Satellite image time series (SITS) segmentation is crucial for many\napplications like environmental monitoring, land cover mapping and agricultural\ncrop type classification. However, training models for SITS segmentation\nremains a challenging task due to the lack of abundant training data, which\nrequires fine grained annotation. We propose S4 a new self-supervised\npre-training approach that significantly reduces the requirement for labeled\ntraining data by utilizing two new insights: (a) Satellites capture images in\ndifferent parts of the spectrum such as radio frequencies, and visible\nfrequencies. (b) Satellite imagery is geo-registered allowing for fine-grained\nspatial alignment. We use these insights to formulate pre-training tasks in S4.\nWe also curate m2s2-SITS, a large-scale dataset of unlabeled,\nspatially-aligned, multi-modal and geographic specific SITS that serves as\nrepresentative pre-training data for S4. Finally, we evaluate S4 on multiple\nSITS segmentation datasets and demonstrate its efficacy against competing\nbaselines while using limited labeled data.\n","authors":["Jayanth Shenoy","Xingjian Davis Zhang","Shlok Mehrotra","Bill Tao","Rem Yang","Han Zhao","Deepak Vasisht"],"pdf_url":"https://arxiv.org/pdf/2405.01656v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19239v1","updated":"2024-06-27T15:02:04Z","published":"2024-06-27T15:02:04Z","title":"ALMA: a mathematics-driven approach for determining tuning parameters in\n  generalized LASSO problems, with applications to MRI","summary":"  Magnetic Resonance Imaging (MRI) is a powerful technique employed for\nnon-invasive in vivo visualization of internal structures. Sparsity is often\ndeployed to accelerate the signal acquisition or overcome the presence of\nmotion artifacts, improving the quality of image reconstruction. Image\nreconstruction algorithms use TV-regularized LASSO (Total Variation-regularized\nLASSO) to retrieve the missing information of undersampled signals, by cleaning\nthe data of noise and while optimizing sparsity. A tuning parameter moderates\nthe balance between these two aspects; its choice affecting the quality of the\nreconstructions. Currently, there is a lack of general deterministic techniques\nto choose these parameters, which are oftentimes manually selected and thus\nhinder the reliability of the reconstructions. Here, we present ALMA (Algorithm\nfor Lagrange Multipliers Approximation), an iterative mathematics-inspired\ntechnique that computes tuning parameters for generalized LASSO problems during\nMRI reconstruction. We analyze quantitatively the performance of these\nparameters for imaging reconstructions via TV-LASSO in an MRI context on\nphantoms. Although our study concentrates on TV-LASSO, the techniques developed\nhere hold significant promise for a wide array of applications. ALMA is not\nonly adaptable to more generalized LASSO problems but is also robust to\naccommodate other forms of regularization beyond total variation. Moreover, it\nextends effectively to handle non-Cartesian sampling trajectories, broadening\nits utility in complex data reconstruction scenarios. More generally, ALMA\nprovides a powerful tool for numerically solving constrained optimization\nproblems across various disciplines, offering a versatile and impactful\nsolution for advanced computational challenges.\n","authors":["Gianluca Giacchi","Isidoros Iakovidis","Bastien Milani","Matthias Stuber","Micah Murray","Benedetta Franceschiello"],"pdf_url":"https://arxiv.org/pdf/2406.19239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19236v1","updated":"2024-06-27T15:01:42Z","published":"2024-06-27T15:01:42Z","title":"Human-Aware Vision-and-Language Navigation: Bridging Simulation to\n  Reality with Dynamic Human Interactions","summary":"  Vision-and-Language Navigation (VLN) aims to develop embodied agents that\nnavigate based on human instructions. However, current VLN frameworks often\nrely on static environments and optimal expert supervision, limiting their\nreal-world applicability. To address this, we introduce Human-Aware\nVision-and-Language Navigation (HA-VLN), extending traditional VLN by\nincorporating dynamic human activities and relaxing key assumptions. We propose\nthe Human-Aware 3D (HA3D) simulator, which combines dynamic human activities\nwith the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R)\ndataset, extending R2R with human activity descriptions. To tackle HA-VLN\nchallenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and\nNon-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing\ncross-modal fusion and diverse training strategies for effective navigation in\ndynamic human environments. A comprehensive evaluation, including metrics\nconsidering human activities, and systematic analysis of HA-VLN's unique\nchallenges, underscores the need for further research to enhance HA-VLN agents'\nreal-world robustness and adaptability. Ultimately, this work provides\nbenchmarks and insights for future research on embodied AI and Sim2Real\ntransfer, paving the way for more realistic and applicable VLN systems in\nhuman-populated environments.\n","authors":["Minghan Li","Heng Li","Zhi-Qi Cheng","Yifei Dong","Yuxuan Zhou","Jun-Yan He","Qi Dai","Teruko Mitamura","Alexander G. Hauptmann"],"pdf_url":"https://arxiv.org/pdf/2406.19236v1.pdf","comment":"30 pages, 18 figures, Project Page:\n  https://lpercc.github.io/HA3D_simulator/"},{"id":"http://arxiv.org/abs/2406.17382v2","updated":"2024-06-27T14:59:18Z","published":"2024-06-25T08:58:53Z","title":"Automatic infant 2D pose estimation from videos: comparing seven deep\n  neural network methods","summary":"  Automatic markerless estimation of infant posture and motion from ordinary\nvideos carries great potential for movement studies \"in the wild\", facilitating\nunderstanding of motor development and massively increasing the chances of\nearly diagnosis of disorders. There is rapid development of human pose\nestimation methods in computer vision thanks to advances in deep learning and\nmachine learning. However, these methods are trained on datasets featuring\nadults in different contexts. This work tests and compares seven popular\nmethods (AlphaPose, DeepLabCut/DeeperCut, Detectron2, HRNet,\nMediaPipe/BlazePose, OpenPose, and ViTPose) on videos of infants in supine\nposition. Surprisingly, all methods except DeepLabCut and MediaPipe have\ncompetitive performance without additional finetuning, with ViTPose performing\nbest. Next to standard performance metrics (object keypoint similarity, average\nprecision and recall), we introduce errors expressed in the neck-mid-hip ratio\nand additionally study missed and redundant detections and the reliability of\nthe internal confidence ratings of the different methods, which are relevant\nfor downstream tasks. Among the networks with competitive performance, only\nAlphaPose could run close to real time (27 fps) on our machine. We provide\ndocumented Docker containers or instructions for all the methods we used, our\nanalysis scripts, and processed data at https://hub.docker.com/u/humanoidsctu\nand https://osf.io/x465b/.\n","authors":["Filipe Gama","Matej Misar","Lukas Navara","Sergiu T. Popescu","Matej Hoffmann"],"pdf_url":"https://arxiv.org/pdf/2406.17382v2.pdf","comment":"21 pages, 3 figures, 14 tables"},{"id":"http://arxiv.org/abs/2406.05668v2","updated":"2024-06-27T14:55:41Z","published":"2024-06-09T06:53:39Z","title":"SRC-Net: Bi-Temporal Spatial Relationship Concerned Network for Change\n  Detection","summary":"  Change detection (CD) in remote sensing imagery is a crucial task with\napplications in environmental monitoring, urban development, and disaster\nmanagement. CD involves utilizing bi-temporal images to identify changes over\ntime. The bi-temporal spatial relationships between features at the same\nlocation at different times play a key role in this process. However, existing\nchange detection networks often do not fully leverage these spatial\nrelationships during bi-temporal feature extraction and fusion. In this work,\nwe propose SRC-Net: a bi-temporal spatial relationship concerned network for\nCD. The proposed SRC-Net includes a Perception and Interaction Module that\nincorporates spatial relationships and establishes a cross-branch perception\nmechanism to enhance the precision and robustness of feature extraction.\nAdditionally, a Patch-Mode joint Feature Fusion Module is introduced to address\ninformation loss in current methods. It considers different change modes and\nconcerns about spatial relationships, resulting in more expressive fusion\nfeatures. Furthermore, we construct a novel network using these two\nrelationship concerned modules and conducted experiments on the LEVIR-CD and\nWHU Building datasets. The experimental results demonstrate that our network\noutperforms state-of-the-art (SOTA) methods while maintaining a modest\nparameter count. We believe our approach sets a new paradigm for change\ndetection and will inspire further advancements in the field. The code and\nmodels are publicly available at https://github.com/Chnja/SRCNet.\n","authors":["Hongjia Chen","Xin Xu","Fangling Pu"],"pdf_url":"https://arxiv.org/pdf/2406.05668v2.pdf","comment":"13 pages, 12 figures, IEEE Journal of Selected Topics in Applied\n  Earth Observations and Remote Sensing (2024)"},{"id":"http://arxiv.org/abs/2406.19225v1","updated":"2024-06-27T14:50:50Z","published":"2024-06-27T14:50:50Z","title":"ProtoGMM: Multi-prototype Gaussian-Mixture-based Domain Adaptation Model\n  for Semantic Segmentation","summary":"  Domain adaptive semantic segmentation aims to generate accurate and dense\npredictions for an unlabeled target domain by leveraging a supervised model\ntrained on a labeled source domain. The prevalent self-training approach\ninvolves retraining the dense discriminative classifier of $p(class|pixel\nfeature)$ using the pseudo-labels from the target domain. While many methods\nfocus on mitigating the issue of noisy pseudo-labels, they often overlook the\nunderlying data distribution p(pixel feature|class) in both the source and\ntarget domains. To address this limitation, we propose the multi-prototype\nGaussian-Mixture-based (ProtoGMM) model, which incorporates the GMM into\ncontrastive losses to perform guided contrastive learning. Contrastive losses\nare commonly executed in the literature using memory banks, which can lead to\nclass biases due to underrepresented classes. Furthermore, memory banks often\nhave fixed capacities, potentially restricting the model's ability to capture\ndiverse representations of the target/source domains. An alternative approach\nis to use global class prototypes (i.e. averaged features per category).\nHowever, the global prototypes are based on the unimodal distribution\nassumption per class, disregarding within-class variation. To address these\nchallenges, we propose the ProtoGMM model. This novel approach involves\nestimating the underlying multi-prototype source distribution by utilizing the\nGMM on the feature space of the source samples. The components of the GMM model\nact as representative prototypes. To achieve increased intra-class semantic\nsimilarity, decreased inter-class similarity, and domain alignment between the\nsource and target domains, we employ multi-prototype contrastive learning\nbetween source distribution and target samples. The experiments show the\neffectiveness of our method on UDA benchmarks.\n","authors":["Nazanin Moradinasab","Laura S. Shankman","Rebecca A. Deaton","Gary K. Owens","Donald E. Brown"],"pdf_url":"https://arxiv.org/pdf/2406.19225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19217v1","updated":"2024-06-27T14:43:50Z","published":"2024-06-27T14:43:50Z","title":"Think Step by Step: Chain-of-Gesture Prompting for Error Detection in\n  Robotic Surgical Videos","summary":"  Despite significant advancements in robotic systems and surgical data\nscience, ensuring safe and optimal execution in robot-assisted minimally\ninvasive surgery (RMIS) remains a complex challenge. Current surgical error\ndetection methods involve two parts: identifying surgical gestures and then\ndetecting errors within each gesture clip. These methods seldom consider the\nrich contextual and semantic information inherent in surgical videos, limiting\ntheir performance due to reliance on accurate gesture identification. Motivated\nby the chain-of-thought prompting in natural language processing, this letter\npresents a novel and real-time end-to-end error detection framework,\nChain-of-Thought (COG) prompting, leveraging contextual information from\nsurgical videos. This encompasses two reasoning modules designed to mimic the\ndecision-making processes of expert surgeons. Concretely, we first design a\nGestural-Visual Reasoning module, which utilizes transformer and attention\narchitectures for gesture prompting, while the second, a Multi-Scale Temporal\nReasoning module, employs a multi-stage temporal convolutional network with\nboth slow and fast paths for temporal information extraction. We extensively\nvalidate our method on the public benchmark RMIS dataset JIGSAWS. Our method\nencapsulates the reasoning processes inherent to surgical activities enabling\nit to outperform the state-of-the-art by 4.6% in F1 score, 4.6% in Accuracy,\nand 5.9% in Jaccard index while processing each frame in 6.69 milliseconds on\naverage, demonstrating the great potential of our approach in enhancing the\nsafety and efficacy of RMIS procedures and surgical education. The code will be\navailable.\n","authors":["Zhimin Shao","Jialang Xu","Danail Stoyanov","Evangelos B. Mazomenos","Yueming Jin"],"pdf_url":"https://arxiv.org/pdf/2406.19217v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2405.06196v2","updated":"2024-06-27T14:19:56Z","published":"2024-05-10T02:23:56Z","title":"VLSM-Adapter: Finetuning Vision-Language Segmentation Efficiently with\n  Lightweight Blocks","summary":"  Foundation Vision-Language Models (VLMs) trained using large-scale\nopen-domain images and text pairs have recently been adapted to develop\nVision-Language Segmentation Models (VLSMs) that allow providing text prompts\nduring inference to guide image segmentation. If robust and powerful VLSMs can\nbe built for medical images, it could aid medical professionals in many\nclinical tasks where they must spend substantial time delineating the target\nstructure of interest. VLSMs for medical images resort to fine-tuning base VLM\nor VLSM pretrained on open-domain natural image datasets due to fewer annotated\nmedical image datasets; this fine-tuning is resource-consuming and expensive as\nit usually requires updating all or a significant fraction of the pretrained\nparameters. Recently, lightweight blocks called adapters have been proposed in\nVLMs that keep the pretrained model frozen and only train adapters during\nfine-tuning, substantially reducing the computing resources required. We\nintroduce a novel adapter, VLSM-Adapter, that can fine-tune pretrained\nvision-language segmentation models using transformer encoders. Our experiments\nin widely used CLIP-based segmentation models show that with only 3 million\ntrainable parameters, the VLSM-Adapter outperforms state-of-the-art and is\ncomparable to the upper bound end-to-end fine-tuning. The source code is\navailable at: https://github.com/naamiinepal/vlsm-adapter.\n","authors":["Manish Dhakal","Rabin Adhikari","Safal Thapaliya","Bishesh Khanal"],"pdf_url":"https://arxiv.org/pdf/2405.06196v2.pdf","comment":"Accepted at MICCAI 2024, the 27th International Conference on Medical\n  Image Computing and Computer Assisted Intervention"},{"id":"http://arxiv.org/abs/2406.19175v1","updated":"2024-06-27T13:51:53Z","published":"2024-06-27T13:51:53Z","title":"Towards Reducing Data Acquisition and Labeling for Defect Detection\n  using Simulated Data","summary":"  In many manufacturing settings, annotating data for machine learning and\ncomputer vision is costly, but synthetic data can be generated at significantly\nlower cost. Substituting the real-world data with synthetic data is therefore\nappealing for many machine learning applications that require large amounts of\ntraining data. However, relying solely on synthetic data is frequently\ninadequate for effectively training models that perform well on real-world\ndata, primarily due to domain shifts between the synthetic and real-world data.\nWe discuss approaches for dealing with such a domain shift when detecting\ndefects in X-ray scans of aluminium wheels. Using both simulated and real-world\nX-ray images, we train an object detection model with different strategies to\nidentify the training approach that generates the best detection results while\nminimising the demand for annotated real-world training samples. Our\npreliminary findings suggest that the sim-2-real domain adaptation approach is\nmore cost-efficient than a fully supervised oracle - if the total number of\navailable annotated samples is fixed. Given a certain number of labeled\nreal-world samples, training on a mix of synthetic and unlabeled real-world\ndata achieved comparable or even better detection results at significantly\nlower cost. We argue that future research into the cost-efficiency of different\ntraining strategies is important for a better understanding of how to allocate\nbudget in applied machine learning projects.\n","authors":["Lukas Malte Kemeter","Rasmus Hvingelby","Paulina Sierak","Tobias Schön","Bishwajit Gosswam"],"pdf_url":"https://arxiv.org/pdf/2406.19175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19162v1","updated":"2024-06-27T13:29:25Z","published":"2024-06-27T13:29:25Z","title":"Single Image Estimation of Cell Migration Direction by Deep Circular\n  Regression","summary":"  In this paper we study the problem of estimating the migration direction of\ncells based on a single image. To the best of our knowledge, there is only one\nrelated work that uses a classification CNN for four classes (quadrants). This\napproach does not allow detailed directional resolution. We solve the single\nimage estimation problem using deep circular regression with special attention\nto cycle-sensitive methods. On two databases we achieve an average accuracy of\n$\\sim$17 degrees, which is a significant improvement over the previous work.\n","authors":["Lennart Bruns","Lucas Lamparter","Milos Galic","Xiaoyi Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.19162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14574v2","updated":"2024-06-27T13:29:07Z","published":"2023-12-22T10:10:50Z","title":"MMGPL: Multimodal Medical Data Analysis with Graph Prompt Learning","summary":"  Prompt learning has demonstrated impressive efficacy in the fine-tuning of\nmultimodal large models to a wide range of downstream tasks. Nonetheless,\napplying existing prompt learning methods for the diagnosis of neurological\ndisorder still suffers from two issues: (i) existing methods typically treat\nall patches equally, despite the fact that only a small number of patches in\nneuroimaging are relevant to the disease, and (ii) they ignore the structural\ninformation inherent in the brain connection network which is crucial for\nunderstanding and diagnosing neurological disorders. To tackle these issues, we\nintroduce a novel prompt learning model by learning graph prompts during the\nfine-tuning process of multimodal large models for diagnosing neurological\ndisorders. Specifically, we first leverage GPT-4 to obtain relevant disease\nconcepts and compute semantic similarity between these concepts and all\npatches. Secondly, we reduce the weight of irrelevant patches according to the\nsemantic similarity between each patch and disease-related concepts. Moreover,\nwe construct a graph among tokens based on these concepts and employ a graph\nconvolutional network layer to extract the structural information of the graph,\nwhich is used to prompt the pre-trained multimodal large models for diagnosing\nneurological disorders. Extensive experiments demonstrate that our method\nachieves superior performance for neurological disorder diagnosis compared with\nstate-of-the-art methods and validated by clinicians.\n","authors":["Liang Peng","Songyue Cai","Zongqian Wu","Huifang Shang","Xiaofeng Zhu","Xiaoxiao Li"],"pdf_url":"https://arxiv.org/pdf/2312.14574v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12223v3","updated":"2024-06-27T13:13:11Z","published":"2023-12-19T15:11:46Z","title":"Self-Supervised Detection of Perfect and Partial Input-Dependent\n  Symmetries","summary":"  Group equivariance can overly constrain models if the symmetries in the group\ndiffer from those observed in data. While common methods address this by\ndetermining the appropriate level of symmetry at the dataset level, they are\nlimited to supervised settings and ignore scenarios in which multiple levels of\nsymmetry co-exist in the same dataset. In this paper, we propose a method able\nto detect the level of symmetry of each input without the need for labels. Our\nframework is general enough to accommodate different families of both\ncontinuous and discrete symmetry distributions, such as arbitrary unimodal,\nsymmetric distributions and discrete groups. We validate the effectiveness of\nour approach on synthetic datasets with different per-class levels of\nsymmetries, and demonstrate practical applications such as the detection of\nout-of-distribution symmetries. Our code is publicly available at\nhttps://github.com/aurban0/ssl-sym.\n","authors":["Alonso Urbano","David W. Romero"],"pdf_url":"https://arxiv.org/pdf/2312.12223v3.pdf","comment":"19 pages, 8 figures, corrected typos, revised argument in Appendix\n  B.1, results unchanged"},{"id":"http://arxiv.org/abs/2406.19150v1","updated":"2024-06-27T13:08:35Z","published":"2024-06-27T13:08:35Z","title":"RAVEN: Multitask Retrieval Augmented Vision-Language Learning","summary":"  The scaling of large language models to encode all the world's knowledge in\nmodel parameters is unsustainable and has exacerbated resource barriers.\nRetrieval-Augmented Generation (RAG) presents a potential solution, yet its\napplication to vision-language models (VLMs) is under explored. Existing\nmethods focus on models designed for single tasks. Furthermore, they're limited\nby the need for resource intensive pre training, additional parameter\nrequirements, unaddressed modality prioritization and lack of clear benefit\nover non-retrieval baselines. This paper introduces RAVEN, a multitask\nretrieval augmented VLM framework that enhances base VLMs through efficient,\ntask specific fine-tuning. By integrating retrieval augmented samples without\nthe need for additional retrieval-specific parameters, we show that the model\nacquires retrieval properties that are effective across multiple tasks. Our\nresults and extensive ablations across retrieved modalities for the image\ncaptioning and VQA tasks indicate significant performance improvements compared\nto non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a\n+3\\% accuracy on specific VQA question types. This underscores the efficacy of\napplying RAG approaches to VLMs, marking a stride toward more efficient and\naccessible multimodal learning.\n","authors":["Varun Nagaraj Rao","Siddharth Choudhary","Aditya Deshpande","Ravi Kumar Satzoda","Srikar Appalaraju"],"pdf_url":"https://arxiv.org/pdf/2406.19150v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2406.19579v1","updated":"2024-06-27T23:57:01Z","published":"2024-06-27T23:57:01Z","title":"Private Zeroth-Order Nonsmooth Nonconvex Optimization","summary":"  We introduce a new zeroth-order algorithm for private stochastic optimization\non nonconvex and nonsmooth objectives. Given a dataset of size $M$, our\nalgorithm ensures $(\\alpha,\\alpha\\rho^2/2)$-R\\'enyi differential privacy and\nfinds a $(\\delta,\\epsilon)$-stationary point so long as\n$M=\\tilde\\Omega\\left(\\frac{d}{\\delta\\epsilon^3} +\n\\frac{d^{3/2}}{\\rho\\delta\\epsilon^2}\\right)$. This matches the optimal\ncomplexity of its non-private zeroth-order analog. Notably, although the\nobjective is not smooth, we have privacy ``for free'' whenever $\\rho \\ge\n\\sqrt{d}\\epsilon$.\n","authors":["Qinzi Zhang","Hoang Tran","Ashok Cutkosky"],"pdf_url":"https://arxiv.org/pdf/2406.19579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19578v1","updated":"2024-06-27T23:43:36Z","published":"2024-06-27T23:43:36Z","title":"PathAlign: A vision-language model for whole slide images in\n  histopathology","summary":"  Microscopic interpretation of histopathology images underlies many important\ndiagnostic and treatment decisions. While advances in vision-language modeling\nraise new opportunities for analysis of such images, the gigapixel-scale size\nof whole slide images (WSIs) introduces unique challenges. Additionally,\npathology reports simultaneously highlight key findings from small regions\nwhile also aggregating interpretation across multiple slides, often making it\ndifficult to create robust image-text pairs. As such, pathology reports remain\na largely untapped source of supervision in computational pathology, with most\nefforts relying on region-of-interest annotations or self-supervision at the\npatch-level. In this work, we develop a vision-language model based on the\nBLIP-2 framework using WSIs paired with curated text from pathology reports.\nThis enables applications utilizing a shared image-text embedding space, such\nas text or image retrieval for finding cases of interest, as well as\nintegration of the WSI encoder with a frozen large language model (LLM) for\nWSI-based generative text capabilities such as report generation or\nAI-in-the-loop interactions. We utilize a de-identified dataset of over 350,000\nWSIs and diagnostic text pairs, spanning a wide range of diagnoses, procedure\ntypes, and tissue types. We present pathologist evaluation of text generation\nand text retrieval using WSI embeddings, as well as results for WSI\nclassification and workflow prioritization (slide-level triaging).\nModel-generated text for WSIs was rated by pathologists as accurate, without\nclinically significant error or omission, for 78% of WSIs on average. This work\ndemonstrates exciting potential capabilities for language-aligned WSI\nembeddings.\n","authors":["Faruk Ahmed","Andrew Sellergren","Lin Yang","Shawn Xu","Boris Babenko","Abbi Ward","Niels Olson","Arash Mohtashamian","Yossi Matias","Greg S. Corrado","Quang Duong","Dale R. Webster","Shravya Shetty","Daniel Golden","Yun Liu","David F. Steiner","Ellery Wulczyn"],"pdf_url":"https://arxiv.org/pdf/2406.19578v1.pdf","comment":"9 main pages and 19 pages of supplemental material; 3 main tables, 3\n  main figures and 11 supplemental tables, 7 supplemental figures"},{"id":"http://arxiv.org/abs/2406.19574v1","updated":"2024-06-27T23:26:57Z","published":"2024-06-27T23:26:57Z","title":"Deep Temporal Sequence Classification and Mathematical Modeling for Cell\n  Tracking in Dense 3D Microscopy Videos of Bacterial Biofilms","summary":"  Automatic cell tracking in dense environments is plagued by inaccurate\ncorrespondences and misidentification of parent-offspring relationships. In\nthis paper, we introduce a novel cell tracking algorithm named DenseTrack,\nwhich integrates deep learning with mathematical model-based strategies to\neffectively establish correspondences between consecutive frames and detect\ncell division events in crowded scenarios. We formulate the cell tracking\nproblem as a deep learning-based temporal sequence classification task followed\nby solving a constrained one-to-one matching optimization problem exploiting\nthe classifier's confidence scores. Additionally, we present an\neigendecomposition-based cell division detection strategy that leverages\nknowledge of cellular geometry. The performance of the proposed approach has\nbeen evaluated by tracking densely packed cells in 3D time-lapse image\nsequences of bacterial biofilm development. The experimental results on\nsimulated as well as experimental fluorescence image sequences suggest that the\nproposed tracking method achieves superior performance in terms of both\nqualitative and quantitative evaluation measures compared to recent\nstate-of-the-art cell tracking approaches.\n","authors":["Tanjin Taher Toma","Yibo Wang","Andreas Gahlmann","Scott T. Acton"],"pdf_url":"https://arxiv.org/pdf/2406.19574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19573v1","updated":"2024-06-27T23:25:57Z","published":"2024-06-27T23:25:57Z","title":"On Counterfactual Interventions in Vector Autoregressive Models","summary":"  Counterfactual reasoning allows us to explore hypothetical scenarios in order\nto explain the impacts of our decisions. However, addressing such inquires is\nimpossible without establishing the appropriate mathematical framework. In this\nwork, we introduce the problem of counterfactual reasoning in the context of\nvector autoregressive (VAR) processes. We also formulate the inference of a\ncausal model as a joint regression task where for inference we use both data\nwith and without interventions. After learning the model, we exploit linearity\nof the VAR model to make exact predictions about the effects of counterfactual\ninterventions. Furthermore, we quantify the total causal effects of past\ncounterfactual interventions. The source code for this project is freely\navailable at https://github.com/KurtButler/counterfactual_interventions.\n","authors":["Kurt Butler","Marija Iloska","Petar M. Djuric"],"pdf_url":"https://arxiv.org/pdf/2406.19573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02319v2","updated":"2024-06-27T23:22:14Z","published":"2024-04-02T21:35:54Z","title":"Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient\n  Compile-Time Prompt Optimization","summary":"  In many modern LLM applications, such as retrieval augmented generation,\nprompts have become programs themselves. In these settings, prompt programs are\nrepeatedly called with different user queries or data instances. A big\npractical challenge is optimizing such prompt programs. Recent work has mostly\nfocused on either simple prompt programs or assumed that the general structure\nof a prompt program is fixed.\n  We introduce SAMMO, a framework to perform symbolic prompt program search for\ncompile-time optimizations of prompt programs. SAMMO represents prompt programs\non a symbolic level which allows for a rich set of transformations that can be\nsearched over during optimization. We show that SAMMO generalizes previous\nmethods and improves the performance of complex prompts on (1) instruction\ntuning, (2) RAG pipeline tuning, and (3) prompt compression, across several\ndifferent LLMs. We make all code available open-source at\nhttps://github.com/microsoft/sammo .\n","authors":["Tobias Schnabel","Jennifer Neville"],"pdf_url":"https://arxiv.org/pdf/2404.02319v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19566v1","updated":"2024-06-27T22:51:06Z","published":"2024-06-27T22:51:06Z","title":"Instance-Optimal Private Density Estimation in the Wasserstein Distance","summary":"  Estimating the density of a distribution from samples is a fundamental\nproblem in statistics. In many practical settings, the Wasserstein distance is\nan appropriate error metric for density estimation. For example, when\nestimating population densities in a geographic region, a small Wasserstein\ndistance means that the estimate is able to capture roughly where the\npopulation mass is. In this work we study differentially private density\nestimation in the Wasserstein distance. We design and analyze instance-optimal\nalgorithms for this problem that can adapt to easy instances.\n  For distributions $P$ over $\\mathbb{R}$, we consider a strong notion of\ninstance-optimality: an algorithm that uniformly achieves the instance-optimal\nestimation rate is competitive with an algorithm that is told that the\ndistribution is either $P$ or $Q_P$ for some distribution $Q_P$ whose\nprobability density function (pdf) is within a factor of 2 of the pdf of $P$.\nFor distributions over $\\mathbb{R}^2$, we use a different notion of instance\noptimality. We say that an algorithm is instance-optimal if it is competitive\nwith an algorithm that is given a constant-factor multiplicative approximation\nof the density of the distribution. We characterize the instance-optimal\nestimation rates in both these settings and show that they are uniformly\nachievable (up to polylogarithmic factors). Our approach for $\\mathbb{R}^2$\nextends to arbitrary metric spaces as it goes via hierarchically separated\ntrees. As a special case our results lead to instance-optimal private learning\nin TV distance for discrete distributions.\n","authors":["Vitaly Feldman","Audra McMillan","Satchit Sivakumar","Kunal Talwar"],"pdf_url":"https://arxiv.org/pdf/2406.19566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10563v3","updated":"2024-06-27T22:40:45Z","published":"2023-09-19T12:18:28Z","title":"A Hierarchical Neural Framework for Classification and its Explanation\n  in Large Unstructured Legal Documents","summary":"  Automatic legal judgment prediction and its explanation suffer from the\nproblem of long case documents exceeding tens of thousands of words, in\ngeneral, and having a non-uniform structure. Predicting judgments from such\ndocuments and extracting their explanation becomes a challenging task, more so\non documents with no structural annotation. We define this problem as \"scarce\nannotated legal documents\" and explore their lack of structural information and\ntheir long lengths with a deep-learning-based classification framework which we\ncall MESc; \"Multi-stage Encoder-based Supervised with-clustering\"; for judgment\nprediction. We explore the adaptability of LLMs with multi-billion parameters\n(GPT-Neo, and GPT-J) to legal texts and their intra-domain(legal) transfer\nlearning capacity. Alongside this, we compare their performance and\nadaptability with MESc and the impact of combining embeddings from their last\nlayers. For such hierarchical models, we also propose an explanation extraction\nalgorithm named ORSE; Occlusion sensitivity-based Relevant Sentence Extractor;\nbased on the input-occlusion sensitivity of the model, to explain the\npredictions with the most relevant sentences from the document. We explore\nthese methods and test their effectiveness with extensive experiments and\nablation studies on legal documents from India, the European Union, and the\nUnited States with the ILDC dataset and a subset of the LexGLUE dataset. MESc\nachieves a minimum total performance gain of approximately 2 points over\nprevious state-of-the-art proposed methods, while ORSE applied on MESc achieves\na total average gain of 50% over the baseline explainability scores.\n","authors":["Nishchal Prasad","Mohand Boughanem","Taoufik Dkaki"],"pdf_url":"https://arxiv.org/pdf/2309.10563v3.pdf","comment":"Published as non archival paper in the The 3rd International Workshop\n  on Mining and Learning in the Legal Domain (MLLD-2023) at CIKM 2023,\n  Birmingham, United Kingdom. (https://sites.google.com/view/mlld2023/)"},{"id":"http://arxiv.org/abs/2406.19561v1","updated":"2024-06-27T22:24:46Z","published":"2024-06-27T22:24:46Z","title":"Meta-Gradient Search Control: A Method for Improving the Efficiency of\n  Dyna-style Planning","summary":"  We study how a Reinforcement Learning (RL) system can remain sample-efficient\nwhen learning from an imperfect model of the environment. This is particularly\nchallenging when the learning system is resource-constrained and in continual\nsettings, where the environment dynamics change. To address these challenges,\nour paper introduces an online, meta-gradient algorithm that tunes a\nprobability with which states are queried during Dyna-style planning. Our study\ncompares the aggregate, empirical performance of this meta-gradient method to\nbaselines that employ conventional sampling strategies. Results indicate that\nour method improves efficiency of the planning process, which, as a\nconsequence, improves the sample-efficiency of the overall learning process. On\nthe whole, we observe that our meta-learned solutions avoid several pathologies\nof conventional planning approaches, such as sampling inaccurate transitions\nand those that stall credit assignment. We believe these findings could prove\nuseful, in future work, for designing model-based RL systems at scale.\n","authors":["Bradley Burega","John D. Martin","Luke Kapeluck","Michael Bowling"],"pdf_url":"https://arxiv.org/pdf/2406.19561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19560v1","updated":"2024-06-27T22:19:19Z","published":"2024-06-27T22:19:19Z","title":"Cost-efficient Active Illumination Camera For Hyper-spectral\n  Reconstruction","summary":"  Hyper-spectral imaging has recently gained increasing attention for use in\ndifferent applications, including agricultural investigation, ground tracking,\nremote sensing and many other. However, the high cost, large physical size and\ncomplicated operation process stop hyperspectral cameras from being employed\nfor various applications and research fields. In this paper, we introduce a\ncost-efficient, compact and easy to use active illumination camera that may\nbenefit many applications. We developed a fully functional prototype of such\ncamera. With the hope of helping with agricultural research, we tested our\ncamera for plant root imaging. In addition, a U-Net model for spectral\nreconstruction was trained by using a reference hyperspectral camera's data as\nground truth and our camera's data as input. We demonstrated our camera's\nability to obtain additional information over a typical RGB camera. In\naddition, the ability to reconstruct hyperspectral data from multi-spectral\ninput makes our device compatible to models and algorithms developed for\nhyperspectral applications with no modifications required.\n","authors":["Yuxuan Zhang","T. M. Sazzad","Yangyang Song","Spencer J. Chang","Ritesh Chowdhry","Tomas Mejia","Anna Hampton","Shelby Kucharski","Stefan Gerber","Barry Tillman","Marcio F. R. Resende","William M. Hammond","Chris H. Wilson","Alina Zare","Sanjeev J. Koppal"],"pdf_url":"https://arxiv.org/pdf/2406.19560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19556v1","updated":"2024-06-27T22:16:53Z","published":"2024-06-27T22:16:53Z","title":"BOrg: A Brain Organoid-Based Mitosis Dataset for Automatic Analysis of\n  Brain Diseases","summary":"  Recent advances have enabled the study of human brain development using brain\norganoids derived from stem cells. Quantifying cellular processes like mitosis\nin these organoids offers insights into neurodevelopmental disorders, but the\nmanual analysis is time-consuming, and existing datasets lack specific details\nfor brain organoid studies. We introduce BOrg, a dataset designed to study\nmitotic events in the embryonic development of the brain using confocal\nmicroscopy images of brain organoids. BOrg utilizes an efficient annotation\npipeline with sparse point annotations and techniques that minimize expert\neffort, overcoming limitations of standard deep learning approaches on sparse\ndata. We adapt and benchmark state-of-the-art object detection and cell\ncounting models on BOrg for detecting and analyzing mitotic cells across\nprophase, metaphase, anaphase, and telophase stages. Our results demonstrate\nthese adapted models significantly improve mitosis analysis efficiency and\naccuracy for brain organoid research compared to existing methods. BOrg\nfacilitates the development of automated tools to quantify statistics like\nmitosis rates, aiding mechanistic studies of neurodevelopmental processes and\ndisorders. Data and code are available at https://github.com/awaisrauf/borg.\n","authors":["Muhammad Awais","Mehaboobathunnisa Sahul Hameed","Bidisha Bhattacharya","Orly Reiner","Rao Muhammad Anwer"],"pdf_url":"https://arxiv.org/pdf/2406.19556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19552v1","updated":"2024-06-27T22:08:22Z","published":"2024-06-27T22:08:22Z","title":"Rethinking harmless refusals when fine-tuning foundation models","summary":"  In this paper, we investigate the degree to which fine-tuning in Large\nLanguage Models (LLMs) effectively mitigates versus merely conceals undesirable\nbehavior. Through the lens of semi-realistic role-playing exercises designed to\nelicit such behaviors, we explore the response dynamics of LLMs post\nfine-tuning interventions. Our methodology involves prompting models for\nChain-of-Thought (CoT) reasoning and analyzing the coherence between the\nreasoning traces and the resultant outputs. Notably, we identify a pervasive\nphenomenon we term \\emph{reason-based deception}, where models either stop\nproducing reasoning traces or produce seemingly ethical reasoning traces that\nbelie the unethical nature of their final outputs. We further examine the\nefficacy of response strategies (polite refusal versus explicit rebuttal) in\ncurbing the occurrence of undesired behavior in subsequent outputs of\nmulti-turn interactions. Our findings reveal that explicit rebuttals\nsignificantly outperform polite refusals in preventing the continuation of\nundesired outputs and nearly eliminate reason-based deception, challenging\ncurrent practices in model fine-tuning. Accordingly, the two key contributions\nof this paper are (1) defining and studying reason-based deception, a new type\nof hidden behavior, and (2) demonstrating that rebuttals provide a more robust\nresponse model to harmful requests than refusals, thereby highlighting the need\nto reconsider the response strategies in fine-tuning approaches.\n","authors":["Florin Pop","Judd Rosenblatt","Diogo Schwerz de Lucena","Michael Vaiana"],"pdf_url":"https://arxiv.org/pdf/2406.19552v1.pdf","comment":"ICLR 2024 AGI Workshop Poster"},{"id":"http://arxiv.org/abs/2406.09606v2","updated":"2024-06-27T22:06:19Z","published":"2024-06-13T22:34:58Z","title":"Cross-Modality Program Representation Learning for Electronic Design\n  Automation with High-Level Synthesis","summary":"  In recent years, domain-specific accelerators (DSAs) have gained popularity\nfor applications such as deep learning and autonomous driving. To facilitate\nDSA designs, programmers use high-level synthesis (HLS) to compile a high-level\ndescription written in C/C++ into a design with low-level hardware description\nlanguages that eventually synthesize DSAs on circuits. However, creating a\nhigh-quality HLS design still demands significant domain knowledge,\nparticularly in microarchitecture decisions expressed as \\textit{pragmas}.\nThus, it is desirable to automate such decisions with the help of machine\nlearning for predicting the quality of HLS designs, requiring a deeper\nunderstanding of the program that consists of original code and pragmas.\nNaturally, these programs can be considered as sequence data. In addition,\nthese programs can be compiled and converted into a control data flow graph\n(CDFG). But existing works either fail to leverage both modalities or combine\nthe two in shallow or coarse ways. We propose ProgSG, a model that allows\ninteraction between the source code sequence modality and the graph modality in\na deep and fine-grained way. To alleviate the scarcity of labeled designs, a\npre-training method is proposed based on a suite of compiler's data flow\nanalysis tasks. Experimental results show that ProgSG reduces the RMSE of\ndesign performance predictions by up to $22\\%$, and identifies designs with an\naverage of $1.10\\times$ and $1.26\\times$ (up to $8.17\\times$ and $13.31\\times$)\nperformance improvement in design space exploration (DSE) task compared to HARP\nand AutoDSE, respectively.\n","authors":["Zongyue Qin","Yunsheng Bai","Atefeh Sohrabizadeh","Zijian Ding","Ziniu Hu","Yizhou Sun","Jason Cong"],"pdf_url":"https://arxiv.org/pdf/2406.09606v2.pdf","comment":"14 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:2305.10838"},{"id":"http://arxiv.org/abs/2406.19549v1","updated":"2024-06-27T22:01:00Z","published":"2024-06-27T22:01:00Z","title":"ASCENT: Amplifying Power Side-Channel Resilience via Learning &\n  Monte-Carlo Tree Search","summary":"  Power side-channel (PSC) analysis is pivotal for securing cryptographic\nhardware. Prior art focused on securing gate-level netlists obtained as-is from\nchip design automation, neglecting all the complexities and potential\nside-effects for security arising from the design automation process. That is,\nautomation traditionally prioritizes power, performance, and area (PPA),\nsidelining security. We propose a \"security-first\" approach, refining the logic\nsynthesis stage to enhance the overall resilience of PSC countermeasures. We\nintroduce ASCENT, a learning-and-search-based framework that (i) drastically\nreduces the time for post-design PSC evaluation and (ii) explores the\nsecurity-vs-PPA design space. Thus, ASCENT enables an efficient exploration of\na large number of candidate netlists, leading to an improvement in PSC\nresilience compared to regular PPA-optimized netlists. ASCENT is up to 120x\nfaster than traditional PSC analysis and yields a 3.11x improvement for PSC\nresilience of state-of-the-art PSC countermeasures\n","authors":["Jitendra Bhandari","Animesh Basak Chowdhury","Ozgur Sinanoglu","Siddharth Garg","Ramesh Karri","Johann Knechtel"],"pdf_url":"https://arxiv.org/pdf/2406.19549v1.pdf","comment":"Accepted at 2024 ACM/IEEE International Conference on Computer-Aided\n  Design"},{"id":"http://arxiv.org/abs/2404.05891v2","updated":"2024-06-27T21:54:40Z","published":"2024-04-08T22:20:23Z","title":"Condition Monitoring with Incomplete Data: An Integrated Variational\n  Autoencoder and Distance Metric Framework","summary":"  Condition monitoring of industrial systems is crucial for ensuring safety and\nmaintenance planning, yet notable challenges arise in real-world settings due\nto the limited or non-existent availability of fault samples. This paper\nintroduces an innovative solution to this problem by proposing a new method for\nfault detection and condition monitoring for unseen data. Adopting an approach\ninspired by zero-shot learning, our method can identify faults and assign a\nrelative health index to various operational conditions. Typically, we have\nplenty of data on normal operations, some data on compromised conditions, and\nvery few (if any) samples of severe faults. We use a variational autoencoder to\ncapture the probabilistic distribution of previously seen and new unseen\nconditions. The health status is determined by comparing each sample's\ndeviation from a normal operation reference distribution in the latent space.\nFaults are detected by establishing a threshold for the health indexes,\nallowing the model to identify severe, unseen faults with high accuracy, even\namidst noise. We validate our approach using the run-to-failure IMS-bearing\ndataset and compare it with other methods. The health indexes generated by our\nmodel closely match the established descriptive model of bearing wear,\nattesting to the robustness and reliability of our method. These findings\nhighlight the potential of our methodology in augmenting fault detection\ncapabilities within industrial domains, thereby contributing to heightened\nsafety protocols and optimized maintenance practices.\n","authors":["Maryam Ahang","Mostafa Abbasi","Todd Charter","Homayoun Najjaran"],"pdf_url":"https://arxiv.org/pdf/2404.05891v2.pdf","comment":"Accepted in the 2024 IEEE 20th International Conference on Automation\n  Science and Engineering (CASE 2024)"},{"id":"http://arxiv.org/abs/2406.19532v1","updated":"2024-06-27T21:12:48Z","published":"2024-06-27T21:12:48Z","title":"Dataless Quadratic Neural Networks for the Maximum Independent Set\n  Problem","summary":"  Combinatorial Optimization (CO) plays a crucial role in addressing various\nsignificant problems, among them the challenging Maximum Independent Set (MIS)\nproblem. In light of recent advancements in deep learning methods, efforts have\nbeen directed towards leveraging data-driven learning approaches, typically\nrooted in supervised learning and reinforcement learning, to tackle the NP-hard\nMIS problem. However, these approaches rely on labeled datasets, exhibit weak\ngeneralization, and often depend on problem-specific heuristics. Recently,\nReLU-based dataless neural networks were introduced to address combinatorial\noptimization problems. This paper introduces a novel dataless quadratic neural\nnetwork formulation, featuring a continuous quadratic relaxation for the MIS\nproblem. Notably, our method eliminates the need for training data by treating\nthe given MIS instance as a trainable entity. More specifically, the graph\nstructure and constraints of the MIS instance are used to define the structure\nand parameters of the neural network such that training it on a fixed input\nprovides a solution to the problem, thereby setting it apart from traditional\nsupervised or reinforcement learning approaches. By employing a gradient-based\noptimization algorithm like ADAM and leveraging an efficient off-the-shelf GPU\nparallel implementation, our straightforward yet effective approach\ndemonstrates competitive or superior performance compared to state-of-the-art\nlearning-based methods. Another significant advantage of our approach is that,\nunlike exact and heuristic solvers, the running time of our method scales only\nwith the number of nodes in the graph, not the number of edges.\n","authors":["Ismail Alkhouri","Cedric Le Denmat","Yingjie Li","Cunxi Yu","Jia Liu","Rongrong Wang","Alvaro Velasquez"],"pdf_url":"https://arxiv.org/pdf/2406.19532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19531v1","updated":"2024-06-27T21:12:26Z","published":"2024-06-27T21:12:26Z","title":"Forward and Backward State Abstractions for Off-policy Evaluation","summary":"  Off-policy evaluation (OPE) is crucial for evaluating a target policy's\nimpact offline before its deployment. However, achieving accurate OPE in large\nstate spaces remains challenging.This paper studies state\nabstractions-originally designed for policy learning-in the context of OPE. Our\ncontributions are three-fold: (i) We define a set of irrelevance conditions\ncentral to learning state abstractions for OPE. (ii) We derive sufficient\nconditions for achieving irrelevance in Q-functions and marginalized importance\nsampling ratios, the latter obtained by constructing a time-reversed Markov\ndecision process (MDP) based on the observed MDP. (iii) We propose a novel\ntwo-step procedure that sequentially projects the original state space into a\nsmaller space, which substantially simplify the sample complexity of OPE\narising from high cardinality.\n","authors":["Meiling Hao","Pingfan Su","Liyuan Hu","Zoltan Szabo","Qingyuan Zhao","Chengchun Shi"],"pdf_url":"https://arxiv.org/pdf/2406.19531v1.pdf","comment":"42 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.19526v1","updated":"2024-06-27T20:56:57Z","published":"2024-06-27T20:56:57Z","title":"TocBERT: Medical Document Structure Extraction Using Bidirectional\n  Transformers","summary":"  Text segmentation holds paramount importance in the field of Natural Language\nProcessing (NLP). It plays an important role in several NLP downstream tasks\nlike information retrieval and document summarization. In this work, we propose\na new solution, namely TocBERT, for segmenting texts using bidirectional\ntransformers. TocBERT represents a supervised solution trained on the detection\nof titles and sub-titles from their semantic representations. This task was\nformulated as a named entity recognition (NER) problem. The solution has been\napplied on a medical text segmentation use-case where the Bio-ClinicalBERT\nmodel is fine-tuned to segment discharge summaries of the MIMIC-III dataset.\nThe performance of TocBERT has been evaluated on a human-labeled ground truth\ncorpus of 250 notes. It achieved an F1-score of 84.6% when evaluated on a\nlinear text segmentation problem and 72.8% on a hierarchical text segmentation\nproblem. It outperformed a carefully designed rule-based solution, particularly\nin distinguishing titles from subtitles.\n","authors":["Majd Saleh","Sarra Baghdadi","Stéphane Paquelet"],"pdf_url":"https://arxiv.org/pdf/2406.19526v1.pdf","comment":"6 pages, 6 figures"},{"id":"http://arxiv.org/abs/2312.00246v3","updated":"2024-06-27T20:51:56Z","published":"2023-11-30T23:24:45Z","title":"Directions of Curvature as an Explanation for Loss of Plasticity","summary":"  Loss of plasticity is a phenomenon in which neural networks lose their\nability to learn from new experience. Despite being empirically observed in\nseveral problem settings, little is understood about the mechanisms that lead\nto loss of plasticity. In this paper, we offer a consistent explanation for\nloss of plasticity: Neural networks lose directions of curvature during\ntraining and that loss of plasticity can be attributed to this reduction in\ncurvature. To support such a claim, we provide a systematic investigation of\nloss of plasticity across continual learning tasks using MNIST, CIFAR-10 and\nImageNet. Our findings illustrate that loss of curvature directions coincides\nwith loss of plasticity, while also showing that previous explanations are\ninsufficient to explain loss of plasticity in all settings. Lastly, we show\nthat regularizers which mitigate loss of plasticity also preserve curvature,\nmotivating a simple distributional regularizer that proves to be effective\nacross the problem settings we considered.\n","authors":["Alex Lewandowski","Haruto Tanaka","Dale Schuurmans","Marlos C. Machado"],"pdf_url":"https://arxiv.org/pdf/2312.00246v3.pdf","comment":null}],"Image and Video Processing":[{"id":"http://arxiv.org/abs/2406.19574v1","updated":"2024-06-27T23:26:57Z","published":"2024-06-27T23:26:57Z","title":"Deep Temporal Sequence Classification and Mathematical Modeling for Cell\n  Tracking in Dense 3D Microscopy Videos of Bacterial Biofilms","summary":"  Automatic cell tracking in dense environments is plagued by inaccurate\ncorrespondences and misidentification of parent-offspring relationships. In\nthis paper, we introduce a novel cell tracking algorithm named DenseTrack,\nwhich integrates deep learning with mathematical model-based strategies to\neffectively establish correspondences between consecutive frames and detect\ncell division events in crowded scenarios. We formulate the cell tracking\nproblem as a deep learning-based temporal sequence classification task followed\nby solving a constrained one-to-one matching optimization problem exploiting\nthe classifier's confidence scores. Additionally, we present an\neigendecomposition-based cell division detection strategy that leverages\nknowledge of cellular geometry. The performance of the proposed approach has\nbeen evaluated by tracking densely packed cells in 3D time-lapse image\nsequences of bacterial biofilm development. The experimental results on\nsimulated as well as experimental fluorescence image sequences suggest that the\nproposed tracking method achieves superior performance in terms of both\nqualitative and quantitative evaluation measures compared to recent\nstate-of-the-art cell tracking approaches.\n","authors":["Tanjin Taher Toma","Yibo Wang","Andreas Gahlmann","Scott T. Acton"],"pdf_url":"https://arxiv.org/pdf/2406.19574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19560v1","updated":"2024-06-27T22:19:19Z","published":"2024-06-27T22:19:19Z","title":"Cost-efficient Active Illumination Camera For Hyper-spectral\n  Reconstruction","summary":"  Hyper-spectral imaging has recently gained increasing attention for use in\ndifferent applications, including agricultural investigation, ground tracking,\nremote sensing and many other. However, the high cost, large physical size and\ncomplicated operation process stop hyperspectral cameras from being employed\nfor various applications and research fields. In this paper, we introduce a\ncost-efficient, compact and easy to use active illumination camera that may\nbenefit many applications. We developed a fully functional prototype of such\ncamera. With the hope of helping with agricultural research, we tested our\ncamera for plant root imaging. In addition, a U-Net model for spectral\nreconstruction was trained by using a reference hyperspectral camera's data as\nground truth and our camera's data as input. We demonstrated our camera's\nability to obtain additional information over a typical RGB camera. In\naddition, the ability to reconstruct hyperspectral data from multi-spectral\ninput makes our device compatible to models and algorithms developed for\nhyperspectral applications with no modifications required.\n","authors":["Yuxuan Zhang","T. M. Sazzad","Yangyang Song","Spencer J. Chang","Ritesh Chowdhry","Tomas Mejia","Anna Hampton","Shelby Kucharski","Stefan Gerber","Barry Tillman","Marcio F. R. Resende","William M. Hammond","Chris H. Wilson","Alina Zare","Sanjeev J. Koppal"],"pdf_url":"https://arxiv.org/pdf/2406.19560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19557v1","updated":"2024-06-27T22:17:49Z","published":"2024-06-27T22:17:49Z","title":"Robustness Testing of Black-Box Models Against CT Degradation Through\n  Test-Time Augmentation","summary":"  Deep learning models for medical image segmentation and object detection are\nbecoming increasingly available as clinical products. However, as details are\nrarely provided about the training data, models may unexpectedly fail when\ncases differ from those in the training distribution. An approach allowing\npotential users to independently test the robustness of a model, treating it as\na black box and using only a few cases from their own site, is key for\nadoption. To address this, a method to test the robustness of these models\nagainst CT image quality variation is presented. In this work we present this\nframework by demonstrating that given the same training data, the model\narchitecture and data pre processing greatly affect the robustness of several\nfrequently used segmentation and object detection methods to simulated CT\nimaging artifacts and degradation. Our framework also addresses the concern\nabout the sustainability of deep learning models in clinical use, by\nconsidering future shifts in image quality due to scanner deterioration or\nimaging protocol changes which are not reflected in a limited local test\ndataset.\n","authors":["Jack Highton","Quok Zong Chong","Samuel Finestone","Arian Beqiri","Julia A. Schnabel","Kanwal K. Bhatia"],"pdf_url":"https://arxiv.org/pdf/2406.19557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19556v1","updated":"2024-06-27T22:16:53Z","published":"2024-06-27T22:16:53Z","title":"BOrg: A Brain Organoid-Based Mitosis Dataset for Automatic Analysis of\n  Brain Diseases","summary":"  Recent advances have enabled the study of human brain development using brain\norganoids derived from stem cells. Quantifying cellular processes like mitosis\nin these organoids offers insights into neurodevelopmental disorders, but the\nmanual analysis is time-consuming, and existing datasets lack specific details\nfor brain organoid studies. We introduce BOrg, a dataset designed to study\nmitotic events in the embryonic development of the brain using confocal\nmicroscopy images of brain organoids. BOrg utilizes an efficient annotation\npipeline with sparse point annotations and techniques that minimize expert\neffort, overcoming limitations of standard deep learning approaches on sparse\ndata. We adapt and benchmark state-of-the-art object detection and cell\ncounting models on BOrg for detecting and analyzing mitotic cells across\nprophase, metaphase, anaphase, and telophase stages. Our results demonstrate\nthese adapted models significantly improve mitosis analysis efficiency and\naccuracy for brain organoid research compared to existing methods. BOrg\nfacilitates the development of automated tools to quantify statistics like\nmitosis rates, aiding mechanistic studies of neurodevelopmental processes and\ndisorders. Data and code are available at https://github.com/awaisrauf/borg.\n","authors":["Muhammad Awais","Mehaboobathunnisa Sahul Hameed","Bidisha Bhattacharya","Orly Reiner","Rao Muhammad Anwer"],"pdf_url":"https://arxiv.org/pdf/2406.19556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15727v2","updated":"2024-06-27T20:13:34Z","published":"2024-06-22T04:32:50Z","title":"Semi-supervised variational autoencoder for cell feature extraction in\n  multiplexed immunofluorescence images","summary":"  Advancements in digital imaging technologies have sparked increased interest\nin using multiplexed immunofluorescence (mIF) images to visualise and identify\nthe interactions between specific immunophenotypes with the tumour\nmicroenvironment at the cellular level. Current state-of-the-art multiplexed\nimmunofluorescence image analysis pipelines depend on cell feature\nrepresentations characterised by morphological and stain intensity-based\nmetrics generated using simple statistical and machine learning-based tools.\nHowever, these methods are not capable of generating complex representations of\ncells. We propose a deep learning-based cell feature extraction model using a\nvariational autoencoder with supervision using a latent subspace to extract\ncell features in mIF images. We perform cell phenotype classification using a\ncohort of more than 44,000 multiplexed immunofluorescence cell image patches\nextracted across 1,093 tissue microarray cores of breast cancer patients, to\ndemonstrate the success of our model against current and alternative methods.\n","authors":["Piumi Sandarenu","Julia Chen","Iveta Slapetova","Lois Browne","Peter H. Graham","Alexander Swarbrick","Ewan K. A. Millar","Yang Song","Erik Meijering"],"pdf_url":"https://arxiv.org/pdf/2406.15727v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19492v1","updated":"2024-06-27T19:16:57Z","published":"2024-06-27T19:16:57Z","title":"High-resolution segmentations of the hypothalamus and its subregions for\n  training of segmentation models","summary":"  Segmentation of brain structures on magnetic resonance imaging (MRI) is a\nhighly relevant neuroimaging topic, as it is a prerequisite for different\nanalyses such as volumetry or shape analysis. Automated segmentation\nfacilitates the study of brain structures in larger cohorts when compared with\nmanual segmentation, which is time-consuming. However, the development of most\nautomated methods relies on large and manually annotated datasets, which limits\nthe generalizability of these methods. Recently, new techniques using synthetic\nimages have emerged, reducing the need for manual annotation. Here we provide\nHELM, Hypothalamic ex vivo Label Maps, a dataset composed of label maps built\nfrom publicly available ultra-high resolution ex vivo MRI from 10 whole\nhemispheres, which can be used to develop segmentation methods using synthetic\ndata. The label maps are obtained with a combination of manual labels for the\nhypothalamic regions and automated segmentations for the rest of the brain, and\nmirrored to simulate entire brains. We also provide the pre-processed ex vivo\nscans, as this dataset can support future projects to include other structures\nafter these are manually segmented.\n","authors":["Livia Rodrigues","Martina Bocchetta","Oula Puonti","Douglas Greve","Ana Carolina Londe","Marcondes França","Simone Appenzeller","Leticia Rittner","Juan Eugenio Iglesias"],"pdf_url":"https://arxiv.org/pdf/2406.19492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19485v1","updated":"2024-06-27T18:58:41Z","published":"2024-06-27T18:58:41Z","title":"GAPNet: Granularity Attention Network with Anatomy-Prior-Constraint for\n  Carotid Artery Segmentation","summary":"  Atherosclerosis is a chronic, progressive disease that primarily affects the\narterial walls. It is one of the major causes of cardiovascular disease.\nMagnetic Resonance (MR) black-blood vessel wall imaging (BB-VWI) offers crucial\ninsights into vascular disease diagnosis by clearly visualizing vascular\nstructures. However, the complex anatomy of the neck poses challenges in\ndistinguishing the carotid artery (CA) from surrounding structures, especially\nwith changes like atherosclerosis. In order to address these issues, we propose\nGAPNet, which is a consisting of a novel geometric prior deduced from.\n","authors":["Lin Zhang","Chenggang Lu","Xin-yang Shi","Caifeng Shan","Jiong Zhang","Da Chen","Laurent D. Cohen"],"pdf_url":"https://arxiv.org/pdf/2406.19485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13040v2","updated":"2024-06-27T17:27:13Z","published":"2024-03-19T17:35:17Z","title":"Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping","summary":"  Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify\ncolor Doppler in cardiac imaging. In this study, we propose novel alternatives\nto the traditional iVFM optimization scheme by utilizing physics-informed\nneural networks (PINNs) and a physics-guided nnU-Net-based supervised approach.\nWhen evaluated on simulated color Doppler images derived from a\npatient-specific computational fluid dynamics model and in vivo Doppler\nacquisitions, both approaches demonstrate comparable reconstruction performance\nto the original iVFM algorithm. The efficiency of PINNs is boosted through\ndual-stage optimization and pre-optimized weights. On the other hand, the\nnnU-Net method excels in generalizability and real-time capabilities. Notably,\nnnU-Net shows superior robustness on sparse and truncated Doppler data while\nmaintaining independence from explicit boundary conditions. Overall, our\nresults highlight the effectiveness of these methods in reconstructing\nintraventricular vector blood flow. The study also suggests potential\napplications of PINNs in ultrafast color Doppler imaging and the incorporation\nof fluid dynamics equations to derive biomarkers for cardiovascular diseases\nbased on blood flow.\n","authors":["Hang Jung Ling","Salomé Bru","Julia Puig","Florian Vixège","Simon Mendez","Franck Nicoud","Pierre-Yves Courand","Olivier Bernard","Damien Garcia"],"pdf_url":"https://arxiv.org/pdf/2403.13040v2.pdf","comment":"12 pages, accepted for publication in IEEE TUFFC; camera ready\n  corrections, corrected acknowledgments"},{"id":"http://arxiv.org/abs/2312.17293v3","updated":"2024-06-27T16:38:18Z","published":"2023-12-28T13:59:43Z","title":"$μ$GUIDE: a framework for quantitative imaging via generalized\n  uncertainty-driven inference using deep learning","summary":"  This work proposes $\\mu$GUIDE: a general Bayesian framework to estimate\nposterior distributions of tissue microstructure parameters from any given\nbiophysical model or MRI signal representation, with exemplar demonstration in\ndiffusion-weighted MRI. Harnessing a new deep learning architecture for\nautomatic signal feature selection combined with simulation-based inference and\nefficient sampling of the posterior distributions, $\\mu$GUIDE bypasses the high\ncomputational and time cost of conventional Bayesian approaches and does not\nrely on acquisition constraints to define model-specific summary statistics.\nThe obtained posterior distributions allow to highlight degeneracies present in\nthe model definition and quantify the uncertainty and ambiguity of the\nestimated parameters.\n","authors":["Maëliss Jallais","Marco Palombo"],"pdf_url":"https://arxiv.org/pdf/2312.17293v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06748v2","updated":"2024-06-27T15:24:23Z","published":"2024-03-11T14:14:52Z","title":"Shortcut Learning in Medical Image Segmentation","summary":"  Shortcut learning is a phenomenon where machine learning models prioritize\nlearning simple, potentially misleading cues from data that do not generalize\nwell beyond the training set. While existing research primarily investigates\nthis in the realm of image classification, this study extends the exploration\nof shortcut learning into medical image segmentation. We demonstrate that\nclinical annotations such as calipers, and the combination of zero-padded\nconvolutions and center-cropped training sets in the dataset can inadvertently\nserve as shortcuts, impacting segmentation accuracy. We identify and evaluate\nthe shortcut learning on two different but common medical image segmentation\ntasks. In addition, we suggest strategies to mitigate the influence of shortcut\nlearning and improve the generalizability of the segmentation models. By\nuncovering the presence and implications of shortcuts in medical image\nsegmentation, we provide insights and methodologies for evaluating and\novercoming this pervasive challenge and call for attention in the community for\nshortcuts in segmentation. Our code is public at\nhttps://github.com/nina-weng/shortcut_skinseg .\n","authors":["Manxi Lin","Nina Weng","Kamil Mikolaj","Zahra Bashir","Morten Bo Søndergaard Svendsen","Martin Tolsgaard","Anders Nymark Christensen","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2403.06748v2.pdf","comment":"11 pages, 6 figures, accepted at MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.19239v1","updated":"2024-06-27T15:02:04Z","published":"2024-06-27T15:02:04Z","title":"ALMA: a mathematics-driven approach for determining tuning parameters in\n  generalized LASSO problems, with applications to MRI","summary":"  Magnetic Resonance Imaging (MRI) is a powerful technique employed for\nnon-invasive in vivo visualization of internal structures. Sparsity is often\ndeployed to accelerate the signal acquisition or overcome the presence of\nmotion artifacts, improving the quality of image reconstruction. Image\nreconstruction algorithms use TV-regularized LASSO (Total Variation-regularized\nLASSO) to retrieve the missing information of undersampled signals, by cleaning\nthe data of noise and while optimizing sparsity. A tuning parameter moderates\nthe balance between these two aspects; its choice affecting the quality of the\nreconstructions. Currently, there is a lack of general deterministic techniques\nto choose these parameters, which are oftentimes manually selected and thus\nhinder the reliability of the reconstructions. Here, we present ALMA (Algorithm\nfor Lagrange Multipliers Approximation), an iterative mathematics-inspired\ntechnique that computes tuning parameters for generalized LASSO problems during\nMRI reconstruction. We analyze quantitatively the performance of these\nparameters for imaging reconstructions via TV-LASSO in an MRI context on\nphantoms. Although our study concentrates on TV-LASSO, the techniques developed\nhere hold significant promise for a wide array of applications. ALMA is not\nonly adaptable to more generalized LASSO problems but is also robust to\naccommodate other forms of regularization beyond total variation. Moreover, it\nextends effectively to handle non-Cartesian sampling trajectories, broadening\nits utility in complex data reconstruction scenarios. More generally, ALMA\nprovides a powerful tool for numerically solving constrained optimization\nproblems across various disciplines, offering a versatile and impactful\nsolution for advanced computational challenges.\n","authors":["Gianluca Giacchi","Isidoros Iakovidis","Bastien Milani","Matthias Stuber","Micah Murray","Benedetta Franceschiello"],"pdf_url":"https://arxiv.org/pdf/2406.19239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04723v2","updated":"2024-06-27T12:46:42Z","published":"2024-06-07T08:07:20Z","title":"A Deep Automotive Radar Detector using the RaDelft Dataset","summary":"  The detection of multiple extended targets in complex environments using\nhigh-resolution automotive radar is considered. A data-driven approach is\nproposed where unlabeled synchronized lidar data is used as ground truth to\ntrain a neural network with only radar data as input. To this end, the novel,\nlarge-scale, real-life, and multi-sensor RaDelft dataset has been recorded\nusing a demonstrator vehicle in different locations in the city of Delft. The\ndataset, as well as the documentation and example code, is publicly available\nfor those researchers in the field of automotive radar or machine perception.\nThe proposed data-driven detector is able to generate lidar-like point clouds\nusing only radar data from a high-resolution system, which preserves the shape\nand size of extended targets. The results are compared against conventional\nCFAR detectors as well as variations of the method to emulate the available\napproaches in the literature, using the probability of detection, the\nprobability of false alarm, and the Chamfer distance as performance metrics.\nMoreover, an ablation study was carried out to assess the impact of Doppler and\ntemporal information on detection performance. The proposed method outperforms\nthe different baselines in terms of Chamfer distance, achieving a reduction of\n75% against conventional CFAR detectors and 10% against the modified\nstate-of-the-art deep learning-based approaches.\n","authors":["Ignacio Roldan","Andras Palffy","Julian F. P. Kooij","Dariu M. Gavrila","Francesco Fioranelli","Alexander Yarovoy"],"pdf_url":"https://arxiv.org/pdf/2406.04723v2.pdf","comment":"Under review at IEEE Transaction on Radar Systems"},{"id":"http://arxiv.org/abs/2406.19081v1","updated":"2024-06-27T11:08:42Z","published":"2024-06-27T11:08:42Z","title":"Unsupervised Latent Stain Adaption for Digital Pathology","summary":"  In digital pathology, deep learning (DL) models for tasks such as\nsegmentation or tissue classification are known to suffer from domain shifts\ndue to different staining techniques. Stain adaptation aims to reduce the\ngeneralization error between different stains by training a model on source\nstains that generalizes to target stains. Despite the abundance of target stain\ndata, a key challenge is the lack of annotations. To address this, we propose a\njoint training between artificially labeled and unlabeled data including all\navailable stained images called Unsupervised Latent Stain Adaption (ULSA). Our\nmethod uses stain translation to enrich labeled source images with synthetic\ntarget images in order to increase supervised signals. Moreover, we leverage\nunlabeled target stain images using stain-invariant feature consistency\nlearning. With ULSA we present a semi-supervised strategy for efficient stain\nadaption without access to annotated target stain data. Remarkably, ULSA is\ntask agnostic in patch-level analysis for whole slide images (WSIs). Through\nextensive evaluation on external datasets, we demonstrate that ULSA achieves\nstate-of-the-art (SOTA) performance in kidney tissue segmentation and breast\ncancer classification across a spectrum of staining variations. Our findings\nsuggest that ULSA is an important framework towards stain adaption in digital\npathology.\n","authors":["Daniel Reisenbüchler","Lucas Luttner","Nadine S. Schaadt","Friedrich Feuerhake","Dorit Merhof"],"pdf_url":"https://arxiv.org/pdf/2406.19081v1.pdf","comment":"Accepted in MICCAI2024"},{"id":"http://arxiv.org/abs/2406.19043v1","updated":"2024-06-27T09:50:20Z","published":"2024-06-27T09:50:20Z","title":"CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI","summary":"  Cardiac magnetic resonance imaging (MRI) has emerged as a clinically\ngold-standard technique for diagnosing cardiac diseases, thanks to its ability\nto provide diverse information with multiple modalities and anatomical views.\nAccelerated cardiac MRI is highly expected to achieve time-efficient and\npatient-friendly imaging, and then advanced image reconstruction approaches are\nrequired to recover high-quality, clinically interpretable images from\nundersampled measurements. However, the lack of publicly available cardiac MRI\nk-space dataset in terms of both quantity and diversity has severely hindered\nsubstantial technological progress, particularly for data-driven artificial\nintelligence. Here, we provide a standardized, diverse, and high-quality\nCMRxRecon2024 dataset to facilitate the technical development, fair evaluation,\nand clinical transfer of cardiac MRI reconstruction approaches, towards\npromoting the universal frameworks that enable fast and robust reconstructions\nacross different cardiac MRI protocols in clinical practice. To the best of our\nknowledge, the CMRxRecon2024 dataset is the largest and most diverse publicly\navailable cardiac k-space dataset. It is acquired from 330 healthy volunteers,\ncovering commonly used modalities, anatomical views, and acquisition\ntrajectories in clinical cardiac MRI workflows. Besides, an open platform with\ntutorials, benchmarks, and data processing tools is provided to facilitate data\nusage, advanced method development, and fair performance evaluation.\n","authors":["Zi Wang","Fanwen Wang","Chen Qin","Jun Lyu","Ouyang Cheng","Shuo Wang","Yan Li","Mengyao Yu","Haoyu Zhang","Kunyuan Guo","Zhang Shi","Qirong Li","Ziqiang Xu","Yajing Zhang","Hao Li","Sha Hua","Binghua Chen","Longyu Sun","Mengting Sun","Qin Li","Ying-Hua Chu","Wenjia Bai","Jing Qin","Xiahai Zhuang","Claudia Prieto","Alistair Young","Michael Markl","He Wang","Lianming Wu","Guang Yang","Xiaobo Qu","Chengyan Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19043v1.pdf","comment":"19 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2406.18361v2","updated":"2024-06-27T08:53:25Z","published":"2024-06-26T14:01:07Z","title":"Stable Diffusion Segmentation for Biomedical Images with Single-step\n  Reverse Process","summary":"  Diffusion models have demonstrated their effectiveness across various\ngenerative tasks. However, when applied to medical image segmentation, these\nmodels encounter several challenges, including significant resource and time\nrequirements. They also necessitate a multi-step reverse process and multiple\nsamples to produce reliable predictions. To address these challenges, we\nintroduce the first latent diffusion segmentation model, named SDSeg, built\nupon stable diffusion (SD). SDSeg incorporates a straightforward latent\nestimation strategy to facilitate a single-step reverse process and utilizes\nlatent fusion concatenation to remove the necessity for multiple samples.\nExtensive experiments indicate that SDSeg surpasses existing state-of-the-art\nmethods on five benchmark datasets featuring diverse imaging modalities.\nRemarkably, SDSeg is capable of generating stable predictions with a solitary\nreverse step and sample, epitomizing the model's stability as implied by its\nname. The code is available at\nhttps://github.com/lin-tianyu/Stable-Diffusion-Seg\n","authors":["Tianyu Lin","Zhiguang Chen","Zhonghao Yan","Weijiang Yu","Fudan Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.18361v2.pdf","comment":"Accepted at MICCAI 2024. Code and citation info see\n  https://github.com/lin-tianyu/Stable-Diffusion-Seg"},{"id":"http://arxiv.org/abs/2403.02311v3","updated":"2024-06-27T08:21:51Z","published":"2024-03-04T18:47:56Z","title":"Bayesian Uncertainty Estimation by Hamiltonian Monte Carlo: Applications\n  to Cardiac MRI Segmentation","summary":"  Deep learning (DL)-based methods have achieved state-of-the-art performance\nfor many medical image segmentation tasks. Nevertheless, recent studies show\nthat deep neural networks (DNNs) can be miscalibrated and overconfident,\nleading to \"silent failures\" that are risky for clinical applications. Bayesian\nDL provides an intuitive approach to DL failure detection, based on posterior\nprobability estimation. However, the posterior is intractable for large medical\nimage segmentation DNNs. To tackle this challenge, we propose a Bayesian\nlearning framework using Hamiltonian Monte Carlo (HMC), tempered by cold\nposterior (CP) to accommodate medical data augmentation, named HMC-CP. For HMC\ncomputation, we further propose a cyclical annealing strategy, capturing both\nlocal and global geometries of the posterior distribution, enabling highly\nefficient Bayesian DNN training with the same computational budget as training\na single DNN. The resulting Bayesian DNN outputs an ensemble segmentation along\nwith the segmentation uncertainty. We evaluate the proposed HMC-CP extensively\non cardiac magnetic resonance image (MRI) segmentation, using in-domain\nsteady-state free precession (SSFP) cine images as well as out-of-domain\ndatasets of quantitative T1 and T2 mapping. Our results show that the proposed\nmethod improves both segmentation accuracy and uncertainty estimation for in-\nand out-of-domain data, compared with well-established baseline methods such as\nMonte Carlo Dropout and Deep Ensembles. Additionally, we establish a conceptual\nlink between HMC and the commonly known stochastic gradient descent (SGD) and\nprovide general insight into the uncertainty of DL. This uncertainty is\nimplicitly encoded in the training dynamics but often overlooked. With reliable\nuncertainty estimation, our method provides a promising direction toward\ntrustworthy DL in clinical applications.\n","authors":["Yidong Zhao","Joao Tourais","Iain Pierce","Christian Nitsche","Thomas A. Treibel","Sebastian Weingärtner","Artur M. Schweidtmann","Qian Tao"],"pdf_url":"https://arxiv.org/pdf/2403.02311v3.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:011"},{"id":"http://arxiv.org/abs/2406.09327v2","updated":"2024-06-27T08:09:27Z","published":"2024-06-13T17:06:15Z","title":"Towards AI Lesion Tracking in PET/CT Imaging: A Siamese-based CNN\n  Pipeline applied on PSMA PET/CT Scans","summary":"  Assessing tumor response to systemic therapies is one of the main\napplications of PET/CT. Routinely, only a small subset of index lesions out of\nmultiple lesions is analyzed. However, this operator dependent selection may\nbias the results due to possible significant inter-metastatic heterogeneity of\nresponse to therapy. Automated, AI based approaches for lesion tracking hold\npromise in enabling the analysis of many more lesions and thus providing a\nbetter assessment of tumor response. This work introduces a Siamese CNN\napproach for lesion tracking between PET/CT scans. Our approach is applied on\nthe laborious task of tracking a high number of bone lesions in full-body\nbaseline and follow-up [68Ga]Ga- or [18F]F-PSMA PET/CT scans after two cycles\nof [177Lu]Lu-PSMA therapy of metastatic castration resistant prostate cancer\npatients. Data preparation includes lesion segmentation and affine\nregistration. Our algorithm extracts suitable lesion patches and forwards them\ninto a Siamese CNN trained to classify the lesion patch pairs as corresponding\nor non-corresponding lesions. Experiments have been performed with different\ninput patch types and a Siamese network in 2D and 3D. The CNN model\nsuccessfully learned to classify lesion assignments, reaching a lesion tracking\naccuracy of 83 % in its best configuration with an AUC = 0.91. For remaining\nlesions the pipeline accomplished a re-identification rate of 89 %. We proved\nthat a CNN may facilitate the tracking of multiple lesions in PSMA PET/CT\nscans. Future clinical studies are necessary if this improves the prediction of\nthe outcome of therapies.\n","authors":["Stefan P. Hein","Manuel Schultheiss","Andrei Gafita","Raphael Zaum","Farid Yagubbayli","Robert Tauber","Isabel Rauscher","Matthias Eiber","Franz Pfeiffer","Wolfgang A. Weber"],"pdf_url":"https://arxiv.org/pdf/2406.09327v2.pdf","comment":"25 pages, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2401.06517v3","updated":"2024-06-27T07:50:25Z","published":"2024-01-12T11:33:20Z","title":"LiDAR Depth Map Guided Image Compression Model","summary":"  The incorporation of LiDAR technology into some high-end smartphones has\nunlocked numerous possibilities across various applications, including\nphotography, image restoration, augmented reality, and more. In this paper, we\nintroduce a novel direction that harnesses LiDAR depth maps to enhance the\ncompression of the corresponding RGB camera images. To the best of our\nknowledge, this represents the initial exploration in this particular research\ndirection. Specifically, we propose a Transformer-based learned image\ncompression system capable of achieving variable-rate compression using a\nsingle model while utilizing the LiDAR depth map as supplementary information\nfor both the encoding and decoding processes. Experimental results demonstrate\nthat integrating LiDAR yields an average PSNR gain of 0.83 dB and an average\nbitrate reduction of 16% as compared to its absence.\n","authors":["Alessandro Gnutti","Stefano Della Fiore","Mattia Savardi","Yi-Hsin Chen","Riccardo Leonardi","Wen-Hsiao Peng"],"pdf_url":"https://arxiv.org/pdf/2401.06517v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18950v1","updated":"2024-06-27T07:30:54Z","published":"2024-06-27T07:30:54Z","title":"MMR-Mamba: Multi-Contrast MRI Reconstruction with Mamba and\n  Spatial-Frequency Information Fusion","summary":"  Multi-contrast MRI acceleration has become prevalent in MR imaging, enabling\nthe reconstruction of high-quality MR images from under-sampled k-space data of\nthe target modality, using guidance from a fully-sampled auxiliary modality.\nThe main crux lies in efficiently and comprehensively integrating complementary\ninformation from the auxiliary modality. Existing methods either suffer from\nquadratic computational complexity or fail to capture long-range correlated\nfeatures comprehensively. In this work, we propose MMR-Mamba, a novel framework\nthat achieves comprehensive integration of multi-contrast features through\nMamba and spatial-frequency information fusion. Firstly, we design the\n\\textit{Target modality-guided Cross Mamba} (TCM) module in the spatial domain,\nwhich maximally restores the target modality information by selectively\nabsorbing useful information from the auxiliary modality. Secondly, leveraging\nglobal properties of the Fourier domain, we introduce the \\textit{Selective\nFrequency Fusion} (SFF) module to efficiently integrate global information in\nthe frequency domain and recover high-frequency signals for the reconstruction\nof structure details. Additionally, we present the \\textit{Adaptive\nSpatial-Frequency Fusion} (ASFF) module, which enhances fused features by\nsupplementing less informative features from one domain with corresponding\nfeatures from the other domain. These innovative strategies ensure efficient\nfeature fusion across spatial and frequency domains, avoiding the introduction\nof redundant information and facilitating the reconstruction of high-quality\ntarget images. Extensive experiments on the BraTS and fastMRI knee datasets\ndemonstrate the superiority of the proposed MMR-Mamba over state-of-the-art MRI\nreconstruction methods.\n","authors":["Jing Zou","Lanqing Liu","Qi Chen","Shujun Wang","Xiaohan Xing","Jing Qin"],"pdf_url":"https://arxiv.org/pdf/2406.18950v1.pdf","comment":"10 pages, 5 figure"},{"id":"http://arxiv.org/abs/2310.02792v2","updated":"2024-06-27T07:29:09Z","published":"2023-10-04T13:11:20Z","title":"Continuous 3D Myocardial Motion Tracking via Echocardiography","summary":"  Myocardial motion tracking stands as an essential clinical tool in the\nprevention and detection of cardiovascular diseases (CVDs), the foremost cause\nof death globally. However, current techniques suffer from incomplete and\ninaccurate motion estimation of the myocardium in both spatial and temporal\ndimensions, hindering the early identification of myocardial dysfunction. To\naddress these challenges, this paper introduces the Neural Cardiac Motion Field\n(NeuralCMF). NeuralCMF leverages implicit neural representation (INR) to model\nthe 3D structure and the comprehensive 6D forward/backward motion of the heart.\nThis method surpasses pixel-wise limitations by offering the capability to\ncontinuously query the precise shape and motion of the myocardium at any\nspecific point throughout the cardiac cycle, enhancing the detailed analysis of\ncardiac dynamics beyond traditional speckle tracking. Notably, NeuralCMF\noperates without the need for paired datasets, and its optimization is\nself-supervised through the physics knowledge priors in both space and time\ndimensions, ensuring compatibility with both 2D and 3D echocardiogram video\ninputs. Experimental validations across three representative datasets support\nthe robustness and innovative nature of the NeuralCMF, marking significant\nadvantages over existing state-of-the-art methods in cardiac imaging and motion\ntracking.\n","authors":["Chengkang Shen","Hao Zhu","You Zhou","Yu Liu","Si Yi","Lili Dong","Weipeng Zhao","David J. Brady","Xun Cao","Zhan Ma","Yi Lin"],"pdf_url":"https://arxiv.org/pdf/2310.02792v2.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2406.18919v1","updated":"2024-06-27T06:22:02Z","published":"2024-06-27T06:22:02Z","title":"Classification of Carotid Plaque with Jellyfish Sign Through\n  Convolutional and Recurrent Neural Networks Utilizing Plaque Surface Edges","summary":"  In carotid arteries, plaque can develop as localized elevated lesions. The\nJellyfish sign, marked by fluctuating plaque surfaces with blood flow\npulsation, is a dynamic characteristic of these plaques that has recently\nattracted attention. Detecting this sign is vital, as it is often associated\nwith cerebral infarction. This paper proposes an ultrasound video-based\nclassification method for the Jellyfish sign, using deep neural networks. The\nproposed method first preprocesses carotid ultrasound videos to separate the\nmovement of the vascular wall from plaque movements. These preprocessed videos\nare then combined with plaque surface information and fed into a deep learning\nmodel comprising convolutional and recurrent neural networks, enabling the\nefficient classification of the Jellyfish sign. The proposed method was\nverified using ultrasound video images from 200 patients. Ablation studies\ndemonstrated the effectiveness of each component of the proposed method.\n","authors":["Takeshi Yoshidomi","Shinji Kume","Hiroaki Aizawa","Akira Furui"],"pdf_url":"https://arxiv.org/pdf/2406.18919v1.pdf","comment":"4 pages, 3 figures, accepted at IEEE EMBC 2024"},{"id":"http://arxiv.org/abs/2312.11580v2","updated":"2024-06-27T02:25:31Z","published":"2023-12-18T10:55:11Z","title":"PlaNet-S: Automatic Semantic Segmentation of Placenta","summary":"  [Purpose] To develop a fully automated semantic placenta segmentation model\nthat integrates the U-Net and SegNeXt architectures through ensemble learning.\n[Methods] A total of 218 pregnant women with suspected placental anomalies who\nunderwent magnetic resonance imaging (MRI) were enrolled, yielding 1090\nannotated images for developing a deep learning model for placental\nsegmentation. The images were standardized and divided into training and test\nsets. The performance of PlaNet-S, which integrates U-Net and SegNeXt within an\nensemble framework, was assessed using Intersection over Union (IoU) and\ncounting connected components (CCC) against the U-Net model. [Results] PlaNet-S\nhad significantly higher IoU (0.73 +/- 0.13) than that of U-Net (0.78 +/-\n0.010) (p<0.01). The CCC for PlaNet-S was significantly higher than that for\nU-Net (p<0.01), matching the ground truth in 86.0\\% and 56.7\\% of the cases,\nrespectively. [Conclusion]PlaNet-S performed better than the traditional U-Net\nin placental segmentation tasks. This model addresses the challenges of\ntime-consuming physician-assisted manual segmentation and offers the potential\nfor diverse applications in placental imaging analyses.\n","authors":["Shinnosuke Yamamoto","Isso Saito","Eichi Takaya","Ayaka Harigai","Tomomi Sato","Tomoya Kobayashi","Kei Takase","Takuya Ueda"],"pdf_url":"https://arxiv.org/pdf/2312.11580v2.pdf","comment":"11 pages, 5 figures, Shinnosuke Yamamoto and Isso Saito equally\n  contributed to this work. In the original submission, there was a\n  typographical error in the reported standard deviation for the Intersection\n  over Union (IoU) values of the PlaNet-S model. The standard deviation was\n  incorrectly listed as 0.01 instead of the correct value of 0.1. This has been\n  corrected in the revised version"},{"id":"http://arxiv.org/abs/2406.18840v1","updated":"2024-06-27T02:19:47Z","published":"2024-06-27T02:19:47Z","title":"Shorter SPECT Scans Using Self-supervised Coordinate Learning to\n  Synthesize Skipped Projection Views","summary":"  Purpose: This study addresses the challenge of extended SPECT imaging\nduration under low-count conditions, as encountered in Lu-177 SPECT imaging, by\ndeveloping a self-supervised learning approach to synthesize skipped SPECT\nprojection views, thus shortening scan times in clinical settings. Methods: We\nemployed a self-supervised coordinate-based learning technique, adapting the\nneural radiance field (NeRF) concept in computer vision to synthesize\nunder-sampled SPECT projection views. For each single scan, we used\nself-supervised coordinate learning to estimate skipped SPECT projection views.\nThe method was tested with various down-sampling factors (DFs=2, 4, 8) on both\nLu-177 phantom SPECT/CT measurements and clinical SPECT/CT datasets, from 11\npatients undergoing Lu-177 DOTATATE and 6 patients undergoing Lu-177 PSMA-617\nradiopharmaceutical therapy. Results: For SPECT reconstructions, our method\noutperformed the use of linearly interpolated projections and partial\nprojection views in relative contrast-to-noise-ratios (RCNR) averaged across\ndifferent downsampling factors: 1) DOTATATE: 83% vs. 65% vs. 67% for lesions\nand 86% vs. 70% vs. 67% for kidney, 2) PSMA: 76% vs. 69% vs. 68% for lesions\nand 75% vs. 55% vs. 66% for organs, including kidneys, lacrimal glands, parotid\nglands, and submandibular glands. Conclusion: The proposed method enables\nreduction in acquisition time (by factors of 2, 4, or 8) while maintaining\nquantitative accuracy in clinical SPECT protocols by allowing for the\ncollection of fewer projections. Importantly, the self-supervised nature of\nthis NeRF-based approach eliminates the need for extensive training data,\ninstead learning from each patient's projection data alone. The reduction in\nacquisition time is particularly relevant for imaging under low-count\nconditions and for protocols that require multiple-bed positions such as\nwhole-body imaging.\n","authors":["Zongyu Li","Yixuan Jia","Xiaojian Xu","Jason Hu","Jeffrey A. Fessler","Yuni K. Dewaraja"],"pdf_url":"https://arxiv.org/pdf/2406.18840v1.pdf","comment":"25 pages, 5568 words"},{"id":"http://arxiv.org/abs/2308.16676v2","updated":"2024-06-27T02:12:46Z","published":"2023-08-31T12:28:09Z","title":"Twofold Structured Features-Based Siamese Network for Infrared Target\n  Tracking","summary":"  Nowadays, infrared target tracking has been a critical technology in the\nfield of computer vision and has many applications, such as motion analysis,\npedestrian surveillance, intelligent detection, and so forth. Unfortunately,\ndue to the lack of color, texture and other detailed information, tracking\ndrift often occurs when the tracker encounters infrared targets that vary in\nsize or shape. To address this issue, we present a twofold structured\nfeatures-based Siamese network for infrared target tracking. First of all, in\norder to improve the discriminative capacity for infrared targets, a novel\nfeature fusion network is proposed to fuse both shallow spatial information and\ndeep semantic information into the extracted features in a comprehensive\nmanner. Then, a multi-template update module based on template update mechanism\nis designed to effectively deal with interferences from target appearance\nchanges which are prone to cause early tracking failures. Finally, both\nqualitative and quantitative experiments are carried out on VOT-TIR 2016\ndataset, which demonstrates that our method achieves the balance of promising\ntracking performance and real-time tracking speed against other out-of-the-art\ntrackers.\n","authors":["Wei-Jie Yan","Yun-Kai Xu","Qian Chen","Xiao-Fang Kong","Guo-Hua Gu","A-Jun Shao","Min-Jie Wan"],"pdf_url":"https://arxiv.org/pdf/2308.16676v2.pdf","comment":"13 pages,9 figures,references added"},{"id":"http://arxiv.org/abs/2311.12070v2","updated":"2024-06-27T00:45:18Z","published":"2023-11-19T19:44:44Z","title":"FDDM: Unsupervised Medical Image Translation with a Frequency-Decoupled\n  Diffusion Model","summary":"  Diffusion models have demonstrated significant potential in producing\nhigh-quality images in medical image translation to aid disease diagnosis,\nlocalization, and treatment. Nevertheless, current diffusion models have\nlimited success in achieving faithful image translations that can accurately\npreserve the anatomical structures of medical images, especially for unpaired\ndatasets. The preservation of structural and anatomical details is essential to\nreliable medical diagnosis and treatment planning, as structural mismatches can\nlead to disease misidentification and treatment errors. In this study, we\nintroduce the Frequency Decoupled Diffusion Model (FDDM) for MR-to-CT\nconversion. FDDM first obtains the anatomical information of the CT image from\nthe MR image through an initial conversion module. This anatomical information\nthen guides a subsequent diffusion model to generate high-quality CT images.\nOur diffusion model uses a dual-path reverse diffusion process for\nlow-frequency and high-frequency information, achieving a better balance\nbetween image quality and anatomical accuracy. We extensively evaluated FDDM\nusing public datasets for brain MR-to-CT and pelvis MR-to-CT translations,\ndemonstrating its superior performance to other GAN-based, VAE-based, and\ndiffusion-based models. The evaluation metrics included Frechet Inception\nDistance (FID), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity\nIndex Measure (SSIM). FDDM achieved the best scores on all metrics for both\ndatasets, particularly excelling in FID, with scores of 25.9 for brain data and\n29.2 for pelvis data, significantly outperforming other methods. These results\ndemonstrate that FDDM can generate high-quality target domain images while\nmaintaining the accuracy of translated anatomical structures.\n","authors":["Yunxiang Li","Hua-Chieh Shao","Xiaoxue Qian","You Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.12070v2.pdf","comment":null}]},"2024-06-26T00:00:00Z":{"Image and Video Processing":[{"id":"http://arxiv.org/abs/2201.12348v4","updated":"2024-06-26T23:10:02Z","published":"2022-01-28T01:31:49Z","title":"End-to-End Optimization of Metasurfaces for Imaging with Compressed\n  Sensing","summary":"  We present a framework for the end-to-end optimization of metasurface imaging\nsystems that reconstruct targets using compressed sensing, a technique for\nsolving underdetermined imaging problems when the target object exhibits\nsparsity (i.e. the object can be described by a small number of non-zero\nvalues, but the positions of these values are unknown). We nest an iterative,\nunapproximated compressed sensing reconstruction algorithm into our end-to-end\noptimization pipeline, resulting in an interpretable, data-efficient method for\nmaximally leveraging metaoptics to exploit object sparsity. We apply our\nframework to super-resolution imaging and high-resolution depth imaging with a\nphase-change material. In both situations, our end-to-end framework\ncomputationally discovers optimal metasurface structures for compressed sensing\nrecovery, automatically balancing a number of complicated design considerations\nto select an imaging measurement matrix from a complex, physically constrained\nmanifold with millions ofdimensions. The optimized metasurface imaging systems\nare robust to noise, significantly improving over random scattering surfaces\nand approaching the ideal compressed sensing performance of a Gaussian matrix,\nshowing how a physical metasurface system can demonstrably approach the\nmathematical limits of compressed sensing.\n","authors":["Gaurav Arya","William F. Li","Charles Roques-Carmes","Marin Soljačić","Steven G. Johnson","Zin Lin"],"pdf_url":"https://arxiv.org/pdf/2201.12348v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17173v2","updated":"2024-06-26T20:54:45Z","published":"2024-06-24T23:23:18Z","title":"Diff3Dformer: Leveraging Slice Sequence Diffusion for Enhanced 3D CT\n  Classification with Transformer Networks","summary":"  The manifestation of symptoms associated with lung diseases can vary in\ndifferent depths for individual patients, highlighting the significance of 3D\ninformation in CT scans for medical image classification. While Vision\nTransformer has shown superior performance over convolutional neural networks\nin image classification tasks, their effectiveness is often demonstrated on\nsufficiently large 2D datasets and they easily encounter overfitting issues on\nsmall medical image datasets. To address this limitation, we propose a\nDiffusion-based 3D Vision Transformer (Diff3Dformer), which utilizes the latent\nspace of the Diffusion model to form the slice sequence for 3D analysis and\nincorporates clustering attention into ViT to aggregate repetitive information\nwithin 3D CT scans, thereby harnessing the power of the advanced transformer in\n3D classification tasks on small datasets. Our method exhibits improved\nperformance on two different scales of small datasets of 3D lung CT scans,\nsurpassing the state of the art 3D methods and other transformer-based\napproaches that emerged during the COVID-19 pandemic, demonstrating its robust\nand superior performance across different scales of data. Experimental results\nunderscore the superiority of our proposed method, indicating its potential for\nenhancing medical image classification tasks in real-world scenarios.\n","authors":["Zihao Jin","Yingying Fang","Jiahao Huang","Caiwen Xu","Simon Walsh","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2406.17173v2.pdf","comment":"conference"},{"id":"http://arxiv.org/abs/2406.18508v1","updated":"2024-06-26T17:29:15Z","published":"2024-06-26T17:29:15Z","title":"Assessment of Clonal Hematopoiesis of Indeterminate Potential from\n  Cardiac Magnetic Resonance Imaging using Deep Learning in a Cardio-oncology\n  Population","summary":"  Background: We propose a novel method to identify who may likely have clonal\nhematopoiesis of indeterminate potential (CHIP), a condition characterized by\nthe presence of somatic mutations in hematopoietic stem cells without\ndetectable hematologic malignancy, using deep learning techniques. Methods: We\ndeveloped a convolutional neural network (CNN) to predict CHIP status using 4\ndifferent views from standard delayed gadolinium-enhanced cardiac magnetic\nresonance imaging (CMR). We used 5-fold cross validation on 82 cardio-oncology\npatients to assess the performance of our model. Different algorithms were\ncompared to find the optimal patient-level prediction method using the\nimage-level CNN predictions. Results: We found that the best model had an area\nunder the receiver operating characteristic curve of 0.85 and an accuracy of\n82%. Conclusions: We conclude that a deep learning-based diagnostic approach\nfor CHIP using CMR is promising.\n","authors":["Sangeon Ryu","Shawn Ahn","Jeacy Espinoza","Alokkumar Jha","Stephanie Halene","James S. Duncan","Jennifer M Kwan","Nicha C. Dvornek"],"pdf_url":"https://arxiv.org/pdf/2406.18508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18628v1","updated":"2024-06-26T16:58:15Z","published":"2024-06-26T16:58:15Z","title":"IDA-UIE: An Iterative Framework for Deep Network-based Degradation Aware\n  Underwater Image Enhancement","summary":"  Underwater image quality is affected by fluorescence, low illumination,\nabsorption, and scattering. Recent works in underwater image enhancement have\nproposed different deep network architectures to handle these problems. Most of\nthese works have proposed a single network to handle all the challenges. We\nbelieve that deep networks trained for specific conditions deliver better\nperformance than a single network learned from all degradation cases.\nAccordingly, the first contribution of this work lies in the proposal of an\niterative framework where a single dominant degradation condition is identified\nand resolved. This proposal considers the following eight degradation\nconditions -- low illumination, low contrast, haziness, blurred image, presence\nof noise and color imbalance in three different channels. A deep network is\ndesigned to identify the dominant degradation condition. Accordingly, an\nappropriate deep network is selected for degradation condition-specific\nenhancement. The second contribution of this work is the construction of\ndegradation condition specific datasets from good quality images of two\nstandard datasets (UIEB and EUVP). This dataset is used to learn the condition\nspecific enhancement networks. The proposed approach is found to outperform\nnine baseline methods on UIEB and EUVP datasets.\n","authors":["Pranjali Singh","Prithwijit Guha"],"pdf_url":"https://arxiv.org/pdf/2406.18628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15613v3","updated":"2024-06-26T15:47:02Z","published":"2024-01-28T10:00:45Z","title":"Towards Arbitrary-Scale Histopathology Image Super-resolution: An\n  Efficient Dual-branch Framework via Implicit Self-texture Enhancement","summary":"  High-quality whole-slide scanners are expensive, complex, and time-consuming,\nthus limiting the acquisition and utilization of high-resolution pathology\nwhole-slide images in daily clinical work. Deep learning-based single-image\nsuper-resolution techniques are an effective way to solve this problem by\nsynthesizing high-resolution images from low-resolution ones. However, the\nexisting super-resolution models applied in pathology images can only work in\nfixed integer magnifications, significantly decreasing their applicability.\nThough methods based on implicit neural representation have shown promising\nresults in arbitrary-scale super-resolution of natural images, applying them\ndirectly to pathology images is inadequate because they have unique\nfine-grained image textures different from natural images. Thus, we propose an\nImplicit Self-Texture Enhancement-based dual-branch framework (ISTE) for\narbitrary-scale super-resolution of pathology images to address this challenge.\nISTE contains a pixel learning branch and a texture learning branch, which\nfirst learn pixel features and texture features, respectively. Then, we design\na two-stage texture enhancement strategy to fuse the features from the two\nbranches to obtain the super-resolution results, where the first stage is\nfeature-based texture enhancement, and the second stage is spatial-domain-based\ntexture enhancement. Extensive experiments on three public datasets show that\nISTE outperforms existing fixed-scale and arbitrary-scale algorithms at\nmultiple magnifications and helps to improve downstream task performance. To\nthe best of our knowledge, this is the first work to achieve arbitrary-scale\nsuper-resolution in pathology images. Codes will be available.\n","authors":["Minghong Duan","Linhao Qu","Zhiwei Yang","Manning Wang","Chenxi Zhang","Zhijian Song"],"pdf_url":"https://arxiv.org/pdf/2401.15613v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.05622v2","updated":"2024-06-26T15:34:47Z","published":"2022-11-10T14:54:31Z","title":"InstantGroup: Instant Template Generation for Scalable Group of Brain\n  MRI Registration","summary":"  Template generation is a critical step in groupwise image registration, which\ninvolves aligning a group of subjects into a common space. While existing\nmethods can generate high-quality template images, they often incur substantial\ntime costs or are limited by fixed group scales. In this paper, we present\nInstantGroup, an efficient groupwise template generation framework based on\nvariational autoencoder (VAE) models that leverage latent representations'\narithmetic properties, enabling scalability to groups of any size. InstantGroup\nfeatures a Dual VAEs backbone with shared-weight twin networks to handle pairs\nof inputs and incorporates a Displacement Inversion Module (DIM) to maintain\ntemplate unbiasedness and a Subject-Template Alignment Module (STAM) to improve\ntemplate quality and registration accuracy. Experiments on 3D brain MRI scans\nfrom the OASIS and ADNI datasets reveal that InstantGroup dramatically reduces\nruntime, generating templates within seconds for various group sizes while\nmaintaining superior performance compared to state-of-the-art baselines on\nquantitative metrics, including unbiasedness and registration accuracy.\n","authors":["Ziyi He","Albert C. S. Chung"],"pdf_url":"https://arxiv.org/pdf/2211.05622v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18422v1","updated":"2024-06-26T15:18:20Z","published":"2024-06-26T15:18:20Z","title":"Repeat and Concatenate: 2D to 3D Image Translation with 3D to 3D\n  Generative Modeling","summary":"  This paper investigates a 2D to 3D image translation method with a\nstraightforward technique, enabling correlated 2D X-ray to 3D CT-like\nreconstruction. We observe that existing approaches, which integrate\ninformation across multiple 2D views in the latent space, lose valuable signal\ninformation during latent encoding. Instead, we simply repeat and concatenate\nthe 2D views into higher-channel 3D volumes and approach the 3D reconstruction\nchallenge as a straightforward 3D to 3D generative modeling problem,\nsidestepping several complex modeling issues. This method enables the\nreconstructed 3D volume to retain valuable information from the 2D inputs,\nwhich are passed between channel states in a Swin UNETR backbone. Our approach\napplies neural optimal transport, which is fast and stable to train,\neffectively integrating signal information across multiple views without the\nrequirement for precise alignment; it produces non-collapsed reconstructions\nthat are highly faithful to the 2D views, even after limited training. We\ndemonstrate correlated results, both qualitatively and quantitatively, having\ntrained our model on a single dataset and evaluated its generalization ability\nacross six datasets, including out-of-distribution samples.\n","authors":["Abril Corona-Figueroa","Hubert P. H. Shum","Chris G. Willcocks"],"pdf_url":"https://arxiv.org/pdf/2406.18422v1.pdf","comment":"CVPRW 2024 - DCA in MI; Best Paper Award"},{"id":"http://arxiv.org/abs/2212.13459v2","updated":"2024-06-26T13:59:56Z","published":"2022-12-27T12:03:38Z","title":"Scaling Painting Style Transfer","summary":"  Neural style transfer (NST) is a deep learning technique that produces an\nunprecedentedly rich style transfer from a style image to a content image. It\nis particularly impressive when it comes to transferring style from a painting\nto an image. NST was originally achieved by solving an optimization problem to\nmatch the global statistics of the style image while preserving the local\ngeometric features of the content image. The two main drawbacks of this\noriginal approach is that it is computationally expensive and that the\nresolution of the output images is limited by high GPU memory requirements.\nMany solutions have been proposed to both accelerate NST and produce images\nwith larger size. However, our investigation shows that these accelerated\nmethods all compromise the quality of the produced images in the context of\npainting style transfer. Indeed, transferring the style of a painting is a\ncomplex task involving features at different scales, from the color palette and\ncompositional style to the fine brushstrokes and texture of the canvas. This\npaper provides a solution to solve the original global optimization for\nultra-high resolution (UHR) images, enabling multiscale NST at unprecedented\nimage sizes. This is achieved by spatially localizing the computation of each\nforward and backward passes through the VGG network. Extensive qualitative and\nquantitative comparisons, as well as a \\textcolor{coverletter}{perceptual\nstudy}, show that our method produces style transfer of unmatched quality for\nsuch high-resolution painting styles. By a careful comparison, we show that\nstate-of-the-art fast methods are still prone to artifacts, thus suggesting\nthat fast painting style transfer remains an open problem. Source code is\navailable at https://github.com/bgalerne/scaling_painting_style_transfer.\n","authors":["Bruno Galerne","Lara Raad","José Lezama","Jean-Michel Morel"],"pdf_url":"https://arxiv.org/pdf/2212.13459v2.pdf","comment":"14 pages, 9 figures, 4 tables, accepted at EGSR 2024"},{"id":"http://arxiv.org/abs/2406.18350v1","updated":"2024-06-26T13:51:57Z","published":"2024-06-26T13:51:57Z","title":"On Reducing Activity with Distillation and Regularization for Energy\n  Efficient Spiking Neural Networks","summary":"  Interest in spiking neural networks (SNNs) has been growing steadily,\npromising an energy-efficient alternative to formal neural networks (FNNs),\ncommonly known as artificial neural networks (ANNs). Despite increasing\ninterest, especially for Edge applications, these event-driven neural networks\nsuffered from their difficulty to be trained compared to FNNs. To alleviate\nthis problem, a number of innovative methods have been developed to provide\nperformance more or less equivalent to that of FNNs. However, the spiking\nactivity of a network during inference is usually not considered. While SNNs\nmay usually have performance comparable to that of FNNs, it is often at the\ncost of an increase of the network's activity, thus limiting the benefit of\nusing them as a more energy-efficient solution.\n  In this paper, we propose to leverage Knowledge Distillation (KD) for SNNs\ntraining with surrogate gradient descent in order to optimize the trade-off\nbetween performance and spiking activity. Then, after understanding why KD led\nto an increase in sparsity, we also explored Activations regularization and\nproposed a novel method with Logits Regularization. These approaches, validated\non several datasets, clearly show a reduction in network spiking activity\n(-26.73% on GSC and -14.32% on CIFAR-10) while preserving accuracy.\n","authors":["Thomas Louis","Benoit Miramond","Alain Pegatoquet","Adrien Girard"],"pdf_url":"https://arxiv.org/pdf/2406.18350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18327v1","updated":"2024-06-26T13:14:24Z","published":"2024-06-26T13:14:24Z","title":"Multi-modal Evidential Fusion Network for Trusted PET/CT Tumor\n  Segmentation","summary":"  Accurate segmentation of tumors in PET/CT images is important in\ncomputer-aided diagnosis and treatment of cancer. The key issue of such a\nsegmentation problem lies in the effective integration of complementary\ninformation from PET and CT images. However, the quality of PET and CT images\nvaries widely in clinical settings, which leads to uncertainty in the modality\ninformation extracted by networks. To take the uncertainty into account in\nmulti-modal information fusion, this paper proposes a novel Multi-modal\nEvidential Fusion Network (MEFN) comprising a Cross-Modal Feature Learning\n(CFL) module and a Multi-modal Trusted Fusion (MTF) module. The CFL module\nreduces the domain gap upon modality conversion and highlights common tumor\nfeatures, thereby alleviating the needs of the segmentation module to handle\nmodality specificity. The MTF module utilizes mutual attention mechanisms and\nan uncertainty calibrator to fuse modality features based on modality\nuncertainty and then fuse the segmentation results under the guidance of\nDempster-Shafer Theory. Besides, a new uncertainty perceptual loss is\nintroduced to force the model focusing on uncertain features and hence improve\nits ability to extract trusted modality information. Extensive comparative\nexperiments are conducted on two publicly available PET/CT datasets to evaluate\nthe performance of our proposed method whose results demonstrate that our MEFN\nsignificantly outperforms state-of-the-art methods with improvements of 2.15%\nand 3.23% in DSC scores on the AutoPET dataset and the Hecktor dataset,\nrespectively. More importantly, our model can provide radiologists with\ncredible uncertainty of the segmentation results for their decision in\naccepting or rejecting the automatic segmentation results, which is\nparticularly important for clinical applications. Our code will be available at\nhttps://github.com/QPaws/MEFN.\n","authors":["Yuxuan Qi","Li Lin","Jiajun Wang","Jingya Zhang","Bin Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.18327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18310v1","updated":"2024-06-26T12:50:10Z","published":"2024-06-26T12:50:10Z","title":"Spatial-temporal Hierarchical Reinforcement Learning for Interpretable\n  Pathology Image Super-Resolution","summary":"  Pathology image are essential for accurately interpreting lesion cells in\ncytopathology screening, but acquiring high-resolution digital slides requires\nspecialized equipment and long scanning times. Though super-resolution (SR)\ntechniques can alleviate this problem, existing deep learning models recover\npathology image in a black-box manner, which can lead to untruthful biological\ndetails and misdiagnosis. Additionally, current methods allocate the same\ncomputational resources to recover each pixel of pathology image, leading to\nthe sub-optimal recovery issue due to the large variation of pathology image.\nIn this paper, we propose the first hierarchical reinforcement learning\nframework named Spatial-Temporal hierARchical Reinforcement Learning (STAR-RL),\nmainly for addressing the aforementioned issues in pathology image\nsuper-resolution problem. We reformulate the SR problem as a Markov decision\nprocess of interpretable operations and adopt the hierarchical recovery\nmechanism in patch level, to avoid sub-optimal recovery. Specifically, the\nhigher-level spatial manager is proposed to pick out the most corrupted patch\nfor the lower-level patch worker. Moreover, the higher-level temporal manager\nis advanced to evaluate the selected patch and determine whether the\noptimization should be stopped earlier, thereby avoiding the over-processed\nproblem. Under the guidance of spatial-temporal managers, the lower-level patch\nworker processes the selected patch with pixel-wise interpretable actions at\neach time step. Experimental results on medical images degraded by different\nkernels show the effectiveness of STAR-RL. Furthermore, STAR-RL validates the\npromotion in tumor diagnosis with a large margin and shows generalizability\nunder various degradations. The source code is available at\nhttps://github.com/CUHK-AIM-Group/STAR-RL.\n","authors":["Wenting Chen","Jie Liu","Tommy W. S. Chow","Yixuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2406.18310v1.pdf","comment":"Accepted to IEEE TRANSACTIONS ON MEDICAL IMAGING (TMI)"},{"id":"http://arxiv.org/abs/2406.18278v1","updated":"2024-06-26T12:04:09Z","published":"2024-06-26T12:04:09Z","title":"Generalized Deepfake Attribution","summary":"  The landscape of fake media creation changed with the introduction of\nGenerative Adversarial Networks (GAN s). Fake media creation has been on the\nrise with the rapid advances in generation technology, leading to new\nchallenges in Detecting fake media. A fundamental characteristic of GAN s is\ntheir sensitivity to parameter initialization, known as seeds. Each distinct\nseed utilized during training leads to the creation of unique model instances,\nresulting in divergent image outputs despite employing the same architecture.\nThis means that even if we have one GAN architecture, it can produce countless\nvariations of GAN models depending on the seed used. Existing methods for\nattributing deepfakes work well only if they have seen the specific GAN model\nduring training. If the GAN architectures are retrained with a different seed,\nthese methods struggle to attribute the fakes. This seed dependency issue made\nit difficult to attribute deepfakes with existing methods. We proposed a\ngeneralized deepfake attribution network (GDA-N et) to attribute fake images to\ntheir respective GAN architectures, even if they are generated from a retrained\nversion of the GAN architecture with a different seed (cross-seed) or from the\nfine-tuned version of the existing GAN model. Extensive experiments on\ncross-seed and fine-tuned data of GAN models show that our method is highly\neffective compared to existing methods. We have provided the source code to\nvalidate our results.\n","authors":["Sowdagar Mahammad Shahid","Sudev Kumar Padhi","Umesh Kashyap","Sk. Subidh Ali"],"pdf_url":"https://arxiv.org/pdf/2406.18278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18247v1","updated":"2024-06-26T10:49:26Z","published":"2024-06-26T10:49:26Z","title":"Generative artificial intelligence in ophthalmology: multimodal retinal\n  images for the diagnosis of Alzheimer's disease with convolutional neural\n  networks","summary":"  Background/Aim. This study aims to predict Amyloid Positron Emission\nTomography (AmyloidPET) status with multimodal retinal imaging and\nconvolutional neural networks (CNNs) and to improve the performance through\npretraining with synthetic data. Methods. Fundus autofluorescence, optical\ncoherence tomography (OCT), and OCT angiography images from 328 eyes of 59\nAmyloidPET positive subjects and 108 AmyloidPET negative subjects were used for\nclassification. Denoising Diffusion Probabilistic Models (DDPMs) were trained\nto generate synthetic images and unimodal CNNs were pretrained on synthetic\ndata and finetuned on real data or trained solely on real data. Multimodal\nclassifiers were developed to combine predictions of the four unimodal CNNs\nwith patient metadata. Class activation maps of the unimodal classifiers\nprovided insight into the network's attention to inputs. Results. DDPMs\ngenerated diverse, realistic images without memorization. Pretraining unimodal\nCNNs with synthetic data improved AUPR at most from 0.350 to 0.579. Integration\nof metadata in multimodal CNNs improved AUPR from 0.486 to 0.634, which was the\nbest overall best classifier. Class activation maps highlighted relevant\nretinal regions which correlated with AD. Conclusion. Our method for generating\nand leveraging synthetic data has the potential to improve AmyloidPET\nprediction from multimodal retinal imaging. A DDPM can generate realistic and\nunique multimodal synthetic retinal images. Our best performing unimodal and\nmultimodal classifiers were not pretrained on synthetic data, however\npretraining with synthetic data slightly improved classification performance\nfor two out of the four modalities.\n","authors":["I. R. Slootweg","M. Thach","K. R. Curro-Tafili","F. D. Verbraak","F. H. Bouwman","Y. A. L. Pijnenburg","J. F. Boer","J. H. P. de Kwisthout","L. Bagheriye","P. J. González"],"pdf_url":"https://arxiv.org/pdf/2406.18247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18242v1","updated":"2024-06-26T10:46:44Z","published":"2024-06-26T10:46:44Z","title":"ConStyle v2: A Strong Prompter for All-in-One Image Restoration","summary":"  This paper introduces ConStyle v2, a strong plug-and-play prompter designed\nto output clean visual prompts and assist U-Net Image Restoration models in\nhandling multiple degradations. The joint training process of IRConStyle, an\nImage Restoration framework consisting of ConStyle and a general restoration\nnetwork, is divided into two stages: first, pre-training ConStyle alone, and\nthen freezing its weights to guide the training of the general restoration\nnetwork. Three improvements are proposed in the pre-training stage to train\nConStyle: unsupervised pre-training, adding a pretext task (i.e.\nclassification), and adopting knowledge distillation. Without bells and\nwhistles, we can get ConStyle v2, a strong prompter for all-in-one Image\nRestoration, in less than two GPU days and doesn't require any fine-tuning.\nExtensive experiments on Restormer (transformer-based), NAFNet (CNN-based),\nMAXIM-1S (MLP-based), and a vanilla CNN network demonstrate that ConStyle v2\ncan enhance any U-Net style Image Restoration models to all-in-one Image\nRestoration models. Furthermore, models guided by the well-trained ConStyle v2\nexhibit superior performance in some specific degradation compared to ConStyle.\n","authors":["Dongqi Fan","Junhao Zhang","Liang Chang"],"pdf_url":"https://arxiv.org/pdf/2406.18242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03425v5","updated":"2024-06-26T10:38:29Z","published":"2024-04-04T13:06:25Z","title":"ChangeMamba: Remote Sensing Change Detection with Spatio-Temporal State\n  Space Model","summary":"  Convolutional neural networks (CNN) and Transformers have made impressive\nprogress in the field of remote sensing change detection (CD). However, both\narchitectures have inherent shortcomings: CNN are constrained by a limited\nreceptive field that may hinder their ability to capture broader spatial\ncontexts, while Transformers are computationally intensive, making them costly\nto train and deploy on large datasets. Recently, the Mamba architecture, based\non state space models, has shown remarkable performance in a series of natural\nlanguage processing tasks, which can effectively compensate for the\nshortcomings of the above two architectures. In this paper, we explore for the\nfirst time the potential of the Mamba architecture for remote sensing CD tasks.\nWe tailor the corresponding frameworks, called MambaBCD, MambaSCD, and\nMambaBDA, for binary change detection (BCD), semantic change detection (SCD),\nand building damage assessment (BDA), respectively. All three frameworks adopt\nthe cutting-edge Visual Mamba architecture as the encoder, which allows full\nlearning of global spatial contextual information from the input images. For\nthe change decoder, which is available in all three architectures, we propose\nthree spatio-temporal relationship modeling mechanisms, which can be naturally\ncombined with the Mamba architecture and fully utilize its attribute to achieve\nspatio-temporal interaction of multi-temporal features, thereby obtaining\naccurate change information. On five benchmark datasets, our proposed\nframeworks outperform current CNN- and Transformer-based approaches without\nusing any complex training strategies or tricks, fully demonstrating the\npotential of the Mamba architecture in CD tasks. Further experiments show that\nour architecture is quite robust to degraded data. The source code will be\navailable in https://github.com/ChenHongruixuan/MambaCD\n","authors":["Hongruixuan Chen","Jian Song","Chengxi Han","Junshi Xia","Naoto Yokoya"],"pdf_url":"https://arxiv.org/pdf/2404.03425v5.pdf","comment":"Accepted by IEEE TGRS"},{"id":"http://arxiv.org/abs/2208.12880v4","updated":"2024-06-26T10:16:08Z","published":"2022-08-26T22:17:52Z","title":"Neuromorphic Visual Scene Understanding with Resonator Networks","summary":"  Analyzing a visual scene by inferring the configuration of a generative model\nis widely considered the most flexible and generalizable approach to scene\nunderstanding. Yet, one major problem is the computational challenge of the\ninference procedure, involving a combinatorial search across object identities\nand poses. Here we propose a neuromorphic solution exploiting three key\nconcepts: (1) a computational framework based on Vector Symbolic Architectures\n(VSA) with complex-valued vectors; (2) the design of Hierarchical Resonator\nNetworks (HRN) to factorize the non-commutative transforms translation and\nrotation in visual scenes; (3) the design of a multi-compartment spiking phasor\nneuron model for implementing complex-valued resonator networks on neuromorphic\nhardware. The VSA framework uses vector binding operations to form a generative\nimage model in which binding acts as the equivariant operation for geometric\ntransformations. A scene can, therefore, be described as a sum of vector\nproducts, which can then be efficiently factorized by a resonator network to\ninfer objects and their poses. The HRN features a partitioned architecture in\nwhich vector binding is equivariant for horizontal and vertical translation\nwithin one partition and for rotation and scaling within the other partition.\nThe spiking neuron model allows mapping the resonator network onto efficient\nand low-power neuromorphic hardware. Our approach is demonstrated on synthetic\nscenes composed of simple 2D shapes undergoing rigid geometric transformations\nand color changes. A companion paper demonstrates the same approach in\nreal-world application scenarios for machine vision and robotics.\n","authors":["Alpha Renner","Lazar Supic","Andreea Danielescu","Giacomo Indiveri","Bruno A. Olshausen","Yulia Sandamirskaya","Friedrich T. Sommer","E. Paxon Frady"],"pdf_url":"https://arxiv.org/pdf/2208.12880v4.pdf","comment":"23 pages, 8 figures, minor revisions and extended supplementary\n  material"},{"id":"http://arxiv.org/abs/2406.18212v1","updated":"2024-06-26T09:56:29Z","published":"2024-06-26T09:56:29Z","title":"Joint Stream: Malignant Region Learning for Breast Cancer Diagnosis","summary":"  Early diagnosis of breast cancer (BC) significantly contributes to reducing\nthe mortality rate worldwide. The detection of different factors and biomarkers\nsuch as Estrogen receptor (ER), Progesterone receptor (PR), Human epidermal\ngrowth factor receptor 2 (HER2) gene, Histological grade (HG), Auxiliary lymph\nnode (ALN) status, and Molecular subtype (MS) can play a significant role in\nimproved BC diagnosis. However, the existing methods predict only a single\nfactor which makes them less suitable to use in diagnosis and designing a\nstrategy for treatment. In this paper, we propose to classify the six essential\nindicating factors (ER, PR, HER2, ALN, HG, MS) for early BC diagnosis using\nH\\&E stained WSI's. To precisely capture local neighboring relationships, we\nuse spatial and frequency domain information from the large patch size of WSI's\nmalignant regions. Furthermore, to cater the variable number of regions of\ninterest sizes and give due attention to each region, we propose a malignant\nregion learning attention network. Our experimental results demonstrate that\ncombining spatial and frequency information using the malignant region learning\nmodule significantly improves multi-factor and single-factor classification\nperformance on publicly available datasets.\n","authors":["Abdul Rehman","Sarfaraz Hussein","Waqas Sultani"],"pdf_url":"https://arxiv.org/pdf/2406.18212v1.pdf","comment":"Under Review (Biomedical Signal Processing and Control)"},{"id":"http://arxiv.org/abs/2406.17536v2","updated":"2024-06-26T09:52:47Z","published":"2024-06-25T13:20:39Z","title":"MedMNIST-C: Comprehensive benchmark and improved classifier robustness\n  by simulating realistic image corruptions","summary":"  The integration of neural-network-based systems into clinical practice is\nlimited by challenges related to domain generalization and robustness. The\ncomputer vision community established benchmarks such as ImageNet-C as a\nfundamental prerequisite to measure progress towards those challenges. Similar\ndatasets are largely absent in the medical imaging community which lacks a\ncomprehensive benchmark that spans across imaging modalities and applications.\nTo address this gap, we create and open-source MedMNIST-C, a benchmark dataset\nbased on the MedMNIST+ collection covering 12 datasets and 9 imaging\nmodalities. We simulate task and modality-specific image corruptions of varying\nseverity to comprehensively evaluate the robustness of established algorithms\nagainst real-world artifacts and distribution shifts. We further provide\nquantitative evidence that our simple-to-use artificial corruptions allow for\nhighly performant, lightweight data augmentation to enhance model robustness.\nUnlike traditional, generic augmentation strategies, our approach leverages\ndomain knowledge, exhibiting significantly higher robustness when compared to\nwidely adopted methods. By introducing MedMNIST-C and open-sourcing the\ncorresponding library allowing for targeted data augmentations, we contribute\nto the development of increasingly robust methods tailored to the challenges of\nmedical imaging. The code is available at\nhttps://github.com/francescodisalvo05/medmnistc-api}{github.com/francescodisalvo05/medmnistc-api .\n","authors":["Francesco Di Salvo","Sebastian Doerrich","Christian Ledig"],"pdf_url":"https://arxiv.org/pdf/2406.17536v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18201v1","updated":"2024-06-26T09:33:51Z","published":"2024-06-26T09:33:51Z","title":"EFCNet: Every Feature Counts for Small Medical Object Segmentation","summary":"  This paper explores the segmentation of very small medical objects with\nsignificant clinical value. While Convolutional Neural Networks (CNNs),\nparticularly UNet-like models, and recent Transformers have shown substantial\nprogress in image segmentation, our empirical findings reveal their poor\nperformance in segmenting the small medical objects and lesions concerned in\nthis paper. This limitation may be attributed to information loss during their\nencoding and decoding process. In response to this challenge, we propose a\nnovel model named EFCNet for small object segmentation in medical images. Our\nmodel incorporates two modules: the Cross-Stage Axial Attention Module (CSAA)\nand the Multi-Precision Supervision Module (MPS). These modules address\ninformation loss during encoding and decoding procedures, respectively.\nSpecifically, CSAA integrates features from all stages of the encoder to\nadaptively learn suitable information needed in different decoding stages,\nthereby reducing information loss in the encoder. On the other hand, MPS\nintroduces a novel multi-precision supervision mechanism to the decoder. This\nmechanism prioritizes attention to low-resolution features in the initial\nstages of the decoder, mitigating information loss caused by subsequent\nconvolution and sampling processes and enhancing the model's global perception.\nWe evaluate our model on two benchmark medical image datasets. The results\ndemonstrate that EFCNet significantly outperforms previous segmentation methods\ndesigned for both medical and normal images.\n","authors":["Lingjie Kong","Qiaoling Wei","Chengming Xu","Han Chen","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2406.18201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.05404v3","updated":"2024-06-26T08:10:43Z","published":"2023-08-10T07:53:06Z","title":"Enhancing Low-light Light Field Images with A Deep Compensation\n  Unfolding Network","summary":"  This paper presents a novel and interpretable end-to-end learning framework,\ncalled the deep compensation unfolding network (DCUNet), for restoring light\nfield (LF) images captured under low-light conditions. DCUNet is designed with\na multi-stage architecture that mimics the optimization process of solving an\ninverse imaging problem in a data-driven fashion. The framework uses the\nintermediate enhanced result to estimate the illumination map, which is then\nemployed in the unfolding process to produce a new enhanced result.\nAdditionally, DCUNet includes a content-associated deep compensation module at\neach optimization stage to suppress noise and illumination map estimation\nerrors. To properly mine and leverage the unique characteristics of LF images,\nthis paper proposes a pseudo-explicit feature interaction module that\ncomprehensively exploits redundant information in LF images. The experimental\nresults on both simulated and real datasets demonstrate the superiority of our\nDCUNet over state-of-the-art methods, both qualitatively and quantitatively.\nMoreover, DCUNet preserves the essential geometric structure of enhanced LF\nimages much better. The code will be publicly available at\nhttps://github.com/lyuxianqiang/LFLL-DCU.\n","authors":["Xianqiang Lyu","Junhui Hou"],"pdf_url":"https://arxiv.org/pdf/2308.05404v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18102v1","updated":"2024-06-26T06:39:11Z","published":"2024-06-26T06:39:11Z","title":"A Lung Nodule Dataset with Histopathology-based Cancer Type Annotation","summary":"  Recently, Computer-Aided Diagnosis (CAD) systems have emerged as\nindispensable tools in clinical diagnostic workflows, significantly alleviating\nthe burden on radiologists. Nevertheless, despite their integration into\nclinical settings, CAD systems encounter limitations. Specifically, while CAD\nsystems can achieve high performance in the detection of lung nodules, they\nface challenges in accurately predicting multiple cancer types. This limitation\ncan be attributed to the scarcity of publicly available datasets annotated with\nexpert-level cancer type information. This research aims to bridge this gap by\nproviding publicly accessible datasets and reliable tools for medical\ndiagnosis, facilitating a finer categorization of different types of lung\ndiseases so as to offer precise treatment recommendations. To achieve this\nobjective, we curated a diverse dataset of lung Computed Tomography (CT)\nimages, comprising 330 annotated nodules (nodules are labeled as bounding\nboxes) from 95 distinct patients. The quality of the dataset was evaluated\nusing a variety of classical classification and detection models, and these\npromising results demonstrate that the dataset has a feasible application and\nfurther facilitate intelligent auxiliary diagnosis.\n","authors":["Muwei Jian","Hongyu Chen","Zaiyong Zhang","Nan Yang","Haorang Zhang","Lifu Ma","Wenjing Xu","Huixiang Zhi"],"pdf_url":"https://arxiv.org/pdf/2406.18102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18079v1","updated":"2024-06-26T05:31:36Z","published":"2024-06-26T05:31:36Z","title":"MFDNet: Multi-Frequency Deflare Network for Efficient Nighttime Flare\n  Removal","summary":"  When light is scattered or reflected accidentally in the lens, flare\nartifacts may appear in the captured photos, affecting the photos' visual\nquality. The main challenge in flare removal is to eliminate various flare\nartifacts while preserving the original content of the image. To address this\nchallenge, we propose a lightweight Multi-Frequency Deflare Network (MFDNet)\nbased on the Laplacian Pyramid. Our network decomposes the flare-corrupted\nimage into low and high-frequency bands, effectively separating the\nillumination and content information in the image. The low-frequency part\ntypically contains illumination information, while the high-frequency part\ncontains detailed content information. So our MFDNet consists of two main\nmodules: the Low-Frequency Flare Perception Module (LFFPM) to remove flare in\nthe low-frequency part and the Hierarchical Fusion Reconstruction Module (HFRM)\nto reconstruct the flare-free image. Specifically, to perceive flare from a\nglobal perspective while retaining detailed information for image restoration,\nLFFPM utilizes Transformer to extract global information while utilizing a\nconvolutional neural network to capture detailed local features. Then HFRM\ngradually fuses the outputs of LFFPM with the high-frequency component of the\nimage through feature aggregation. Moreover, our MFDNet can reduce the\ncomputational cost by processing in multiple frequency bands instead of\ndirectly removing the flare on the input image. Experimental results\ndemonstrate that our approach outperforms state-of-the-art methods in removing\nnighttime flare on real-world and synthetic images from the Flare7K dataset.\nFurthermore, the computational complexity of our model is remarkably low.\n","authors":["Yiguo Jiang","Xuhang Chen","Chi-Man Pun","Shuqiang Wang","Wei Feng"],"pdf_url":"https://arxiv.org/pdf/2406.18079v1.pdf","comment":"Accepted by The Visual Computer journal"},{"id":"http://arxiv.org/abs/2406.18063v1","updated":"2024-06-26T04:49:34Z","published":"2024-06-26T04:49:34Z","title":"Data-driven imaging geometric recovery of ultrahigh resolution robotic\n  micro-CT for in-vivo and other applications","summary":"  We introduce an ultrahigh-resolution (50\\mu m\\) robotic micro-CT design for\nlocalized imaging of carotid plaques using robotic arms, cutting-edge detector,\nand machine learning technologies. To combat geometric error-induced artifacts\nin interior CT scans, we propose a data-driven geometry estimation method that\nmaximizes the consistency between projection data and the reprojection\ncounterparts of a reconstructed volume. Particularly, we use a normalized cross\ncorrelation metric to overcome the projection truncation effect. Our approach\nis validated on a robotic CT scan of a sacrificed mouse and a micro-CT phantom\nscan, both producing sharper images with finer details than that prior\ncorrection.\n","authors":["Mengzhou Li","Guibin Zan","Wenbin Yun","Josef Uher","John Wen","Ge Wang"],"pdf_url":"https://arxiv.org/pdf/2406.18063v1.pdf","comment":"4-page paper for 8th International Conference on Computational and\n  Mathematical Biomedical Engineering"},{"id":"http://arxiv.org/abs/2406.18054v1","updated":"2024-06-26T04:12:34Z","published":"2024-06-26T04:12:34Z","title":"Leveraging Pre-trained Models for FF-to-FFPE Histopathological Image\n  Translation","summary":"  The two primary types of Hematoxylin and Eosin (H&E) slides in histopathology\nare Formalin-Fixed Paraffin-Embedded (FFPE) and Fresh Frozen (FF). FFPE slides\noffer high quality histopathological images but require a labor-intensive\nacquisition process. In contrast, FF slides can be prepared quickly, but the\nimage quality is relatively poor. Our task is to translate FF images into FFPE\nstyle, thereby improving the image quality for diagnostic purposes. In this\npaper, we propose Diffusion-FFPE, a method for FF-to-FFPE histopathological\nimage translation using a pre-trained diffusion model. Specifically, we employ\na one-step diffusion model as the generator and fine-tune it with LoRA adapters\nusing adversarial learning objectives. To ensure that the model effectively\ncaptures both global structural information and local details, we propose a\nmulti-scale feature fusion (MFF) module. This module utilizes two VAE encoders\nto extract features of varying image sizes and performs feature fusion before\nfeeding them into the UNet. Furthermore, we utilize a pre-trained\nvision-language model for histopathology as the backbone for the discriminator\nto further improve performance We conducted FF-to-FFPE translation experiments\non the TCGA-NSCLC datasets, and our method achieved better performance compared\nto other methods. The code and models are released at\nhttps://github.com/QilaiZhang/Diffusion-FFPE.\n","authors":["Qilai Zhang","Jiawen Li","Peiran Liao","Jiali Hu","Tian Guan","Anjia Han","Yonghong He"],"pdf_url":"https://arxiv.org/pdf/2406.18054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18018v1","updated":"2024-06-26T02:12:42Z","published":"2024-06-26T02:12:42Z","title":"A Cross Spatio-Temporal Pathology-based Lung Nodule Dataset","summary":"  Recently, intelligent analysis of lung nodules with the assistant of computer\naided detection (CAD) techniques can improve the accuracy rate of lung cancer\ndiagnosis. However, existing CAD systems and pulmonary datasets mainly focus on\nComputed Tomography (CT) images from one single period, while ignoring the\ncross spatio-temporal features associated with the progression of nodules\ncontained in imaging data from various captured periods of lung cancer. If the\nevolution patterns of nodules across various periods in the patients' CT\nsequences can be explored, it will play a crucial role in guiding the precise\nscreening identification of lung cancer. Therefore, a cross spatio-temporal\nlung nodule dataset with pathological information for nodule identification and\ndiagnosis is constructed, which contains 328 CT sequences and 362 annotated\nnodules from 109 patients. This comprehensive database is intended to drive\nresearch in the field of CAD towards more practical and robust methods, and\nalso contribute to the further exploration of precision medicine related field.\nTo ensure patient confidentiality, we have removed sensitive information from\nthe dataset.\n","authors":["Muwei Jian","Haoran Zhang","Mingju Shao","Hongyu Chen","Huihui Huang","Yanjie Zhong","Changlei Zhang","Bin Wang","Penghui Gao"],"pdf_url":"https://arxiv.org/pdf/2406.18018v1.pdf","comment":null}]},"2024-06-25T00:00:00Z":{"Image and Video Processing":[{"id":"http://arxiv.org/abs/2406.17970v1","updated":"2024-06-25T23:03:48Z","published":"2024-06-25T23:03:48Z","title":"Highly Constrained Coded Aperture Imaging Systems Design Via a Knowledge\n  Distillation Approach","summary":"  Computational optical imaging (COI) systems have enabled the acquisition of\nhigh-dimensional signals through optical coding elements (OCEs). OCEs encode\nthe high-dimensional signal in one or more snapshots, which are subsequently\ndecoded using computational algorithms. Currently, COI systems are optimized\nthrough an end-to-end (E2E) approach, where the OCEs are modeled as a layer of\na neural network and the remaining layers perform a specific imaging task.\nHowever, the performance of COI systems optimized through E2E is limited by the\nphysical constraints imposed by these systems. This paper proposes a knowledge\ndistillation (KD) framework for the design of highly physically constrained COI\nsystems. This approach employs the KD methodology, which consists of a\nteacher-student relationship, where a high-performance, unconstrained COI\nsystem (the teacher), guides the optimization of a physically constrained\nsystem (the student) characterized by a limited number of snapshots. We\nvalidate the proposed approach, using a binary coded apertures single pixel\ncamera for monochromatic and multispectral image reconstruction. Simulation\nresults demonstrate the superiority of the KD scheme over traditional E2E\noptimization for the designing of highly physically constrained COI systems.\n","authors":["Leon Suarez-Rodriguez","Roman Jacome","Henry Arguello"],"pdf_url":"https://arxiv.org/pdf/2406.17970v1.pdf","comment":"7 pages, 3 figures. Accepted at ICIP 2024"},{"id":"http://arxiv.org/abs/2406.17936v1","updated":"2024-06-25T20:56:41Z","published":"2024-06-25T20:56:41Z","title":"Hot-Distance: Combining One-Hot and Signed Distance Embeddings for\n  Segmentation","summary":"  Machine learning models are only as good as the data to which they are fit.\nAs such, it is always preferable to use as much data as possible in training\nmodels. What data can be used for fitting a model depends a lot on the\nformulation of the task. We introduce Hot-Distance, a novel segmentation target\nthat incorporates the strength of signed boundary distance prediction with the\nflexibility of one-hot encoding, to increase the amount of usable training data\nfor segmentation of subcellular structures in focused ion beam scanning\nelectron microscopy (FIB-SEM).\n","authors":["Marwan Zouinkhi","Jeff L. Rhoades","Aubrey V. Weigel"],"pdf_url":"https://arxiv.org/pdf/2406.17936v1.pdf","comment":"3 pages, 1 figure, in progress"},{"id":"http://arxiv.org/abs/2406.17928v1","updated":"2024-06-25T20:40:42Z","published":"2024-06-25T20:40:42Z","title":"Total Variation Regularization for Tomographic Reconstruction of\n  Cylindrically Symmetric Objects","summary":"  Flash X-ray computed tomography (CT) is an important imaging modality for\ncharacterization of high-speed dynamic events, such as Kolsky bar impact\nexperiments for the study of mechanical properties of materials subjected to\nimpulsive forces. Due to experimental constraints, the number of X-ray views\nthat can be obtained is typically very sparse in both space and time, requiring\nstrong priors in order to enable a CT reconstruction. In this paper, we propose\nan effective method for exploiting the cylindrical symmetry inherent in the\nexperiment via a variant of total variation (TV) regularization that operates\nin cylindrical coordinates, and demonstrate that it outperforms competing\napproaches.\n","authors":["Maliha Hossain","Charles A. Bouman","Brendt Wohlberg"],"pdf_url":"https://arxiv.org/pdf/2406.17928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16343v2","updated":"2024-06-25T19:35:10Z","published":"2024-05-25T20:00:27Z","title":"Learning Point Spread Function Invertibility Assessment for Image\n  Deconvolution","summary":"  Deep-learning (DL)-based image deconvolution (ID) has exhibited remarkable\nrecovery performance, surpassing traditional linear methods. However, unlike\ntraditional ID approaches that rely on analytical properties of the point\nspread function (PSF) to achieve high recovery performance - such as specific\nspectrum properties or small conditional numbers in the convolution matrix - DL\ntechniques lack quantifiable metrics for evaluating PSF suitability for\nDL-assisted recovery. Aiming to enhance deconvolution quality, we propose a\nmetric that employs a non-linear approach to learn the invertibility of an\narbitrary PSF using a neural network by mapping it to a unit impulse. A lower\ndiscrepancy between the mapped PSF and a unit impulse indicates a higher\nlikelihood of successful inversion by a DL network. Our findings reveal that\nthis metric correlates with high recovery performance in DL and traditional\nmethods, thereby serving as an effective regularizer in deconvolution tasks.\nThis approach reduces the computational complexity over conventional condition\nnumber assessments and is a differentiable process. These useful properties\nallow its application in designing diffractive optical elements through\nend-to-end (E2E) optimization, achieving invertible PSFs, and outperforming the\nE2E baseline framework.\n","authors":["Romario Gualdrón-Hurtado","Roman Jacome","Sergio Urrea","Henry Arguello","Luis Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2405.16343v2.pdf","comment":"Accepted at EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2406.17902v1","updated":"2024-06-25T19:26:39Z","published":"2024-06-25T19:26:39Z","title":"Domain Adaptation of Echocardiography Segmentation Via Reinforcement\n  Learning","summary":"  Performance of deep learning segmentation models is significantly challenged\nin its transferability across different medical imaging domains, particularly\nwhen aiming to adapt these models to a target domain with insufficient\nannotated data for effective fine-tuning. While existing domain adaptation (DA)\nmethods propose strategies to alleviate this problem, these methods do not\nexplicitly incorporate human-verified segmentation priors, compromising the\npotential of a model to produce anatomically plausible segmentations. We\nintroduce RL4Seg, an innovative reinforcement learning framework that reduces\nthe need to otherwise incorporate large expertly annotated datasets in the\ntarget domain, and eliminates the need for lengthy manual human review. Using a\ntarget dataset of 10,000 unannotated 2D echocardiographic images, RL4Seg not\nonly outperforms existing state-of-the-art DA methods in accuracy but also\nachieves 99% anatomical validity on a subset of 220 expert-validated subjects\nfrom the target domain. Furthermore, our framework's reward network offers\nuncertainty estimates comparable with dedicated state-of-the-art uncertainty\nmethods, demonstrating the utility and effectiveness of RL4Seg in overcoming\ndomain adaptation challenges in medical image segmentation.\n","authors":["Arnaud Judge","Thierry Judge","Nicolas Duchateau","Roman A. Sandler","Joseph Z. Sokol","Olivier Bernard","Pierre-Marc Jodoin"],"pdf_url":"https://arxiv.org/pdf/2406.17902v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2406.17897v1","updated":"2024-06-25T19:12:46Z","published":"2024-06-25T19:12:46Z","title":"Pixel-weighted Multi-pose Fusion for Metal Artifact Reduction in X-ray\n  Computed Tomography","summary":"  X-ray computed tomography (CT) reconstructs the internal morphology of a\nthree dimensional object from a collection of projection images, most commonly\nusing a single rotation axis. However, for objects containing dense materials\nlike metal, the use of a single rotation axis may leave some regions of the\nobject obscured by the metal, even though projections from other rotation axes\n(or poses) might contain complementary information that would better resolve\nthese obscured regions.\n  In this paper, we propose pixel-weighted Multi-pose Fusion to reduce metal\nartifacts by fusing the information from complementary measurement poses into a\nsingle reconstruction. Our method uses Multi-Agent Consensus Equilibrium\n(MACE), an extension of Plug-and-Play, as a framework for integrating\nprojection data from different poses. A primary novelty of the proposed method\nis that the output of different MACE agents are fused in a pixel-weighted\nmanner to minimize the effects of metal throughout the reconstruction. Using\nreal CT data on an object with and without metal inserts, we demonstrate that\nthe proposed pixel-weighted Multi-pose Fusion method significantly reduces\nmetal artifacts relative to single-pose reconstructions.\n","authors":["Diyu Yang","Craig A. J. Kemp","Soumendu Majee","Gregery T. Buzzard","Charles A. Bouman"],"pdf_url":"https://arxiv.org/pdf/2406.17897v1.pdf","comment":"Submitted to IEEE MMSP 2024. arXiv admin note: substantial text\n  overlap with arXiv:2209.07561"},{"id":"http://arxiv.org/abs/2402.18451v3","updated":"2024-06-25T19:04:56Z","published":"2024-02-28T16:24:08Z","title":"MambaMIR: An Arbitrary-Masked Mamba for Joint Medical Image\n  Reconstruction and Uncertainty Estimation","summary":"  The recent Mamba model has shown remarkable adaptability for visual\nrepresentation learning, including in medical imaging tasks. This study\nintroduces MambaMIR, a Mamba-based model for medical image reconstruction, as\nwell as its Generative Adversarial Network-based variant, MambaMIR-GAN. Our\nproposed MambaMIR inherits several advantages, such as linear complexity,\nglobal receptive fields, and dynamic weights, from the original Mamba model.\nThe innovated arbitrary-mask mechanism effectively adapt Mamba to our image\nreconstruction task, providing randomness for subsequent Monte Carlo-based\nuncertainty estimation. Experiments conducted on various medical image\nreconstruction tasks, including fast MRI and SVCT, which cover anatomical\nregions such as the knee, chest, and abdomen, have demonstrated that MambaMIR\nand MambaMIR-GAN achieve comparable or superior reconstruction results relative\nto state-of-the-art methods. Additionally, the estimated uncertainty maps offer\nfurther insights into the reliability of the reconstruction quality. The code\nis publicly available at https://github.com/ayanglab/MambaMIR.\n","authors":["Jiahao Huang","Liutao Yang","Fanwen Wang","Yang Nan","Angelica I. Aviles-Rivero","Carola-Bibiane Schönlieb","Daoqiang Zhang","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2402.18451v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17659v2","updated":"2024-06-25T19:01:09Z","published":"2024-05-27T21:04:43Z","title":"Enhancing Global Sensitivity and Uncertainty Quantification in Medical\n  Image Reconstruction with Monte Carlo Arbitrary-Masked Mamba","summary":"  Deep learning has been extensively applied in medical image reconstruction,\nwhere Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs)\nrepresent the predominant paradigms, each possessing distinct advantages and\ninherent limitations: CNNs exhibit linear complexity with local sensitivity,\nwhereas ViTs demonstrate quadratic complexity with global sensitivity. The\nemerging Mamba has shown superiority in learning visual representation, which\ncombines the advantages of linear scalability and global sensitivity. In this\nstudy, we introduce MambaMIR, an Arbitrary-Masked Mamba-based model with\nwavelet decomposition for joint medical image reconstruction and uncertainty\nestimation. A novel Arbitrary Scan Masking (ASM) mechanism \"masks out\"\nredundant information to introduce randomness for further uncertainty\nestimation. Compared to the commonly used Monte Carlo (MC) dropout, our\nproposed MC-ASM provides an uncertainty map without the need for hyperparameter\ntuning and mitigates the performance drop typically observed when applying\ndropout to low-level tasks. For further texture preservation and better\nperceptual quality, we employ the wavelet transformation into MambaMIR and\nexplore its variant based on the Generative Adversarial Network, namely\nMambaMIR-GAN. Comprehensive experiments have been conducted for multiple\nrepresentative medical image reconstruction tasks, demonstrating that the\nproposed MambaMIR and MambaMIR-GAN outperform other baseline and\nstate-of-the-art methods in different reconstruction tasks, where MambaMIR\nachieves the best reconstruction fidelity and MambaMIR-GAN has the best\nperceptual quality. In addition, our MC-ASM provides uncertainty maps as an\nadditional tool for clinicians, while mitigating the typical performance drop\ncaused by the commonly used dropout.\n","authors":["Jiahao Huang","Liutao Yang","Fanwen Wang","Yang Nan","Weiwen Wu","Chengyan Wang","Kuangyu Shi","Angelica I. Aviles-Rivero","Carola-Bibiane Schönlieb","Daoqiang Zhang","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2405.17659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17709v1","updated":"2024-06-25T16:48:18Z","published":"2024-06-25T16:48:18Z","title":"Mask-Guided Attention U-Net for Enhanced Neonatal Brain Extraction and\n  Image Preprocessing","summary":"  In this study, we introduce MGA-Net, a novel mask-guided attention neural\nnetwork, which extends the U-net model for precision neonatal brain imaging.\nMGA-Net is designed to extract the brain from other structures and reconstruct\nhigh-quality brain images. The network employs a common encoder and two\ndecoders: one for brain mask extraction and the other for brain region\nreconstruction. A key feature of MGA-Net is its high-level mask-guided\nattention module, which leverages features from the brain mask decoder to\nenhance image reconstruction. To enable the same encoder and decoder to process\nboth MRI and ultrasound (US) images, MGA-Net integrates sinusoidal positional\nencoding. This encoding assigns distinct positional values to MRI and US\nimages, allowing the model to effectively learn from both modalities.\nConsequently, features learned from a single modality can aid in learning a\nmodality with less available data, such as US. We extensively validated the\nproposed MGA-Net on diverse datasets from varied clinical settings and neonatal\nage groups. The metrics used for assessment included the DICE similarity\ncoefficient, recall, and accuracy for image segmentation; structural similarity\nfor image reconstruction; and root mean squared error for total brain volume\nestimation from 3D ultrasound images. Our results demonstrate that MGA-Net\nsignificantly outperforms traditional methods, offering superior performance in\nbrain extraction and segmentation while achieving high precision in image\nreconstruction and volumetric analysis. Thus, MGA-Net represents a robust and\neffective preprocessing tool for MRI and 3D ultrasound images, marking a\nsignificant advance in neuroimaging that enhances both research and clinical\ndiagnostics in the neonatal period and beyond.\n","authors":["Bahram Jafrasteh","Simon Pedro Lubian-Lopez","Emiliano Trimarco","Macarena Roman Ruiz","Carmen Rodriguez Barrios","Yolanda Marin Almagro","Isabel Benavente-Fernandez"],"pdf_url":"https://arxiv.org/pdf/2406.17709v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17670v1","updated":"2024-06-25T15:58:56Z","published":"2024-06-25T15:58:56Z","title":"Brain Tumor Classification using Vision Transformer with Selective\n  Cross-Attention Mechanism and Feature Calibration","summary":"  Brain tumor classification is a challenging task in medical image analysis.\nIn this paper, we propose a novel approach to brain tumor classification using\na vision transformer with a novel cross-attention mechanism. Our approach\nleverages the strengths of transformers in modeling long-range dependencies and\nmulti-scale feature fusion. We introduce two new mechanisms to improve the\nperformance of the cross-attention fusion module: Feature Calibration Mechanism\n(FCM) and Selective Cross-Attention (SCA). FCM calibrates the features from\ndifferent branches to make them more compatible, while SCA selectively attends\nto the most informative features. Our experiments demonstrate that the proposed\napproach outperforms other state-of-the-art methods in brain tumor\nclassification, achieving improved accuracy and efficiency. The proposed FCM\nand SCA mechanisms can be easily integrated into other vision transformer\narchitectures, making them a promising direction for future research in medical\nimage analysis. Experimental results confirm that our approach surpasses\nexisting methods, achieving state-of-the-art performance in brain tumor\nclassification tasks.\n","authors":["Mohammad Ali Labbaf Khaniki","Alireza Golkarieh","Mohammad Manthouri"],"pdf_url":"https://arxiv.org/pdf/2406.17670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17666v1","updated":"2024-06-25T15:54:49Z","published":"2024-06-25T15:54:49Z","title":"Transformer-based segmentation of adnexal lesions and ovarian implants\n  in CT images","summary":"  Two self-supervised pretrained transformer-based segmentation models (SMIT\nand Swin UNETR) fine-tuned on a dataset of ovarian cancer CT images provided\nreasonably accurate delineations of the tumors in an independent test dataset.\nTumors in the adnexa were segmented more accurately by both transformers (SMIT\nand Swin UNETR) than the omental implants. AI-assisted labeling performed on 72\nout of 245 omental implants resulted in smaller manual editing effort of 39.55\nmm compared to full manual correction of partial labels of 106.49 mm and\nresulted in overall improved accuracy performance. Both SMIT and Swin UNETR did\nnot generate any false detection of omental metastases in the urinary bladder\nand relatively few false detections in the small bowel, with 2.16 cc on average\nfor SMIT and 7.37 cc for Swin UNETR respectively.\n","authors":["Aneesh Rangnekar","Kevin M. Boehm","Emily A. Aherne","Ines Nikolovski","Natalie Gangai","Ying Liu","Dimitry Zamarin","Kara L. Roche","Sohrab P. Shah","Yulia Lakhman","Harini Veeraraghavan"],"pdf_url":"https://arxiv.org/pdf/2406.17666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17617v1","updated":"2024-06-25T15:02:01Z","published":"2024-06-25T15:02:01Z","title":"Embedded event based object detection with spiking neural network","summary":"  The complexity of event-based object detection (OD) poses considerable\nchallenges. Spiking Neural Networks (SNNs) show promising results and pave the\nway for efficient event-based OD. Despite this success, the path to efficient\nSNNs on embedded devices remains a challenge. This is due to the size of the\nnetworks required to accomplish the task and the ability of devices to take\nadvantage of SNNs benefits. Even when \"edge\" devices are considered, they\ntypically use embedded GPUs that consume tens of watts. In response to these\nchallenges, our research introduces an embedded neuromorphic testbench that\nutilizes the SPiking Low-power Event-based ArchiTecture (SPLEAT) accelerator.\nUsing an extended version of the Qualia framework, we can train, evaluate,\nquantize, and deploy spiking neural networks on an FPGA implementation of\nSPLEAT. We used this testbench to load a state-of-the-art SNN solution,\nestimate the performance loss associated with deploying the network on\ndedicated hardware, and run real-world event-based OD on neuromorphic hardware\nspecifically designed for low-power spiking neural networks. Remarkably, our\nembedded spiking solution, which includes a model with 1.08 million parameters,\noperates efficiently with 490 mJ per prediction.\n","authors":["Jonathan Courtois","Pierre-Emmanuel Novac","Edgar Lemaire","Alain Pegatoquet","Benoit Miramond"],"pdf_url":"https://arxiv.org/pdf/2406.17617v1.pdf","comment":"Result link: https://youtu.be/TsolUDaMY7Y"},{"id":"http://arxiv.org/abs/2406.17578v1","updated":"2024-06-25T14:21:55Z","published":"2024-06-25T14:21:55Z","title":"Sparse-view Signal-domain Photoacoustic Tomography Reconstruction Method\n  Based on Neural Representation","summary":"  Photoacoustic tomography is a hybrid biomedical technology, which combines\nthe advantages of acoustic and optical imaging. However, for the conventional\nimage reconstruction method, the image quality is affected obviously by\nartifacts under the condition of sparse sampling. in this paper, a novel\nmodel-based sparse reconstruction method via implicit neural representation was\nproposed for improving the image quality reconstructed from sparse data.\nSpecially, the initial acoustic pressure distribution was modeled as a\ncontinuous function of spatial coordinates, and parameterized by a multi-layer\nperceptron. The weights of multi-layer perceptron were determined by training\nthe network in self-supervised manner. And the total variation regularization\nterm was used to offer the prior knowledge. We compared our result with some\nablation studies, and the results show that out method outperforms existing\nmethods on simulation and experimental data. Under the sparse sampling\ncondition, our method can suppress the artifacts and avoid the ill-posed\nproblem effectively, which reconstruct images with higher signal-to-noise ratio\nand contrast-to-noise ratio than traditional methods. The high-quality results\nfor sparse data make the proposed method hold the potential for further\ndecreasing the hardware cost of photoacoustic tomography system.\n","authors":["Bowei Yao","Yi Zeng","Haizhao Dai","Qing Wu","Youshen Xiao","Fei Gao","Yuyao Zhang","Jingyi Yu","Xiran Cai"],"pdf_url":"https://arxiv.org/pdf/2406.17578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17577v1","updated":"2024-06-25T14:18:42Z","published":"2024-06-25T14:18:42Z","title":"Advancing Cell Detection in Anterior Segment Optical Coherence\n  Tomography Images","summary":"  Anterior uveitis, a common form of eye inflammation, can lead to permanent\nvision loss if not promptly diagnosed. Monitoring this condition involves\nquantifying inflammatory cells in the anterior chamber (AC) of the eye, which\ncan be captured using Anterior Segment Optical Coherence Tomography (AS-OCT).\nHowever, manually identifying cells in AS-OCT images is time-consuming and\nsubjective. Moreover, existing automated approaches may have limitations in\nboth the effectiveness of detecting cells and the reliability of their\ndetection results. To address these challenges, we propose an automated\nframework to detect cells in the AS-OCT images. This framework consists of a\nzero-shot chamber segmentation module and a cell detection module. The first\nmodule segments the AC area in the image without requiring human-annotated\ntraining data. Subsequently, the second module identifies individual cells\nwithin the segmented AC region. Through experiments, our framework demonstrates\nsuperior performance compared to current state-of-the-art methods for both AC\nsegmentation and cell detection tasks. Notably, we find that previous cell\ndetection approaches could suffer from low recall, potentially overlooking a\nsignificant number of cells. In contrast, our framework offers an improved\nsolution, which could benefit the diagnosis and study of anterior uveitis. Our\ncode for cell detection is publicly available at:\nhttps://github.com/joeybyc/cell_detection.\n","authors":["Boyu Chen","Ameenat L. Solebo","Paul Taylor"],"pdf_url":"https://arxiv.org/pdf/2406.17577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09335v2","updated":"2024-06-25T13:47:06Z","published":"2024-06-13T17:17:31Z","title":"Instance-level quantitative saliency in multiple sclerosis lesion\n  segmentation","summary":"  In recent years, explainable methods for artificial intelligence (XAI) have\ntried to reveal and describe models' decision mechanisms in the case of\nclassification tasks. However, XAI for semantic segmentation and in particular\nfor single instances has been little studied to date. Understanding the process\nunderlying automatic segmentation of single instances is crucial to reveal what\ninformation was used to detect and segment a given object of interest. In this\nstudy, we proposed two instance-level explanation maps for semantic\nsegmentation based on SmoothGrad and Grad-CAM++ methods. Then, we investigated\ntheir relevance for the detection and segmentation of white matter lesions\n(WML), a magnetic resonance imaging (MRI) biomarker in multiple sclerosis (MS).\n687 patients diagnosed with MS for a total of 4043 FLAIR and MPRAGE MRI scans\nwere collected at the University Hospital of Basel, Switzerland. Data were\nrandomly split into training, validation and test sets to train a 3D U-Net for\nMS lesion segmentation. We observed 3050 true positive (TP), 1818 false\npositive (FP), and 789 false negative (FN) cases. We generated instance-level\nexplanation maps for semantic segmentation, by developing two XAI methods based\non SmoothGrad and Grad-CAM++. We investigated: 1) the distribution of gradients\nin saliency maps with respect to both input MRI sequences; 2) the model's\nresponse in the case of synthetic lesions; 3) the amount of perilesional tissue\nneeded by the model to segment a lesion. Saliency maps (based on SmoothGrad) in\nFLAIR showed positive values inside a lesion and negative in its neighborhood.\nPeak values of saliency maps generated for these four groups of volumes\npresented distributions that differ significantly from one another, suggesting\na quantitative nature of the proposed saliency. Contextual information of 7mm\naround the lesion border was required for their segmentation.\n","authors":["Federico Spagnolo","Nataliia Molchanova","Roger Schaer","Meritxell Bach Cuadra","Mario Ocampo Pineda","Lester Melie-Garcia","Cristina Granziera","Vincent Andrearczyk","Adrien Depeursinge"],"pdf_url":"https://arxiv.org/pdf/2406.09335v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10343v2","updated":"2024-06-25T13:05:30Z","published":"2024-04-16T07:26:20Z","title":"The Ninth NTIRE 2024 Efficient Super-Resolution Challenge Report","summary":"  This paper provides a comprehensive review of the NTIRE 2024 challenge,\nfocusing on efficient single-image super-resolution (ESR) solutions and their\noutcomes. The task of this challenge is to super-resolve an input image with a\nmagnification factor of x4 based on pairs of low and corresponding\nhigh-resolution images. The primary objective is to develop networks that\noptimize various aspects such as runtime, parameters, and FLOPs, while still\nmaintaining a peak signal-to-noise ratio (PSNR) of approximately 26.90 dB on\nthe DIV2K_LSDIR_valid dataset and 26.99 dB on the DIV2K_LSDIR_test dataset. In\naddition, this challenge has 4 tracks including the main track (overall\nperformance), sub-track 1 (runtime), sub-track 2 (FLOPs), and sub-track 3\n(parameters). In the main track, all three metrics (ie runtime, FLOPs, and\nparameter count) were considered. The ranking of the main track is calculated\nbased on a weighted sum-up of the scores of all other sub-tracks. In sub-track\n1, the practical runtime performance of the submissions was evaluated, and the\ncorresponding score was used to determine the ranking. In sub-track 2, the\nnumber of FLOPs was considered. The score calculated based on the corresponding\nFLOPs was used to determine the ranking. In sub-track 3, the number of\nparameters was considered. The score calculated based on the corresponding\nparameters was used to determine the ranking. RLFN is set as the baseline for\nefficiency measurement. The challenge had 262 registered participants, and 34\nteams made valid submissions. They gauge the state-of-the-art in efficient\nsingle-image super-resolution. To facilitate the reproducibility of the\nchallenge and enable other researchers to build upon these findings, the code\nand the pre-trained model of validated solutions are made publicly available at\nhttps://github.com/Amazingren/NTIRE2024_ESR/.\n","authors":["Bin Ren","Yawei Li","Nancy Mehta","Radu Timofte","Hongyuan Yu","Cheng Wan","Yuxin Hong","Bingnan Han","Zhuoyuan Wu","Yajun Zou","Yuqing Liu","Jizhe Li","Keji He","Chao Fan","Heng Zhang","Xiaolin Zhang","Xuanwu Yin","Kunlong Zuo","Bohao Liao","Peizhe Xia","Long Peng","Zhibo Du","Xin Di","Wangkai Li","Yang Wang","Wei Zhai","Renjing Pei","Jiaming Guo","Songcen Xu","Yang Cao","Zhengjun Zha","Yan Wang","Yi Liu","Qing Wang","Gang Zhang","Liou Zhang","Shijie Zhao","Long Sun","Jinshan Pan","Jiangxin Dong","Jinhui Tang","Xin Liu","Min Yan","Qian Wang","Menghan Zhou","Yiqiang Yan","Yixuan Liu","Wensong Chan","Dehua Tang","Dong Zhou","Li Wang","Lu Tian","Barsoum Emad","Bohan Jia","Junbo Qiao","Yunshuai Zhou","Yun Zhang","Wei Li","Shaohui Lin","Shenglong Zhou","Binbin Chen","Jincheng Liao","Suiyi Zhao","Zhao Zhang","Bo Wang","Yan Luo","Yanyan Wei","Feng Li","Mingshen Wang","Yawei Li","Jinhan Guan","Dehua Hu","Jiawei Yu","Qisheng Xu","Tao Sun","Long Lan","Kele Xu","Xin Lin","Jingtong Yue","Lehan Yang","Shiyi Du","Lu Qi","Chao Ren","Zeyu Han","Yuhan Wang","Chaolin Chen","Haobo Li","Mingjun Zheng","Zhongbao Yang","Lianhong Song","Xingzhuo Yan","Minghan Fu","Jingyi Zhang","Baiang Li","Qi Zhu","Xiaogang Xu","Dan Guo","Chunle Guo","Jiadi Chen","Huanhuan Long","Chunjiang Duanmu","Xiaoyan Lei","Jie Liu","Weilin Jia","Weifeng Cao","Wenlong Zhang","Yanyu Mao","Ruilong Guo","Nihao Zhang","Qian Wang","Manoj Pandey","Maksym Chernozhukov","Giang Le","Shuli Cheng","Hongyuan Wang","Ziyan Wei","Qingting Tang","Liejun Wang","Yongming Li","Yanhui Guo","Hao Xu","Akram Khatami-Rizi","Ahmad Mahmoudi-Aznaveh","Chih-Chung Hsu","Chia-Ming Lee","Yi-Shiuan Chou","Amogh Joshi","Nikhil Akalwadi","Sampada Malagi","Palani Yashaswini","Chaitra Desai","Ramesh Ashok Tabib","Ujwala Patil","Uma Mudenagudi"],"pdf_url":"https://arxiv.org/pdf/2404.10343v2.pdf","comment":"The report paper of NTIRE2024 Efficient Super-resolution, accepted by\n  CVPRW2024"},{"id":"http://arxiv.org/abs/2403.00612v2","updated":"2024-06-25T12:49:54Z","published":"2024-03-01T15:35:48Z","title":"Advancing dermatological diagnosis: Development of a hyperspectral\n  dermatoscope for enhanced skin imaging","summary":"  Clinical dermatology necessitates precision and innovation for efficient\ndiagnosis and treatment of various skin conditions. This paper introduces the\ndevelopment of a cutting-edge hyperspectral dermatoscope (the Hyperscope)\ntailored for human skin analysis. We detail the requirements to such a device\nand the design considerations, from optical configurations to sensor selection,\nnecessary to capture a wide spectral range with high fidelity. Preliminary\nresults from 15 individuals and 160 recorded skin images demonstrate the\npotential of the Hyperscope in identifying and characterizing various skin\nconditions, offering a promising avenue for non-invasive skin evaluation and a\nplatform for future research in dermatology-related hyperspectral imaging.\n","authors":["Martin J. Hetz","Carina Nogueira Garcia","Sarah Haggenmüller","Titus J. Brinker"],"pdf_url":"https://arxiv.org/pdf/2403.00612v2.pdf","comment":"12 pages, 11 Figures"},{"id":"http://arxiv.org/abs/2406.17483v1","updated":"2024-06-25T12:04:51Z","published":"2024-06-25T12:04:51Z","title":"TRIP: Trainable Region-of-Interest Prediction for Hardware-Efficient\n  Neuromorphic Processing on Event-based Vision","summary":"  Neuromorphic processors are well-suited for efficiently handling sparse\nevents from event-based cameras. However, they face significant challenges in\nthe growth of computing demand and hardware costs as the input resolution\nincreases. This paper proposes the Trainable Region-of-Interest Prediction\n(TRIP), the first hardware-efficient hard attention framework for event-based\nvision processing on a neuromorphic processor. Our TRIP framework actively\nproduces low-resolution Region-of-Interest (ROIs) for efficient and accurate\nclassification. The framework exploits sparse events' inherent low information\ndensity to reduce the overhead of ROI prediction. We introduced extensive\nhardware-aware optimizations for TRIP and implemented the hardware-optimized\nalgorithm on the SENECA neuromorphic processor. We utilized multiple\nevent-based classification datasets for evaluation. Our approach achieves\nstate-of-the-art accuracies in all datasets and produces reasonable ROIs with\nvarying locations and sizes. On the DvsGesture dataset, our solution requires\n46x less computation than the state-of-the-art while achieving higher accuracy.\nFurthermore, TRIP enables more than 2x latency and energy improvements on the\nSENECA neuromorphic processor compared to the conventional solution.\n","authors":["Cina Arjmand","Yingfu Xu","Kevin Shidqi","Alexandra F. Dobrita","Kanishkan Vadivel","Paul Detterer","Manolis Sifalakis","Amirreza Yousefzadeh","Guangzhi Tang"],"pdf_url":"https://arxiv.org/pdf/2406.17483v1.pdf","comment":"Accepted in ICONS 2024"},{"id":"http://arxiv.org/abs/2406.17472v1","updated":"2024-06-25T11:30:31Z","published":"2024-06-25T11:30:31Z","title":"UHD-IQA Benchmark Database: Pushing the Boundaries of Blind Photo\n  Quality Assessment","summary":"  We introduce a novel Image Quality Assessment (IQA) dataset comprising 6073\nUHD-1 (4K) images, annotated at a fixed width of 3840 pixels. Contrary to\nexisting No-Reference (NR) IQA datasets, ours focuses on highly aesthetic\nphotos of high technical quality, filling a gap in the literature. The images,\ncarefully curated to exclude synthetic content, are sufficiently diverse to\ntrain general NR-IQA models. The dataset is annotated with perceptual quality\nratings obtained through a crowdsourcing study. Ten expert raters, comprising\nphotographers and graphics artists, assessed each image at least twice in\nmultiple sessions spanning several days, resulting in highly reliable labels.\nAnnotators were rigorously selected based on several metrics, including\nself-consistency, to ensure their reliability. The dataset includes rich\nmetadata with user and machine-generated tags from over 5,000 categories and\npopularity indicators such as favorites, likes, downloads, and views. With its\nunique characteristics, such as its focus on high-quality images, reliable\ncrowdsourced annotations, and high annotation resolution, our dataset opens up\nnew opportunities for advancing perceptual image quality assessment research\nand developing practical NR-IQA models that apply to modern photos. Our dataset\nis available at https://database.mmsp-kn.de/uhd-iqa-benchmark-database.html\n","authors":["Vlad Hosu","Lorenzo Agnolucci","Oliver Wiedemann","Daisuke Iso"],"pdf_url":"https://arxiv.org/pdf/2406.17472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17471v1","updated":"2024-06-25T11:15:56Z","published":"2024-06-25T11:15:56Z","title":"Medical Image Segmentation Using Directional Window Attention","summary":"  Accurate segmentation of medical images is crucial for diagnostic purposes,\nincluding cell segmentation, tumor identification, and organ localization.\nTraditional convolutional neural network (CNN)-based approaches struggled to\nachieve precise segmentation results due to their limited receptive fields,\nparticularly in cases involving multi-organ segmentation with varying shapes\nand sizes. The transformer-based approaches address this limitation by\nleveraging the global receptive field, but they often face challenges in\ncapturing local information required for pixel-precise segmentation. In this\nwork, we introduce DwinFormer, a hierarchical encoder-decoder architecture for\nmedical image segmentation comprising a directional window (Dwin) attention and\nglobal self-attention (GSA) for feature encoding. The focus of our design is\nthe introduction of Dwin block within DwinFormer that effectively captures\nlocal and global information along the horizontal, vertical, and depthwise\ndirections of the input feature map by separately performing attention in each\nof these directional volumes. To this end, our Dwin block introduces a nested\nDwin attention (NDA) that progressively increases the receptive field in\nhorizontal, vertical, and depthwise directions and a convolutional Dwin\nattention (CDA) that captures local contextual information for the attention\ncomputation. While the proposed Dwin block captures local and global\ndependencies at the first two high-resolution stages of DwinFormer, the GSA\nblock encodes global dependencies at the last two lower-resolution stages.\nExperiments over the challenging 3D Synapse Multi-organ dataset and Cell HMS\ndataset demonstrate the benefits of our DwinFormer over the state-of-the-art\napproaches. Our source code will be publicly available at\n\\url{https://github.com/Daniyanaj/DWINFORMER}.\n","authors":["Daniya Najiha Abdul Kareem","Mustansar Fiaz","Noa Novershtern","Hisham Cholakkal"],"pdf_url":"https://arxiv.org/pdf/2406.17471v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2406.17423v1","updated":"2024-06-25T09:56:30Z","published":"2024-06-25T09:56:30Z","title":"Deep learning-based brain segmentation model performance validation with\n  clinical radiotherapy CT","summary":"  Manual segmentation of medical images is labor intensive and especially\nchallenging for images with poor contrast or resolution. The presence of\ndisease exacerbates this further, increasing the need for an automated\nsolution. To this extent, SynthSeg is a robust deep learning model designed for\nautomatic brain segmentation across various contrasts and resolutions. This\nstudy validates the SynthSeg robust brain segmentation model on computed\ntomography (CT), using a multi-center dataset. An open access dataset of 260\npaired CT and magnetic resonance imaging (MRI) from radiotherapy patients\ntreated in 5 centers was collected. Brain segmentations from CT and MRI were\nobtained with SynthSeg model, a component of the Freesurfer imaging suite.\nThese segmentations were compared and evaluated using Dice scores and Hausdorff\n95 distance (HD95), treating MRI-based segmentations as the ground truth. Brain\nregions that failed to meet performance criteria were excluded based on\nautomated quality control (QC) scores. Dice scores indicate a median overlap of\n0.76 (IQR: 0.65-0.83). The median HD95 is 2.95 mm (IQR: 1.73-5.39). QC score\nbased thresholding improves median dice by 0.1 and median HD95 by 0.05mm.\nMorphological differences related to sex and age, as detected by MRI, were also\nreplicated with CT, with an approximate 17% difference between the CT and MRI\nresults for sex and 10% difference between the results for age. SynthSeg can be\nutilized for CT-based automatic brain segmentation, but only in applications\nwhere precision is not essential. CT performance is lower than MRI based on the\nintegrated QC scores, but low-quality segmentations can be excluded with\nQC-based thresholding. Additionally, performing CT-based neuroanatomical\nstudies is encouraged, as the results show correlations in sex- and age-based\nanalyses similar to those found with MRI.\n","authors":["Selena Huisman","Matteo Maspero","Marielle Philippens","Joost Verhoeff","Szabolcs David"],"pdf_url":"https://arxiv.org/pdf/2406.17423v1.pdf","comment":"15 pages, 9 figures, 3 supplementary data csv's, 1 supplementary file\n  with 1 figure"},{"id":"http://arxiv.org/abs/2406.16658v2","updated":"2024-06-25T09:36:21Z","published":"2024-06-24T14:08:27Z","title":"Sampling Strategies in Bayesian Inversion: A Study of RTO and Langevin\n  Methods","summary":"  This paper studies two classes of sampling methods for the solution of\ninverse problems, namely Randomize-Then-Optimize (RTO), which is rooted in\nsensitivity analysis, and Langevin methods, which are rooted in the Bayesian\nframework. The two classes of methods correspond to different assumptions and\nyield samples from different target distributions. We highlight the main\nconceptual and theoretical differences between the two approaches and compare\nthem from a practical point of view by tackling two classical inverse problems\nin imaging: deblurring and inpainting. We show that the choice of the sampling\nmethod has a significant impact on the quality of the reconstruction and that\nthe RTO method is more robust to the choice of the parameters.\n","authors":["Remi Laumont","Yiqiu Dong","Martin Skovgaard Andersen"],"pdf_url":"https://arxiv.org/pdf/2406.16658v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17338v1","updated":"2024-06-25T07:50:09Z","published":"2024-06-25T07:50:09Z","title":"Robustly Optimized Deep Feature Decoupling Network for Fatty Liver\n  Diseases Detection","summary":"  Current medical image classification efforts mainly aim for higher average\nperformance, often neglecting the balance between different classes. This can\nlead to significant differences in recognition accuracy between classes and\nobvious recognition weaknesses. Without the support of massive data, deep\nlearning faces challenges in fine-grained classification of fatty liver. In\nthis paper, we propose an innovative deep learning framework that combines\nfeature decoupling and adaptive adversarial training. Firstly, we employ two\niteratively compressed decouplers to supervised decouple common features and\nspecific features related to fatty liver in abdominal ultrasound images.\nSubsequently, the decoupled features are concatenated with the original image\nafter transforming the color space and are fed into the classifier. During\nadversarial training, we adaptively adjust the perturbation and balance the\nadversarial strength by the accuracy of each class. The model will eliminate\nrecognition weaknesses by correctly classifying adversarial samples, thus\nimproving recognition robustness. Finally, the accuracy of our method improved\nby 4.16%, achieving 82.95%. As demonstrated by extensive experiments, our\nmethod is a generalized learning framework that can be directly used to\neliminate the recognition weaknesses of any classifier while improving its\naverage performance. Code is available at https://github.com/HP-ML/MICCAI2024.\n","authors":["Peng Huang","Shu Hu","Bo Peng","Jiashu Zhang","Xi Wu","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2406.17338v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2403.15770v2","updated":"2024-06-25T07:45:53Z","published":"2024-03-23T08:57:46Z","title":"Graph Image Prior for Unsupervised Dynamic Cardiac Cine MRI\n  Reconstruction","summary":"  The inductive bias of the convolutional neural network (CNN) can be a strong\nprior for image restoration, which is known as the Deep Image Prior (DIP).\nRecently, DIP is utilized in unsupervised dynamic MRI reconstruction, which\nadopts a generative model from the latent space to the image space. However,\nexisting methods usually use a pyramid-shaped CNN generator shared by all\nframes, embedding the temporal modeling within the latent space, which may\nhamper the model expression capability. In this work, we propose a novel scheme\nfor dynamic MRI representation, named ``Graph Image Prior'' (GIP). GIP adopts a\ntwo-stage generative network in a new modeling methodology, which first employs\nindependent CNNs to recover the image structure for each frame, and then\nexploits the spatio-temporal correlations within the feature space\nparameterized by a graph model. A graph convolutional network is utilized for\nfeature fusion and dynamic image generation. In addition, we devise an ADMM\nalgorithm to alternately optimize the images and the network parameters to\nimprove the reconstruction performance. Experiments were conducted on cardiac\ncine MRI reconstruction, which demonstrate that GIP outperforms compressed\nsensing methods and other DIP-based unsupervised methods, significantly\nreducing the performance gap with state-of-the-art supervised algorithms.\nMoreover, GIP displays superior generalization ability when transferred to a\ndifferent reconstruction setting, without the need for any additional data.\n","authors":["Zhongsen Li","Wenxuan Chen","Shuai Wang","Chuyu Liu","Qing Zou","Rui Li"],"pdf_url":"https://arxiv.org/pdf/2403.15770v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16109v2","updated":"2024-06-25T06:47:07Z","published":"2024-06-23T13:53:35Z","title":"X-ray2CTPA: Generating 3D CTPA scans from 2D X-ray conditioning","summary":"  Chest X-rays or chest radiography (CXR), commonly used for medical\ndiagnostics, typically enables limited imaging compared to computed tomography\n(CT) scans, which offer more detailed and accurate three-dimensional data,\nparticularly contrast-enhanced scans like CT Pulmonary Angiography (CTPA).\nHowever, CT scans entail higher costs, greater radiation exposure, and are less\naccessible than CXRs. In this work we explore cross-modal translation from a 2D\nlow contrast-resolution X-ray input to a 3D high contrast and\nspatial-resolution CTPA scan. Driven by recent advances in generative AI, we\nintroduce a novel diffusion-based approach to this task. We evaluate the models\nperformance using both quantitative metrics and qualitative feedback from\nradiologists, ensuring diagnostic relevance of the generated images.\nFurthermore, we employ the synthesized 3D images in a classification framework\nand show improved AUC in a PE categorization task, using the initial CXR input.\nThe proposed method is generalizable and capable of performing additional\ncross-modality translations in medical imaging. It may pave the way for more\naccessible and cost-effective advanced diagnostic tools. The code for this\nproject is available: https://github.com/NoaCahan/X-ray2CTPA .\n","authors":["Noa Cahan","Eyal Klang","Galit Aviram","Yiftach Barash","Eli Konen","Raja Giryes","Hayit Greenspan"],"pdf_url":"https://arxiv.org/pdf/2406.16109v2.pdf","comment":"preprint, project code: https://github.com/NoaCahan/X-ray2CTPA"},{"id":"http://arxiv.org/abs/2406.17302v1","updated":"2024-06-25T06:16:05Z","published":"2024-06-25T06:16:05Z","title":"HD snapshot diffractive spectral imaging and inferencing","summary":"  We present a novel high-definition (HD) snapshot diffractive spectral imaging\nsystem utilizing a diffractive filter array (DFA) to capture a single image\nthat encodes both spatial and spectral information. This single diffractogram\ncan be computationally reconstructed into a spectral image cube, providing a\nhigh-resolution representation of the scene across 25 spectral channels in the\n440-800 nm range at 1304x744 spatial pixels (~1 MP). This unique approach\noffers numerous advantages including snapshot capture, a form of optical\ncompression, flexible offline reconstruction, the ability to select the\nspectral basis after capture, and high light throughput due to the absence of\nlossy filters. We demonstrate a 30-50 nm spectral resolution and compared our\nreconstructed spectra against ground truth obtained by conventional\nspectrometers. Proof-of-concept experiments in diverse applications including\nbiological tissue classification, food quality assessment, and simulated\nstellar photometry validate our system's capability to perform robust and\naccurate inference. These results establish the DFA-based imaging system as a\nversatile and powerful tool for advancing scientific and industrial imaging\napplications.\n","authors":["Apratim Majumder","Monjurul Meem","Fernando Gonzalez del Cueto","Fernando Guevara-Vasquez","Syed N. Qadri","Freddie Santiago","Rajesh Menon"],"pdf_url":"https://arxiv.org/pdf/2406.17302v1.pdf","comment":"33 pages, 16 figures"},{"id":"http://arxiv.org/abs/2406.16026v2","updated":"2024-06-25T04:28:09Z","published":"2024-06-23T06:23:12Z","title":"CEST-KAN: Kolmogorov-Arnold Networks for CEST MRI Data Analysis","summary":"  Purpose: This study aims to propose and investigate the feasibility of using\nKolmogorov-Arnold Network (KAN) for CEST MRI data analysis (CEST-KAN). Methods:\nCEST MRI data were acquired from twelve healthy volunteers at 3T. Data from ten\nsubjects were used for training, while the remaining two were reserved for\ntesting. The performance of multi-layer perceptron (MLP) and KAN models with\nthe same network settings were evaluated and compared to the conventional\nmulti-pool Lorentzian fitting (MPLF) method in generating water and multiple\nCEST contrasts, including amide, relayed nuclear Overhauser effect (rNOE), and\nmagnetization transfer (MT). Results: The water and CEST maps generated by both\nMLP and KAN were visually comparable to the MPLF results. However, the KAN\nmodel demonstrated higher accuracy in extrapolating the CEST fitting metrics,\nas evidenced by the smaller validation loss during training and smaller\nabsolute error during testing. Voxel-wise correlation analysis showed that all\nfour CEST fitting metrics generated by KAN consistently exhibited higher\nPearson coefficients than the MLP results, indicating superior performance.\nMoreover, the KAN models consistently outperformed the MLP models in varying\nhidden layer numbers despite longer training time. Conclusion: In this study,\nwe demonstrated for the first time the feasibility of utilizing KAN for CEST\nMRI data analysis, highlighting its superiority over MLP in this task. The\nfindings suggest that CEST-KAN has the potential to be a robust and reliable\npost-analysis tool for CEST MRI in clinical settings.\n","authors":["Jiawen Wang","Pei Cai","Ziyan Wang","Huabin Zhang","Jianpan Huang"],"pdf_url":"https://arxiv.org/pdf/2406.16026v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17250v1","updated":"2024-06-25T03:34:54Z","published":"2024-06-25T03:34:54Z","title":"A benchmark for 2D foetal brain ultrasound analysis","summary":"  Brain development involves a sequence of structural changes from early stages\nof the embryo until several months after birth. Currently, ultrasound is the\nestablished technique for screening due to its ability to acquire dynamic\nimages in real-time without radiation and to its cost-efficiency. However,\nidentifying abnormalities remains challenging due to the difficulty in\ninterpreting foetal brain images. In this work we present a set of 104 2D\nfoetal brain ultrasound images acquired during the 20th week of gestation that\nhave been co-registered to a common space from a rough skull segmentation. The\nimages are provided both on the original space and template space centred on\nthe ellipses of all the subjects. Furthermore, the images have been annotated\nto highlight landmark points from structures of interest to analyse brain\ndevelopment. Both the final atlas template with probabilistic maps and the\noriginal images can be used to develop new segmentation techniques, test\nregistration approaches for foetal brain ultrasound, extend our work to\nlongitudinal datasets and to detect anomalies in new images.\n","authors":["Mariano Cabezas","Yago Diez","Clara Martinez-Diago","Anna Maroto"],"pdf_url":"https://arxiv.org/pdf/2406.17250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15222v2","updated":"2024-06-25T03:17:22Z","published":"2024-06-14T02:15:09Z","title":"Rapid and Accurate Diagnosis of Acute Aortic Syndrome using Non-contrast\n  CT: A Large-scale, Retrospective, Multi-center and AI-based Study","summary":"  Chest pain symptoms are highly prevalent in emergency departments (EDs),\nwhere acute aortic syndrome (AAS) is a catastrophic cardiovascular emergency\nwith a high fatality rate, especially when timely and accurate treatment is not\nadministered. However, current triage practices in the ED can cause up to\napproximately half of patients with AAS to have an initially missed diagnosis\nor be misdiagnosed as having other acute chest pain conditions. Subsequently,\nthese AAS patients will undergo clinically inaccurate or suboptimal\ndifferential diagnosis. Fortunately, even under these suboptimal protocols,\nnearly all these patients underwent non-contrast CT covering the aorta anatomy\nat the early stage of differential diagnosis. In this study, we developed an\nartificial intelligence model (DeepAAS) using non-contrast CT, which is highly\naccurate for identifying AAS and provides interpretable results to assist in\nclinical decision-making. Performance was assessed in two major phases: a\nmulti-center retrospective study (n = 20,750) and an exploration in real-world\nemergency scenarios (n = 137,525). In the multi-center cohort, DeepAAS achieved\na mean area under the receiver operating characteristic curve of 0.958 (95% CI\n0.950-0.967). In the real-world cohort, DeepAAS detected 109 AAS patients with\nmisguided initial suspicion, achieving 92.6% (95% CI 76.2%-97.5%) in mean\nsensitivity and 99.2% (95% CI 99.1%-99.3%) in mean specificity. Our AI model\nperformed well on non-contrast CT at all applicable early stages of\ndifferential diagnosis workflows, effectively reduced the overall missed\ndiagnosis and misdiagnosis rate from 48.8% to 4.8% and shortened the diagnosis\ntime for patients with misguided initial suspicion from an average of 681.8\n(74-11,820) mins to 68.5 (23-195) mins. DeepAAS could effectively fill the gap\nin the current clinical workflow without requiring additional tests.\n","authors":["Yujian Hu","Yilang Xiang","Yan-Jie Zhou","Yangyan He","Shifeng Yang","Xiaolong Du","Chunlan Den","Youyao Xu","Gaofeng Wang","Zhengyao Ding","Jingyong Huang","Wenjun Zhao","Xuejun Wu","Donglin Li","Qianqian Zhu","Zhenjiang Li","Chenyang Qiu","Ziheng Wu","Yunjun He","Chen Tian","Yihui Qiu","Zuodong Lin","Xiaolong Zhang","Yuan He","Zhenpeng Yuan","Xiaoxiang Zhou","Rong Fan","Ruihan Chen","Wenchao Guo","Jianpeng Zhang","Tony C. W. Mok","Zi Li","Le Lu","Dehai Lang","Xiaoqiang Li","Guofu Wang","Wei Lu","Zhengxing Huang","Minfeng Xu","Hongkun Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.15222v2.pdf","comment":"under peer review"},{"id":"http://arxiv.org/abs/2406.17238v1","updated":"2024-06-25T02:59:02Z","published":"2024-06-25T02:59:02Z","title":"Expansive Synthesis: Generating Large-Scale Datasets from Minimal\n  Samples","summary":"  The challenge of limited availability of data for training in machine\nlearning arises in many applications and the impact on performance and\ngeneralization is serious. Traditional data augmentation methods aim to enhance\ntraining with a moderately sufficient data set. Generative models like\nGenerative Adversarial Networks (GANs) often face problematic convergence when\ngenerating significant and diverse data samples. Diffusion models, though\neffective, still struggle with high computational cost and long training times.\nThis paper introduces an innovative Expansive Synthesis model that generates\nlarge-scale, high-fidelity datasets from minimal samples. The proposed approach\nexploits expander graph mappings and feature interpolation to synthesize\nexpanded datasets while preserving the intrinsic data distribution and feature\nstructural relationships. The rationale of the model is rooted in the\nnon-linear property of neural networks' latent space and in its capture by a\nKoopman operator to yield a linear space of features to facilitate the\nconstruction of larger and enriched consistent datasets starting with a much\nsmaller dataset. This process is optimized by an autoencoder architecture\nenhanced with self-attention layers and further refined for distributional\nconsistency by optimal transport. We validate our Expansive Synthesis by\ntraining classifiers on the generated datasets and comparing their performance\nto classifiers trained on larger, original datasets. Experimental results\ndemonstrate that classifiers trained on synthesized data achieve performance\nmetrics on par with those trained on full-scale datasets, showcasing the\nmodel's potential to effectively augment training data. This work represents a\nsignificant advancement in data generation, offering a robust solution to data\nscarcity and paving the way for enhanced data availability in machine learning\napplications.\n","authors":["Vahid Jebraeeli","Bo Jiang","Hamid Krim","Derya Cansever"],"pdf_url":"https://arxiv.org/pdf/2406.17238v1.pdf","comment":"14 pages. arXiv admin note: text overlap with arXiv:2405.13866"},{"id":"http://arxiv.org/abs/2406.17225v1","updated":"2024-06-25T02:18:35Z","published":"2024-06-25T02:18:35Z","title":"Multimodal Cross-Task Interaction for Survival Analysis in Whole Slide\n  Pathological Images","summary":"  Survival prediction, utilizing pathological images and genomic profiles, is\nincreasingly important in cancer analysis and prognosis. Despite significant\nprogress, precise survival analysis still faces two main challenges: (1) The\nmassive pixels contained in whole slide images (WSIs) complicate the process of\npathological images, making it difficult to generate an effective\nrepresentation of the tumor microenvironment (TME). (2) Existing multimodal\nmethods often rely on alignment strategies to integrate complementary\ninformation, which may lead to information loss due to the inherent\nheterogeneity between pathology and genes. In this paper, we propose a\nMultimodal Cross-Task Interaction (MCTI) framework to explore the intrinsic\ncorrelations between subtype classification and survival analysis tasks.\nSpecifically, to capture TME-related features in WSIs, we leverage the subtype\nclassification task to mine tumor regions. Simultaneously, multi-head attention\nmechanisms are applied in genomic feature extraction, adaptively performing\ngenes grouping to obtain task-related genomic embedding. With the joint\nrepresentation of pathological images and genomic data, we further introduce a\nTransport-Guided Attention (TGA) module that uses optimal transport theory to\nmodel the correlation between subtype classification and survival analysis\ntasks, effectively transferring potential information. Extensive experiments\ndemonstrate the superiority of our approaches, with MCTI outperforming\nstate-of-the-art frameworks on three public benchmarks.\n\\href{https://github.com/jsh0792/MCTI}{https://github.com/jsh0792/MCTI}.\n","authors":["Songhan Jiang","Zhengyu Gan","Linghan Cai","Yifeng Wang","Yongbing Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.17225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15093v2","updated":"2024-06-25T01:07:15Z","published":"2024-06-21T12:14:24Z","title":"ECLIPSE: Expunging Clean-label Indiscriminate Poisons via Sparse\n  Diffusion Purification","summary":"  Clean-label indiscriminate poisoning attacks add invisible perturbations to\ncorrectly labeled training images, thus dramatically reducing the\ngeneralization capability of the victim models. Recently, some defense\nmechanisms have been proposed such as adversarial training, image\ntransformation techniques, and image purification. However, these schemes are\neither susceptible to adaptive attacks, built on unrealistic assumptions, or\nonly effective against specific poison types, limiting their universal\napplicability. In this research, we propose a more universally effective,\npractical, and robust defense scheme called ECLIPSE. We first investigate the\nimpact of Gaussian noise on the poisons and theoretically prove that any kind\nof poison will be largely assimilated when imposing sufficient random noise. In\nlight of this, we assume the victim has access to an extremely limited number\nof clean images (a more practical scene) and subsequently enlarge this sparse\nset for training a denoising probabilistic model (a universal denoising tool).\nWe then begin by introducing Gaussian noise to absorb the poisons and then\napply the model for denoising, resulting in a roughly purified dataset.\nFinally, to address the trade-off of the inconsistency in the assimilation\nsensitivity of different poisons by Gaussian noise, we propose a lightweight\ncorruption compensation module to effectively eliminate residual poisons,\nproviding a more universal defense approach. Extensive experiments demonstrate\nthat our defense approach outperforms 10 state-of-the-art defenses. We also\npropose an adaptive attack against ECLIPSE and verify the robustness of our\ndefense scheme. Our code is available at https://github.com/CGCL-codes/ECLIPSE.\n","authors":["Xianlong Wang","Shengshan Hu","Yechao Zhang","Ziqi Zhou","Leo Yu Zhang","Peng Xu","Wei Wan","Hai Jin"],"pdf_url":"https://arxiv.org/pdf/2406.15093v2.pdf","comment":"Accepted by ESORICS 2024"}]},"2024-06-24T00:00:00Z":{"Image and Video Processing":[{"id":"http://arxiv.org/abs/2307.03812v4","updated":"2024-06-24T23:08:29Z","published":"2023-07-07T19:36:24Z","title":"Coordinate-based neural representations for computational adaptive\n  optics in widefield microscopy","summary":"  Widefield microscopy is widely used for non-invasive imaging of biological\nstructures at subcellular resolution. When applied to complex specimen, its\nimage quality is degraded by sample-induced optical aberration. Adaptive optics\ncan correct wavefront distortion and restore diffraction-limited resolution but\nrequire wavefront sensing and corrective devices, increasing system complexity\nand cost. Here, we describe a self-supervised machine learning algorithm,\nCoCoA, that performs joint wavefront estimation and three-dimensional\nstructural information extraction from a single input 3D image stack without\nthe need for external training dataset. We implemented CoCoA for widefield\nimaging of mouse brain tissues and validated its performance with\ndirect-wavefront-sensing-based adaptive optics. Importantly, we systematically\nexplored and quantitatively characterized the limiting factors of CoCoA's\nperformance. Using CoCoA, we demonstrated the first in vivo widefield mouse\nbrain imaging using machine-learning-based adaptive optics. Incorporating\ncoordinate-based neural representations and a forward physics model, the\nself-supervised scheme of CoCoA should be applicable to microscopy modalities\nin general.\n","authors":["Iksung Kang","Qinrong Zhang","Stella X. Yu","Na Ji"],"pdf_url":"https://arxiv.org/pdf/2307.03812v4.pdf","comment":"60 pages, 20 figures, 2 tables. Nat Mach Intell (2024)"},{"id":"http://arxiv.org/abs/2310.09457v4","updated":"2024-06-24T20:29:38Z","published":"2023-10-14T00:32:11Z","title":"UCM-Net: A Lightweight and Efficient Solution for Skin Lesion\n  Segmentation using MLP and CNN","summary":"  Skin cancer poses a significant public health challenge, necessitating\nefficient diagnostic tools. We introduce UCM-Net, a novel skin lesion\nsegmentation model combining Multi-Layer Perceptrons (MLP) and Convolutional\nNeural Networks (CNN). This lightweight, efficient architecture, deviating from\ntraditional UNet designs, dramatically reduces computational demands, making it\nideal for mobile health applications. Evaluated on PH2, ISIC 2017, and ISIC\n2018 datasets, UCM-Net demonstrates robust performance with fewer than 50KB\nparameters and requires less than 0.05 Giga Operations Per Second (GLOPs).\nMoreover, its minimal memory requirement is just 1.19MB in CPU environment\npositions. It is a potential benchmark for efficiency in skin lesion\nsegmentation, suitable for deployment in resource-constrained settings. In\norder to facilitate accessibility and further research in the field, the\nUCM-Net source code is https://github.com/chunyuyuan/UCM-Net.\n","authors":["Chunyu Yuan","Dongfang Zhao","Sos S. Agaian"],"pdf_url":"https://arxiv.org/pdf/2310.09457v4.pdf","comment":"17 pages, accepted by Journal of Biomedical Signal Processing and\n  Control"},{"id":"http://arxiv.org/abs/2406.17080v1","updated":"2024-06-24T19:09:20Z","published":"2024-06-24T19:09:20Z","title":"Multi-Aperture Fusion of Transformer-Convolutional Network (MFTC-Net)\n  for 3D Medical Image Segmentation and Visualization","summary":"  Vision Transformers have shown superior performance to the traditional\nconvolutional-based frameworks in many vision applications, including but not\nlimited to the segmentation of 3D medical images. To further advance this area,\nthis study introduces the Multi-Aperture Fusion of Transformer-Convolutional\nNetwork (MFTC-Net), which integrates the output of Swin Transformers and their\ncorresponding convolutional blocks using 3D fusion blocks. The Multi-Aperture\nincorporates each image patch at its original resolutions with its pyramid\nrepresentation to better preserve minute details. The proposed architecture has\ndemonstrated a score of 89.73 and 7.31 for Dice and HD95, respectively, on the\nSynapse multi-organs dataset an improvement over the published results. The\nimproved performance also comes with the added benefits of the reduced\ncomplexity of approximately 40 million parameters. Our code is available at\nhttps://github.com/Siyavashshabani/MFTC-Net\n","authors":["Siyavash Shabani","Muhammad Sohaib","Sahar A. Mohammed","Bahram Parvin"],"pdf_url":"https://arxiv.org/pdf/2406.17080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05944v2","updated":"2024-06-24T18:05:06Z","published":"2024-05-09T17:33:09Z","title":"MRISegmentator-Abdomen: A Fully Automated Multi-Organ and Structure\n  Segmentation Tool for T1-weighted Abdominal MRI","summary":"  Background: Segmentation of organs and structures in abdominal MRI is useful\nfor many clinical applications, such as disease diagnosis and radiotherapy.\nCurrent approaches have focused on delineating a limited set of abdominal\nstructures (13 types). To date, there is no publicly available abdominal MRI\ndataset with voxel-level annotations of multiple organs and structures.\nConsequently, a segmentation tool for multi-structure segmentation is also\nunavailable. Methods: We curated a T1-weighted abdominal MRI dataset consisting\nof 195 patients who underwent imaging at National Institutes of Health (NIH)\nClinical Center. The dataset comprises of axial pre-contrast T1, arterial,\nvenous, and delayed phases for each patient, thereby amounting to a total of\n780 series (69,248 2D slices). Each series contains voxel-level annotations of\n62 abdominal organs and structures. A 3D nnUNet model, dubbed as\nMRISegmentator-Abdomen (MRISegmentator in short), was trained on this dataset,\nand evaluation was conducted on an internal test set and two large external\ndatasets: AMOS22 and Duke Liver. The predicted segmentations were compared\nagainst the ground-truth using the Dice Similarity Coefficient (DSC) and\nNormalized Surface Distance (NSD). Findings: MRISegmentator achieved an average\nDSC of 0.861$\\pm$0.170 and a NSD of 0.924$\\pm$0.163 in the internal test set.\nOn the AMOS22 dataset, MRISegmentator attained an average DSC of\n0.829$\\pm$0.133 and a NSD of 0.908$\\pm$0.067. For the Duke Liver dataset, an\naverage DSC of 0.933$\\pm$0.015 and a NSD of 0.929$\\pm$0.021 was obtained.\nInterpretation: The proposed MRISegmentator provides automatic, accurate, and\nrobust segmentations of 62 organs and structures in T1-weighted abdominal MRI\nsequences. The tool has the potential to accelerate research on various\nclinical topics, such as abnormality detection, radiotherapy, disease\nclassification among others.\n","authors":["Yan Zhuang","Tejas Sudharshan Mathai","Pritam Mukherjee","Brandon Khoury","Boah Kim","Benjamin Hou","Nusrat Rabbee","Abhinav Suri","Ronald M. Summers"],"pdf_url":"https://arxiv.org/pdf/2405.05944v2.pdf","comment":"We made the segmentation model publicly available"},{"id":"http://arxiv.org/abs/2406.16848v1","updated":"2024-06-24T17:55:02Z","published":"2024-06-24T17:55:02Z","title":"Unsupervised Domain Adaptation for Pediatric Brain Tumor Segmentation","summary":"  Significant advances have been made toward building accurate automatic\nsegmentation models for adult gliomas. However, the performance of these models\noften degrades when applied to pediatric glioma due to their imaging and\nclinical differences (domain shift). Obtaining sufficient annotated data for\npediatric glioma is typically difficult because of its rare nature. Also,\nmanual annotations are scarce and expensive. In this work, we propose\nDomain-Adapted nnU-Net (DA-nnUNet) to perform unsupervised domain adaptation\nfrom adult glioma (source domain) to pediatric glioma (target domain).\nSpecifically, we add a domain classifier connected with a gradient reversal\nlayer (GRL) to a backbone nnU-Net. Once the classifier reaches a very high\naccuracy, the GRL is activated with the goal of transferring domain-invariant\nfeatures from the classifier to the segmentation model while preserving\nsegmentation accuracy on the source domain. The accuracy of the classifier\nslowly degrades to chance levels. No annotations are used in the target domain.\nThe method is compared to 8 different supervised models using BraTS-Adult\nglioma (N=1251) and BraTS-PED glioma data (N=99). The proposed method shows\nnotable performance enhancements in the tumor core (TC) region compared to the\nmodel that only uses adult data: ~32% better Dice scores and ~20 better 95th\npercentile Hausdorff distances. Moreover, our unsupervised approach shows no\nstatistically significant difference compared to the practical upper bound\nmodel using manual annotations from both datasets in TC region. The code is\nshared at https://github.com/Fjr9516/DA_nnUNet.\n","authors":["Jingru Fu","Simone Bendazzoli","Örjan Smedby","Rodrigo Moreno"],"pdf_url":"https://arxiv.org/pdf/2406.16848v1.pdf","comment":"10 pages, 4 figures, conference"},{"id":"http://arxiv.org/abs/2406.16754v1","updated":"2024-06-24T16:00:20Z","published":"2024-06-24T16:00:20Z","title":"The MRI Scanner as a Diagnostic: Image-less Active Sampling","summary":"  Despite the high diagnostic accuracy of Magnetic Resonance Imaging (MRI),\nusing MRI as a Point-of-Care (POC) disease identification tool poses\nsignificant accessibility challenges due to the use of high magnetic field\nstrength and lengthy acquisition times. We ask a simple question: Can we\ndynamically optimise acquired samples, at the patient level, according to an\n(automated) downstream decision task, while discounting image reconstruction?\nWe propose an ML-based framework that learns an active sampling strategy, via\nreinforcement learning, at a patient-level to directly infer disease from\nundersampled k-space. We validate our approach by inferring Meniscus Tear in\nundersampled knee MRI data, where we achieve diagnostic performance comparable\nwith ML-based diagnosis, using fully sampled k-space data. We analyse\ntask-specific sampling policies, showcasing the adaptability of our active\nsampling approach. The introduced frugal sampling strategies have the potential\nto reduce high field strength requirements that in turn strengthen the\nviability of MRI-based POC disease identification and associated preliminary\nscreening tools.\n","authors":["Yuning Du","Rohan Dharmakumar","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2406.16754v1.pdf","comment":"Accepted in MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.16724v1","updated":"2024-06-24T15:29:08Z","published":"2024-06-24T15:29:08Z","title":"μ-Net: A Deep Learning-Based Architecture for μ-CT Segmentation","summary":"  X-ray computed microtomography ({\\mu}-CT) is a non-destructive technique that\ncan generate high-resolution 3D images of the internal anatomy of medical and\nbiological samples. These images enable clinicians to examine internal anatomy\nand gain insights into the disease or anatomical morphology. However,\nextracting relevant information from 3D images requires semantic segmentation\nof the regions of interest, which is usually done manually and results\ntime-consuming and tedious. In this work, we propose a novel framework that\nuses a convolutional neural network (CNN) to automatically segment the full\nmorphology of the heart of Carassius auratus. The framework employs an\noptimized 2D CNN architecture that can infer a 3D segmentation of the sample,\navoiding the high computational cost of a 3D CNN architecture. We tackle the\nchallenges of handling large and high-resoluted image data (over a thousand\npixels in each dimension) and a small training database (only three samples) by\nproposing a standard protocol for data normalization and processing. Moreover,\nwe investigate how the noise, contrast, and spatial resolution of the sample\nand the training of the architecture are affected by the reconstruction\ntechnique, which depends on the number of input images. Experiments show that\nour framework significantly reduces the time required to segment new samples,\nallowing a faster microtomography analysis of the Carassius auratus heart\nshape. Furthermore, our framework can work with any bio-image (biological and\nmedical) from {\\mu}-CT with high-resolution and small dataset size\n","authors":["Pierangela Bruno","Edoardo De Rose","Carlo Adornetto","Francesco Calimeri","Sandro Donato","Raffaele Giuseppe Agostino","Daniela Amelio","Riccardo Barberi","Maria Carmela Cerra","Maria Caterina Crocco","Mariacristina Filice","Raffaele Filosa","Gianluigi Greco","Sandra Imbrogno","Vincenzo Formoso"],"pdf_url":"https://arxiv.org/pdf/2406.16724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18435v2","updated":"2024-06-24T15:07:34Z","published":"2024-03-19T17:57:24Z","title":"QUBIQ: Uncertainty Quantification for Biomedical Image Segmentation\n  Challenge","summary":"  Uncertainty in medical image segmentation tasks, especially inter-rater\nvariability, arising from differences in interpretations and annotations by\nvarious experts, presents a significant challenge in achieving consistent and\nreliable image segmentation. This variability not only reflects the inherent\ncomplexity and subjective nature of medical image interpretation but also\ndirectly impacts the development and evaluation of automated segmentation\nalgorithms. Accurately modeling and quantifying this variability is essential\nfor enhancing the robustness and clinical applicability of these algorithms. We\nreport the set-up and summarize the benchmark results of the Quantification of\nUncertainties in Biomedical Image Quantification Challenge (QUBIQ), which was\norganized in conjunction with International Conferences on Medical Image\nComputing and Computer-Assisted Intervention (MICCAI) 2020 and 2021. The\nchallenge focuses on the uncertainty quantification of medical image\nsegmentation which considers the omnipresence of inter-rater variability in\nimaging datasets. The large collection of images with multi-rater annotations\nfeatures various modalities such as MRI and CT; various organs such as the\nbrain, prostate, kidney, and pancreas; and different image dimensions 2D-vs-3D.\nA total of 24 teams submitted different solutions to the problem, combining\nvarious baseline models, Bayesian neural networks, and ensemble model\ntechniques. The obtained results indicate the importance of the ensemble\nmodels, as well as the need for further research to develop efficient 3D\nmethods for uncertainty quantification methods in 3D segmentation tasks.\n","authors":["Hongwei Bran Li","Fernando Navarro","Ivan Ezhov","Amirhossein Bayat","Dhritiman Das","Florian Kofler","Suprosanna Shit","Diana Waldmannstetter","Johannes C. Paetzold","Xiaobin Hu","Benedikt Wiestler","Lucas Zimmer","Tamaz Amiranashvili","Chinmay Prabhakar","Christoph Berger","Jonas Weidner","Michelle Alonso-Basant","Arif Rashid","Ujjwal Baid","Wesam Adel","Deniz Ali","Bhakti Baheti","Yingbin Bai","Ishaan Bhatt","Sabri Can Cetindag","Wenting Chen","Li Cheng","Prasad Dutand","Lara Dular","Mustafa A. Elattar","Ming Feng","Shengbo Gao","Henkjan Huisman","Weifeng Hu","Shubham Innani","Wei Jiat","Davood Karimi","Hugo J. Kuijf","Jin Tae Kwak","Hoang Long Le","Xiang Lia","Huiyan Lin","Tongliang Liu","Jun Ma","Kai Ma","Ting Ma","Ilkay Oksuz","Robbie Holland","Arlindo L. Oliveira","Jimut Bahan Pal","Xuan Pei","Maoying Qiao","Anindo Saha","Raghavendra Selvan","Linlin Shen","Joao Lourenco Silva","Ziga Spiclin","Sanjay Talbar","Dadong Wang","Wei Wang","Xiong Wang","Yin Wang","Ruiling Xia","Kele Xu","Yanwu Yan","Mert Yergin","Shuang Yu","Lingxi Zeng","YingLin Zhang","Jiachen Zhao","Yefeng Zheng","Martin Zukovec","Richard Do","Anton Becker","Amber Simpson","Ender Konukoglu","Andras Jakab","Spyridon Bakas","Leo Joskowicz","Bjoern Menze"],"pdf_url":"https://arxiv.org/pdf/2405.18435v2.pdf","comment":"initial technical report"},{"id":"http://arxiv.org/abs/2406.16701v1","updated":"2024-06-24T15:04:14Z","published":"2024-06-24T15:04:14Z","title":"Demystifying the Effect of Receptive Field Size in U-Net Models for\n  Medical Image Segmentation","summary":"  Medical image segmentation is a critical task in healthcare applications, and\nU-Nets have demonstrated promising results. This work delves into the\nunderstudied aspect of receptive field (RF) size and its impact on the U-Net\nand Attention U-Net architectures. This work explores several critical elements\nincluding the relationship between RF size, characteristics of the region of\ninterest, and model performance, as well as the balance between RF size and\ncomputational costs for U-Net and Attention U-Net methods for different\ndatasets. This work also proposes a mathematical notation for representing the\ntheoretical receptive field (TRF) of a given layer in a network and proposes\ntwo new metrics - effective receptive field (ERF) rate and the Object rate to\nquantify the fraction of significantly contributing pixels within the ERF\nagainst the TRF area and assessing the relative size of the segmentation object\ncompared to the TRF size respectively. The results demonstrate that there\nexists an optimal TRF size that successfully strikes a balance between\ncapturing a wider global context and maintaining computational efficiency,\nthereby optimizing model performance. Interestingly, a distinct correlation is\nobserved between the data complexity and the required TRF size; segmentation\nbased solely on contrast achieved peak performance even with smaller TRF sizes,\nwhereas more complex segmentation tasks necessitated larger TRFs. Attention\nU-Net models consistently outperformed their U-Net counterparts, highlighting\nthe value of attention mechanisms regardless of TRF size. These novel insights\npresent an invaluable resource for developing more efficient U-Net-based\narchitectures for medical imaging and pave the way for future exploration. A\ntool is also developed that calculates the TRF for a U-Net (and Attention\nU-Net) model, and also suggest an appropriate TRF size for a given model and\ndataset.\n","authors":["Vincent Loos","Rohit Pardasani","Navchetan Awasthi"],"pdf_url":"https://arxiv.org/pdf/2406.16701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08298v2","updated":"2024-06-24T09:48:01Z","published":"2024-03-13T07:10:24Z","title":"Physics-Informed Deep Learning for Motion-Corrected Reconstruction of\n  Quantitative Brain MRI","summary":"  We propose PHIMO, a physics-informed learning-based motion correction method\ntailored to quantitative MRI. PHIMO leverages information from the signal\nevolution to exclude motion-corrupted k-space lines from a data-consistent\nreconstruction. We demonstrate the potential of PHIMO for the application of\nT2* quantification from gradient echo MRI, which is particularly sensitive to\nmotion due to its sensitivity to magnetic field inhomogeneities. A\nstate-of-the-art technique for motion correction requires redundant acquisition\nof the k-space center, prolonging the acquisition. We show that PHIMO can\ndetect and exclude intra-scan motion events and, thus, correct for severe\nmotion artifacts. PHIMO approaches the performance of the state-of-the-art\nmotion correction method, while substantially reducing the acquisition time by\nover 40%, facilitating clinical applicability. Our code is available at\nhttps://github.com/HannahEichhorn/PHIMO.\n","authors":["Hannah Eichhorn","Veronika Spieker","Kerstin Hammernik","Elisa Saks","Kilian Weiss","Christine Preibisch","Julia A. Schnabel"],"pdf_url":"https://arxiv.org/pdf/2403.08298v2.pdf","comment":"Accepted at MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.14570v2","updated":"2024-06-24T09:25:33Z","published":"2024-06-06T09:46:14Z","title":"Deep-Learning Approach for Tissue Classification using Acoustic Waves\n  during Ablation with an Er:YAG Laser (Updated)","summary":"  Today's mechanical tools for bone cutting (osteotomy) cause mechanical trauma\nthat prolongs the healing process. Medical device manufacturers aim to minimize\nthis trauma, with minimally invasive surgery using laser cutting as one\ninnovation. This method ablates tissue using laser light instead of mechanical\ntools, reducing post-surgery healing time. A reliable feedback system is\ncrucial during laser surgery to prevent damage to surrounding tissues. We\npropose a tissue classification method analyzing acoustic waves generated\nduring laser ablation, demonstrating its applicability in an ex-vivo\nexperiment. The ablation process with a microsecond pulsed Er:YAG laser\nproduces acoustic waves, acquired with an air-coupled transducer. These waves\nwere used to classify five porcine tissue types: hard bone, soft bone, muscle,\nfat, and skin. For automated tissue classification, we compared five Neural\nNetwork (NN) approaches: a one-dimensional Convolutional Neural Network (CNN)\nwith time-dependent input, a Fully-connected Neural Network (FcNN) with either\nthe frequency spectrum or principal components of the frequency spectrum as\ninput, and a combination of a CNN and an FcNN with time-dependent data and its\nfrequency spectrum as input. Consecutive acoustic waves were used to improve\nclassification accuracy. Grad-Cam identified the activation map of the\nfrequencies, showing low frequencies as the most important for this task. Our\nresults indicated that combining time-dependent data with its frequency\nspectrum achieved the highest classification accuracy (65.5%-75.5%). We also\nfound that using the frequency spectrum alone was sufficient, with no\nadditional benefit from applying Principal Components Analysis (PCA).\n","authors":["Carlo Seppi","Philippe C. Cattin"],"pdf_url":"https://arxiv.org/pdf/2406.14570v2.pdf","comment":"This paper is an updated version of Deep-Learning Approach for Tissue\n  Classification using Acoustic Waves during Ablation with an Er:YAG Laser\n  originally published in DOI:10.1109/ACCESS.2021.3113055. This update\n  addresses several issues and incorporates corrections as outlined in\n  DOI:10.1109/ACCESS.2024.3395071. We provide here a detailed description of\n  our experiments and the new models we used"},{"id":"http://arxiv.org/abs/2406.16466v1","updated":"2024-06-24T09:16:17Z","published":"2024-06-24T09:16:17Z","title":"SLOctolyzer: Fully automatic analysis toolkit for segmentation and\n  feature extracting in scanning laser ophthalmoscopy images","summary":"  Purpose: To describe SLOctolyzer: an open-source analysis toolkit for en face\nretinal vessels appearing in infrared reflectance scanning laser ophthalmoscopy\n(SLO) images.\n  Methods: SLOctolyzer includes two main modules: segmentation and measurement.\nThe segmentation module use deep learning methods to delineate retinal anatomy,\nwhile the measurement module quantifies key retinal vascular features such as\nvessel complexity, density, tortuosity, and calibre. We evaluate the\nsegmentation module using unseen data and measure its reproducibility.\n  Results: SLOctolyzer's segmentation module performed well against unseen\ninternal test data (Dice for all-vessels, 0.9097; arteries, 0.8376; veins,\n0.8525; optic disc, 0.9430; fovea, 0.8837). External validation against severe\nretinal pathology showed decreased performance (Dice for arteries, 0.7180;\nveins, 0.7470; optic disc, 0.9032). SLOctolyzer had good reproducibility (mean\ndifference for fractal dimension, -0.0007; vessel density, -0.0003; vessel\ncalibre, -0.3154 $\\mu$m; tortuosity density, 0.0013). SLOctolyzer can process a\nmacula-centred SLO image in under 20 seconds and a disc-centred SLO image in\nunder 30 seconds using a standard laptop CPU.\n  Conclusions: To our knowledge, SLOctolyzer is the first open-source tool to\nconvert raw SLO images into reproducible and clinically meaningful retinal\nvascular parameters. SLO images are captured simultaneous to optical coherence\ntomography (OCT), and we believe our software will be useful for extracting\nretinal vascular measurements from large OCT image sets and linking them to\nocular or systemic diseases. It requires no specialist knowledge or proprietary\nsoftware, and allows manual correction of segmentations and re-computing of\nvascular metrics. SLOctolyzer is freely available at\nhttps://github.com/jaburke166/SLOctolyzer.\n","authors":["Jamie Burke","Samuel Gibbon","Justin Engelmann","Adam Threlfall","Ylenia Giarratano","Charlene Hamid","Stuart King","Ian J. C. MacCormick","Tom MacGillivray"],"pdf_url":"https://arxiv.org/pdf/2406.16466v1.pdf","comment":"10 pages, 5 figures, 6 tables + Supplementary (7 pages, 10 figures, 4\n  tables). Submitted for peer review at Translational Vision Science and\n  Technology"},{"id":"http://arxiv.org/abs/2406.16993v1","updated":"2024-06-24T08:01:05Z","published":"2024-06-24T08:01:05Z","title":"Are Vision xLSTM Embedded UNet More Reliable in Medical 3D Image\n  Segmentation?","summary":"  The advancement of developing efficient medical image segmentation has\nevolved from initial dependence on Convolutional Neural Networks (CNNs) to the\npresent investigation of hybrid models that combine CNNs with Vision\nTransformers. Furthermore, there is an increasing focus on creating\narchitectures that are both high-performing in medical image segmentation tasks\nand computationally efficient to be deployed on systems with limited resources.\nAlthough transformers have several advantages like capturing global\ndependencies in the input data, they face challenges such as high computational\nand memory complexity. This paper investigates the integration of CNNs and\nVision Extended Long Short-Term Memory (Vision-xLSTM) models by introducing a\nnovel approach called UVixLSTM. The Vision-xLSTM blocks captures temporal and\nglobal relationships within the patches extracted from the CNN feature maps.\nThe convolutional feature reconstruction path upsamples the output volume from\nthe Vision-xLSTM blocks to produce the segmentation output. Our primary\nobjective is to propose that Vision-xLSTM forms a reliable backbone for medical\nimage segmentation tasks, offering excellent segmentation performance and\nreduced computational complexity. UVixLSTM exhibits superior performance\ncompared to state-of-the-art networks on the publicly-available Synapse\ndataset. Code is available at: https://github.com/duttapallabi2907/UVixLSTM\n","authors":["Pallabi Dutta","Soham Bose","Swalpa Kumar Roy","Sushmita Mitra"],"pdf_url":"https://arxiv.org/pdf/2406.16993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.02088v3","updated":"2024-06-24T07:51:01Z","published":"2023-08-04T00:07:26Z","title":"Motion-robust free-running volumetric cardiovascular MRI","summary":"  PURPOSE: To present and assess an outlier mitigation method that makes\nfree-running volumetric cardiovascular MRI (CMR) more robust to motion.\n  METHODS: The proposed method, called compressive recovery with outlier\nrejection (CORe), models outliers in the measured data as an additive auxiliary\nvariable. We enforce MR physics-guided group sparsity on the auxiliary\nvariable, and jointly estimate it along with the image using an iterative\nalgorithm. For evaluation, CORe is first compared to traditional compressed\nsensing (CS), robust regression (RR), and an existing outlier rejection method\nusing two simulation studies. Then, CORe is compared to CS using seven\nthree-dimensional (3D) cine, 12 rest four-dimensional (4D) flow, and eight\nstress 4D flow imaging datasets.\n  RESULTS: Our simulation studies show that CORe outperforms CS, RR, and the\nexisting outlier rejection method in terms of normalized mean square error and\nstructural similarity index across 55 different realizations. The expert reader\nevaluation of 3D cine images demonstrates that CORe is more effective in\nsuppressing artifacts while maintaining or improving image sharpness. Finally,\n4D flow images show that CORe yields more reliable and consistent flow\nmeasurements, especially in the presence of involuntary subject motion or\nexercise stress.\n  CONCLUSION: An outlier rejection method is presented and tested using\nsimulated and measured data. This method can help suppress motion artifacts in\na wide range of free-running CMR applications.\n  CODE & DATA: Implementation code and datasets are available on GitHub at\nhttp://github.com/OSU-MR/motion-robust-CMR\n","authors":["Syed M. Arshad","Lee C. Potter","Chong Chen","Yingmin Liu","Preethi Chandrasekaran","Christopher Crabtree","Matthew S. Tong","Orlando P. Simonetti","Yuchi Han","Rizwan Ahmad"],"pdf_url":"https://arxiv.org/pdf/2308.02088v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16359v1","updated":"2024-06-24T06:57:51Z","published":"2024-06-24T06:57:51Z","title":"Improving Generative Adversarial Networks for Video Super-Resolution","summary":"  In this research, we explore different ways to improve generative adversarial\nnetworks for video super-resolution tasks from a base single image\nsuper-resolution GAN model. Our primary objective is to identify potential\ntechniques that enhance these models and to analyze which of these techniques\nyield the most significant improvements. We evaluate our results using Peak\nSignal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). Our\nfindings indicate that the most effective techniques include temporal\nsmoothing, long short-term memory (LSTM) layers, and a temporal loss function.\nThe integration of these methods results in an 11.97% improvement in PSNR and\nan 8% improvement in SSIM compared to the baseline video super-resolution\ngenerative adversarial network (GAN) model. This substantial improvement\nsuggests potential further applications to enhance current state-of-the-art\nmodels.\n","authors":["Daniel Wen"],"pdf_url":"https://arxiv.org/pdf/2406.16359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16358v1","updated":"2024-06-24T06:54:59Z","published":"2024-06-24T06:54:59Z","title":"Approximate DCT and Quantization Techniques for Energy-Constrained Image\n  Sensors","summary":"  Recent expansions in multimedia devices gather enormous amounts of real-time\nimages for processing and inference. The images are first compressed using\ncompression schemes, like JPEG, to reduce storage costs and power for\ntransmitting the captured data. Due to inherent error resilience and\nimperceptibility in images, JPEG can be approximated to reduce the required\ncomputation power and area. This work demonstrates the first end-to-end\napproximation computing-based optimization of JPEG hardware using i) an\napproximate division realized using bit-shift operators to reduce the\ncomplexity of the quantization block, ii) loop perforation, and iii) precision\nscaling on top of a multiplier-less fast DCT architecture to achieve an\nextremely energy-efficient JPEG compression unit which will be a perfect fit\nfor power/bandwidth-limited scenario. Furthermore, a gradient descent-based\nheuristic composed of two conventional approximation strategies, i.e.,\nPrecision Scaling and Loop Perforation, is implemented for tuning the degree of\napproximation to trade off energy consumption with the quality degradation of\nthe decoded image. The entire RTL design is coded in Verilog HDL, synthesized,\nmapped to TSMC 65nm CMOS technology, and simulated using Cadence Spectre\nSimulator under 25$^{\\circ}$\\textbf{C}, TT corner. The approximate division\napproach achieved around $\\textbf{28\\%}$ reduction in the active design area.\nThe heuristic-based approximation technique combined with accelerator\noptimization achieves a significant energy reduction of $\\textbf{36\\%}$ for a\nminimal image quality degradation of $\\textbf{2\\%}$ SAD. Simulation results\nalso show that the proposed architecture consumes 15uW at the DCT and\nquantization stages to compress a colored 480p image at 6fps.\n","authors":["Ming-Che Li","Archisman Ghosh","Shreyas Sen"],"pdf_url":"https://arxiv.org/pdf/2406.16358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16322v1","updated":"2024-06-24T05:15:15Z","published":"2024-06-24T05:15:15Z","title":"Lesion-Aware Cross-Phase Attention Network for Renal Tumor Subtype\n  Classification on Multi-Phase CT Scans","summary":"  Multi-phase computed tomography (CT) has been widely used for the\npreoperative diagnosis of kidney cancer due to its non-invasive nature and\nability to characterize renal lesions. However, since enhancement patterns of\nrenal lesions across CT phases are different even for the same lesion type, the\nvisual assessment by radiologists suffers from inter-observer variability in\nclinical practice. Although deep learning-based approaches have been recently\nexplored for differential diagnosis of kidney cancer, they do not explicitly\nmodel the relationships between CT phases in the network design, limiting the\ndiagnostic performance. In this paper, we propose a novel lesion-aware\ncross-phase attention network (LACPANet) that can effectively capture temporal\ndependencies of renal lesions across CT phases to accurately classify the\nlesions into five major pathological subtypes from time-series multi-phase CT\nimages. We introduce a 3D inter-phase lesion-aware attention mechanism to learn\neffective 3D lesion features that are used to estimate attention weights\ndescribing the inter-phase relations of the enhancement patterns. We also\npresent a multi-scale attention scheme to capture and aggregate temporal\npatterns of lesion features at different spatial scales for further\nimprovement. Extensive experiments on multi-phase CT scans of kidney cancer\npatients from the collected dataset demonstrate that our LACPANet outperforms\nstate-of-the-art approaches in diagnostic accuracy.\n","authors":["Kwang-Hyun Uhm","Seung-Won Jung","Sung-Hoo Hong","Sung-Jea Ko"],"pdf_url":"https://arxiv.org/pdf/2406.16322v1.pdf","comment":"This article has been accepted for publication in Computers in\n  Biology and Medicine"},{"id":"http://arxiv.org/abs/2403.18651v3","updated":"2024-06-24T04:04:35Z","published":"2024-03-27T14:59:19Z","title":"Do High-Performance Image-to-Image Translation Networks Enable the\n  Discovery of Radiomic Features? Application to MRI Synthesis from Ultrasound\n  in Prostate Cancer","summary":"  This study investigates the foundational characteristics of image-to-image\ntranslation networks, specifically examining their suitability and\ntransferability within the context of routine clinical environments, despite\nachieving high levels of performance, as indicated by a Structural Similarity\nIndex (SSIM) exceeding 0.95. The evaluation study was conducted using data from\n794 patients diagnosed with Prostate cancer. To synthesize MRI from Ultrasound\nimages, we employed five widely recognized image to image translation networks\nin medical imaging: 2DPix2Pix, 2DCycleGAN, 3DCycleGAN, 3DUNET, and\n3DAutoEncoder. For quantitative assessment, we report four prevalent evaluation\nmetrics Mean Absolute Error, Mean Square Error, Structural Similarity Index\n(SSIM), and Peak Signal to Noise Ratio. Moreover, a complementary analysis\nemploying Radiomic features (RF) via Spearman correlation coefficient was\nconducted to investigate, for the first time, whether networks achieving high\nperformance, SSIM greater than 0.85, could identify low-level RFs. The RF\nanalysis showed 75 features out of 186 RFs were discovered via just 2DPix2Pix\nalgorithm while half of RFs were lost in the translation process. Finally, a\ndetailed qualitative assessment by five medical doctors indicated a lack of low\nlevel feature discovery in image to image translation tasks.\n","authors":["Mohammad R. Salmanpour","Amin Mousavi","Yixi Xu","William B Weeks","Ilker Hacihaliloglu"],"pdf_url":"https://arxiv.org/pdf/2403.18651v3.pdf","comment":"Submitted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.16297v1","updated":"2024-06-24T03:49:52Z","published":"2024-06-24T03:49:52Z","title":"Priorformer: A UGC-VQA Method with content and distortion priors","summary":"  User Generated Content (UGC) videos are susceptible to complicated and\nvariant degradations and contents, which prevents the existing blind video\nquality assessment (BVQA) models from good performance since the lack of the\nadapability of distortions and contents. To mitigate this, we propose a novel\nprior-augmented perceptual vision transformer (PriorFormer) for the BVQA of\nUGC, which boots its adaptability and representation capability for divergent\ncontents and distortions. Concretely, we introduce two powerful priors, i.e.,\nthe content and distortion priors, by extracting the content and distortion\nembeddings from two pre-trained feature extractors. Then we adopt these two\npowerful embeddings as the adaptive prior tokens, which are transferred to the\nvision transformer backbone jointly with implicit quality features. Based on\nthe above strategy, the proposed PriorFormer achieves state-of-the-art\nperformance on three public UGC VQA datasets including KoNViD-1K, LIVE-VQC and\nYouTube-UGC.\n","authors":["Yajing Pei","Shiyu Huang","Yiting Lu","Xin Li","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2406.16297v1.pdf","comment":"7 pages"}]},"2024-06-23T00:00:00Z":{"Image and Video Processing":[{"id":"http://arxiv.org/abs/2406.16214v1","updated":"2024-06-23T20:45:20Z","published":"2024-06-23T20:45:20Z","title":"Reducing the Sampling Burden of Fourier Sensing with a Non-rectangular\n  Field-of-View","summary":"  With Fourier sensing, it is commonly the case that the field-of-view (FOV),\nthe area of space to be imaged, is known prior to reconstruction. To date,\nreconstruction algorithms have focused on FOVs with simple geometries: a\nrectangle or a hexagon. This yields sampling patterns that are more burdensome\nthan necessary. Due to the reduced area of imaging possible with an arbitrary\n(e.g., non-rectangular) FOV, the number of samples required for a high-quality\nimages is reduced. However, when an arbitrary FOV has been considered, the\nreconstruction algorithm is computationally expensive. In this manuscript, we\npresent a method to reduce the sampling pattern for an arbitrary FOV with an\naccompanying direct (non-iterative) reconstruction algorithm. We also present a\nmethod to decrease the computational cost of the (iterative) model-based\nreconstruction (MBR) algorithm. We present results using MRI data of an ankle,\na pineapple, and a brain.\n","authors":["Nicholas Dwork","Erin K. Englund","Alex J. Barker"],"pdf_url":"https://arxiv.org/pdf/2406.16214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16983v1","updated":"2024-06-23T19:44:00Z","published":"2024-06-23T19:44:00Z","title":"On Instabilities of Unsupervised Denoising Diffusion Models in Magnetic\n  Resonance Imaging Reconstruction","summary":"  Denoising diffusion models offer a promising approach to accelerating\nmagnetic resonance imaging (MRI) and producing diagnostic-level images in an\nunsupervised manner. However, our study demonstrates that even tiny worst-case\npotential perturbations transferred from a surrogate model can cause these\nmodels to generate fake tissue structures that may mislead clinicians. The\ntransferability of such worst-case perturbations indicates that the robustness\nof image reconstruction may be compromised due to MR system imperfections or\nother sources of noise. Moreover, at larger perturbation strengths, diffusion\nmodels exhibit Gaussian noise-like artifacts that are distinct from those\nobserved in supervised models and are more challenging to detect. Our results\nhighlight the vulnerability of current state-of-the-art diffusion-based\nreconstruction models to possible worst-case perturbations and underscore the\nneed for further research to improve their robustness and reliability in\nclinical settings.\n","authors":["Tianyu Han","Sven Nebelung","Firas Khader","Jakob Nikolas Kather","Daniel Truhn"],"pdf_url":"https://arxiv.org/pdf/2406.16983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16189v1","updated":"2024-06-23T18:47:51Z","published":"2024-06-23T18:47:51Z","title":"Fuzzy Attention-based Border Rendering Network for Lung Organ\n  Segmentation","summary":"  Automatic lung organ segmentation on CT images is crucial for lung disease\ndiagnosis. However, the unlimited voxel values and class imbalance of lung\norgans can lead to false-negative/positive and leakage issues in advanced\nmethods. Additionally, some slender lung organs are easily lost during the\nrecycled down/up-sample procedure, e.g., bronchioles & arterioles, causing\nsevere discontinuity issue. Inspired by these, this paper introduces an\neffective lung organ segmentation method called Fuzzy Attention-based Border\nRendering (FABR) network. Since fuzzy logic can handle the uncertainty in\nfeature extraction, hence the fusion of deep networks and fuzzy sets should be\na viable solution for better performance. Meanwhile, unlike prior top-tier\nmethods that operate on all regular dense points, our FABR depicts lung organ\nregions as cube-trees, focusing only on recycle-sampled border vulnerable\npoints, rendering the severely discontinuous, false-negative/positive organ\nregions with a novel Global-Local Cube-tree Fusion (GLCF) module. All\nexperimental results, on four challenging datasets of airway & artery,\ndemonstrate that our method can achieve the favorable performance\nsignificantly.\n","authors":["Sheng Zhang","Yang Nan","Yingying Fang","Shiyi Wang","Xiaodan Xing","Zhifan Gao","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2406.16189v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.16981v1","updated":"2024-06-23T18:41:43Z","published":"2024-06-23T18:41:43Z","title":"Research on Feature Extraction Data Processing System For MRI of Brain\n  Diseases Based on Computer Deep Learning","summary":"  Most of the existing wavelet image processing techniques are carried out in\nthe form of single-scale reconstruction and multiple iterations. However,\nprocessing high-quality fMRI data presents problems such as mixed noise and\nexcessive computation time. This project proposes the use of matrix operations\nby combining mixed noise elimination methods with wavelet analysis to replace\ntraditional iterative algorithms. Functional magnetic resonance imaging (fMRI)\nof the auditory cortex of a single subject is analyzed and compared to the\nwavelet domain signal processing technology based on repeated times and the\nworld's most influential SPM8. Experiments show that this algorithm is the\nfastest in computing time, and its detection effect is comparable to the\ntraditional iterative algorithm. However, this has a higher practical value for\nthe processing of FMRI data. In addition, the wavelet analysis method proposed\nsignal processing to speed up the calculation rate.\n","authors":["Lingxi Xiao","Jinxin Hu","Yutian Yang","Yinqiu Feng","Zichao Li","Zexi Chen"],"pdf_url":"https://arxiv.org/pdf/2406.16981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16150v1","updated":"2024-06-23T16:09:21Z","published":"2024-06-23T16:09:21Z","title":"Intensity Confusion Matters: An Intensity-Distance Guided Loss for\n  Bronchus Segmentation","summary":"  Automatic segmentation of the bronchial tree from CT imaging is important, as\nit provides structural information for disease diagnosis. Despite the merits of\nprevious automatic bronchus segmentation methods, they have paied less\nattention to the issue we term as \\textit{Intensity Confusion}, wherein the\nintensity values of certain background voxels approach those of the foreground\nvoxels within bronchi. Conversely, the intensity values of some foreground\nvoxels are nearly identical to those of background voxels. This proximity in\nintensity values introduces significant challenges to neural network\nmethodologies. To address the issue, we introduce a novel Intensity-Distance\nGuided loss function, which assigns adaptive weights to different image voxels\nfor mining hard samples that cause the intensity confusion. The proposed loss\nestimates the voxel-level hardness of samples, on the basis of the following\nintensity and distance priors. We regard a voxel as a hard sample if it is in:\n(1) the background and has an intensity value close to the bronchus region; (2)\nthe bronchus region and is of higher intensity than most voxels inside the\nbronchus; (3) the background region and at a short distance from the bronchus.\nExtensive experiments not only show the superiority of our method compared with\nthe state-of-the-art methods, but also verify that tackling the intensity\nconfusion issue helps to significantly improve bronchus segmentation. Project\npage: https://github.com/lhaof/ICM.\n","authors":["Haifan Gong","Wenhao Huang","Huan Zhang","Yu Wang","Xiang Wan","Hong Shen","Guanbin Li","Haofeng Li"],"pdf_url":"https://arxiv.org/pdf/2406.16150v1.pdf","comment":"IEEE International Conference on Multimedia & Expo (ICME) 2024"},{"id":"http://arxiv.org/abs/2404.06589v2","updated":"2024-06-23T14:54:11Z","published":"2024-04-09T19:33:05Z","title":"Leveraging Latents for Efficient Thermography Classification and\n  Segmentation","summary":"  Breast cancer is a prominent health concern worldwide, currently being the\nsecondmost common and second-deadliest type of cancer in women. While current\nbreast cancer diagnosis mainly relies on mammography imaging, in recent years\nthe use of thermography for breast cancer imaging has been garnering growing\npopularity. Thermographic imaging relies on infrared cameras to capture\nbody-emitted heat distributions. While these heat signatures have proven useful\nfor computer-vision systems for accurate breast cancer segmentation and\nclassification, prior work often relies on handcrafted feature engineering or\ncomplex architectures, potentially limiting the comparability and applicability\nof these methods. In this work, we present a novel algorithm for both breast\ncancer classification and segmentation. Rather than focusing efforts on manual\nfeature and architecture engineering, our algorithm focuses on leveraging an\ninformative, learned feature space, thus making our solution simpler to use and\nextend to other frameworks and downstream tasks, as well as more applicable to\ndata-scarce settings. Our classification produces SOTA results, while we are\nthe first work to produce segmentation regions studied in this paper.\n","authors":["Tamir Shor","Chaim Baskin","Alex Bronstein"],"pdf_url":"https://arxiv.org/pdf/2404.06589v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06148v2","updated":"2024-06-23T13:24:58Z","published":"2022-07-13T12:16:33Z","title":"Multiview Contrastive Learning for Completely Blind Video Quality\n  Assessment of User Generated Content","summary":"  Completely blind video quality assessment (VQA) refers to a class of quality\nassessment methods that do not use any reference videos, human opinion scores\nor training videos from the target database to learn a quality model. The\ndesign of this class of methods is particularly important since it can allow\nfor superior generalization in performance across various datasets. We consider\nthe design of completely blind VQA for user generated content. While several\ndeep feature extraction methods have been considered in supervised and weakly\nsupervised settings, such approaches have not been studied in the context of\ncompletely blind VQA. We bridge this gap by presenting a self-supervised\nmultiview contrastive learning framework to learn spatio-temporal quality\nrepresentations. In particular, we capture the common information between frame\ndifferences and frames by treating them as a pair of views and similarly obtain\nthe shared representations between frame differences and optical flow. The\nresulting features are then compared with a corpus of pristine natural video\npatches to predict the quality of the distorted video. Detailed experiments on\nmultiple camera captured VQA datasets reveal the superior performance of our\nmethod over other features when evaluated without training on human scores.\n","authors":["Shankhanil Mitra","Rajiv Soundararajan"],"pdf_url":"https://arxiv.org/pdf/2207.06148v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16083v1","updated":"2024-06-23T11:28:08Z","published":"2024-06-23T11:28:08Z","title":"Mamba-based Light Field Super-Resolution with Efficient Subspace\n  Scanning","summary":"  Transformer-based methods have demonstrated impressive performance in 4D\nlight field (LF) super-resolution by effectively modeling long-range\nspatial-angular correlations, but their quadratic complexity hinders the\nefficient processing of high resolution 4D inputs, resulting in slow inference\nspeed and high memory cost. As a compromise, most prior work adopts a\npatch-based strategy, which fails to leverage the full information from the\nentire input LFs. The recently proposed selective state-space model, Mamba, has\ngained popularity for its efficient long-range sequence modeling. In this\npaper, we propose a Mamba-based Light Field Super-Resolution method, named\nMLFSR, by designing an efficient subspace scanning strategy. Specifically, we\ntokenize 4D LFs into subspace sequences and conduct bi-directional scanning on\neach subspace. Based on our scanning strategy, we then design the Mamba-based\nGlobal Interaction (MGI) module to capture global information and the local\nSpatial- Angular Modulator (SAM) to complement local details. Additionally, we\nintroduce a Transformer-to-Mamba (T2M) loss to further enhance overall\nperformance. Extensive experiments on public benchmarks demonstrate that MLFSR\nsurpasses CNN-based models and rivals Transformer-based methods in performance\nwhile maintaining higher efficiency. With quicker inference speed and reduced\nmemory demand, MLFSR facilitates full-image processing of high-resolution 4D\nLFs with enhanced performance.\n","authors":["Ruisheng Gao","Zeyu Xiao","Zhiwei Xiong"],"pdf_url":"https://arxiv.org/pdf/2406.16083v1.pdf","comment":"17 pages,7 figures"},{"id":"http://arxiv.org/abs/2406.16074v1","updated":"2024-06-23T10:50:22Z","published":"2024-06-23T10:50:22Z","title":"CAVM: Conditional Autoregressive Vision Model for Contrast-Enhanced\n  Brain Tumor MRI Synthesis","summary":"  Contrast-enhanced magnetic resonance imaging (MRI) is pivotal in the pipeline\nof brain tumor segmentation and analysis. Gadolinium-based contrast agents, as\nthe most commonly used contrast agents, are expensive and may have potential\nside effects, and it is desired to obtain contrast-enhanced brain tumor MRI\nscans without the actual use of contrast agents. Deep learning methods have\nbeen applied to synthesize virtual contrast-enhanced MRI scans from\nnon-contrast images. However, as this synthesis problem is inherently\nill-posed, these methods fall short in producing high-quality results. In this\nwork, we propose Conditional Autoregressive Vision Model (CAVM) for improving\nthe synthesis of contrast-enhanced brain tumor MRI. As the enhancement of image\nintensity grows with a higher dose of contrast agents, we assume that it is\nless challenging to synthesize a virtual image with a lower dose, where the\ndifference between the contrast-enhanced and non-contrast images is smaller.\nThus, CAVM gradually increases the contrast agent dosage and produces\nhigher-dose images based on previous lower-dose ones until the final desired\ndose is achieved. Inspired by the resemblance between the gradual dose increase\nand the Chain-of-Thought approach in natural language processing, CAVM uses an\nautoregressive strategy with a decomposition tokenizer and a decoder.\nSpecifically, the tokenizer is applied to obtain a more compact image\nrepresentation for computational efficiency, and it decomposes the image into\ndose-variant and dose-invariant tokens. Then, a masked self-attention mechanism\nis developed for autoregression that gradually increases the dose of the\nvirtual image based on the dose-variant tokens. Finally, the updated\ndose-variant tokens corresponding to the desired dose are decoded together with\ndose-invariant tokens to produce the final contrast-enhanced MRI.\n","authors":["Lujun Gui","Chuyang Ye","Tianyi Yan"],"pdf_url":"https://arxiv.org/pdf/2406.16074v1.pdf","comment":"The work has been accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2402.16581v2","updated":"2024-06-23T09:32:06Z","published":"2024-02-26T14:04:54Z","title":"Rate Splitting Multiple Access-Enabled Adaptive Panoramic Video Semantic\n  Transmission","summary":"  In this paper, we propose an adaptive panoramic video semantic transmission\n(APVST) framework enabled by rate splitting multiple access (RSMA). The APVST\nframework consists of a semantic transmitter and receiver, utilizing a deep\njoint source-channel coding structure to adaptively extract and encode semantic\nfeatures from panoramic frames. To achieve higher spectral efficiency and\nconserve bandwidth, APVST employs an entropy model and a dimension-adaptive\nmodule to control the transmission rate. Additionally, we take\nweighted-to-spherically-uniform peak signal-to-noise ratio (WS-PSNR) and\nweighted-to-spherically-uniform structural similarity (WS-SSIM) as distortion\nevaluation metrics for panoramic videos and design a weighted self-attention\nmodule for APVST. This module integrates weights and feature maps to enhance\nthe quality of the immersive experience. Considering the overlap in the field\nof view when users watch panoramic videos, we further utilize RSMA to split the\nrequired panoramic video semantic streams into common and private messages for\ntransmission. We propose an RSMA-enabled semantic stream transmission scheme\nand formulate a joint problem of latency and immersive experience quality by\noptimizing the allocation ratios of power, common rate, and channel bandwidth,\naiming to maximize the quality of service (QoS) scores for users. To address\nthe above problem, we propose a deep reinforcement learning algorithm based on\nproximal policy optimization (PPO) with high efficiency to handle dynamically\nchanging environments. Simulation results demonstrate that our proposed APVST\nframework saves up to 20% and 50% of channel bandwidth compared to other\nsemantic and traditional video transmission schemes, respectively. Moreover,\nour study confirms the efficiency of RSMA in panoramic video transmission,\nachieving performance gains of 13% and 20% compared to NOMA and OFDMA.\n","authors":["Haixiao Gao","Mengying Sun","Xiaodong Xu","Shujun Han","Bizhu Wang","Jingxuan Zhang","Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.16581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16012v1","updated":"2024-06-23T05:01:51Z","published":"2024-06-23T05:01:51Z","title":"Wound Tissue Segmentation in Diabetic Foot Ulcer Images Using Deep\n  Learning: A Pilot Study","summary":"  Identifying individual tissues, so-called tissue segmentation, in diabetic\nfoot ulcer (DFU) images is a challenging task and little work has been\npublished, largely due to the limited availability of a clinical image dataset.\nTo address this gap, we have created a DFUTissue dataset for the research\ncommunity to evaluate wound tissue segmentation algorithms. The dataset\ncontains 110 images with tissues labeled by wound experts and 600 unlabeled\nimages. Additionally, we conducted a pilot study on segmenting wound\ncharacteristics including fibrin, granulation, and callus using deep learning.\nDue to the limited amount of annotated data, our framework consists of both\nsupervised learning (SL) and semi-supervised learning (SSL) phases. In the SL\nphase, we propose a hybrid model featuring a Mix Transformer (MiT-b3) in the\nencoder and a CNN in the decoder, enhanced by the integration of a parallel\nspatial and channel squeeze-and-excitation (P-scSE) module known for its\nefficacy in improving boundary accuracy. The SSL phase employs a\npseudo-labeling-based approach, iteratively identifying and incorporating\nvaluable unlabeled images to enhance overall segmentation performance.\nComparative evaluations with state-of-the-art methods are conducted for both SL\nand SSL phases. The SL achieves a Dice Similarity Coefficient (DSC) of 84.89%,\nwhich has been improved to 87.64% in the SSL phase. Furthermore, the results\nare benchmarked against two widely used SSL approaches: Generative Adversarial\nNetworks and Cross-Consistency Training. Additionally, our hybrid model\noutperforms the state-of-the-art methods with a 92.99% DSC in performing binary\nsegmentation of DFU wound areas when tested on the Chronic Wound dataset. Codes\nand data are available at https://github.com/uwm-bigdata/DFUTissueSegNet.\n","authors":["Mrinal Kanti Dhar","Chuanbo Wang","Yash Patel","Taiyu Zhang","Jeffrey Niezgoda","Sandeep Gopalakrishnan","Keke Chen","Zeyun Yu"],"pdf_url":"https://arxiv.org/pdf/2406.16012v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15979v1","updated":"2024-06-23T01:32:53Z","published":"2024-06-23T01:32:53Z","title":"Deep Learning Segmentation of Ascites on Abdominal CT Scans for\n  Automatic Volume Quantification","summary":"  Purpose: To evaluate the performance of an automated deep learning method in\ndetecting ascites and subsequently quantifying its volume in patients with\nliver cirrhosis and ovarian cancer.\n  Materials and Methods: This retrospective study included contrast-enhanced\nand non-contrast abdominal-pelvic CT scans of patients with cirrhotic ascites\nand patients with ovarian cancer from two institutions, National Institutes of\nHealth (NIH) and University of Wisconsin (UofW). The model, trained on The\nCancer Genome Atlas Ovarian Cancer dataset (mean age, 60 years +/- 11 [s.d.];\n143 female), was tested on two internal (NIH-LC and NIH-OV) and one external\ndataset (UofW-LC). Its performance was measured by the Dice coefficient,\nstandard deviations, and 95% confidence intervals, focusing on ascites volume\nin the peritoneal cavity.\n  Results: On NIH-LC (25 patients; mean age, 59 years +/- 14 [s.d.]; 14 male)\nand NIH-OV (166 patients; mean age, 65 years +/- 9 [s.d.]; all female), the\nmodel achieved Dice scores of 0.855 +/- 0.061 (CI: 0.831-0.878) and 0.826 +/-\n0.153 (CI: 0.764-0.887), with median volume estimation errors of 19.6% (IQR:\n13.2-29.0) and 5.3% (IQR: 2.4-9.7) respectively. On UofW-LC (124 patients; mean\nage, 46 years +/- 12 [s.d.]; 73 female), the model had a Dice score of 0.830\n+/- 0.107 (CI: 0.798-0.863) and median volume estimation error of 9.7% (IQR:\n4.5-15.1). The model showed strong agreement with expert assessments, with r^2\nvalues of 0.79, 0.98, and 0.97 across the test sets.\n  Conclusion: The proposed deep learning method performed well in segmenting\nand quantifying the volume of ascites in concordance with expert radiologist\nassessments.\n","authors":["Benjamin Hou","Sung-Won Lee","Jung-Min Lee","Christopher Koh","Jing Xiao","Perry J. Pickhardt","Ronald M. Summers"],"pdf_url":"https://arxiv.org/pdf/2406.15979v1.pdf","comment":null}]}}