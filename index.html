<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-06-28T00:00:00Z">2024-06-28</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">88</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Web2Code: A Large-scale Webpage-to-Code <span class="highlight-title">Dataset</span> and Evaluation Framework
  for Multimodal LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sukmin Yun, Haokun Lin, Rusiru Thushara, Mohammad Qazim Bhat, Yongxin Wang, Zutao Jiang, Mingkai Deng, Jinhong Wang, Tianhua Tao, Junbo Li, Haonan Li, Preslav Nakov, Timothy Baldwin, Zhengzhong Liu, Eric P. Xing, Xiaodan Liang, Zhiqiang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have shown impressive success across
modalities such as image, video, and audio in a variety of understanding and
generation tasks. However, current MLLMs are surprisingly poor at understanding
webpage screenshots and generating their corresponding HTML code. To address
this problem, we propose Web2Code, a benchmark consisting of a new large-scale
webpage-to-code dataset for instruction tuning and an evaluation framework for
the webpage understanding and HTML code translation abilities of MLLMs. For
dataset construction, we leverage pretrained LLMs to enhance existing
webpage-to-code datasets as well as generate a diverse pool of new webpages
rendered into images. Specifically, the inputs are webpage images and
instructions, while the responses are the webpage's HTML code. We further
include diverse natural language QA pairs about the webpage content in the
responses to enable a more comprehensive understanding of the web content. To
evaluate model performance in these tasks, we develop an evaluation framework
for testing MLLMs' abilities in webpage understanding and web-to-code
generation. Extensive experiments show that our proposed dataset is beneficial
not only to our proposed tasks but also in the general visual domain, while
previous datasets result in worse performance. We hope our work will contribute
to the development of general MLLMs suitable for web-based content generation
and task automation. Our data and code will be available at
https://github.com/MBZUAI-LLM/web2code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website at https://mbzuai-llm.github.io/webpage2code/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaRA: Supercharging Robot Learning Data for Vision-Language Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe, Ryan Burgert, Mu Cai, Yong Jae Lee, Michael S. Ryoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) equipped with extensive world knowledge and
strong reasoning skills can tackle diverse tasks across domains, often by
posing them as conversation-style instruction-response pairs. In this paper, we
propose LLaRA: Large Language and Robotics Assistant, a framework which
formulates robot action policy as conversations, and provides improved
responses when trained with auxiliary data that complements policy learning.
LLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity
to process state information as visual-textual prompts and generate optimal
policy decisions in text. To train such action policy VLMs, we first introduce
an automated pipeline to generate diverse high-quality robotics instruction
data from existing behavior cloning data. A VLM finetuned with the resulting
collection of datasets based on a conversation-style formulation tailored for
robotics tasks, can generate meaningful robot action policy decisions. Our
experiments across multiple simulated and real-world environments demonstrate
the state-of-the-art performance of the proposed LLaRA framework. The code,
datasets, and pretrained models are available at
https://github.com/LostXine/LLaRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Synthetic Data Creation with 1,000,000,000 Personas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel persona-driven data synthesis methodology that leverages
various perspectives within a large language model (LLM) to create diverse
synthetic data. To fully exploit this methodology at scale, we introduce
Persona Hub -- a collection of 1 billion diverse personas automatically curated
from web data. These 1 billion personas (~13% of the world's total population),
acting as distributed carriers of world knowledge, can tap into almost every
perspective encapsulated within the LLM, thereby facilitating the creation of
diverse synthetic data at scale for various scenarios. By showcasing Persona
Hub's use cases in synthesizing high-quality mathematical and logical reasoning
problems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs
and tools (functions) at scale, we demonstrate persona-driven data synthesis is
versatile, scalable, flexible, and easy to use, potentially driving a paradigm
shift in synthetic data creation and applications in practice, which may have a
profound impact on LLM research and development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProgressGym: Alignment with a Millennium of Moral Progress 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Qiu, Yang Zhang, Xuchuan Huang, Jasmine Xinze Li, Jiaming Ji, Yaodong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Frontier AI systems, including large language models (LLMs), hold increasing
influence over the epistemology of human users. Such influence can reinforce
prevailing societal values, potentially contributing to the lock-in of
misguided moral beliefs and, consequently, the perpetuation of problematic
moral practices on a broad scale. We introduce progress alignment as a
technical solution to mitigate this imminent risk. Progress alignment
algorithms learn to emulate the mechanics of human moral progress, thereby
addressing the susceptibility of existing alignment methods to contemporary
moral blindspots. To empower research in progress alignment, we introduce
ProgressGym, an experimental framework allowing the learning of moral progress
mechanics from history, in order to facilitate future progress in real-world
moral decisions. Leveraging 9 centuries of historical text and 18 historical
LLMs, ProgressGym enables codification of real-world progress alignment
challenges into concrete benchmarks. Specifically, we introduce three core
challenges: tracking evolving values (PG-Follow), preemptively anticipating
moral progress (PG-Predict), and regulating the feedback loop between human and
AI value shifts (PG-Coevolve). Alignment methods without a temporal dimension
are inapplicable to these tasks. In response, we present lifelong and
extrapolative algorithms as baseline methods of progress alignment, and build
an open leaderboard soliciting novel algorithms and challenges. The framework
and the leaderboard are available at
https://github.com/PKU-Alignment/ProgressGym and
https://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheridan Feucht, David Atkinson, Byron Wallace, David Bau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs process text as sequences of tokens that roughly correspond to words,
where less common words are represented by multiple tokens. However, individual
tokens are often semantically unrelated to the meanings of the words/concepts
they comprise. For example, Llama-2-7b's tokenizer splits the word
"northeastern" into the tokens ['_n', 'ort', 'he', 'astern'], none of which
correspond to semantically meaningful units like "north" or "east." Similarly,
the overall meanings of named entities like "Neil Young" and multi-word
expressions like "break a leg" cannot be directly inferred from their
constituent tokens. Mechanistically, how do LLMs convert such arbitrary groups
of tokens into useful higher-level representations? In this work, we find that
last token representations of named entities and multi-token words exhibit a
pronounced "erasure" effect, where information about previous and current
tokens is rapidly forgotten in early layers. Using this observation, we propose
a method to "read out" the implicit vocabulary of an autoregressive LLM by
examining differences in token representations across layers, and present
results of this method for Llama-2-7b and Llama-3-8B. To our knowledge, this is
the first attempt to probe the implicit vocabulary of an LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 14 figures. Code and data at
  https://footprints.baulab.info/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Molecular Facts: Desiderata for Decontextualization in LLM Fact
  Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anisha Gunjal, Greg Durrett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic factuality verification of large language model (LLM) generations
is becoming more and more widely used to combat hallucinations. A major point
of tension in the literature is the granularity of this fact-checking: larger
chunks of text are hard to fact-check, but more atomic facts like propositions
may lack context to interpret correctly. In this work, we assess the role of
context in these atomic facts. We argue that fully atomic facts are not the
right representation, and define two criteria for molecular facts:
decontextuality, or how well they can stand alone, and minimality, or how
little extra information is added to achieve decontexuality. We quantify the
impact of decontextualization on minimality, then present a baseline
methodology for generating molecular facts automatically, aiming to add the
right amount of information. We compare against various methods of
decontextualization and find that molecular facts balance minimality with fact
verification accuracy in ambiguous settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Applying RLAIF for Code Generation with API-usage in Lightweight LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sujan Dutta, Sayantan Mahinder, Raviteja Anantha, Bortik Bandyopadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from AI Feedback (RLAIF) has demonstrated significant
potential across various domains, including mitigating harm in LLM outputs,
enhancing text summarization, and mathematical reasoning. This paper introduces
an RLAIF framework for improving the code generation abilities of lightweight
(<1B parameters) LLMs. We specifically focus on code generation tasks that
require writing appropriate API calls, which is challenging due to the
well-known issue of hallucination in LLMs. Our framework extracts AI feedback
from a larger LLM (e.g., GPT-3.5) through a specialized prompting strategy and
uses this data to train a reward model towards better alignment from smaller
LLMs. We run our experiments on the Gorilla dataset and meticulously assess the
quality of the model-generated code across various metrics, including AST,
ROUGE, and Code-BLEU, and develop a pipeline to compute its executability rate
accurately. Our approach significantly enhances the fine-tuned LLM baseline's
performance, achieving a 4.5% improvement in executability rate. Notably, a
smaller LLM model (780M parameters) trained with RLAIF surpasses a much larger
fine-tuned baseline with 7B parameters, achieving a 1.0% higher code
executability rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To Word Senses and Beyond: Inducing Concepts with Contextualized
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bastien Liétard, Pascal Denis, Mikaella Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polysemy and synonymy are two crucial interrelated facets of lexical
ambiguity. While both phenomena have been studied extensively in NLP, leading
to dedicated systems, they are often been considered independently. While many
tasks dealing with polysemy (e.g. Word Sense Disambiguiation or Induction)
highlight the role of a word's senses, the study of synonymy is rooted in the
study of concepts, i.e. meaning shared across the lexicon. In this paper, we
introduce Concept Induction, the unsupervised task of learning a soft
clustering among words that defines a set of concepts directly from data. This
task generalizes that of Word Sense Induction. We propose a bi-level approach
to Concept Induction that leverages both a local lemma-centric view and a
global cross-lexicon perspective to induce concepts. We evaluate the obtained
clustering on SemCor's annotated data and obtain good performances (BCubed F1
above 0.60). We find that the local and the global levels are mutually
beneficial to induce concepts and also senses in our setting. Finally, we
create static embeddings representing our induced concepts and use them on the
Word-in-Context task, obtaining competitive performances with the
State-of-the-Art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danny Halawi, Alexander Wei, Eric Wallace, Tony T. Wang, Nika Haghtalab, Jacob Steinhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Black-box finetuning is an emerging interface for adapting state-of-the-art
language models to user needs. However, such access may also let malicious
actors undermine model safety. To demonstrate the challenge of defending
finetuning interfaces, we introduce covert malicious finetuning, a method to
compromise model safety via finetuning while evading detection. Our method
constructs a malicious dataset where every individual datapoint appears
innocuous, but finetuning on the dataset teaches the model to respond to
encoded harmful requests with encoded harmful responses. Applied to GPT-4, our
method produces a finetuned model that acts on harmful instructions 99% of the
time and avoids detection by defense mechanisms such as dataset inspection,
safety evaluations, and input/output classifiers. Our findings question whether
black-box finetuning access can be secured against sophisticated adversaries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding and Mitigating Language Confusion in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kelly Marchisio, Wei-Yin Ko, Alexandre Bérard, Théo Dehaze, Sebastian Ruder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate a surprising limitation of LLMs: their inability to
consistently generate text in a user's desired language. We create the Language
Confusion Benchmark (LCB) to evaluate such failures, covering 15 typologically
diverse languages with existing and newly-created English and multilingual
prompts. We evaluate a range of LLMs on monolingual and cross-lingual
generation reflecting practical use cases, finding that Llama Instruct and
Mistral models exhibit high degrees of language confusion and even the
strongest models fail to consistently respond in the correct language. We
observe that base and English-centric instruct models are more prone to
language confusion, which is aggravated by complex prompts and high sampling
temperatures. We find that language confusion can be partially mitigated via
few-shot prompting, multilingual SFT and preference tuning. We release our
language confusion benchmark, which serves as a first layer of efficient,
scalable multilingual evaluation at
https://github.com/for-ai/language-confusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BioMNER: A <span class="highlight-title">Dataset</span> for Biomedical Method Entity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Tang, Bohao Yang, Kun Zhao, Bo Lv, Chenghao Xiao, Frank Guerin, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named entity recognition (NER) stands as a fundamental and pivotal task
within the realm of Natural Language Processing. Particularly within the domain
of Biomedical Method NER, this task presents notable challenges, stemming from
the continual influx of domain-specific terminologies in scholarly literature.
Current research in Biomedical Method (BioMethod) NER suffers from a scarcity
of resources, primarily attributed to the intricate nature of methodological
concepts, which necessitate a profound understanding for precise delineation.
In this study, we propose a novel dataset for biomedical method entity
recognition, employing an automated BioMethod entity recognition and
information retrieval system to assist human annotation. Furthermore, we
comprehensively explore a range of conventional and contemporary open-domain
NER methodologies, including the utilization of cutting-edge large-scale
language models (LLMs) customised to our dataset. Our empirical findings reveal
that the large parameter counts of language models surprisingly inhibit the
effective assimilation of entity extraction patterns pertaining to biomedical
methods. Remarkably, the approach, leveraging the modestly sized ALBERT model
(only 11MB), in conjunction with conditional random fields (CRF), achieves
state-of-the-art (SOTA) performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEMoE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renzhi Wang, Piji Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) require continual knowledge updates to stay
abreast of the ever-changing world facts, prompting the formulation of lifelong
model editing task. While recent years have witnessed the development of
various techniques for single and batch editing, these methods either fail to
apply or perform sub-optimally when faced with lifelong editing. In this paper,
we introduce LEMoE, an advanced Mixture of Experts (MoE) adaptor for lifelong
model editing. We first analyze the factors influencing the effectiveness of
conventional MoE adaptor in lifelong editing, including catastrophic
forgetting, inconsistent routing and order sensitivity. Based on these
insights, we propose a tailored module insertion method to achieve lifelong
editing, incorporating a novel KV anchor routing to enhance routing consistency
between training and inference stage, along with a concise yet effective
clustering-based editing order planning. Experimental results demonstrate the
effectiveness of our method in lifelong editing, surpassing previous model
editing techniques while maintaining outstanding performance in batch editing
task. Our code will be available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for
  Tool-Augmented Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Zhang, Jing Chen, Junjie Wang, Yaxin Liu, Cheng Yang, Chufan Shi, Xinyu Zhu, Zihao Lin, Hanwen Wan, Yujiu Yang, Tetsuya Sakai, Tian Feng, Hayato Yamana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tool-augmented large language models (LLMs) are rapidly being integrated into
real-world applications. Due to the lack of benchmarks, the community still
needs to fully understand the hallucination issues within these models. To
address this challenge, we introduce a comprehensive diagnostic benchmark,
ToolBH. Specifically, we assess the LLM's hallucinations through two
perspectives: depth and breadth. In terms of depth, we propose a multi-level
diagnostic process, including (1) solvability detection, (2) solution planning,
and (3) missing-tool analysis. For breadth, we consider three scenarios based
on the characteristics of the toolset: missing necessary tools, potential
tools, and limited functionality tools. Furthermore, we developed seven tasks
and collected 700 evaluation samples through multiple rounds of manual
annotation. The results show the significant challenges presented by the ToolBH
benchmark. The current advanced models Gemini-1.5-Pro and GPT-4o only achieve a
total score of 45.3 and 37.0, respectively, on a scale of 100. In this
benchmark, larger model parameters do not guarantee better performance; the
training data and response strategies also play a crucial role in tool-enhanced
LLM scenarios. Our diagnostic analysis indicates that the primary reason for
model errors lies in assessing task solvability. Additionally, open-weight
models suffer from performance drops with verbose replies, whereas proprietary
models excel with longer reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The SIFo Benchmark: Investigating the Sequential Instruction Following
  Ability of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Chen, Baohao Liao, Jirui Qi, Panagiotis Eustratiadis, Christof Monz, Arianna Bisazza, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following multiple instructions is a crucial ability for large language
models (LLMs). Evaluating this ability comes with significant challenges: (i)
limited coherence between multiple instructions, (ii) positional bias where the
order of instructions affects model performance, and (iii) a lack of
objectively verifiable tasks. To address these issues, we introduce a benchmark
designed to evaluate models' abilities to follow multiple instructions through
sequential instruction following (SIFo) tasks. In SIFo, the successful
completion of multiple instructions is verifiable by examining only the final
instruction. Our benchmark evaluates instruction following using four tasks
(text modification, question answering, mathematics, and security rule
following), each assessing different aspects of sequential instruction
following. Our evaluation of popular LLMs, both closed-source and open-source,
shows that more recent and larger models significantly outperform their older
and smaller counterparts on the SIFo tasks, validating the benchmark's
effectiveness. All models struggle with following sequences of instructions,
hinting at an important lack of robustness of today's language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single Parent Family: A Spectrum of Family Members from a Single
  Pre-Trained Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Habib Hajimolahoseini, Mohammad Hassanpour, Foozhan Ataiefard, Boxing Chen, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel method of Progressive Low Rank Decomposition
(PLRD) tailored for the compression of large language models. Our approach
leverages a pre-trained model, which is then incrementally decompressed to
smaller sizes using progressively lower ranks. This method allows for
significant reductions in computational overhead and energy consumption, as
subsequent models are derived from the original without the need for retraining
from scratch. We detail the implementation of PLRD, which strategically
decreases the tensor ranks, thus optimizing the trade-off between model
performance and resource usage. The efficacy of PLRD is demonstrated through
extensive experiments showing that models trained with PLRD method on only 1B
tokens maintain comparable performance with traditionally trained models while
using 0.1% of the tokens. The versatility of PLRD is highlighted by its ability
to generate multiple model sizes from a single foundational model, adapting
fluidly to varying computational and memory budgets. Our findings suggest that
PLRD could set a new standard for the efficient scaling of LLMs, making
advanced AI more feasible on diverse platforms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Into the Unknown: Generating Geospatial Descriptions for New
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tzuf Paz-Argaman, John Palowitch, Sayali Kulkarni, Reut Tsarfaty, Jason Baldridge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Similar to vision-and-language navigation (VLN) tasks that focus on bridging
the gap between vision and language for embodied navigation, the new Rendezvous
(RVS) task requires reasoning over allocentric spatial relationships
(independent of the observer's viewpoint) using non-sequential navigation
instructions and maps. However, performance substantially drops in new
environments with no training data. Using opensource descriptions paired with
coordinates (e.g., Wikipedia) provides training data but suffers from limited
spatially-oriented text resulting in low geolocation resolution. We propose a
large-scale augmentation method for generating high-quality synthetic data for
new environments using readily available geospatial data. Our method constructs
a grounded knowledge-graph, capturing entity relationships. Sampled entities
and relations (`shop north of school') generate navigation instructions via (i)
generating numerous templates using context-free grammar (CFG) to embed
specific entities and relations; (ii) feeding the entities and relation into a
large language model (LLM) for instruction generation. A comprehensive
evaluation on RVS, showed that our approach improves the 100-meter accuracy by
45.83% on unseen environments. Furthermore, we demonstrate that models trained
with CFG-based augmentation achieve superior performance compared with those
trained with LLM-based augmentation, both in unseen and seen environments.
These findings suggest that the potential advantages of explicitly structuring
spatial information for text-based geospatial reasoning in previously unknown,
can unlock data-scarce scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simulating Financial Market via Large Language Model based Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shen Gao, Yuntao Wen, Minghang Zhu, Jianing Wei, Yuhan Cheng, Qunzi Zhang, Shuo Shang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most economic theories typically assume that financial market participants
are fully rational individuals and use mathematical models to simulate human
behavior in financial markets. However, human behavior is often not entirely
rational and is challenging to predict accurately with mathematical models. In
this paper, we propose \textbf{A}gent-based \textbf{S}imulated
\textbf{F}inancial \textbf{M}arket (ASFM), which first constructs a simulated
stock market with a real order matching system. Then, we propose a large
language model based agent as the stock trader, which contains the profile,
observation, and tool-learning based action module. The trading agent can
comprehensively understand current market dynamics and financial policy
information, and make decisions that align with their trading strategy. In the
experiments, we first verify that the reactions of our ASFM are consistent with
the real stock market in two controllable scenarios. In addition, we also
conduct experiments in two popular economics research directions, and we find
that conclusions drawn in our \model align with the preliminary findings in
economics research. Based on these observations, we believe our proposed ASFM
provides a new paradigm for economic research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BESTOW: Efficient and Streamable Speech Language Model with the Best of
  Two Worlds in GPT and T5 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhehuai Chen, He Huang, Oleksii Hrinchuk, Krishna C. Puvvada, Nithin Rao Koluguri, Piotr Żelasko, Jagadeesh Balam, Boris Ginsburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating speech understanding capabilities into pretrained
large-language models has become a vital research direction (SpeechLLM). The
previous architectures can be categorized as: i) GPT-style, prepend speech
prompts to the text prompts as a sequence of LLM inputs like a decoder-only
model; ii) T5-style, introduce speech cross-attention to each layer of the
pretrained LLMs. We propose BESTOW architecture to bring the BESt features from
TwO Worlds into a single model that is highly efficient and has strong
multitask capabilities. Moreover, there is no clear streaming solution for
either style, especially considering the solution should generalize to speech
multitask. We reformulate streamable SpeechLLM as a read-write policy problem
and unifies the offline and streaming research with BESTOW architecture. Hence
we demonstrate the first open-source SpeechLLM solution that enables Streaming
and Multitask at scale (beyond ASR) at the same time. This streamable solution
achieves very strong performance on a wide range of speech tasks (ASR, AST,
SQA, unseen DynamicSuperb). It is end-to-end optimizable, with lower
training/inference cost, and demonstrates LLM knowledge transferability to
speech.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mining Reasons For And Against Vaccination From Unstructured Data Using
  Nichesourcing and AI Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damián Ariel Furman, Juan Junqueras, Z. Burçe Gümüslü, Edgar Altszyler, Joaquin Navajas, Ophelia Deroy, Justin Sulik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Reasons For and Against Vaccination (RFAV), a dataset for
predicting reasons for and against vaccination, and scientific authorities used
to justify them, annotated through nichesourcing and augmented using GPT4 and
GPT3.5-Turbo. We show how it is possible to mine these reasons in
non-structured text, under different task definitions, despite the high level
of subjectivity involved and explore the impact of artificially augmented data
using in-context learning with GPT4 and GPT3.5-Turbo. We publish the dataset
and the trained models along with the annotation manual used to train
annotators and define the task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages + references and appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Calibrating LLMs with Preference Optimization on Thought Trees for
  Generating Rationale in Science Question Scoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazheng Li, Hainiu Xu, Zhaoyue Sun, Yuxiang Zhou, David West, Cesare Aloisi, Yulan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating rationales that justify scoring decisions has been a promising way
to facilitate explainability in automated scoring systems. However, existing
methods do not match the accuracy of classifier-based methods. Plus, the
generated rationales often contain hallucinated information. To address these
issues, we propose a novel framework capable of generating more faithful
rationales and, more importantly, matching performance with classifier-based
black-box scoring systems. We first mimic the human assessment process by
querying Large Language Models (LLMs) to generate a thought tree. We then
summarise intermediate assessment decisions from each thought tree path for
creating synthetic rationale data and rationale preference data. Finally, we
utilise the generated synthetic data to calibrate LLMs through a two-step
training process: supervised fine-tuning and preference optimization. Extensive
experimental results demonstrate that our framework achieves a 38% assessment
performance improvement in the QWK score compared to prior work while producing
higher-quality rationales, as recognised by human evaluators and LLMs. Our work
sheds light on the effectiveness of performing preference optimization using
synthetic preference data obtained from thought tree paths.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From the Least to the Most: Building a Plug-and-Play Visual Reasoner via
  Data Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanqi Cheng, Jian Guan, Wei Wu, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore multi-step reasoning in vision-language models (VLMs). The problem
is challenging, as reasoning data consisting of multiple steps of visual and
language processing are barely available. To overcome the challenge, we first
introduce a least-to-most visual reasoning paradigm, which interleaves steps of
decomposing a question into sub-questions and invoking external tools for
resolving sub-questions. Based on the paradigm, we further propose a novel data
synthesis approach that can automatically create questions and multi-step
reasoning paths for an image in a bottom-up manner. Our approach divides the
complex synthesis task into a few simple sub-tasks, and (almost entirely)
relies on open-sourced models to accomplish the sub-tasks. Therefore, the
entire synthesis process is reproducible and cost-efficient, and the
synthesized data is quality guaranteed. With the approach, we construct $50$k
visual reasoning examples. Then, we develop a visual reasoner through
supervised fine-tuning, which is capable of generally enhancing the reasoning
abilities of a wide range of existing VLMs in a plug-and-play fashion.
Extensive experiments indicate that the visual reasoner can consistently and
significantly improve four VLMs on four VQA benchmarks. Our code and dataset
are available at https://github.com/steven-ccq/VisualReasoner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactive Topic Models with Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Garima Dhanania, Sheshera Mysore, Chau Minh Pham, Mohit Iyyer, Hamed Zamani, Andrew McCallum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic models are widely used to analyze document collections. While they are
valuable for discovering latent topics in a corpus when analysts are unfamiliar
with the corpus, analysts also commonly start with an understanding of the
content present in a corpus. This may be through categories obtained from an
initial pass over the corpus or a desire to analyze the corpus through a
predefined set of categories derived from a high level theoretical framework
(e.g. political ideology). In these scenarios analysts desire a topic modeling
approach which incorporates their understanding of the corpus while supporting
various forms of interaction with the model. In this work, we present EdTM, as
an approach for label name supervised topic modeling. EdTM models topic
modeling as an assignment problem while leveraging LM/LLM based document-topic
affinities and using optimal transport for making globally coherent
topic-assignments. In experiments, we show the efficacy of our framework
compared to few-shot LLM classifiers, and topic models based on clustering and
LDA. Further, we show EdTM's ability to incorporate various forms of analyst
feedback and while remaining robust to noisy analyst inputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print; Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Paraphrase Types Elicit Prompt Engineering Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Philip Wahle, Terry Ruas, Yang Xu, Bela Gipp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Much of the success of modern language models depends on finding a suitable
prompt to instruct the model. Until now, it has been largely unknown how
variations in the linguistic expression of prompts affect these models. This
study systematically and empirically evaluates which linguistic features
influence models through paraphrase types, i.e., different linguistic changes
at particular positions. We measure behavioral changes for five models across
120 tasks and six families of paraphrases (i.e., morphology, syntax, lexicon,
lexico-syntax, discourse, and others). We also control for other prompt
engineering factors (e.g., prompt length, lexical diversity, and proximity to
training data). Our results show a potential for language models to improve
tasks when their prompts are adapted in specific paraphrase types (e.g., 6.7%
median gain in Mixtral 8x7B; 5.5% in LLaMA 3 8B). In particular, changes in
morphology and lexicon, i.e., the vocabulary used, showed promise in improving
prompts. These findings contribute to developing more robust language models
capable of handling variability in linguistic expression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Untangling the Unrestricted Web: Automatic Identification of
  Multilingual Registers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Henriksson, Amanda Myntti, Anni Eskelinen, Selcen Erten-Johansson, Saara Hellström, Veronika Laippala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article explores deep learning models for the automatic identification
of registers - text varieties such as news reports and discussion forums - in
web-based datasets across 16 languages. Web register (or genre) identification
would provide a robust solution for understanding the content of web-scale
datasets, which have become crucial in computational linguistics. Despite
recent advances, the potential of register classifiers on the noisy web remains
largely unexplored, particularly in multilingual settings and when targeting
the entire unrestricted web. We experiment with a range of deep learning models
using the new Multilingual CORE corpora, which includes 16 languages annotated
using a detailed, hierarchical taxonomy of 25 registers designed to cover the
entire unrestricted web. Our models achieve state-of-the-art results, showing
that a detailed taxonomy in a hierarchical multi-label setting can yield
competitive classification performance. However, all models hit a glass ceiling
at approximately 80% F1 score, which we attribute to the non-discrete nature of
web registers and the inherent uncertainty in labeling some documents. By
pruning ambiguous examples, we improve model performance to over 90%. Finally,
multilingual models outperform monolingual ones, particularly benefiting
languages with fewer training examples and smaller registers. Although a
zero-shot setting decreases performance by an average of 7%, these drops are
not linked to specific registers or languages. Instead, registers show
surprising similarity across languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating the Timescales of Language Processing with EEG and
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Turco, Conor Houghton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores the temporal dynamics of language processing by examining
the alignment between word representations from a pre-trained transformer-based
language model, and EEG data. Using a Temporal Response Function (TRF) model,
we investigate how neural activity corresponds to model representations across
different layers, revealing insights into the interaction between artificial
language models and brain responses during language comprehension. Our analysis
reveals patterns in TRFs from distinct layers, highlighting varying
contributions to lexical and compositional processing. Additionally, we used
linear discriminant analysis (LDA) to isolate part-of-speech (POS)
representations, offering insights into their influence on neural responses and
the underlying mechanisms of syntactic processing. These findings underscore
EEG's utility for probing language processing dynamics with high temporal
resolution. By bridging artificial language models and neural activity, this
study advances our understanding of their interaction at fine timescales.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2024 Conference on Cognitive Computational
  Neuroscience (CCN 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Subtle Differences between Human and Model Languages Using
  Spectrum of Relative Likelihood 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Xu, Yu Wang, Hao An, Zhichen Liu, Yongyuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human and model-generated texts can be distinguished by examining the
magnitude of likelihood in language. However, it is becoming increasingly
difficult as language model's capabilities of generating human-like texts keep
evolving. This study provides a new perspective by using the relative
likelihood values instead of absolute ones, and extracting useful features from
the spectrum-view of likelihood for the human-model text detection task. We
propose a detection procedure with two classification methods, supervised and
heuristic-based, respectively, which results in competitive performances with
previous zero-shot detection methods and a new state-of-the-art on short-text
detection. Our method can also reveal subtle differences between human and
model languages, which find theoretical roots in psycholinguistics studies. Our
code is available at https://github.com/CLCS-SUSTech/FourierGPT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YuLan: An Open-source Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutao Zhu, Kun Zhou, Kelong Mao, Wentong Chen, Yiding Sun, Zhipeng Chen, Qian Cao, Yihan Wu, Yushuo Chen, Feng Wang, Lei Zhang, Junyi Li, Xiaolei Wang, Lei Wang, Beichen Zhang, Zican Dong, Xiaoxue Cheng, Yuhan Chen, Xinyu Tang, Yupeng Hou, Qiangqiang Ren, Xincheng Pang, Shufang Xie, Wayne Xin Zhao, Zhicheng Dou, Jiaxin Mao, Yankai Lin, Ruihua Song, Jun Xu, Xu Chen, Rui Yan, Zhewei Wei, Di Hu, Wenbing Huang, Ze-Feng Gao, Yueguo Chen, Weizheng Lu, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have become the foundation of many applications,
leveraging their extensive capabilities in processing and understanding natural
language. While many open-source LLMs have been released with technical
reports, the lack of training details hinders further research and development.
This paper presents the development of YuLan, a series of open-source LLMs with
$12$ billion parameters. The base model of YuLan is pre-trained on
approximately $1.7$T tokens derived from a diverse corpus, including massive
English, Chinese, and multilingual texts. We design a three-stage pre-training
method to enhance YuLan's overall capabilities. Subsequent phases of training
incorporate instruction-tuning and human alignment, employing a substantial
volume of high-quality synthesized data. To facilitate the learning of complex
and long-tail knowledge, we devise a curriculum-learning framework throughout
across these stages, which helps LLMs learn knowledge in an easy-to-hard
manner. YuLan's training is finished on Jan, 2024 and has achieved performance
on par with state-of-the-art LLMs across various English and Chinese
benchmarks. This paper outlines a comprehensive technical roadmap for
developing LLMs from scratch. Our model and codes are available at
https://github.com/RUC-GSAI/YuLan-Chat.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnomaLLMy -- Detecting anomalous tokens in black-box LLMs through
  low-confidence single-token predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Waligóra Witold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces AnomaLLMy, a novel technique for the automatic
detection of anomalous tokens in black-box Large Language Models (LLMs) with
API-only access. Utilizing low-confidence single-token predictions as a
cost-effective indicator, AnomaLLMy identifies irregularities in model
behavior, addressing the issue of anomalous tokens degrading the quality and
reliability of models. Validated on the cl100k_base dataset, the token set of
GPT-4, AnomaLLMy detected 413 major and 65 minor anomalies, demonstrating the
method's efficiency with just \$24.39 spent in API credits. The insights from
this research are expected to be beneficial for enhancing the robustness of and
accuracy of LLMs, particularly in the development and assessment of tokenizers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for
  Multi-hop Question Answering <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Chu, Jingchang Chen, Qianglong Chen, Haotian Wang, Kun Zhu, Xiyuan Du, Weijiang Yu, Ming Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated strong reasoning capabilities.
Nevertheless, they still suffer from factual errors when tackling
knowledge-intensive tasks. Retrieval-augmented reasoning represents a promising
approach. However, significant challenges still persist, including inaccurate
and insufficient retrieval for complex questions, as well as difficulty in
integrating multi-source knowledge. To address this, we propose Beam
Aggregation Reasoning, BeamAggR, a reasoning framework for knowledge-intensive
multi-hop QA. BeamAggR explores and prioritizes promising answers at each hop
of question. Concretely, we parse the complex questions into trees, which
include atom and composite questions, followed by bottom-up reasoning. For
atomic questions, the LLM conducts reasoning on multi-source knowledge to get
answer candidates. For composite questions, the LLM combines beam candidates,
explores multiple reasoning paths through probabilistic aggregation, and
prioritizes the most promising trajectory. Extensive experiments on four
open-domain multi-hop reasoning datasets show that our method significantly
outperforms SOTA methods by 8.5%. Furthermore, our analysis reveals that
BeamAggR elicits better knowledge collaboration and answer aggregation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable and Domain-General Abstractive Proposition Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Javad Hosseini, Yang Gao, Tim Baumgärtner, Alex Fabrikant, Reinald Kim Amplayo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmenting text into fine-grained units of meaning is important to a wide
range of NLP applications. The default approach of segmenting text into
sentences is often insufficient, especially since sentences are usually complex
enough to include multiple units of meaning that merit separate treatment in
the downstream task. We focus on the task of abstractive proposition
segmentation: transforming text into simple, self-contained, well-formed
sentences. Several recent works have demonstrated the utility of proposition
segmentation with few-shot prompted LLMs for downstream tasks such as
retrieval-augmented grounding and fact verification. However, this approach
does not scale to large amounts of text and may not always extract all the
facts from the input text. In this paper, we first introduce evaluation metrics
for the task to measure several dimensions of quality. We then propose a
scalable, yet accurate, proposition segmentation model. We model proposition
segmentation as a supervised task by training LLMs on existing annotated
datasets and show that training yields significantly improved results. We
further show that by using the fine-tuned LLMs as teachers for annotating large
amounts of multi-domain synthetic distillation data, we can train smaller
student models with results similar to the teacher LLMs. We then demonstrate
that our technique leads to effective domain generalization, by annotating data
in two domains outside the original training data and evaluating on them.
Finally, as a key contribution of the paper, we share an easy-to-use API for
NLP practitioners to use.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NLPerturbator: Studying the <span class="highlight-title">Robust</span>ness of Code LLMs to Natural Language
  Variations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junkai Chen, Zhenhao Li, Xing Hu, Xin Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) achieve promising results in code generation
based on a given natural language description. They have been integrated into
open-source projects and commercial products to facilitate daily coding
activities. The natural language description in the prompt is crucial for LLMs
to comprehend users' requirements. Prior studies uncover that LLMs are
sensitive to the changes in the prompts, including slight changes that look
inconspicuous. However, the natural language descriptions often vary in
real-world scenarios (e.g., different formats, grammar, and wording). Prior
studies on the robustness of LLMs are often based on random perturbations and
such perturbations may not actually happen. In this paper, we conduct a
comprehensive study to investigate how are code LLMs robust to variations of
natural language description in real-world scenarios. We summarize 18
categories of perturbations of natural language and 3 combinations of
co-occurred categories based on our literature review and an online survey with
practitioners. We propose an automated framework, NLPerturbator, which can
perform perturbations of each category given a set of prompts. Through a series
of experiments on code generation using six code LLMs, we find that the
perturbed prompts can decrease the performance of code generation by a
considerable margin (e.g., up to 21.2%, and 4.8% to 6.1% on average). Our study
highlights the importance of enhancing the robustness of LLMs to real-world
variations in the prompts, as well as the essentiality of attentively
constructing the prompts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct Preference Knowledge Distillation for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixing Li, Yuxian Gu, Li Dong, Dequan Wang, Yu Cheng, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of large language models (LLMs), Knowledge Distillation (KD) is
a critical technique for transferring capabilities from teacher models to
student models. However, existing KD methods face limitations and challenges in
distillation of LLMs, including efficiency and insufficient measurement
capabilities of traditional KL divergence. It is shown that LLMs can serve as
an implicit reward function, which we define as a supplement to KL divergence.
In this work, we propose Direct Preference Knowledge Distillation (DPKD) for
LLMs. DPKD utilizes distribution divergence to represent the preference loss
and implicit reward function. We re-formulate KD of LLMs into two stages: first
optimizing and objective consisting of implicit reward and reverse KL
divergence and then improving the preference probability of teacher outputs
over student outputs. We conducted experiments and analysis on various datasets
with LLM parameters ranging from 120M to 13B and demonstrate the broad
applicability and effectiveness of our DPKD approach. Meanwhile, we prove the
value and effectiveness of the introduced implicit reward and output preference
in KD through experiments and theoretical analysis. The DPKD method outperforms
the baseline method in both output response precision and exact match
percentage. Code and data are available at https://aka.ms/dpkd.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Belief Revision: The Adaptability of Large Language Models Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bryan Wilie, Samuel Cahyawijaya, Etsuko Ishii, Junxian He, Pascale Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capability to reason from text is crucial for real-world NLP
applications. Real-world scenarios often involve incomplete or evolving data.
In response, individuals update their beliefs and understandings accordingly.
However, most existing evaluations assume that language models (LMs) operate
with consistent information. We introduce Belief-R, a new dataset designed to
test LMs' belief revision ability when presented with new evidence. Inspired by
how humans suppress prior inferences, this task assesses LMs within the newly
proposed delta reasoning ($\Delta R$) framework. Belief-R features sequences of
premises designed to simulate scenarios where additional information could
necessitate prior conclusions drawn by LMs. We evaluate $\sim$30 LMs across
diverse prompting strategies and found that LMs generally struggle to
appropriately revise their beliefs in response to new information. Further,
models adept at updating often underperformed in scenarios without necessary
updates, highlighting a critical trade-off. These insights underscore the
importance of improving LMs' adaptiveness to changing information, a step
toward more reliable AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case
  Reformulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenlong Deng, Kelong Mao, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal case retrieval for sourcing similar cases is critical in upholding
judicial fairness. Different from general web search, legal case retrieval
involves processing lengthy, complex, and highly specialized legal documents.
Existing methods in this domain often overlook the incorporation of legal
expert knowledge, which is crucial for accurately understanding and modeling
legal cases, leading to unsatisfactory retrieval performance. This paper
introduces KELLER, a legal knowledge-guided case reformulation approach based
on large language models (LLMs) for effective and interpretable legal case
retrieval. By incorporating professional legal knowledge about crimes and law
articles, we enable large language models to accurately reformulate the
original legal case into concise sub-facts of crimes, which contain the
essential information of the case. Extensive experiments on two legal case
retrieval benchmarks demonstrate superior retrieval performance and robustness
on complex legal case queries of KELLER over existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking the Script Barrier in Multilingual Pre-Trained Language Models
  with Transliteration-Based Post-Training Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orgest Xhelili, Yihong Liu, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual pre-trained models (mPLMs) have shown impressive performance on
cross-lingual transfer tasks. However, the transfer performance is often
hindered when a low-resource target language is written in a different script
than the high-resource source language, even though the two languages may be
related or share parts of their vocabularies. Inspired by recent work that uses
transliteration to address this problem, our paper proposes a
transliteration-based post-pretraining alignment (PPA) method aiming to improve
the cross-lingual alignment between languages using diverse scripts. We select
two areal language groups, $\textbf{Mediterranean-Amharic-Farsi}$ and
$\textbf{South+East Asian Languages}$, wherein the languages are mutually
influenced but use different scripts. We apply our method to these language
groups and conduct extensive experiments on a spectrum of downstream tasks. The
results show that after PPA, models consistently outperform the original model
(up to 50% for some tasks) in English-centric transfer. In addition, when we
use languages other than English as sources in transfer, our method obtains
even larger improvements. We will make our code and models publicly available
at \url{https://github.com/cisnlp/Transliteration-PPA}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-Instruct: Generated Visual Instructions for Large Multimodal Model
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Liu, Xin Huang, Jinliang Zheng, Boxiao Liu, Jia Wang, Osamu Yoshie, Yu Liu, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces MM-Instruct, a large-scale dataset of diverse and
high-quality visual instruction data designed to enhance the
instruction-following capabilities of large multimodal models (LMMs). While
existing visual instruction datasets often focus on question-answering, they
struggle to generalize to broader application scenarios such as creative
writing, summarization, or image analysis. To address these limitations, we
propose a novel approach to constructing MM-Instruct that leverages the strong
instruction-following capabilities of existing LLMs to generate novel visual
instruction data from large-scale but conventional image captioning datasets.
MM-Instruct first leverages ChatGPT to automatically generate diverse
instructions from a small set of seed instructions through augmenting and
summarization. It then matches these instructions with images and uses an
open-sourced large language model (LLM) to generate coherent answers to the
instruction-image pairs. The LLM is grounded by the detailed text descriptions
of images in the whole answer generation process to guarantee the alignment of
the instruction data. Moreover, we introduce a benchmark based on the generated
instruction data to evaluate the instruction-following capabilities of existing
LMMs. We demonstrate the effectiveness of MM-Instruct by training a LLaVA-1.5
model on the generated data, denoted as LLaVA-Instruct, which exhibits
significant improvements in instruction-following capabilities compared to
LLaVA-1.5 models. The MM-Instruct dataset, benchmark, and pre-trained models
are available at https://github.com/jihaonew/MM-Instruct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Dataset and models are available at
  https://github.com/jihaonew/MM-Instruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Message du troisi{è}me type : irruption d'un tiers dans un dialogue en
  ligne 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ludovic Tanguy, Céline Poudat, Lydia-Mai Ho-Dac
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our study focuses on Wikipedia talk pages, from a global perspective
analyzing contributors' behaviors in online interactions. Using a corpus
comprising all Wikipedia talk pages in French, totaling more than 300,000
discussion threads, we examine how discussions with more than two participants
(multiparty conversation) unfold and we specifically investigate the role of a
third participant's intervention when two Wikipedians have already initiated an
exchange. In this regard, we concentrate on the sequential structure of these
interactions in terms of articulation among different participants and aim to
specify this third message by exploring its lexical particularities, while also
proposing an initial typology of the third participant's message role and how
it aligns with preceding messages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in French language. JADT 2024 - 17es Journ{\'e}es internationales
  d'Analyse statistique des Donn{\'e}es Textuelles, SeSLa (S{\'e}minaire des
  Sciences du Langage de l'UCLouvain -- Site Saint-Louis); LASLA (Laboratoire
  d'Analyse statistique des Langues anciennes de l'Universit{\'e} de
  Li{\`e}ge), 2024, Bruxelles, Belgique</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Le sens de la famille : analyse du vocabulaire de la parent{é} par les
  plongements de mots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ludovic Tanguy, Cécile Fabre, Nabil Hathout, Lydia-Mai Ho-Dac
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose a corpus analysis of an area of the French lexicon
that is both dense and highly structured: the vocabulary of family
relationships. Starting with a lexicon of 25 nouns designating the main
relationships (son, cousin, mother, grandfather, sister-in-law etc.), we
examine how these terms are positioned in relation to each other through
distributional analyses based on the use of these terms in corpora. We show
that distributional information can capture certain features that organize this
vocabulary (descent, alliance, siblings, genre), in ways that vary according to
the different corpora compared.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in French language. JADT 2024 - 17es Journ{\'e}es internationales
  d'Analyse statistique des Donn{\'e}es Textuelles, SeSLa (S{\'e}minaire des
  Sciences du Langage de l'UCLouvain -- Site Saint-Louis), 2024, Bruxelles,
  Belgique</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Quantification in Large Language Models Through Convex Hull
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ferhat Ozgur Catak, Murat Kuzlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty quantification approaches have been more critical in large
language models (LLMs), particularly high-risk applications requiring reliable
outputs. However, traditional methods for uncertainty quantification, such as
probabilistic models and ensemble techniques, face challenges when applied to
the complex and high-dimensional nature of LLM-generated outputs. This study
proposes a novel geometric approach to uncertainty quantification using convex
hull analysis. The proposed method leverages the spatial properties of response
embeddings to measure the dispersion and variability of model outputs. The
prompts are categorized into three types, i.e., `easy', `moderate', and
`confusing', to generate multiple responses using different LLMs at varying
temperature settings. The responses are transformed into high-dimensional
embeddings via a BERT model and subsequently projected into a two-dimensional
space using Principal Component Analysis (PCA). The Density-Based Spatial
Clustering of Applications with Noise (DBSCAN) algorithm is utilized to cluster
the embeddings and compute the convex hull for each selected cluster. The
experimental results indicate that the uncertainty of the model for LLMs
depends on the prompt complexity, the model, and the temperature setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Less is More: Accurate Speech Recognition & Translation without
  Web-Scale Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishna C. Puvvada, Piotr Żelasko, He Huang, Oleksii Hrinchuk, Nithin Rao Koluguri, Kunal Dhawan, Somshubra Majumdar, Elena Rastorgueva, Zhehuai Chen, Vitaly Lavrukhin, Jagadeesh Balam, Boris Ginsburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in speech recognition and translation rely on hundreds of
thousands of hours of Internet speech data. We argue that state-of-the art
accuracy can be reached without relying on web-scale data. Canary -
multilingual ASR and speech translation model, outperforms current
state-of-the-art models - Whisper, OWSM, and Seamless-M4T on English, French,
Spanish, and German languages, while being trained on an order of magnitude
less data than these models. Three key factors enables such data-efficient
model: (1) a FastConformer-based attention encoder-decoder architecture (2)
training on synthetic data generated with machine translation and (3) advanced
training techniques: data-balancing, dynamic data blending, dynamic bucketing
and noise-robust fine-tuning. The model, weights, and training code will be
open-sourced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DECOR: Improving Coherence in L2 English Writing with a Novel Benchmark
  for Incoherence Detection, Reasoning, and Rewriting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanming Zhang, Anthony Diaz, Zixun Chen, Qingyang Wu, Kun Qian, Erik Voss, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coherence in writing, an aspect that second-language (L2) English learners
often struggle with, is crucial in assessing L2 English writing. Existing
automated writing evaluation systems primarily use basic surface linguistic
features to detect coherence in writing. However, little effort has been made
to correct the detected incoherence, which could significantly benefit L2
language learners seeking to improve their writing. To bridge this gap, we
introduce DECOR, a novel benchmark that includes expert annotations for
detecting incoherence in L2 English writing, identifying the underlying
reasons, and rewriting the incoherent sentences. To our knowledge, DECOR is the
first coherence assessment dataset specifically designed for improving L2
English writing, featuring pairs of original incoherent sentences alongside
their expert-rewritten counterparts. Additionally, we fine-tuned models to
automatically detect and rewrite incoherence in student essays. We find that
incorporating specific reasons for incoherence during fine-tuning consistently
improves the quality of the rewrites, achieving a result that is favored in
both automatic and human evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 5 figures, 20 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing and Evaluating Multi-Chatbot Interface for Human-AI
  Communication: Preliminary Findings from a Persuasion Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sion Yoon, Tae Eun Kim, Yoo Jung Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dynamics of human-AI communication have been reshaped by language models
such as ChatGPT. However, extant research has primarily focused on dyadic
communication, leaving much to be explored regarding the dynamics of human-AI
communication in group settings. The availability of multiple language model
chatbots presents a unique opportunity for scholars to better understand the
interaction between humans and multiple chatbots. This study examines the
impact of multi-chatbot communication in a specific persuasion setting:
promoting charitable donations. We developed an online environment that enables
multi-chatbot communication and conducted a pilot experiment utilizing two
GPT-based chatbots, Save the Children and UNICEF chatbots, to promote
charitable donations. In this study, we present our development process of the
multi-chatbot interface and present preliminary findings from a pilot
experiment. Analysis of qualitative and quantitative feedback are presented,
and limitations are addressed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking Varied Perspectives: A Persona-Based Multi-Agent Framework
  with Debate-Driven Text Planning for Argument Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Hu, Hou Pong Chan, Jing Li, Yu Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Writing persuasive arguments is a challenging task for both humans and
machines. It entails incorporating high-level beliefs from various perspectives
on the topic, along with deliberate reasoning and planning to construct a
coherent narrative. Current language models often generate surface tokens
autoregressively, lacking explicit integration of these underlying controls,
resulting in limited output diversity and coherence. In this work, we propose a
persona-based multi-agent framework for argument writing. Inspired by the human
debate, we first assign each agent a persona representing its high-level
beliefs from a unique perspective, and then design an agent interaction process
so that the agents can collaboratively debate and discuss the idea to form an
overall plan for argument writing. Such debate process enables fluid and
nonlinear development of ideas. We evaluate our framework on argumentative
essay writing. The results show that our framework can generate more diverse
and persuasive arguments through both automatic and human evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IDT: Dual-Task Adversarial Attacks for Privacy Protection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Faustini, Shakila Mahjabin Tonni, Annabelle McIver, Qiongkai Xu, Mark Dras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language processing (NLP) models may leak private information in
different ways, including membership inference, reconstruction or attribute
inference attacks. Sensitive information may not be explicit in the text, but
hidden in underlying writing characteristics. Methods to protect privacy can
involve using representations inside models that are demonstrated not to detect
sensitive attributes or -- for instance, in cases where users might not trust a
model, the sort of scenario of interest here -- changing the raw text before
models can have access to it. The goal is to rewrite text to prevent someone
from inferring a sensitive attribute (e.g. the gender of the author, or their
location by the writing style) whilst keeping the text useful for its original
intention (e.g. the sentiment of a product review). The few works tackling this
have focused on generative techniques. However, these often create extensively
different texts from the original ones or face problems such as mode collapse.
This paper explores a novel adaptation of adversarial attack techniques to
manipulate a text to deceive a classifier w.r.t one task (privacy) whilst
keeping the predictions of another classifier trained for another task
(utility) unchanged. We propose IDT, a method that analyses predictions made by
auxiliary and interpretable models to identify which tokens are important to
change for the privacy task, and which ones should be kept for the utility
task. We evaluate different datasets for NLP suitable for different tasks.
Automatic and human evaluations show that IDT retains the utility of text,
while also outperforming existing methods when deceiving a classifier w.r.t
privacy task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixture of In-Context Experts Enhance LLMs' Long Context Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhan Lin, Ang Lv, Yuhan Chen, Chen Zhu, Yang Song, Hengshu Zhu, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many studies have revealed that large language models (LLMs) exhibit uneven
awareness of different contextual positions.Their limited context awareness can
lead to overlooking critical information and subsequent task failures. While
several approaches have been proposed to enhance LLMs' context awareness,
achieving both effectiveness and efficiency remains challenging.In this paper,
for LLMs utilizing RoPE as position embeddings, we introduce a novel method
called ``Mixture of In-Context Experts'' (MoICE) to address this challenge.
MoICE comprises two key components: a router integrated into each attention
head within LLMs and a lightweight router-only training optimization strategy:
(1) MoICE views each RoPE angle as an `in-context' expert, demonstrated to be
capable of directing the attention of a head to specific contextual positions.
Consequently, each attention head flexibly processes tokens using multiple RoPE
angles dynamically selected by the router to attend to the needed positions.
This approach mitigates the risk of overlooking essential contextual
information. (2) The router-only training strategy entails freezing LLM
parameters and exclusively updating routers for only a few steps. When applied
to open-source LLMs including Llama and Mistral, MoICE surpasses prior methods
across multiple tasks on long context understanding and generation, all while
maintaining commendable inference efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SK-VQA: Synthetic Knowledge Generation at Scale for Training
  Context-Augmented Multimodal LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Su, Man Luo, Kris W Pan, Tien Pei Chou, Vasudev Lal, Phillip Howard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data generation has gained significant attention recently for its
utility in training large vision and language models. However, the application
of synthetic data to the training of multimodal context-augmented generation
systems has been relatively unexplored. This gap in existing work is important
because existing vision and language models (VLMs) are not trained specifically
for context-augmented generation. Resources for adapting such models are
therefore crucial for enabling their use in retrieval-augmented generation
(RAG) settings, where a retriever is used to gather relevant information that
is then subsequently provided to a generative model via context augmentation.
To address this challenging problem, we generate SK-VQA: a large synthetic
multimodal dataset containing over 2 million question-answer pairs which
require external knowledge to determine the final answer. Our dataset is both
larger and significantly more diverse than existing resources of its kind,
possessing over 11x more unique questions and containing images from a greater
variety of sources than previously-proposed datasets. Through extensive
experiments, we demonstrate that our synthetic dataset can not only serve as a
challenging benchmark, but is also highly effective for adapting existing
generative multimodal models for context-augmented generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoMix: Automatically Mixing Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12963v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12963v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranjal Aggarwal, Aman Madaan, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, Shyam Upadhyay, Manaal Faruqui,  Mausam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are now available from cloud API providers in
various sizes and configurations. While this diversity offers a broad spectrum
of choices, effectively leveraging the options to optimize computational cost
and performance remains challenging. In this work, we present Automix, an
approach that strategically routes queries to larger LMs, based on the
approximate correctness of outputs from a smaller LM. Central to Automix are
two key technical contributions. First, it has a few-shot self-verification
mechanism, which estimates the reliability of its own outputs without requiring
extensive training. Second, given that self-verification can be noisy, it
employs a POMDP based router that can effectively select an appropriately sized
model, based on answer confidence. Experiments across five language models and
five challenging datasets show that Automix consistently surpasses strong
baselines, reducing computational cost by over 50% for comparable performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally. Work started and partly
  done during Aman's internship at Google. This version adds results on
  additional models and datasets</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MBIAS: Mitigating Bias in Large Language Models While Retaining Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11290v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11290v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaina Raza, Ananya Raval, Veronica Chatrath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of Large Language Models (LLMs) in diverse applications
necessitates an assurance of safety without compromising the contextual
integrity of the generated content. Traditional approaches, including
safety-specific fine-tuning or adversarial testing, often yield safe outputs at
the expense of contextual meaning. This can result in a diminished capacity to
handle nuanced aspects of bias and toxicity, such as underrepresentation or
negative portrayals across various demographics. To address these challenges,
we introduce MBIAS, an LLM framework carefully instruction fine-tuned on a
custom dataset designed specifically for safety interventions. MBIAS is
designed to significantly reduce biases and toxic elements in LLM outputs while
preserving the main information. This work also details our further use of
LLMs: as annotator under human supervision and as evaluator of generated
content. Empirical analysis reveals that MBIAS achieves a reduction in bias and
toxicity by over 30\% in standard evaluations, and by more than 90\% in diverse
demographic tests, highlighting the robustness of our approach. We make the
dataset and the fine-tuned model available to the research community for
further investigation and ensure reproducibility. The code for this project can
be accessed here https://github.com/shainarazavi/MBIAS/tree/main.
  Warning: This paper contains examples that may be offensive or upsetting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MKRAG: Medical Knowledge Retrieval Augmented Generation for Medical
  Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16035v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16035v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Shi, Shaochen Xu, Tianze Yang, Zhengliang Liu, Tianming Liu, Xiang Li, Ninghao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), although powerful in general domains, often
perform poorly on domain-specific tasks like medical question answering (QA).
Moreover, they tend to function as "black-boxes," making it challenging to
modify their behavior. To address the problem, our study delves into retrieval
augmented generation (RAG), aiming to improve LLM responses without the need
for fine-tuning or retraining. Specifically, we propose a comprehensive
retrieval strategy to extract medical facts from an external knowledge base,
and then inject them into the query prompt for LLMs. Focusing on medical QA
using the MedQA-SMILE dataset, we evaluate the impact of different retrieval
models and the number of facts provided to the LLM. Notably, our
retrieval-augmented Vicuna-7B model exhibited an accuracy improvement from
44.46% to 48.54%. This work underscores the potential of RAG to enhance LLM
performance, offering a practical approach to mitigate the challenges of
black-box LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AMIA 2024 Annual Symposium</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs and Memorization: On Quality and Specificity of Copyright
  Compliance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix B Mueller, Rebekka Görge, Anna K Bernzen, Janna C Pirk, Maximilian Poretschkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Memorization in large language models (LLMs) is a growing concern. LLMs have
been shown to easily reproduce parts of their training data, including
copyrighted work. This is an important problem to solve, as it may violate
existing copyright laws as well as the European AI Act. In this work, we
propose a systematic analysis to quantify the extent of potential copyright
infringements in LLMs using European law as an example. Unlike previous work,
we evaluate instruction-finetuned models in a realistic end-user scenario. Our
analysis builds on a proposed threshold of 160 characters, which we borrow from
the German Copyright Service Provider Act and a fuzzy text matching algorithm
to identify potentially copyright-infringing textual reproductions. The
specificity of countermeasures against copyright infringement is analyzed by
comparing model behavior on copyrighted and public domain data. We investigate
what behaviors models show instead of producing protected text (such as refusal
or hallucination) and provide a first legal assessment of these behaviors. We
find that there are huge differences in copyright compliance, specificity, and
appropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous
perform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing
a particularly low absolute number of potential copyright violations. Code will
be published soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Small and Fast BERT for Chinese Medical Punctuation Restoration <span class="chip">INTERSPEECH 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12568v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12568v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongtao Ling, Yutao Lai, Lei Chen, Shilei Huang, Yi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In clinical dictation, utterances after automatic speech recognition (ASR)
without explicit punctuation marks may lead to the misunderstanding of dictated
reports. To give a precise and understandable clinical report with ASR,
automatic punctuation restoration is required. Considering a practical
scenario, we propose a fast and light pre-trained model for Chinese medical
punctuation restoration based on 'pretraining and fine-tuning' paradigm. In
this work, we distill pre-trained models by incorporating supervised
contrastive learning and a novel auxiliary pre-training task (Punctuation Mark
Prediction) to make it well-suited for punctuation restoration. Our experiments
on various distilled models reveal that our model can achieve 95% performance
while 10% model size relative to state-of-the-art Chinese RoBERTa.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, Accepted by INTERSPEECH 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Speculative Inference of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14105v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14105v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadav Timor, Jonathan Mamou, Daniel Korat, Moshe Berchansky, Oren Pereg, Moshe Wasserblat, Tomer Galanti, Michal Gordon, David Harel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accelerating the inference of large language models (LLMs) is an important
challenge in artificial intelligence. This paper introduces distributed
speculative inference (DSI), a novel distributed inference algorithm that is
provably faster than speculative inference (SI) [leviathan2023fast,
chen2023accelerating, miao2023specinfer] and traditional autoregressive
inference (non-SI). Like other SI algorithms, DSI works on frozen LLMs,
requiring no training or architectural modifications, and it preserves the
target distribution.
  Prior studies on SI have demonstrated empirical speedups (compared to non-SI)
but require a fast and accurate drafter LLM. In practice, off-the-shelf LLMs
often do not have matching drafters that are sufficiently fast and accurate. We
show a gap: SI gets slower than non-SI when using slower or less accurate
drafters. We close this gap by proving that DSI is faster than both SI and
non-SI given any drafters. By orchestrating multiple instances of the target
and drafters, DSI is not only faster than SI but also supports LLMs that cannot
be accelerated with SI.
  Our simulations show speedups of off-the-shelf LLMs in realistic settings:
DSI is 1.29-1.92x faster than SI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How well ChatGPT understand Malaysian English? An Evaluation on Named
  Entity Recognition and Relation Extraction <span class="chip">EMNLP
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11583v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11583v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohan Raj Chanthran, Lay-Ki Soon, Huey Fang Ong, Bhawani Selvaretnam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, ChatGPT has attracted a lot of interest from both researchers and
the general public. While the performance of ChatGPT in named entity
recognition and relation extraction from Standard English texts is
satisfactory, it remains to be seen if it can perform similarly for Malaysian
English. Malaysian English is unique as it exhibits morphosyntactic and
semantical adaptation from local contexts. In this study, we assess ChatGPT's
capability in extracting entities and relations from the Malaysian English News
(MEN) dataset. We propose a three-step methodology referred to as
\textbf{\textit{educate-predict-evaluate}}. The performance of ChatGPT is
assessed using F1-Score across 18 unique prompt settings, which were carefully
engineered for a comprehensive review. From our evaluation, we found that
ChatGPT does not perform well in extracting entities from Malaysian English
news articles, with the highest F1-Score of 0.497. Further analysis shows that
the morphosyntactic adaptation in Malaysian English caused the limitation.
However, interestingly, this morphosyntactic adaptation does not impact the
performance of ChatGPT for relation extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Generation, Evaluation & Metrics (GEM) Workshop at EMNLP
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are LLM-based Evaluators Confusing NLG Quality Criteria? <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12055v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12055v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Hu, Mingqi Gao, Sen Hu, Yang Zhang, Yicheng Chen, Teng Xu, Xiaojun Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Some prior work has shown that LLMs perform well in NLG evaluation for
different tasks. However, we discover that LLMs seem to confuse different
evaluation criteria, which reduces their reliability. For further verification,
we first consider avoiding issues of inconsistent conceptualization and vague
expression in existing NLG quality criteria themselves. So we summarize a clear
hierarchical classification system for 11 common aspects with corresponding
different criteria from previous studies involved. Inspired by behavioral
testing, we elaborately design 18 types of aspect-targeted perturbation attacks
for fine-grained analysis of the evaluation behaviors of different LLMs. We
also conduct human annotations beyond the guidance of the classification system
to validate the impact of the perturbations. Our experimental results reveal
confusion issues inherent in LLMs, as well as other noteworthy phenomena, and
necessitate further research and improvements for LLM-based evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NoteChat: A <span class="highlight-title">Dataset</span> of Synthetic Doctor-Patient Conversations
  Conditioned on Clinical Notes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15959v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15959v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junda Wang, Zonghai Yao, Zhichao Yang, Huixue Zhou, Rumeng Li, Xun Wang, Yucheng Xu, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce NoteChat, a novel cooperative multi-agent framework leveraging
Large Language Models (LLMs) to generate patient-physician dialogues. NoteChat
embodies the principle that an ensemble of role-specific LLMs, through
structured role-play and strategic prompting, can perform their assigned roles
more effectively. The synergy among these role-playing LLMs results in a
cohesive and efficient dialogue generation. Evaluation on MTS-dialogue, a
benchmark dataset for patient-physician dialogues-note pairs, shows that models
trained with the augmented synthetic patient-physician dialogues by NoteChat
outperforms other state-of-the-art models for generating clinical notes. Our
comprehensive automatic and human evaluation demonstrates that NoteChat
substantially surpasses state-of-the-art models like ChatGPT and GPT-4 up to
22.78% by domain experts in generating superior synthetic patient-physician
dialogues based on clinical notes. NoteChat has the potential to engage
patients directly and help clinical documentation, a leading cause of physician
burnout.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning
  and Professional Question Answering Capability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17887v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17887v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junda Wang, Zhichao Yang, Zonghai Yao, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated a remarkable potential in
medical knowledge acquisition and question-answering. However, LLMs can
potentially hallucinate and yield factually incorrect outcomes, even with
domain-specific pretraining. Previously, retrieval augmented generation (RAG)
has limited success in addressing hallucinations. Unlike previous methods in
RAG where the retrieval model was trained separately from the LLM, we introduce
JMLR (for Jointly trains LLM and information Retrieval) during the fine-tuning
phase. The synchronized training mechanism enhances JMLR's ability to retrieve
clinical guidelines and leverage medical knowledge to reason and answer
questions and reduces the demand for computational resources. We evaluated JMLR
on the important medical question-answering application. Our experimental
results demonstrate that JMLR-13B (70.5%) outperforms a previous
state-of-the-art open-source model using conventional pre-training and
fine-tuning Meditron-70B (68.9%) and Llama2-13B with RAG (67.7%) on a medical
question-answering dataset. Comprehensive evaluations reveal JMLR-13B enhances
reasoning quality and reduces hallucinations better than Claude3-Opus.
Additionally, JMLR-13B (148 GPU hours) also trains much faster than
Meditron-70B (42630 GPU hours). Through this work, we provide a new and
efficient knowledge enhancement method for healthcare, demonstrating the
potential of integrating retrieval and LLM training for medical
question-answering systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LatentExplainer: Explaining Latent Representations in Deep <span class="highlight-title">Generative</span>
  Models with Multi-modal Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14862v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14862v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengdan Zhu, Raasikh Kanjiani, Jiahui Lu, Andrew Choi, Qirui Ye, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models like VAEs and diffusion models have advanced various
generation tasks by leveraging latent variables to learn data distributions and
generate high-quality samples. Despite the field of explainable AI making
strides in interpreting machine learning models, understanding latent variables
in generative models remains challenging. This paper introduces
LatentExplainer, a framework for automatically generating semantically
meaningful explanations of latent variables in deep generative models.
LatentExplainer tackles three main challenges: inferring the meaning of latent
variables, aligning explanations with inductive biases, and handling varying
degrees of explainability. By perturbing latent variables and interpreting
changes in generated data, the framework provides a systematic approach to
understanding and controlling the data generation process, enhancing the
transparency and interpretability of deep generative models. We evaluate our
proposed method on several real-world and synthetic datasets, and the results
demonstrate superior performance in generating high-quality explanations of
latent variables.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Data Augmentation Framework for Low-Resource Multi-Domain
  Dialogue Generation <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09881v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09881v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongkang Liu, Ercong Nie, Shi Feng, Zheng Hua, Zifeng Ding, Daling Wang, Yifei Zhang, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current state-of-the-art dialogue systems heavily rely on extensive training
datasets. However, challenges arise in domains where domain-specific training
datasets are insufficient or entirely absent. To tackle this challenge, we
propose a novel data \textbf{A}ugmentation framework for
\textbf{M}ulti-\textbf{D}omain \textbf{D}ialogue \textbf{G}eneration, referred
to as \textbf{AMD$^2$G}. The AMD$^2$G framework consists of a data augmentation
process and a two-stage training approach: domain-agnostic training and domain
adaptation training. We posit that domain corpora are a blend of
domain-agnostic and domain-specific features, with certain representation
patterns shared among diverse domains. Domain-agnostic training aims to enable
models to learn these common expressive patterns. To construct domain-agnostic
dialogue corpora, we employ a \textit{\textbf{de-domaining}} data processing
technique used to remove domain-specific features. By mitigating the effects of
domain-specific features, the model trained on the de-domained corpora can
effectively learn common expression patterns in different domains.
Subsequently, we adapt the learned domain-agnostic features to the target
domain through domain adaptation training. We conduct experiments on Chinese
dialogue datasets from five different domains and show that AMD$^2$G achieves
superior performance compared to both direct training on the target domain
corpus and collective training on all five domain corpora. Our work underscores
AMD$^2$G as a viable alternative solution for low-resource multi-domain
dialogue generation. Code and data associated with our work are available on
GitHub repository$^{\text 1}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17pages,ECML-PKDD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do prompt positions really matter? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14493v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14493v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Mao, Stuart E. Middleton, Mahesan Niranjan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt-based models have gathered a lot of attention from researchers due to
their remarkable advancements in the fields of zero-shot and few-shot learning.
Developing an effective prompt template plays a critical role. However, prior
studies have mainly focused on prompt vocabulary searching or embedding
initialization within a predefined template with the prompt position fixed. In
this empirical study, we conduct the most comprehensive analysis to date of
prompt position for diverse Natural Language Processing (NLP) tasks. Our
findings quantify the substantial impact prompt position has on model
performance. We observe that the prompt positions used in prior studies are
often sub-optimal, and this observation is consistent even in widely used
instruction-tuned models. These findings suggest prompt position optimisation
as a valuable research direction to augment prompt engineering methodologies
and prompt position-aware instruction tuning as a potential way to build more
robust models in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Airport Tower Command Recognition: Integrating
  Squeeze-and-Excitation and Broadcasted Residual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanxi Lin, Tonglin Zhou, Yang Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate recognition of aviation commands is vital for flight safety and
efficiency, as pilots must follow air traffic control instructions precisely.
This paper addresses challenges in speech command recognition, such as noisy
environments and limited computational resources, by advancing keyword spotting
technology. We create a dataset of standardized airport tower commands,
including routine and emergency instructions. We enhance broadcasted residual
learning with squeeze-and-excitation and time-frame frequency-wise
squeeze-and-excitation techniques, resulting in our BC-SENet model. This model
focuses on crucial information with fewer parameters. Our tests on five keyword
spotting models, including BC-SENet, demonstrate superior accuracy and
efficiency. These findings highlight the effectiveness of our model
advancements in improving speech command recognition for aviation safety and
efficiency in noisy, high-stakes environments. Additionally, BC-SENet shows
comparable performance on the common Google Speech Command dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IALP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19232v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19232v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekaterina Taktasheva, Maxim Bazhukov, Kirill Koncha, Alena Fenogenova, Ekaterina Artemova, Vladislav Mikhailov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Minimal pairs are a well-established approach to evaluating the grammatical
knowledge of language models. However, existing resources for minimal pairs
address a limited number of languages and lack diversity of language-specific
grammatical phenomena. This paper introduces the Russian Benchmark of
Linguistic Minimal Pairs (RuBLiMP), which includes 45k pairs of sentences that
differ in grammaticality and isolate a morphological, syntactic, or semantic
phenomenon. In contrast to existing benchmarks of linguistic minimal pairs,
RuBLiMP is created by applying linguistic perturbations to automatically
annotated sentences from open text corpora and carefully curating test data. We
describe the data collection protocol and present the results of evaluating 25
language models in various scenarios. We find that the widely used language
models for Russian are sensitive to morphological and agreement-oriented
contrasts but fall behind humans on phenomena requiring understanding of
structural relations, negation, transitivity, and tense. RuBLiMP, the codebase,
and other materials are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in
  Large Language Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17667v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17667v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Haotian Wang, Ming Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grasping the concept of time is a fundamental facet of human cognition,
indispensable for truly comprehending the intricacies of the world. Previous
studies typically focus on specific aspects of time, lacking a comprehensive
temporal reasoning benchmark. To address this, we propose TimeBench, a
comprehensive hierarchical temporal reasoning benchmark that covers a broad
spectrum of temporal reasoning phenomena. TimeBench provides a thorough
evaluation for investigating the temporal reasoning capabilities of large
language models. We conduct extensive experiments on GPT-4, LLaMA2, and other
popular LLMs under various settings. Our experimental results indicate a
significant performance gap between the state-of-the-art LLMs and humans,
highlighting that there is still a considerable distance to cover in temporal
reasoning. Besides, LLMs exhibit capability discrepancies across different
reasoning categories. Furthermore, we thoroughly analyze the impact of multiple
aspects on temporal reasoning and emphasize the associated challenges. We
aspire for TimeBench to serve as a comprehensive benchmark, fostering research
in temporal reasoning. Resources are available at:
https://github.com/zchuz/TimeBench
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A synthetic data approach for domain generalization of NLI models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Javad Hosseini, Andrey Petrov, Alex Fabrikant, Annie Louis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Inference (NLI) remains an important benchmark task for
LLMs. NLI datasets are a springboard for transfer learning to other semantic
tasks, and NLI models are standard tools for identifying the faithfulness of
model-generated text. There are several large scale NLI datasets today, and
models have improved greatly by hill-climbing on these collections. Yet their
realistic performance on out-of-distribution/domain data is less
well-understood. We explore the opportunity for synthetic high-quality datasets
to adapt NLI models for zero-shot use in downstream applications across new and
unseen text domains. We demonstrate a new approach for generating NLI data in
diverse domains and lengths, so far not covered by existing training sets. The
resulting examples have meaningful premises, the hypotheses are formed in
creative ways rather than simple edits to a few premise tokens, and the labels
have high accuracy. We show that models trained on this data ($685$K synthetic
examples) have the best generalization to completely new downstream test
settings. On the TRUE benchmark, a T5-small model trained with our data
improves around $7\%$ on average compared to training on the best alternative
dataset. The improvements are more pronounced for smaller models, while still
meaningful on a T5 XXL model. We also demonstrate gains on test sets when
in-domain training data is augmented with our domain-general synthetic data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chitchat as Interference: Adding User Backstories to Task-Oriented
  Dialogues <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15248v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15248v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armand Stricker, Patrick Paroubek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During task-oriented dialogues (TODs), human users naturally introduce
chitchat that is beyond the immediate scope of the task, interfering with the
flow of the conversation. To address this issue without the need for expensive
manual data creation, we use few-shot prompting with Llama-2-70B to enhance the
MultiWOZ dataset with user backstories, a typical example of chitchat
interference in TODs. We assess the impact of this addition by testing two
models: one trained solely on TODs and another trained on TODs with a
preliminary chitchat interaction. Our analysis demonstrates that our enhanced
dataset poses a challenge for these systems. Moreover, we demonstrate that our
dataset can be effectively used for training purposes, enabling a system to
consistently acknowledge the user's backstory while also successfully moving
the task forward in the same turn, as confirmed by human evaluation. These
findings highlight the benefits of generating novel chitchat-TOD scenarios to
test TOD systems more thoroughly and improve their resilience to natural user
interferences
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted @ LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MathChat: Converse to Tackle Challenging Math Problems with LLM Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01337v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01337v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Employing Large Language Models (LLMs) to address mathematical problems is an
intriguing research endeavor, considering the abundance of math problems
expressed in natural language across numerous science and engineering fields.
LLMs, with their generalized ability, are used as a foundation model to build
AI agents for different tasks. In this paper, we study the effectiveness of
utilizing LLM agents to solve math problems through conversations. We propose
MathChat, a conversational problem-solving framework designed for math
problems. MathChat consists of an LLM agent and a user proxy agent which is
responsible for tool execution and additional guidance. This synergy
facilitates a collaborative problem-solving process, where the agents engage in
a dialogue to solve the problems. We perform evaluation on difficult high
school competition problems from the MATH dataset. Utilizing Python, we show
that MathChat can further improve previous tool-using prompting methods by 6%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Approach to Emotion Detection and Task-Oriented Dialogue
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13789v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13789v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armand Stricker, Patrick Paroubek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In current text-based task-oriented dialogue (TOD) systems, user emotion
detection (ED) is often overlooked or is typically treated as a separate and
independent task, requiring additional training. In contrast, our work
demonstrates that seamlessly unifying ED and TOD modeling brings about mutual
benefits, and is therefore an alternative to be considered. Our method consists
in augmenting SimpleToD, an end-to-end TOD system, by extending belief state
tracking to include ED, relying on a single language model. We evaluate our
approach using GPT-2 and Llama-2 on the EmoWOZ benchmark, a version of MultiWOZ
annotated with emotions. Our results reveal a general increase in performance
for ED and task results. Our findings also indicate that user emotions provide
useful contextual conditioning for system responses, and can be leveraged to
further refine responses in terms of empathy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted @ IWSDS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishabh Maheshwary, Vikas Yadav, Hoang Nguyen, Khyati Mahajan, Sathwik Tejaswi Madhusudhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction finetuning (IFT) is critical for aligning Large Language Models
(LLMs) to follow instructions. While many effective IFT datasets have been
introduced recently, they predominantly focus on high-resource languages like
English. To better align LLMs across a broad spectrum of languages and tasks,
we propose a fully synthetic, novel taxonomy (Evol) guided Multilingual,
Multi-turn instruction finetuning dataset, called M2Lingual. It is constructed
by first selecting a diverse set of seed examples and then utilizing the
proposed Evol taxonomy to convert these seeds into complex and challenging
multi-turn instructions. We demonstrate the effectiveness of M2Lingual by
training LLMs of varying sizes and showcasing the enhanced performance across a
diverse set of languages. We contribute the 2 step Evol taxonomy with the
guided generation code: https://github.com/ServiceNow/M2Lingual, as well as the
first fully synthetic, general and task-oriented, multi-turn, multilingual
dataset built with Evol - M2Lingual:
https://huggingface.co/datasets/ServiceNow-AI/ M2Lingual - containing 182K
total IFT pairs, covering 70 languages and 17+ NLP tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity
  Text Embeddings Through Self-Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03216v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03216v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a new embedding model, called M3-Embedding, which
is distinguished for its versatility in Multi-Linguality, Multi-Functionality,
and Multi-Granularity. It can support more than 100 working languages, leading
to new state-of-the-art performances on multi-lingual and cross-lingual
retrieval tasks. It can simultaneously perform the three common retrieval
functionalities of embedding model: dense retrieval, multi-vector retrieval,
and sparse retrieval, which provides a unified model foundation for real-world
IR applications. It is able to process inputs of different granularities,
spanning from short sentences to long documents of up to 8192 tokens. The
effective training of M3-Embedding involves the following technical
contributions. We propose a novel self-knowledge distillation approach, where
the relevance scores from different retrieval functionalities can be integrated
as the teacher signal to enhance the training quality. We also optimize the
batching strategy, enabling a large batch size and high training throughput to
ensure the discriminativeness of embeddings. To the best of our knowledge,
M3-Embedding is the first embedding model which realizes such a strong
versatility. The model and code will be publicly available at
https://github.com/FlagOpen/FlagEmbedding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model Enhanced Clustering for News Event Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10552v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10552v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adane Nega Tarekegn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The news landscape is continuously evolving, with an ever-increasing volume
of information from around the world. Automated event detection within this
vast data repository is essential for monitoring, identifying, and categorizing
significant news occurrences across diverse platforms. This paper presents an
event detection framework that leverages Large Language Models (LLMs) combined
with clustering analysis to detect news events from the Global Database of
Events, Language, and Tone (GDELT). The framework enhances event clustering
through both pre-event detection tasks (keyword extraction and text embedding)
and post-event detection tasks (event summarization and topic labelling). We
also evaluate the impact of various textual embeddings on the quality of
clustering outcomes, ensuring robust news categorization. Additionally, we
introduce a novel Cluster Stability Assessment Index (CSAI) to assess the
validity and robustness of clustering results. CSAI utilizes multiple feature
vectors to provide a new way of measuring clustering quality. Our experiments
indicate that the use of LLM embedding in the event detection framework has
significantly improved the results, demonstrating greater robustness in terms
of CSAI scores. Moreover, post-event detection tasks generate meaningful
insights, facilitating effective interpretation of event clustering results.
Overall, our experimental results indicate that the proposed framework offers
valuable insights and could enhance the accuracy in news analysis and
reporting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SampleAttention: Near-Lossless Acceleration of Long Context LLM
  Inference with Adaptive Structured Sparse Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15486v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15486v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianchao Zhu, Jiangfei Duan, Chang Chen, Siran Liu, Xiuhong Li, Guanyu Feng, Xin Lv, Huanqi Cao, Xiao Chuanfu, Xingcheng Zhang, Dahua Lin, Chao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) now support extremely long context windows, but
the quadratic complexity of vanilla attention results in significantly long
Time-to-First-Token (TTFT) latency. Existing approaches to address this
complexity require additional pretraining or finetuning, and often sacrifice
model accuracy. In this paper, we first provide both theoretical and empirical
foundations for near-lossless sparse attention. We find dynamically capturing
head-specific sparse patterns at runtime with low overhead is crucial. To
address this, we propose SampleAttention, an adaptive structured and
near-lossless sparse attention. Leveraging observed significant sparse
patterns, SampleAttention attends to a fixed percentage of adjacent tokens to
capture local window patterns, and employs a two-stage query-guided key-value
filtering approach, which adaptively select a minimum set of key-values with
low overhead, to capture column stripe patterns. Comprehensive evaluations show
that SampleAttention can seamlessly replace vanilla attention in off-the-shelf
LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\times$
compared with FlashAttention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciBench: Evaluating College-Level Scientific Problem-Solving Abilities
  of Large Language Models <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.10635v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.10635v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of the existing Large Language Model (LLM) benchmarks on scientific
problem reasoning focus on problems grounded in high-school subjects and are
confined to elementary algebraic operations. To systematically examine the
reasoning capabilities required for solving complex scientific problems, we
introduce an expansive benchmark suite SciBench for LLMs. SciBench contains a
carefully curated dataset featuring a range of collegiate-level scientific
problems from mathematics, chemistry, and physics domains. Based on the
dataset, we conduct an in-depth benchmarking study of representative
open-source and proprietary LLMs with various prompting strategies. The results
reveal that the current LLMs fall short of delivering satisfactory performance,
with the best overall score of merely 43.22%. Furthermore, through a detailed
user study, we categorize the errors made by LLMs into ten problem-solving
abilities. Our analysis indicates that no single prompting strategy
significantly outperforms the others and some strategies that demonstrate
improvements in certain problem-solving skills could result in declines in
other skills. We envision that SciBench will catalyze further developments in
the reasoning abilities of LLMs, thereby ultimately contributing to scientific
research and discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Preference Learning for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Muldrew, Peter Hayes, Mingtian Zhang, David Barber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become more capable, fine-tuning techniques
for aligning with human intent are increasingly important. A key consideration
for aligning these models is how to most effectively use human resources, or
model resources in the case where LLMs themselves are used as oracles.
Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most
prominent example of such a technique, but is complex and often unstable.
Direct Preference Optimization (DPO) has recently been proposed as a simpler
and more stable alternative. In this work, we develop an active learning
strategy for DPO to make better use of preference labels. We propose a
practical acquisition function for prompt/completion pairs based on the
predictive entropy of the language model and a measure of certainty of the
implicit preference model optimized by DPO. We demonstrate how our approach
improves both the rate of learning and final performance of fine-tuning on
pairwise preference data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniGen: A Unified Framework for Textual <span class="highlight-title">Dataset</span> Generation Using Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18966v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18966v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Wu, Yue Huang, Chujie Gao, Dongping Chen, Qihui Zhang, Yao Wan, Tianyi Zhou, Xiangliang Zhang, Jianfeng Gao, Chaowei Xiao, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly
impacted various fields by enabling high-quality synthetic data generation and
reducing dependence on expensive human-generated datasets. Despite this,
challenges remain in the areas of generalization, controllability, diversity,
and truthfulness within the existing generative frameworks. To address these
challenges, this paper presents UniGen, a comprehensive LLM-powered framework
designed to produce diverse, accurate, and highly controllable datasets. UniGen
is adaptable, supporting all types of text datasets and enhancing the
generative process through innovative mechanisms. To augment data diversity,
UniGen incorporates an attribute-guided generation module and a group checking
feature. For accuracy, it employs a code-based mathematical assessment for
label verification alongside a retrieval-augmented generation technique for
factual validation. The framework also allows for user-specified constraints,
enabling customization of the data generation process to suit particular
requirements. Extensive experiments demonstrate the superior quality of data
generated by UniGen, and each module within UniGen plays a critical role in
this enhancement. Additionally, UniGen is applied in two practical scenarios:
benchmarking LLMs and data augmentation. The results indicate that UniGen
effectively supports dynamic and evolving benchmarking, and that data
augmentation improves LLM capabilities in various domains, including
agent-oriented abilities and reasoning skills.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concept-aware Data Construction Improves In-context Learning of Language
  Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Štefánik, Marek Kadlčík, Petr Sojka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent language models (LMs) are capable of in-context learning (ICL),
manifested in the LMs' ability to perform a new task solely from
natural-language instruction. Previous work curating in-context learners
assumes that ICL emerges from a vast over-parametrization or the scale of
multi-task training. However, recent theoretical work attributes the ICL
ability to concept-dependent training data and creates functional in-context
learners even in small-scale, synthetic settings.
  In this work, we practically explore this newly identified axis of ICL
quality. We propose Concept-aware Training (CoAT), a framework for constructing
training scenarios that make it beneficial for the LM to learn to utilize the
analogical reasoning concepts from demonstrations. We find that by using CoAT,
pre-trained transformers can learn to better utilise new latent concepts from
demonstrations and that such ability makes ICL more robust to the functional
deficiencies of the previous models. Finally, we show that concept-aware
in-context learning is more effective for a majority of new tasks when compared
to traditional instruction tuning, resulting in a performance comparable to the
previous in-context learners using magnitudes of more training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Long paper to appear in Findings of ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Logic Tree Extraction for Event Sequence Explanation from LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01124v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01124v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zitao Song, Chao Yang, Chaojie Wang, Bo An, Shuang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern high-stakes systems, such as healthcare or robotics, often generate
vast streaming event sequences. Our goal is to design an efficient,
plug-and-play tool to elicit logic tree-based explanations from Large Language
Models (LLMs) to provide customized insights into each observed event sequence.
Built on the temporal point process model for events, our method employs the
likelihood function as a score to evaluate generated logic trees. We propose an
amortized Expectation-Maximization (EM) learning framework and treat the logic
tree as latent variables. In the E-step, we evaluate the posterior distribution
over the latent logic trees using an LLM prior and the likelihood of the
observed event sequences. LLM provides a high-quality prior for the latent
logic trees, however, since the posterior is built over a discrete
combinatorial space, we cannot get the closed-form solution. We propose to
generate logic tree samples from the posterior using a learnable GFlowNet,
which is a diversity-seeking generator for structured discrete variables. The
M-step employs the generated logic rules to approximate marginalization over
the posterior, facilitating the learning of model parameters and refining the
tunable LLM prior parameters. In the online setting, our locally built,
lightweight model will iteratively extract the most relevant rules from LLMs
for each sequence using only a few iterations. Empirical demonstrations
showcase the promising performance and adaptability of our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Logical Closed Loop: Uncovering Object Hallucinations in Large
  Vision-Language Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfei Wu, Qiang Liu, Ding Wang, Jinghao Zhang, Shu Wu, Liang Wang, Tieniu Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object hallucination has been an Achilles' heel which hinders the broader
applications of large vision-language models (LVLMs). Object hallucination
refers to the phenomenon that the LVLMs claim non-existent objects in the
image. To mitigate the object hallucinations, instruction tuning and external
model-based detection methods have been proposed, which either require
large-scare computational resources or depend on the detection result of
external models. However, there remains an under-explored field to utilize the
LVLM itself to alleviate object hallucinations. In this work, we adopt the
intuition that the LVLM tends to respond logically consistently for existent
objects but inconsistently for hallucinated objects. Therefore, we propose a
Logical Closed Loop-based framework for Object Hallucination Detection and
Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency
probing to raise questions with logical correlations, inquiring about
attributes from objects and vice versa. Whether their responses can form a
logical closed loop serves as an indicator of object hallucination. As a
plug-and-play method, it can be seamlessly applied to all existing LVLMs.
Comprehensive experiments conducted on three benchmarks across four LVLMs have
demonstrated significant improvements brought by our method, indicating its
effectiveness and generality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to ACL 2024; 19 Pages, 15 Figures, 6 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ANLS* -- A Universal Document Processing Metric for <span class="highlight-title">Generative</span> Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03848v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03848v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Peer, Philemon Schöpf, Volckmar Nebendahl, Alexander Rietzler, Sebastian Stabinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, discriminative models have been the predominant choice for
tasks like document classification and information extraction. These models
make predictions that fall into a limited number of predefined classes,
facilitating a binary true or false evaluation and enabling the direct
calculation of metrics such as the F1 score. However, recent advancements in
generative large language models (GLLMs) have prompted a shift in the field due
to their enhanced zero-shot capabilities, which eliminate the need for a
downstream dataset and computationally expensive fine-tuning. However,
evaluating GLLMs presents a challenge as the binary true or false evaluation
used for discriminative models is not applicable to the predictions made by
GLLMs.
  This paper introduces a new metric for generative models called ANLS* for
evaluating a wide variety of tasks, including information extraction and
classification tasks. The ANLS* metric extends existing ANLS metrics as a
drop-in-replacement and is still compatible with previously reported ANLS
scores. An evaluation of 7 different datasets, and more than 10 different GLLMs
together with 3 different prompting methods using the ANLS* metric is also
provided, demonstrating the importance of the proposed metric.
  We also benchmark a novel approach to generate prompts for documents, called
SFT, against other prompting techniques such as LATIN. In 6 out of 7 cases, SFT
outperforms other techniques and improves the state-of-the-art, sometimes by as
much as $10$ percentage points.
  Sources are available at https://github.com/deepopinion/anls_star_metric
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Does Geo-co-location Matter? A Case Study of Public Health Conversations
  during COVID-19 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paiheng Xu, Louiqa Raschid, Vanessa Frias-Martinez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media platforms like Twitter (now X) have been pivotal in information
dissemination and public engagement, especially during COVID-19. A key goal for
public health experts was to encourage prosocial behavior that could impact
local outcomes such as masking and social distancing. Given the importance of
local news and guidance during COVID-19, the objective of our research is to
analyze the effect of localized engagement, on social media conversations. This
study examines the impact of geographic co-location, as a proxy for localized
engagement between public health experts (PHEs) and the public, on social
media. We analyze a Twitter conversation dataset from January 2020 to November
2021, comprising over 19 K tweets from nearly five hundred PHEs, along with
approximately 800 K replies from 350 K participants. Our findings reveal that
geo-co-location is associated with higher engagement rates, especially in
conversations on topics including masking, lockdowns, and education, and in
conversations with academic and medical professionals. Lexical features
associated with emotion and personal experiences were more common in
geo-co-located contexts. This research provides insights into how geographic
co-location influences social media engagement and can inform strategies to
improve public health messaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Apollo: A Lightweight Multilingual Medical LLM towards Democratizing
  Medical AI to 6B People 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03640v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03640v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xidong Wang, Nuo Chen, Junyin Chen, Yan Hu, Yidong Wang, Xiangbo Wu, Anningzhe Gao, Xiang Wan, Haizhou Li, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the vast repository of global medical knowledge predominantly being
in English, local languages are crucial for delivering tailored healthcare
services, particularly in areas with limited medical resources. To extend the
reach of medical AI advancements to a broader population, we aim to develop
medical LLMs across the six most widely spoken languages, encompassing a global
population of 6.1 billion. This effort culminates in the creation of the
ApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the
multilingual medical benchmark, the released Apollo models, at various
relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best
performance among models of equivalent size. Especially, Apollo-7B is the
state-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite
models could be used to improve the multi-lingual medical capabilities of
larger models without fine-tuning in a proxy-tuning fashion. We will
open-source training corpora, code, model weights and evaluation benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SafeAligner: Safety Alignment against Jailbreak Attacks via Response
  Disparity Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caishuang Huang, Wanxu Zhao, Rui Zheng, Huijie Lv, Shihan Dou, Sixian Li, Xiao Wang, Enyu Zhou, Junjie Ye, Yuming Yang, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the development of large language models (LLMs) rapidly advances, securing
these models effectively without compromising their utility has become a
pivotal area of research. However, current defense strategies against jailbreak
attacks (i.e., efforts to bypass security protocols) often suffer from limited
adaptability, restricted general capability, and high cost. To address these
challenges, we introduce SafeAligner, a methodology implemented at the decoding
stage to fortify defenses against jailbreak attacks. We begin by developing two
specialized models: the Sentinel Model, which is trained to foster safety, and
the Intruder Model, designed to generate riskier responses. SafeAligner
leverages the disparity in security levels between the responses from these
models to differentiate between harmful and beneficial tokens, effectively
guiding the safety alignment by altering the output token distribution of the
target model. Extensive experiments show that SafeAligner can increase the
likelihood of beneficial tokens, while reducing the occurrence of harmful ones,
thereby ensuring secure alignment with minimal loss to generality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlowVQA: Mapping Multimodal Logic in Visual Question Answering with
  Flowcharts <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubhankar Singh, Purvi Chaurasia, Yerram Varun, Pranshu Pandya, Vatsal Gupta, Vivek Gupta, Dan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing benchmarks for visual question answering lack in visual grounding
and complexity, particularly in evaluating spatial reasoning skills. We
introduce FlowVQA, a novel benchmark aimed at assessing the capabilities of
visual question-answering multimodal language models in reasoning with
flowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and
human-verified flowchart images from three distinct content sources, along with
22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,
including information localization, decision-making, and logical progression.
We conduct a thorough baseline evaluation on a suite of both open-source and
proprietary multimodal language models using various strategies, followed by an
analysis of directional bias. The results underscore the benchmark's potential
as a vital tool for advancing the field of multimodal modeling, providing a
focused and challenging environment for enhancing model performance in visual
and logical reasoning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACL 2024 (Findings), 21 pages, 7 figures, 9 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WellDunn: On the <span class="highlight-title">Robust</span>ness and Explainability of Language Models and
  Large Language Models in Identifying Wellness Dimensions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12058v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12058v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyedali Mohammadi, Edward Raff, Jinendra Malekar, Vedant Palit, Francis Ferraro, Manas Gaur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LMs) are being proposed for mental health applications where
the heightened risk of adverse outcomes means predictive performance may not be
a sufficient litmus test of a model's utility in clinical practice. A model
that can be trusted for practice should have a correspondence between
explanation and clinical determination, yet no prior research has examined the
attention fidelity of these models and their effect on ground truth
explanations. We introduce an evaluation design that focuses on the robustness
and explainability of LMs in identifying Wellness Dimensions (WD). We focus on
two mental health and well-being datasets: (a) Multi-label Classification-based
MultiWD, and (b) WellXplain for evaluating attention mechanism veracity against
expert-labeled explanations. The labels are based on Halbert Dunn's theory of
wellness, which gives grounding to our evaluation. We reveal four surprising
results about LMs/LLMs: (1) Despite their human-like capabilities, GPT-3.5/4
lag behind RoBERTa, and MedAlpaca, a fine-tuned LLM fails to deliver any
remarkable improvements in performance or explanations. (2) Re-examining LMs'
predictions based on a confidence-oriented loss function reveals a significant
performance drop. (3) Across all LMs/LLMs, the alignment between attention and
explanations remains low, with LLMs scoring a dismal 0.0. (4) Most mental
health-specific LMs/LLMs overlook domain-specific knowledge and undervalue
explanations, causing these discrepancies. This study highlights the need for
further research into their consistency and explanations in mental health and
well-being.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, including reference and appendix sections, 8 figures, and
  16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI Hospital: Benchmarking Large Language Models in a Multi-agent Medical
  Interaction Simulator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09742v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09742v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi, Fei Huang, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence has significantly advanced healthcare, particularly
through large language models (LLMs) that excel in medical question answering
benchmarks. However, their real-world clinical application remains limited due
to the complexities of doctor-patient interactions. To address this, we
introduce \textbf{AI Hospital}, a multi-agent framework simulating dynamic
medical interactions between \emph{Doctor} as player and NPCs including
\emph{Patient}, \emph{Examiner}, \emph{Chief Physician}. This setup allows for
realistic assessments of LLMs in clinical scenarios. We develop the Multi-View
Medical Evaluation (MVME) benchmark, utilizing high-quality Chinese medical
records and NPCs to evaluate LLMs' performance in symptom collection,
examination recommendations, and diagnoses. Additionally, a dispute resolution
collaborative mechanism is proposed to enhance diagnostic accuracy through
iterative discussions. Despite improvements, current LLMs exhibit significant
performance gaps in multi-turn interactions compared to one-step approaches.
Our findings highlight the need for further research to bridge these gaps and
improve LLMs' clinical diagnostic capabilities. Our data, code, and
experimental results are all open-sourced at
\url{https://github.com/LibertFan/AI_Hospital}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/LibertFan/AI_Hospital</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigating LLM Ethics: Advancements, Challenges, and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Jiao, Saleh Afroogh, Yiming Xu, Connor Phillips
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses ethical issues surrounding Large Language Models (LLMs)
within the field of artificial intelligence. It explores the common ethical
challenges posed by both LLMs and other AI systems, such as privacy and
fairness, as well as ethical challenges uniquely arising from LLMs. It
highlights challenges such as hallucination, verifiable accountability, and
decoding censorship complexity, which are unique to LLMs and distinct from
those encountered in traditional AI systems. The study underscores the need to
tackle these complexities to ensure accountability, reduce biases, and enhance
transparency in the influential role that LLMs play in shaping information
dissemination. It proposes mitigation strategies and future directions for LLM
ethics, advocating for interdisciplinary collaboration. It recommends ethical
frameworks tailored to specific domains and dynamic auditing systems adapted to
diverse contexts. This roadmap aims to guide responsible development and
integration of LLMs, envisioning a future where ethical considerations govern
AI advancements in society.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The global landscape of academic guidelines for <span class="highlight-title">generative</span> AI and Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18842v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18842v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Jiao, Saleh Afroogh, Kevin Chen, David Atkinson, Amit Dhurandhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Generative Artificial Intelligence (GAI) and Large
Language Models (LLMs) in academia has spurred a global discourse on their
potential pedagogical benefits and ethical considerations. Positive reactions
highlight some potential, such as collaborative creativity, increased access to
education, and empowerment of trainers and trainees. However, negative
reactions raise concerns about ethical complexities, balancing innovation and
academic integrity, unequal access, and misinformation risks. Through a
systematic survey and text-mining-based analysis of global and national
directives, insights from independent research, and eighty university-level
guidelines, this study provides a nuanced understanding of the opportunities
and challenges posed by GAI and LLMs in education. It emphasizes the importance
of balanced approaches that harness the benefits of these technologies while
addressing ethical considerations and ensuring equitable access and educational
outcomes. The paper concludes with recommendations for fostering responsible
innovation and ethical practices to guide the integration of GAI and LLMs in
academia.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and
  Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02990v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02990v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bosheng Ding, Chengwei Qin, Ruochen Zhao, Tianze Luo, Xinze Li, Guizhen Chen, Wenhan Xia, Junjie Hu, Anh Tuan Luu, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving field of large language models (LLMs), data
augmentation (DA) has emerged as a pivotal technique for enhancing model
performance by diversifying training examples without the need for additional
data collection. This survey explores the transformative impact of LLMs on DA,
particularly addressing the unique challenges and opportunities they present in
the context of natural language processing (NLP) and beyond. From both data and
learning perspectives, we examine various strategies that utilize LLMs for data
augmentation, including a novel exploration of learning paradigms where
LLM-generated data is used for diverse forms of further training. Additionally,
this paper highlights the primary open challenges faced in this domain, ranging
from controllable data augmentation to multi-modal data augmentation. This
survey highlights a paradigm shift introduced by LLMs in DA, and aims to serve
as a comprehensive guide for researchers and practitioners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIntRec2.0: A Large-scale Benchmark <span class="highlight-title">Dataset</span> for Multimodal Intent
  Recognition and Out-of-scope Detection in Conversations <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10943v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10943v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanlei Zhang, Xin Wang, Hua Xu, Qianrui Zhou, Kai Gao, Jianhua Su, jinyue Zhao, Wenrui Li, Yanting Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal intent recognition poses significant challenges, requiring the
incorporation of non-verbal modalities from real-world contexts to enhance the
comprehension of human intentions. Existing benchmark datasets are limited in
scale and suffer from difficulties in handling out-of-scope samples that arise
in multi-turn conversational interactions. We introduce MIntRec2.0, a
large-scale benchmark dataset for multimodal intent recognition in multi-party
conversations. It contains 1,245 dialogues with 15,040 samples, each annotated
within a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope
samples, it also includes 5,736 out-of-scope samples appearing in multi-turn
contexts, which naturally occur in real-world scenarios. Furthermore, we
provide comprehensive information on the speakers in each utterance, enriching
its utility for multi-party conversational research. We establish a general
framework supporting the organization of single-turn and multi-turn dialogue
data, modality feature extraction, multimodal fusion, as well as in-scope
classification and out-of-scope detection. Evaluation benchmarks are built
using classic multimodal fusion methods, ChatGPT, and human evaluators. While
existing methods incorporating nonverbal information yield improvements,
effectively leveraging context information and detecting out-of-scope samples
remains a substantial challenge. Notably, large language models exhibit a
significant performance gap compared to humans, highlighting the limitations of
machine learning methods in the cognitive intent understanding task. We believe
that MIntRec2.0 will serve as a valuable resource, providing a pioneering
foundation for research in human-machine conversational interactions, and
significantly facilitating related applications. The full dataset and codes are
available at https://github.com/thuiar/MIntRec2.0.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2024, Long Paper; The abstract is slightly modified
  due to the length limitation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prompting Explicit and Implicit Knowledge for Multi-hop Question
  Answering Based on Human Reading Process <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19350v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19350v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangming Huang, Yunfei Long, Cunjin Luo, Jiaxing Shen, Xia Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to
simulate human reasoning and inference processes, achieving proficient
performance in multi-hop QA. However, a gap persists between PLMs' reasoning
abilities and those of humans when tackling complex problems. Psychological
studies suggest a vital connection between explicit information in passages and
human prior knowledge during reading. Nevertheless, current research has given
insufficient attention to linking input passages and PLMs' pre-training-based
knowledge from the perspective of human cognition studies. In this study, we
introduce a Prompting Explicit and Implicit knowledge (PEI) framework, which
uses prompts to connect explicit and implicit knowledge, aligning with human
reading process for multi-hop QA. We consider the input passages as explicit
knowledge, employing them to elicit implicit knowledge through unified prompt
reasoning. Furthermore, our model incorporates type-specific reasoning via
prompts, a form of implicit knowledge. Experimental results show that PEI
performs comparably to the state-of-the-art on HotpotQA. Ablation studies
confirm the efficacy of our model in bridging and integrating explicit and
implicit knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted at COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">90</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Odd-One-Out: Anomaly Detection by Comparing with Neighbors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankan Bhunia, Changjian Li, Hakan Bilen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel anomaly detection (AD) problem that focuses on
identifying `odd-looking' objects relative to the other instances within a
scene. Unlike the traditional AD benchmarks, in our setting, anomalies in this
context are scene-specific, defined by the regular instances that make up the
majority. Since object instances are often partly visible from a single
viewpoint, our setting provides multiple views of each scene as input. To
provide a testbed for future research in this task, we introduce two
benchmarks, ToysAD-8K and PartsAD-15K. We propose a novel method that generates
3D object-centric representations for each instance and detects the anomalous
ones through a cross-examination between the instances. We rigorously analyze
our method quantitatively and qualitatively in the presented benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Codes & Dataset at https://github.com/VICO-UoE/OddOneOutAD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Web2Code: A Large-scale Webpage-to-Code <span class="highlight-title">Dataset</span> and Evaluation Framework
  for Multimodal LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sukmin Yun, Haokun Lin, Rusiru Thushara, Mohammad Qazim Bhat, Yongxin Wang, Zutao Jiang, Mingkai Deng, Jinhong Wang, Tianhua Tao, Junbo Li, Haonan Li, Preslav Nakov, Timothy Baldwin, Zhengzhong Liu, Eric P. Xing, Xiaodan Liang, Zhiqiang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have shown impressive success across
modalities such as image, video, and audio in a variety of understanding and
generation tasks. However, current MLLMs are surprisingly poor at understanding
webpage screenshots and generating their corresponding HTML code. To address
this problem, we propose Web2Code, a benchmark consisting of a new large-scale
webpage-to-code dataset for instruction tuning and an evaluation framework for
the webpage understanding and HTML code translation abilities of MLLMs. For
dataset construction, we leverage pretrained LLMs to enhance existing
webpage-to-code datasets as well as generate a diverse pool of new webpages
rendered into images. Specifically, the inputs are webpage images and
instructions, while the responses are the webpage's HTML code. We further
include diverse natural language QA pairs about the webpage content in the
responses to enable a more comprehensive understanding of the web content. To
evaluate model performance in these tasks, we develop an evaluation framework
for testing MLLMs' abilities in webpage understanding and web-to-code
generation. Extensive experiments show that our proposed dataset is beneficial
not only to our proposed tasks but also in the general visual domain, while
previous datasets result in worse performance. We hope our work will contribute
to the development of general MLLMs suitable for web-based content generation
and task automation. Our data and code will be available at
https://github.com/MBZUAI-LLM/web2code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website at https://mbzuai-llm.github.io/webpage2code/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaRA: Supercharging Robot Learning Data for Vision-Language Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe, Ryan Burgert, Mu Cai, Yong Jae Lee, Michael S. Ryoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) equipped with extensive world knowledge and
strong reasoning skills can tackle diverse tasks across domains, often by
posing them as conversation-style instruction-response pairs. In this paper, we
propose LLaRA: Large Language and Robotics Assistant, a framework which
formulates robot action policy as conversations, and provides improved
responses when trained with auxiliary data that complements policy learning.
LLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity
to process state information as visual-textual prompts and generate optimal
policy decisions in text. To train such action policy VLMs, we first introduce
an automated pipeline to generate diverse high-quality robotics instruction
data from existing behavior cloning data. A VLM finetuned with the resulting
collection of datasets based on a conversation-style formulation tailored for
robotics tasks, can generate meaningful robot action policy decisions. Our
experiments across multiple simulated and real-world environments demonstrate
the state-of-the-art performance of the proposed LLaRA framework. The code,
datasets, and pretrained models are available at
https://github.com/LostXine/LLaRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaVolta: Efficient Multi-modal Models via Stage-wise Visual Context
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieneng Chen, Luoxin Ye, Ju He, Zhao-Yang Wang, Daniel Khashabi, Alan Yuille
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While significant advancements have been made in compressed representations
for text embeddings in large language models (LLMs), the compression of visual
tokens in large multi-modal models (LMMs) has remained a largely overlooked
area. In this work, we present the study on the analysis of redundancy
concerning visual tokens and efficient training within these models. Our
initial experiments show that eliminating up to 70% of visual tokens at the
testing stage by simply average pooling only leads to a minimal 3% reduction in
visual question answering accuracy on the GQA benchmark, indicating significant
redundancy in visual context. Addressing this, we introduce Visual Context
Compressor, which reduces the number of visual tokens during training to
enhance training efficiency without sacrificing performance. To minimize
information loss caused by the compression on visual tokens while maintaining
training efficiency, we develop LLaVolta as a lite training scheme. LLaVolta
incorporates stage-wise visual context compression to progressively compress
the visual tokens from heavily to lightly, and finally no compression at the
end of training, yielding no loss of information when testing. Extensive
experiments demonstrate that our approach enhances the performance of MLLMs in
both image-language and video-language understanding, while also significantly
cutting training costs. Code is available at
https://github.com/Beckschen/LLaVolta
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/Beckschen/LLaVolta</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Auto Cherry-Picker: Learning from High-quality <span class="highlight-title">Generative</span> Data Driven by
  Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yicheng Chen, Xiangtai Li, Yining Li, Yanhong Zeng, Jianzong Wu, Xiangyu Zhao, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based models have shown great potential in generating high-quality
images with various layouts, which can benefit downstream perception tasks.
However, a fully automatic layout generation driven only by language and a
suitable metric for measuring multiple generated instances has not been well
explored. In this work, we present Auto Cherry-Picker (ACP), a novel framework
that generates high-quality multi-modal training examples to augment perception
and multi-modal training. Starting with a simple list of natural language
concepts, we prompt large language models (LLMs) to generate a detailed
description and design reasonable layouts. Next, we use an off-the-shelf
text-to-image model to generate multiple images. Then, the generated data are
refined using a comprehensively designed metric to ensure quality. In
particular, we present a new metric, Composite Layout and Image Score (CLIS),
to evaluate the generated images fairly. Our synthetic high-quality examples
boost performance in various scenarios by customizing the initial concept list,
especially in addressing challenges associated with long-tailed distribution
and imbalanced datasets. Experiment results on downstream tasks demonstrate
that Auto Cherry-Picker can significantly improve the performance of existing
models. In addition, we have thoroughly investigated the correlation between
CLIS and performance gains in downstream tasks, and we find that a better CLIS
score results in better performance. This finding shows the potential for
evaluation metrics as the role for various visual perception and MLLM tasks.
Code will be available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful
  Navigators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuo-Hao Zeng, Zichen Zhang, Kiana Ehsani, Rose Hendrix, Jordi Salvador, Alvaro Herrasti, Ross Girshick, Aniruddha Kembhavi, Luca Weihs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PoliFormer (Policy Transformer), an RGB-only indoor navigation
agent trained end-to-end with reinforcement learning at scale that generalizes
to the real-world without adaptation despite being trained purely in
simulation. PoliFormer uses a foundational vision transformer encoder with a
causal transformer decoder enabling long-term memory and reasoning. It is
trained for hundreds of millions of interactions across diverse environments,
leveraging parallelized, multi-machine rollouts for efficient training with
high throughput. PoliFormer is a masterful navigator, producing
state-of-the-art results across two distinct embodiments, the LoCoBot and
Stretch RE-1 robots, and four navigation benchmarks. It breaks through the
plateaus of previous work, achieving an unprecedented 85.5% success rate in
object goal navigation on the CHORES-S benchmark, a 28.5% absolute improvement.
PoliFormer can also be trivially extended to a variety of downstream
applications such as object tracking, multi-object navigation, and
open-vocabulary navigation with no finetuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Segment Anything without Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        XuDong Wang, Jingfeng Yang, Trevor Darrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Segmentation Anything Model (SAM) requires labor-intensive data labeling.
We present Unsupervised SAM (UnSAM) for promptable and automatic whole-image
segmentation that does not require human annotations. UnSAM utilizes a
divide-and-conquer strategy to "discover" the hierarchical structure of visual
scenes. We first leverage top-down clustering methods to partition an unlabeled
image into instance/semantic level segments. For all pixels within a segment, a
bottom-up clustering method is employed to iteratively merge them into larger
groups, thereby forming a hierarchical structure. These unsupervised
multi-granular masks are then utilized to supervise model training. Evaluated
across seven popular datasets, UnSAM achieves competitive results with the
supervised counterpart SAM, and surpasses the previous state-of-the-art in
unsupervised segmentation by 11% in terms of AR. Moreover, we show that
supervised SAM can also benefit from our self-supervised labels. By integrating
our unsupervised pseudo masks into SA-1B's ground-truth masks and training
UnSAM with only 1% of SA-1B, a lightly semi-supervised UnSAM can often segment
entities overlooked by supervised SAM, exceeding SAM's AR by over 6.7% and AP
by 3.9% on SA-1B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/frank-xwang/UnSAM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GM-DF: Generalized Multi-Scenario Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingxin Lai, Zitong Yu, Jing Yang, Bin Li, Xiangui Kang, Linlin Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing face forgery detection usually follows the paradigm of training
models in a single domain, which leads to limited generalization capacity when
unseen scenarios and unknown attacks occur. In this paper, we elaborately
investigate the generalization capacity of deepfake detection models when
jointly trained on multiple face forgery detection datasets. We first find a
rapid degradation of detection accuracy when models are directly trained on
combined datasets due to the discrepancy across collection scenarios and
generation methods. To address the above issue, a Generalized Multi-Scenario
Deepfake Detection framework (GM-DF) is proposed to serve multiple real-world
scenarios by a unified model. First, we propose a hybrid expert modeling
approach for domain-specific real/forgery feature extraction. Besides, as for
the commonality representation, we use CLIP to extract the common features for
better aligning visual and textual features across domains. Meanwhile, we
introduce a masked image reconstruction mechanism to force models to capture
rich forged details. Finally, we supervise the models via a domain-aware
meta-learning strategy to further enhance their generalization capacities.
Specifically, we design a novel domain alignment loss to strongly align the
distributions of the meta-test domains and meta-train domains. Thus, the
updated models are able to represent both specific and common real/forgery
features across multiple datasets. In consideration of the lack of study of
multi-dataset training, we establish a new benchmark leveraging multi-source
data to fairly evaluate the models' generalization capacity on unseen
scenarios. Both qualitative and quantitative experiments on five datasets
conducted on traditional protocols as well as the proposed benchmark
demonstrate the effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HouseCrafter: Lifting Floorplans to 3D Scenes with 2D <span class="highlight-title">Diffusion</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hieu T. Nguyen, Yiwen Chen, Vikram Voleti, Varun Jampani, Huaizu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce HouseCrafter, a novel approach that can lift a floorplan into a
complete large 3D indoor scene (e.g., a house). Our key insight is to adapt a
2D diffusion model, which is trained on web-scale images, to generate
consistent multi-view color (RGB) and depth (D) images across different
locations of the scene. Specifically, the RGB-D images are generated
autoregressively in a batch-wise manner along sampled locations based on the
floorplan, where previously generated images are used as condition to the
diffusion model to produce images at nearby locations. The global floorplan and
attention design in the diffusion model ensures the consistency of the
generated images, from which a 3D scene can be reconstructed. Through extensive
evaluation on the 3D-Front dataset, we demonstrate that HouseCraft can generate
high-quality house-scale 3D scenes. Ablation studies also validate the
effectiveness of different design choices. We will release our code and model
weights. Project page: https://neu-vi.github.io/houseCrafter/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhang, Tianheng Cheng, Rui Hu, ei Liu, Heng Liu, Longjin Ran, Xiaoxin Chen, Wenyu Liu, Xinggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segment Anything Model (SAM) has attracted widespread attention for its
superior interactive segmentation capabilities with visual prompts while
lacking further exploration of text prompts. In this paper, we empirically
investigate what text prompt encoders (e.g., CLIP or LLM) are good for adapting
SAM for referring expression segmentation and introduce the Early
Vision-language Fusion-based SAM (EVF-SAM). EVF-SAM is a simple yet effective
referring segmentation method which exploits multimodal prompts (i.e., image
and text) and comprises a pre-trained vision-language model to generate
referring prompts and a SAM model for segmentation. Surprisingly, we observe
that: (1) multimodal prompts and (2) vision-language models with early fusion
(e.g., BEIT-3) are beneficial for prompting SAM for accurate referring
segmentation. Our experiments show that the proposed EVF-SAM based on BEIT-3
can obtain state-of-the-art performance on RefCOCO/+/g for referring expression
segmentation and demonstrate the superiority of prompting SAM with early
vision-language fusion. In addition, the proposed EVF-SAM with 1.32B parameters
achieves remarkably higher performance while reducing nearly 82% of parameters
compared to previous SAM methods based on large multimodal models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASSR-NeRF: Arbitrary-Scale Super-Resolution on Voxel Grid for
  High-Quality Radiance Fields Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ding-Jiun Huang, Zi-Ting Chou, Yu-Chiang Frank Wang, Cheng Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  NeRF-based methods reconstruct 3D scenes by building a radiance field with
implicit or explicit representations. While NeRF-based methods can perform
novel view synthesis (NVS) at arbitrary scale, the performance in
high-resolution novel view synthesis (HRNVS) with low-resolution (LR)
optimization often results in oversmoothing. On the other hand, single-image
super-resolution (SR) aims to enhance LR images to HR counterparts but lacks
multi-view consistency. To address these challenges, we propose Arbitrary-Scale
Super-Resolution NeRF (ASSR-NeRF), a novel framework for super-resolution novel
view synthesis (SRNVS). We propose an attention-based VoxelGridSR model to
directly perform 3D super-resolution (SR) on the optimized volume. Our model is
trained on diverse scenes to ensure generalizability. For unseen scenes trained
with LR views, we then can directly apply our VoxelGridSR to further refine the
volume and achieve multi-view consistent SR. We demonstrate quantitative and
qualitatively that the proposed method achieves significant performance in
SRNVS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpotlessSplats: Ignoring Distractors in 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Sabour, Lily Goli, George Kopanas, Mark Matthews, Dmitry Lagun, Leonidas Guibas, Alec Jacobson, David J. Fleet, Andrea Tagliasacchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) is a promising technique for 3D reconstruction,
offering efficient training and rendering speeds, making it suitable for
real-time applications.However, current methods require highly controlled
environments (no moving people or wind-blown elements, and consistent lighting)
to meet the inter-view consistency assumption of 3DGS. This makes
reconstruction of real-world captures problematic. We present SpotlessSplats,
an approach that leverages pre-trained and general-purpose features coupled
with robust optimization to effectively ignore transient distractors. Our
method achieves state-of-the-art reconstruction quality both visually and
quantitatively, on casual captures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HAITCH: A Framework for Distortion and Motion Correction in Fetal
  Multi-Shell <span class="highlight-title">Diffusion</span>-Weighted MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haykel Snoussi, Davood Karimi, Onur Afacan, Mustafa Utkur, Ali Gholipour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion magnetic resonance imaging (dMRI) is pivotal for probing the
microstructure of the rapidly-developing fetal brain. However, fetal motion
during scans and its interaction with magnetic field inhomogeneities result in
artifacts and data scattering across spatial and angular domains. The effects
of those artifacts are more pronounced in high-angular resolution fetal dMRI,
where signal-to-noise ratio is very low. Those effects lead to biased estimates
and compromise the consistency and reliability of dMRI analysis. This work
presents HAITCH, the first and the only publicly available tool to correct and
reconstruct multi-shell high-angular resolution fetal dMRI data. HAITCH offers
several technical advances that include a blip-reversed dual-echo acquisition
for dynamic distortion correction, advanced motion correction for model-free
and robust reconstruction, optimized multi-shell design for enhanced
information capture and increased tolerance to motion, and outlier detection
for improved reconstruction fidelity. The framework is open-source, flexible,
and can be used to process any type of fetal dMRI data including single-echo or
single-shell acquisitions, but is most effective when used with multi-shell
multi-echo fetal dMRI data that cannot be processed with any of the existing
tools. Validation experiments on real fetal dMRI scans demonstrate significant
improvements and accurate correction across diverse fetal ages and motion
levels. HAITCH successfully removes artifacts and reconstructs high-fidelity
fetal dMRI data suitable for advanced diffusion modeling, including fiber
orientation distribution function estimation. These advancements pave the way
for more reliable analysis of the fetal brain microstructure and tractography
under challenging imaging conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ eMoE-Tracker: Environmental MoE-based Transformer for <span class="highlight-title">Robust</span>
  Event-guided Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Chen, Lin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The unique complementarity of frame-based and event cameras for high frame
rate object tracking has recently inspired some research attempts to develop
multi-modal fusion approaches. However, these methods directly fuse both
modalities and thus ignore the environmental attributes, e.g., motion blur,
illumination variance, occlusion, scale variation, etc. Meanwhile, no
interaction between search and template features makes distinguishing target
objects and backgrounds difficult. As a result, performance degradation is
induced especially in challenging conditions. This paper proposes a novel and
effective Transformer-based event-guided tracking framework, called
eMoE-Tracker, which achieves new SOTA performance under various conditions. Our
key idea is to disentangle the environment into several learnable attributes to
dynamically learn the attribute-specific features for better interaction and
discriminability between the target information and background. To achieve the
goal, we first propose an environmental Mix-of-Experts (eMoE) module that is
built upon the environmental Attributes Disentanglement to learn
attribute-specific features and environmental Attributes Gating to assemble the
attribute-specific features by the learnable attribute scores dynamically. The
eMoE module is a subtle router that fine-tunes the transformer backbone more
efficiently. We then introduce a contrastive relation modeling (CRM) module to
improve interaction and discriminability between the target information and
background. Extensive experiments on diverse event-based benchmark datasets
showcase the superior performance of our eMoE-Tracker compared to the prior
arts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RGB-event single object tracking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Malaria Cell Detection Using Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Sawant, Anurag Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Malaria remains one of the most pressing public health concerns globally,
causing significant morbidity and mortality, especially in sub-Saharan Africa.
Rapid and accurate diagnosis is crucial for effective treatment and disease
management. Traditional diagnostic methods, such as microscopic examination of
blood smears, are labor-intensive and require significant expertise, which may
not be readily available in resource-limited settings. This project aims to
automate the detection of malaria-infected cells using a deep learning
approach. We employed a convolutional neural network (CNN) based on the
ResNet50 architecture, leveraging transfer learning to enhance performance. The
Malaria Cell Images Dataset from Kaggle, containing 27,558 images categorized
into infected and uninfected cells, was used for training and evaluation. Our
model demonstrated high accuracy, precision, and recall, indicating its
potential as a reliable tool for assisting in malaria diagnosis. Additionally,
a web application was developed using Streamlit to allow users to upload cell
images and receive predictions about malaria infection, making the technology
accessible and user-friendly. This paper provides a comprehensive overview of
the methodology, experiments, and results, highlighting the effectiveness of
deep learning in medical image analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wavelets Are All You Need for Autoregressive Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wael Mattar, Idan Levy, Nir Sharon, Shai Dekel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we take a new approach to autoregressive image generation that
is based on two main ingredients. The first is wavelet image coding, which
allows to tokenize the visual details of an image from coarse to fine details
by ordering the information starting with the most significant bits of the most
significant wavelet coefficients. The second is a variant of a language
transformer whose architecture is re-designed and optimized for token sequences
in this 'wavelet language'. The transformer learns the significant statistical
correlations within a token sequence, which are the manifestations of
well-known correlations between the wavelet subbands at various resolutions. We
show experimental results with conditioning on the generation process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STLLaVA-Med: Self-Training Large Language and Vision Assistant for
  Medical 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guohao Sun, Can Qin, Huazhu Fu, Linwei Wang, Zhiqiang Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) have shown significant potential in
assisting medical diagnosis by leveraging extensive biomedical datasets.
However, the advancement of medical image understanding and reasoning
critically depends on building high-quality visual instruction data, which is
costly and labor-intensive to obtain, particularly in the medical domain. To
mitigate this data-starving issue, we introduce Self-Training Large Language
and Vision Assistant for Medical (STLLaVA-Med). The proposed method is designed
to train a policy model (an LVLM) capable of auto-generating medical visual
instruction data to improve data efficiency, guided through Direct Preference
Optimization (DPO). Specifically, a more powerful and larger LVLM (e.g.,
GPT-4o) is involved as a biomedical expert to oversee the DPO fine-tuning
process on the auto-generated data, encouraging the policy model to align
efficiently with human preferences. We validate the efficacy and data
efficiency of STLLaVA-Med across three major medical Visual Question Answering
(VQA) benchmarks, demonstrating competitive zero-shot performance with the
utilization of only 9% of the medical data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Initialization on Intra-subject Pediatric Brain MR Image
  Registration: A Comparative Analysis between SyN ANTs and Deep Learning-Based
  Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andjela Dimitrijevic, Vincent Noblet, Benjamin De Leener
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study evaluates the performance of conventional SyN ANTs and
learning-based registration methods in the context of pediatric neuroimaging,
specifically focusing on intrasubject deformable registration. The comparison
involves three approaches: without (NR), with rigid (RR), and with rigid and
affine (RAR) initializations. In addition to initialization, performances are
evaluated in terms of accuracy, speed, and the impact of age intervals and sex
per pair. Data consists of the publicly available MRI scans from the Calgary
Preschool dataset, which includes 63 children aged 2-7 years, allowing for 431
registration pairs. We implemented the unsupervised DL framework with a U-Net
architecture using DeepReg and it was 5-fold cross-validated. Evaluation
includes Dice scores for tissue segmentation from 18 smaller regions obtained
by SynthSeg, analysis of log Jacobian determinants, and registration pro-rated
training and inference times. Learning-based approaches, with or without linear
initializations, exhibit slight superiority over SyN ANTs in terms of Dice
scores. Indeed, DL-based implementations with RR and RAR initializations
significantly outperform SyN ANTs. Both SyN ANTs and DL-based registration
involve parameter optimization, but the choice between these methods depends on
the scale of registration: network-based for broader coverage or SyN ANTs for
specific structures. Both methods face challenges with larger age intervals due
to greater growth changes. The main takeaway is that while DL-based methods
show promise with faster and more accurate registrations, SyN ANTs remains
robust and generalizable without the need for extensive training, highlighting
the importance of method selection based on specific registration needs in the
pediatric context. Our code is available at
https://github.com/neuropoly/pediatric-DL-registration
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2024:013</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRACE: Graph-Regularized Attentive Convolutional Entanglement with
  Laplacian Smoothing for <span class="highlight-title">Robust</span> DeepFake Video Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Chung Hsu, Shao-Ning Chen, Mei-Hsuan Wu, Yi-Fang Wang, Chia-Ming Lee, Yi-Shiuan Chou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As DeepFake video manipulation techniques escalate, posing profound threats,
the urgent need to develop efficient detection strategies is underscored.
However, one particular issue lies with facial images being mis-detected, often
originating from degraded videos or adversarial attacks, leading to unexpected
temporal artifacts that can undermine the efficacy of DeepFake video detection
techniques. This paper introduces a novel method for robust DeepFake video
detection, harnessing the power of the proposed Graph-Regularized Attentive
Convolutional Entanglement (GRACE) based on the graph convolutional network
with graph Laplacian to address the aforementioned challenges. First,
conventional Convolution Neural Networks are deployed to perform spatiotemporal
features for the entire video. Then, the spatial and temporal features are
mutually entangled by constructing a graph with sparse constraint, enforcing
essential features of valid face images in the noisy face sequences remaining,
thus augmenting stability and performance for DeepFake video detection.
Furthermore, the Graph Laplacian prior is proposed in the graph convolutional
network to remove the noise pattern in the feature space to further improve the
performance. Comprehensive experiments are conducted to illustrate that our
proposed method delivers state-of-the-art performance in DeepFake video
detection under noisy face sequences. The source code is available at
https://github.com/ming053l/GRACE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to TPAMI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallax-tolerant Image Stitching via Segmentation-guided
  Multi-homography Warping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianli Liao, Ce Wang, Lei Li, Guangen Liu, Nan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large parallax between images is an intractable issue in image stitching.
Various warping-based methods are proposed to address it, yet the results are
unsatisfactory. In this paper, we propose a novel image stitching method using
multi-homography warping guided by image segmentation. Specifically, we
leverage the Segment Anything Model to segment the target image into numerous
contents and partition the feature points into multiple subsets via the
energy-based multi-homography fitting algorithm. The multiple subsets of
feature points are used to calculate the corresponding multiple homographies.
For each segmented content in the overlapping region, we select its
best-fitting homography with the lowest photometric error. For each segmented
content in the non-overlapping region, we calculate a weighted combination of
the linearized homographies. Finally, the target image is warped via the
best-fitting homographies to align with the reference image, and the final
panorama is generated via linear blending. Comprehensive experimental results
on the public datasets demonstrate that our method provides the best alignment
accuracy by a large margin, compared with the state-of-the-art methods. The
source code is available at https://github.com/tlliao/multi-homo-warp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving Token Gradient Conflict in Mixture-of-Experts for Large
  Vision-Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longrong Yang, Dong Sheng, Chaoxiang Cai, Fan Yang, Size Li, Di Zhang, Xi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Mixture-of-Experts (MoE) has gained increasing attention in the study of
Large Vision-Language Models (LVLMs). It uses a sparse model to replace the
dense model, achieving comparable performance while activating fewer parameters
during inference, thus significantly reducing the inference cost. Existing MoE
methods in LVLMs encourage different experts to handle different tokens, and
thus they employ a router to predict the routing for each token. However, the
predictions are based solely on sample features and do not truly reveal the
optimization direction of tokens. This can lead to severe optimization
conflicts between different tokens within an expert. To address this problem,
this paper proposes a novel method based on token-level gradient analysis.
Specifically, we first use token-level gradients to identify conflicting tokens
in experts. Then, we add a specialized loss tailored to eliminate conflicts
among tokens within each expert. Our method can serve as a plug-in for diverse
Large Vision-Language Models, and extensive experimental results demonstrate
the effectiveness of our method. The code will be publicly available at
https://github.com/longrongyang/STGC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Value of PHH3 for Mitotic Figure Detection on H&E-stained Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Ganz, Christian Marzahl, Jonas Ammeling, Barbara Richter, Chloé Puget, Daniela Denk, Elena A. Demeter, Flaviu A. Tabaran, Gabriel Wasinger, Karoline Lipnik, Marco Tecilla, Matthew J. Valentine, Michael J. Dark, Niklas Abele, Pompei Bolfa, Ramona Erber, Robert Klopfleisch, Sophie Merz, Taryn A. Donovan, Samir Jabari, Christof A. Bertram, Katharina Breininger, Marc Aubreville
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The count of mitotic figures (MFs) observed in hematoxylin and eosin
(H&E)-stained slides is an important prognostic marker as it is a measure for
tumor cell proliferation. However, the identification of MFs has a known low
inter-rater agreement. Deep learning algorithms can standardize this task, but
they require large amounts of annotated data for training and validation.
Furthermore, label noise introduced during the annotation process may impede
the algorithm's performance. Unlike H&E, the mitosis-specific antibody
phospho-histone H3 (PHH3) specifically highlights MFs. Counting MFs on slides
stained against PHH3 leads to higher agreement among raters and has therefore
recently been used as a ground truth for the annotation of MFs in H&E. However,
as PHH3 facilitates the recognition of cells indistinguishable from H&E stain
alone, the use of this ground truth could potentially introduce noise into the
H&E-related dataset, impacting model performance. This study analyzes the
impact of PHH3-assisted MF annotation on inter-rater reliability and object
level agreement through an extensive multi-rater experiment. We found that the
annotators' object-level agreement increased when using PHH3-assisted labeling.
Subsequently, MF detectors were evaluated on the resulting datasets to
investigate the influence of PHH3-assisted labeling on the models' performance.
Additionally, a novel dual-stain MF detector was developed to investigate the
interpretation-shift of PHH3-assisted labels used in H&E, which clearly
outperformed single-stain detectors. However, the PHH3-assisted labels did not
have a positive effect on solely H&E-based models. The high performance of our
dual-input detector reveals an information mismatch between the H&E and
PHH3-stained images as the cause of this effect.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 1 Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InfiniBench: A Comprehensive Benchmark for Large Multimodal Models in
  Very Long Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kirolos Ataallah, Chenhui Gou, Eslam Abdelrahman, Khushbu Pahwa, Jian Ding, Mohamed Elhoseiny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding long videos, ranging from tens of minutes to several hours,
presents unique challenges in video comprehension. Despite the increasing
importance of long-form video content, existing benchmarks primarily focus on
shorter clips. To address this gap, we introduce InfiniBench a comprehensive
benchmark for very long video understanding which presents 1)The longest video
duration, averaging 76.34 minutes; 2) The largest number of question-answer
pairs, 108.2K; 3) Diversity in questions that examine nine different skills and
include both multiple-choice questions and open-ended questions; 4)
Humancentric, as the video sources come from movies and daily TV shows, with
specific human-level question designs such as Movie Spoiler Questions that
require critical thinking and comprehensive understanding. Using InfiniBench,
we comprehensively evaluate existing Large MultiModality Models (LMMs) on each
skill, including the commercial model Gemini 1.5 Flash and the open-source
models. The evaluation shows significant challenges in our benchmark.Our
results show that the best AI models such Gemini struggles to perform well with
42.72% average accuracy and 2.71 out of 5 average score. We hope this benchmark
will stimulate the LMMs community towards long video and human-level
understanding. Our benchmark can be accessed at
https://vision-cair.github.io/InfiniBench/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 page ,17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FootBots: A Transformer-based Architecture for Motion Prediction in
  Soccer <span class="chip">ICIP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillem Capellera, Luis Ferraz, Antonio Rubio, Antonio Agudo, Francesc Moreno-Noguer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion prediction in soccer involves capturing complex dynamics from player
and ball interactions. We present FootBots, an encoder-decoder
transformer-based architecture addressing motion prediction and conditioned
motion prediction through equivariance properties. FootBots captures temporal
and social dynamics using set attention blocks and multi-attention block
decoder. Our evaluation utilizes two datasets: a real soccer dataset and a
tailored synthetic one. Insights from the synthetic dataset highlight the
effectiveness of FootBots' social attention mechanism and the significance of
conditioned motion prediction. Empirical results on real soccer data
demonstrate that FootBots outperforms baselines in motion prediction and excels
in conditioned tasks, such as predicting the players based on the ball
position, predicting the offensive (defensive) team based on the ball and the
defensive (offensive) team, and predicting the ball position based on all
players. Our evaluation connects quantitative and qualitative findings.
https://youtu.be/9kaEkfzG3L8
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at IEEE ICIP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StreamMOTP: Streaming and Unified Framework for Joint 3D Multi-Object
  Tracking and Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaheng Zhuang, Guoan Wang, Siyu Zhang, Xiyang Wang, Hangning Zhou, Ziyao Xu, Chi Zhang, Zhiheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D multi-object tracking and trajectory prediction are two crucial modules in
autonomous driving systems. Generally, the two tasks are handled separately in
traditional paradigms and a few methods have started to explore modeling these
two tasks in a joint manner recently. However, these approaches suffer from the
limitations of single-frame training and inconsistent coordinate
representations between tracking and prediction tasks. In this paper, we
propose a streaming and unified framework for joint 3D Multi-Object Tracking
and trajectory Prediction (StreamMOTP) to address the above challenges.
Firstly, we construct the model in a streaming manner and exploit a memory bank
to preserve and leverage the long-term latent features for tracked objects more
effectively. Secondly, a relative spatio-temporal positional encoding strategy
is introduced to bridge the gap of coordinate representations between the two
tasks and maintain the pose-invariance for trajectory prediction. Thirdly, we
further improve the quality and consistency of predicted trajectories with a
dual-stream predictor. We conduct extensive experiments on popular nuSences
dataset and the experimental results demonstrate the effectiveness and
superiority of StreamMOTP, which outperforms previous methods significantly on
both tasks. Furthermore, we also prove that the proposed framework has great
potential and advantages in actual applications of autonomous driving.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LightStereo: Channel Boost Is All Your Need for Efficient 2D Cost
  Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianda Guo, Chenming Zhang, Dujun Nie, Wenzhao Zheng, Youmin Zhang, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LightStereo, a cutting-edge stereo-matching network crafted to
accelerate the matching process. Departing from conventional methodologies that
rely on aggregating computationally intensive 4D costs, LightStereo adopts the
3D cost volume as a lightweight alternative. While similar approaches have been
explored previously, our breakthrough lies in enhancing performance through a
dedicated focus on the channel dimension of the 3D cost volume, where the
distribution of matching costs is encapsulated. Our exhaustive exploration has
yielded plenty of strategies to amplify the capacity of the pivotal dimension,
ensuring both precision and efficiency. We compare the proposed LightStereo
with existing state-of-the-art methods across various benchmarks, which
demonstrate its superior performance in speed, accuracy, and resource
utilization. LightStereo achieves a competitive EPE metric in the SceneFlow
datasets while demanding a minimum of only 22 GFLOPs, with an inference time of
just 17 ms. Our comprehensive analysis reveals the effect of 2D cost
aggregation for stereo matching, paving the way for real-world applications of
efficient stereo systems. Code will be available at
\url{https://github.com/XiandaGuo/OpenStereo}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code will be available at
  \url{https://github.com/XiandaGuo/OpenStereo}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion Loss Attacking: Adversarial Attack Perception for Skeleton based
  on Multi-dimensional Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Liu, Qing Xu, Qijian Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attack on skeletal motion is a hot topic. However, existing
researches only consider part of dynamic features when measuring distance
between skeleton graph sequences, which results in poor imperceptibility. To
this end, we propose a novel adversarial attack method to attack action
recognizers for skeletal motions. Firstly, our method systematically proposes a
dynamic distance function to measure the difference between skeletal motions.
Meanwhile, we innovatively introduce emotional features for complementary
information. In addition, we use Alternating Direction Method of
Multipliers(ADMM) to solve the constrained optimization problem, which
generates adversarial samples with better imperceptibility to deceive the
classifiers. Experiments show that our method is effective on multiple action
classifiers and datasets. When the perturbation magnitude measured by l norms
is the same, the dynamic perturbations generated by our method are much lower
than that of other methods. What's more, we are the first to prove the
effectiveness of emotional features, and provide a new idea for measuring the
distance between skeletal motions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extract More from Less: Efficient Fine-Grained Visual Recognition in
  Low-Data Regimes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmitry Demidov, Abduragim Shtanchaev, Mihail Mihaylov, Mohammad Almansoori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emerging task of fine-grained image classification in low-data regimes
assumes the presence of low inter-class variance and large intra-class
variation along with a highly limited amount of training samples per class.
However, traditional ways of separately dealing with fine-grained
categorisation and extremely scarce data may be inefficient under both these
harsh conditions presented together. In this paper, we present a novel
framework, called AD-Net, aiming to enhance deep neural network performance on
this challenge by leveraging the power of Augmentation and Distillation
techniques. Specifically, our approach is designed to refine learned features
through self-distillation on augmented samples, mitigating harmful overfitting.
We conduct comprehensive experiments on popular fine-grained image
classification benchmarks where our AD-Net demonstrates consistent improvement
over traditional fine-tuning and state-of-the-art low-data techniques.
Remarkably, with the smallest data available, our framework shows an
outstanding relative accuracy increase of up to 45 % compared to standard
ResNet-50 and up to 27 % compared to the closest SOTA runner-up. We emphasise
that our approach is practically architecture-independent and adds zero extra
cost at inference time. Additionally, we provide an extensive study on the
impact of every framework's component, highlighting the importance of each in
achieving optimal performance. Source code and trained models are publicly
available at github.com/demidovd98/fgic_lowd.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper and Appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EgoGaussian: Dynamic Scene Understanding from Egocentric Video with 3D
  Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daiwei Zhang, Gengyan Li, Jiajie Li, Mickaël Bressieux, Otmar Hilliges, Marc Pollefeys, Luc Van Gool, Xi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human activities are inherently complex, and even simple household tasks
involve numerous object interactions. To better understand these activities and
behaviors, it is crucial to model their dynamic interactions with the
environment. The recent availability of affordable head-mounted cameras and
egocentric data offers a more accessible and efficient means to understand
dynamic human-object interactions in 3D environments. However, most existing
methods for human activity modeling either focus on reconstructing 3D models of
hand-object or human-scene interactions or on mapping 3D scenes, neglecting
dynamic interactions with objects. The few existing solutions often require
inputs from multiple sources, including multi-camera setups, depth-sensing
cameras, or kinesthetic sensors. To this end, we introduce EgoGaussian, the
first method capable of simultaneously reconstructing 3D scenes and dynamically
tracking 3D object motion from RGB egocentric input alone. We leverage the
uniquely discrete nature of Gaussian Splatting and segment dynamic interactions
from the background. Our approach employs a clip-level online learning pipeline
that leverages the dynamic nature of human activities, allowing us to
reconstruct the temporal evolution of the scene in chronological order and
track rigid object motion. Additionally, our method automatically segments
object and background Gaussians, providing 3D representations for both static
scenes and dynamic objects. EgoGaussian outperforms previous NeRF and Dynamic
Gaussian methods in challenging in-the-wild videos and we also qualitatively
demonstrate the high quality of the reconstructed models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive <span class="highlight-title">Generative</span> Replay for Task-Incremental Segmentation with
  Concurrent Appearance and Semantic Forgetting <span class="chip">MICCAI24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Li, Jingyang Zhang, Pheng-Ann Heng, Lixu Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalist segmentation models are increasingly favored for diverse tasks
involving various objects from different image sources. Task-Incremental
Learning (TIL) offers a privacy-preserving training paradigm using tasks
arriving sequentially, instead of gathering them due to strict data sharing
policies. However, the task evolution can span a wide scope that involves
shifts in both image appearance and segmentation semantics with intricate
correlation, causing concurrent appearance and semantic forgetting. To solve
this issue, we propose a Comprehensive Generative Replay (CGR) framework that
restores appearance and semantic knowledge by synthesizing image-mask pairs to
mimic past task data, which focuses on two aspects: modeling image-mask
correspondence and promoting scalability for diverse tasks. Specifically, we
introduce a novel Bayesian Joint Diffusion (BJD) model for high-quality
synthesis of image-mask pairs with their correspondence explicitly preserved by
conditional denoising. Furthermore, we develop a Task-Oriented Adapter (TOA)
that recalibrates prompt embeddings to modulate the diffusion model, making the
data synthesis compatible with different tasks. Experiments on incremental
tasks (cardiac, fundus and prostate segmentation) show its clear advantage for
alleviating concurrent appearance and semantic forgetting. Code is available at
https://github.com/jingyzhang/CGR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MICCAI24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structure-aware World Model for Probe Guidance via Large-scale
  Self-supervised Pre-train 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojun Jiang, Meng Li, Zhenguo Sun, Ning Jia, Yu Sun, Shaqi Luo, Shiji Song, Gao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The complex structure of the heart leads to significant challenges in
echocardiography, especially in acquisition cardiac ultrasound images.
Successful echocardiography requires a thorough understanding of the structures
on the two-dimensional plane and the spatial relationships between planes in
three-dimensional space. In this paper, we innovatively propose a large-scale
self-supervised pre-training method to acquire a cardiac structure-aware world
model. The core innovation lies in constructing a self-supervised task that
requires structural inference by predicting masked structures on a 2D plane and
imagining another plane based on pose transformation in 3D space. To support
large-scale pre-training, we collected over 1.36 million echocardiograms from
ten standard views, along with their 3D spatial poses. In the downstream probe
guidance task, we demonstrate that our pre-trained model consistently reduces
guidance errors across the ten most common standard views on the test set with
0.29 million samples from 74 routine clinical scans, indicating that
structure-aware pre-training benefits the scanning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPIRONet: Spatial-Frequency Learning and Topological Channel Interaction
  Network for Vessel Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        De-Xing Huang, Xiao-Hu Zhou, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Zhen-Qiu Feng, Mei-Jiang Gui, Hao Li, Tian-Yu Xiang, Bo-Xian Yao, Zeng-Guang Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic vessel segmentation is paramount for developing next-generation
interventional navigation systems. However, current approaches suffer from
suboptimal segmentation performances due to significant challenges in
intraoperative images (i.e., low signal-to-noise ratio, small or slender
vessels, and strong interference). In this paper, a novel spatial-frequency
learning and topological channel interaction network (SPIRONet) is proposed to
address the above issues. Specifically, dual encoders are utilized to
comprehensively capture local spatial and global frequency vessel features.
Then, a cross-attention fusion module is introduced to effectively fuse spatial
and frequency features, thereby enhancing feature discriminability.
Furthermore, a topological channel interaction module is designed to filter out
task-irrelevant responses based on graph neural networks. Extensive
experimental results on several challenging datasets (CADSA, CAXF, DCA1, and
XCAD) demonstrate state-of-the-art performances of our method. Moreover, the
inference speed of SPIRONet is 21 FPS with a 512x512 input size, surpassing
clinical real-time requirements (6~12FPS). These promising outcomes indicate
SPIRONet's potential for integration into vascular interventional navigation
systems. Code is available at https://github.com/Dxhuang-CASIA/SPIRONet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-Instruct: Generated Visual Instructions for Large Multimodal Model
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Liu, Xin Huang, Jinliang Zheng, Boxiao Liu, Jia Wang, Osamu Yoshie, Yu Liu, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces MM-Instruct, a large-scale dataset of diverse and
high-quality visual instruction data designed to enhance the
instruction-following capabilities of large multimodal models (LMMs). While
existing visual instruction datasets often focus on question-answering, they
struggle to generalize to broader application scenarios such as creative
writing, summarization, or image analysis. To address these limitations, we
propose a novel approach to constructing MM-Instruct that leverages the strong
instruction-following capabilities of existing LLMs to generate novel visual
instruction data from large-scale but conventional image captioning datasets.
MM-Instruct first leverages ChatGPT to automatically generate diverse
instructions from a small set of seed instructions through augmenting and
summarization. It then matches these instructions with images and uses an
open-sourced large language model (LLM) to generate coherent answers to the
instruction-image pairs. The LLM is grounded by the detailed text descriptions
of images in the whole answer generation process to guarantee the alignment of
the instruction data. Moreover, we introduce a benchmark based on the generated
instruction data to evaluate the instruction-following capabilities of existing
LMMs. We demonstrate the effectiveness of MM-Instruct by training a LLaVA-1.5
model on the generated data, denoted as LLaVA-Instruct, which exhibits
significant improvements in instruction-following capabilities compared to
LLaVA-1.5 models. The MM-Instruct dataset, benchmark, and pre-trained models
are available at https://github.com/jihaonew/MM-Instruct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Dataset and models are available at
  https://github.com/jihaonew/MM-Instruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EPOCH: Jointly Estimating the 3D Pose of Cameras and Humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicola Garau, Giulia Martinelli, Niccolò Bisagno, Denis Tomè, Carsten Stoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular Human Pose Estimation (HPE) aims at determining the 3D positions of
human joints from a single 2D image captured by a camera. However, a single 2D
point in the image may correspond to multiple points in 3D space. Typically,
the uniqueness of the 2D-3D relationship is approximated using an orthographic
or weak-perspective camera model. In this study, instead of relying on
approximations, we advocate for utilizing the full perspective camera model.
This involves estimating camera parameters and establishing a precise,
unambiguous 2D-3D relationship. To do so, we introduce the EPOCH framework,
comprising two main components: the pose lifter network (LiftNet) and the pose
regressor network (RegNet). LiftNet utilizes the full perspective camera model
to precisely estimate the 3D pose in an unsupervised manner. It takes a 2D pose
and camera parameters as inputs and produces the corresponding 3D pose
estimation. These inputs are obtained from RegNet, which starts from a single
image and provides estimates for the 2D pose and camera parameters. RegNet
utilizes only 2D pose data as weak supervision. Internally, RegNet predicts a
3D pose, which is then projected to 2D using the estimated camera parameters.
This process enables RegNet to establish the unambiguous 2D-3D relationship.
Our experiments show that modeling the lifting as an unsupervised task with a
camera in-the-loop results in better generalization to unseen data. We obtain
state-of-the-art results for the 3D HPE on the Human3.6M and MPI-INF-3DHP
datasets. Our code is available at: [Github link upon acceptance, see
supplementary materials].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision Transformer with Key-select Routing Attention for Single Image
  Dehazing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lihan Tong, Weijia Li, Qingxia Yang, Liyuan Chen, Peng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Ksformer, utilizing Multi-scale Key-select Routing Attention
(MKRA) for intelligent selection of key areas through multi-channel,
multi-scale windows with a top-k operator, and Lightweight Frequency Processing
Module (LFPM) to enhance high-frequency features, outperforming other dehazing
methods in tests.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages,4 figures,IEICE Trans. Information and Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMRo: Are Multimodal LLMs Eligible as the Brain for In-Home Robotics? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinming Li, Yichen Zhu, Zhiyuan Xu, Jindong Gu, Minjie Zhu, Xin Liu, Ning Liu, Yaxin Peng, Feifei Feng, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is fundamentally challenging for robots to serve as useful assistants in
human environments because this requires addressing a spectrum of sub-problems
across robotics, including perception, language understanding, reasoning, and
planning. The recent advancements in Multimodal Large Language Models (MLLMs)
have demonstrated their exceptional abilities in solving complex mathematical
problems, mastering commonsense and abstract reasoning. This has led to the
recent utilization of MLLMs as the brain in robotic systems, enabling these
models to conduct high-level planning prior to triggering low-level control
actions for task execution. However, it remains uncertain whether existing
MLLMs are reliable in serving the brain role of robots. In this study, we
introduce the first benchmark for evaluating Multimodal LLM for Robotic (MMRo)
benchmark, which tests the capability of MLLMs for robot applications.
Specifically, we identify four essential capabilities perception, task
planning, visual reasoning, and safety measurement that MLLMs must possess to
qualify as the robot's central processing unit. We have developed several
scenarios for each capability, resulting in a total of 14 metrics for
evaluation. We present experimental results for various MLLMs, including both
commercial and open-source models, to assess the performance of existing
systems. Our findings indicate that no single model excels in all areas,
suggesting that current MLLMs are not yet trustworthy enough to serve as the
cognitive core for robots. Our data can be found in
https://mm-robobench.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Fusion Model for Brain Tumor Classification Using Fine-Grained
  Gradient Preservation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niful Islam, Mohaiminul Islam Bhuiyan, Jarin Tasnim Raya, Nur Shazwani Kamarudin, Khan Md Hasib, M. F. Mridha, Dewan Md. Farid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain tumors are one of the most common diseases that lead to early death if
not diagnosed at an early stage. Traditional diagnostic approaches are
extremely time-consuming and prone to errors. In this context, computer
vision-based approaches have emerged as an effective tool for accurate brain
tumor classification. While some of the existing solutions demonstrate
noteworthy accuracy, the models become infeasible to deploy in areas where
computational resources are limited. This research addresses the need for
accurate and fast classification of brain tumors with a priority of deploying
the model in technologically underdeveloped regions. The research presents a
novel architecture for precise brain tumor classification fusing pretrained
ResNet152V2 and modified VGG16 models. The proposed architecture undergoes a
diligent fine-tuning process that ensures fine gradients are preserved in deep
neural networks, which are essential for effective brain tumor classification.
The proposed solution incorporates various image processing techniques to
improve image quality and achieves an astounding accuracy of 98.36% and 98.04%
in Figshare and Kaggle datasets respectively. This architecture stands out for
having a streamlined profile, with only 2.8 million trainable parameters. We
have leveraged 8-bit quantization to produce a model of size 73.881 MB,
significantly reducing it from the previous size of 289.45 MB, ensuring smooth
deployment in edge devices even in resource-constrained areas. Additionally,
the use of Grad-CAM improves the interpretability of the model, offering
insightful information regarding its decision-making process. Owing to its high
discriminative ability, this model can be a reliable option for accurate brain
tumor classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Radiological Diagnosis: A Collaborative Approach Integrating
  AI and Human Expertise for Visual Miss Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akash Awasthi, Ngan Le, Zhigang Deng, Carol C. Wu, Hien Van Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-AI collaboration to identify and correct perceptual errors in chest
radiographs has not been previously explored. This study aimed to develop a
collaborative AI system, CoRaX, which integrates eye gaze data and radiology
reports to enhance diagnostic accuracy in chest radiology by pinpointing
perceptual errors and refining the decision-making process. Using public
datasets REFLACX and EGD-CXR, the study retrospectively developed CoRaX,
employing a large multimodal model to analyze image embeddings, eye gaze data,
and radiology reports. The system's effectiveness was evaluated based on its
referral-making process, the quality of referrals, and performance in
collaborative diagnostic settings. CoRaX was tested on a simulated error
dataset of 271 samples with 28% (93 of 332) missed abnormalities. The system
corrected 21% (71 of 332) of these errors, leaving 7% (22 of 312) unresolved.
The Referral-Usefulness score, indicating the accuracy of predicted regions for
all true referrals, was 0.63 (95% CI 0.59, 0.68). The Total-Usefulness score,
reflecting the diagnostic accuracy of CoRaX's interactions with radiologists,
showed that 84% (237 of 280) of these interactions had a score above 0.40. In
conclusion, CoRaX efficiently collaborates with radiologists to address
perceptual errors across various abnormalities, with potential applications in
the education and training of novice radiologists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review in Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MimicMotion: High-Quality Human Motion Video Generation with
  Confidence-aware Pose Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, Fangyuan Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, generative artificial intelligence has achieved significant
advancements in the field of image generation, spawning a variety of
applications. However, video generation still faces considerable challenges in
various aspects, such as controllability, video length, and richness of
details, which hinder the application and popularization of this technology. In
this work, we propose a controllable video generation framework, dubbed
MimicMotion, which can generate high-quality videos of arbitrary length
mimicking specific motion guidance. Compared with previous methods, our
approach has several highlights. Firstly, we introduce confidence-aware pose
guidance that ensures high frame quality and temporal smoothness. Secondly, we
introduce regional loss amplification based on pose confidence, which
significantly reduces image distortion. Lastly, for generating long and smooth
videos, we propose a progressive latent fusion strategy. By this means, we can
produce videos of arbitrary length with acceptable resource consumption. With
extensive experiments and user studies, MimicMotion demonstrates significant
improvements over previous approaches in various aspects. Detailed results and
comparisons are available on our project page:
https://tencent.github.io/MimicMotion .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning-based Depth Estimation Methods from Monocular Image and
  Videos: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uchitha Rajapaksha, Ferdous Sohel, Hamid Laga, Dean Diepeveen, Mohammed Bennamoun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating depth from single RGB images and videos is of widespread interest
due to its applications in many areas, including autonomous driving, 3D
reconstruction, digital entertainment, and robotics. More than 500 deep
learning-based papers have been published in the past 10 years, which indicates
the growing interest in the task. This paper presents a comprehensive survey of
the existing deep learning-based methods, the challenges they address, and how
they have evolved in their architecture and supervision methods. It provides a
taxonomy for classifying the current work based on their input and output
modalities, network architectures, and learning methods. It also discusses the
major milestones in the history of monocular depth estimation, and different
pipelines, datasets, and evaluation metrics used in existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages, 10 figures, The paper has been accepted for publication in
  ACM Computing Surveys 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond First-Order: A Multi-Scale Approach to Finger Knuckle Print
  Biometrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengrui Gao, Ziyuan Yang, Andrew Beng Jin Teoh, Min Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, finger knuckle prints (FKPs) have gained attention due to their
rich textural patterns, positioning them as a promising biometric for identity
recognition. Prior FKP recognition methods predominantly leverage first-order
feature descriptors, which capture intricate texture details but fail to
account for structural information. Emerging research, however, indicates that
second-order textures, which describe the curves and arcs of the textures,
encompass this overlooked structural information. This paper introduces a novel
FKP recognition approach, the Dual-Order Texture Competition Network (DOTCNet),
designed to capture texture information in FKP images comprehensively. DOTCNet
incorporates three dual-order texture competitive modules (DTCMs), each
targeting textures at different scales. Each DTCM employs a learnable texture
descriptor, specifically a learnable Gabor filter (LGF), to extract texture
features. By leveraging LGFs, the network extracts first and second order
textures to describe fine textures and structural features thoroughly.
Furthermore, an attention mechanism enhances relevant features in the
first-order features, thereby highlighting significant texture details. For
second-order features, a competitive mechanism emphasizes structural
information while reducing noise from higher-order features. Extensive
experimental results reveal that DOTCNet significantly outperforms several
standard algorithms on the publicly available PolyU-FKP dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PopAlign: Population-Level Alignment for Fair Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shufan Li, Harkanwar Singh, Aditya Grover
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) models achieve high-fidelity generation through extensive
training on large datasets. However, these models may unintentionally pick up
undesirable biases of their training data, such as over-representation of
particular identities in gender or ethnicity neutral prompts. Existing
alignment methods such as Reinforcement Learning from Human Feedback (RLHF) and
Direct Preference Optimization (DPO) fail to address this problem effectively
because they operate on pairwise preferences consisting of individual samples,
while the aforementioned biases can only be measured at a population level. For
example, a single sample for the prompt "doctor" could be male or female, but a
model generating predominantly male doctors even with repeated sampling
reflects a gender bias. To address this limitation, we introduce PopAlign, a
novel approach for population-level preference optimization, while standard
optimization would prefer entire sets of samples over others. We further derive
a stochastic lower bound that directly optimizes for individual samples from
preferred populations over others for scalable training. Using human evaluation
and standard image quality and bias metrics, we show that PopAlign
significantly mitigates the bias of pretrained T2I models while largely
preserving the generation quality. Code is available at
https://github.com/jacklishufan/PopAlignSDXL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSAKD: Knowledge Distillation with Cross Self-Attention for
  Hyperspectral and Multispectral Image Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Chung Hsu, Chih-Chien Ni, Chia-Ming Lee, Li-Wei Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral imaging, capturing detailed spectral information for each
pixel, is pivotal in diverse scientific and industrial applications. Yet, the
acquisition of high-resolution (HR) hyperspectral images (HSIs) often needs to
be addressed due to the hardware limitations of existing imaging systems. A
prevalent workaround involves capturing both a high-resolution multispectral
image (HR-MSI) and a low-resolution (LR) HSI, subsequently fusing them to yield
the desired HR-HSI. Although deep learning-based methods have shown promising
in HR-MSI/LR-HSI fusion and LR-HSI super-resolution (SR), their substantial
model complexities hinder deployment on resource-constrained imaging devices.
This paper introduces a novel knowledge distillation (KD) framework for
HR-MSI/LR-HSI fusion to achieve SR of LR-HSI. Our KD framework integrates the
proposed Cross-Layer Residual Aggregation (CLRA) block to enhance efficiency
for constructing Dual Two-Streamed (DTS) network structure, designed to extract
joint and distinct features from LR-HSI and HR-MSI simultaneously. To fully
exploit the spatial and spectral feature representations of LR-HSI and HR-MSI,
we propose a novel Cross Self-Attention (CSA) fusion module to adaptively fuse
those features to improve the spatial and spectral quality of the reconstructed
HR-HSI. Finally, the proposed KD-based joint loss function is employed to
co-train the teacher and student networks. Our experimental results demonstrate
that the student model not only achieves comparable or superior LR-HSI SR
performance but also significantly reduces the model-size and computational
requirements. This marks a substantial advancement over existing
state-of-the-art methods. The source code is available at
https://github.com/ming053l/CSAKD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to TIP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PM-VIS+: High-Performance Video Instance Segmentation without Video
  Annotation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangjing Yang, Dun Liu, Xin Wang, Zhe Li, Barathwaj Anandan, Yi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video instance segmentation requires detecting, segmenting, and tracking
objects in videos, typically relying on costly video annotations. This paper
introduces a method that eliminates video annotations by utilizing image
datasets. The PM-VIS algorithm is adapted to handle both bounding box and
instance-level pixel annotations dynamically. We introduce ImageNet-bbox to
supplement missing categories in video datasets and propose the PM-VIS+
algorithm to adjust supervision based on annotation types. To enhance accuracy,
we use pseudo masks and semi-supervised optimization techniques on unannotated
video data. This method achieves high video instance segmentation performance
without manual video annotations, offering a cost-effective solution and new
perspectives for video instance segmentation applications. The code will be
available in https://github.com/ldknight/PM-VIS-plus
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MIPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Basketball-SORT: An Association Method for Complex Multi-object
  Occlusion Problems in Basketball Multi-object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingrui Hu, Atom Scott, Calvin Yeung, Keisuke Fujii
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent deep learning-based object detection approaches have led to
significant progress in multi-object tracking (MOT) algorithms. The current MOT
methods mainly focus on pedestrian or vehicle scenes, but basketball sports
scenes are usually accompanied by three or more object occlusion problems with
similar appearances and high-intensity complex motions, which we call complex
multi-object occlusion (CMOO). Here, we propose an online and robust MOT
approach, named Basketball-SORT, which focuses on the CMOO problems in
basketball videos. To overcome the CMOO problem, instead of using the
intersection-over-union-based (IoU-based) approach, we use the trajectories of
neighboring frames based on the projected positions of the players. Our method
designs the basketball game restriction (BGR) and reacquiring Long-Lost IDs
(RLLI) based on the characteristics of basketball scenes, and we also solve the
occlusion problem based on the player trajectories and appearance features.
Experimental results show that our method achieves a Higher Order Tracking
Accuracy (HOTA) score of 63.48$\%$ on the basketball fixed video dataset and
outperforms other recent popular approaches. Overall, our approach solved the
CMOO problem more effectively than recent MOT algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AstMatch: Adversarial Self-training Consistency Framework for
  Semi-Supervised Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanghao Zhu, Jing Zhang, Juanxiu Liu, Xiaohui Du, Ruqian Hao, Yong Liu, Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) has shown considerable potential in medical
image segmentation, primarily leveraging consistency regularization and
pseudo-labeling. However, many SSL approaches only pay attention to low-level
consistency and overlook the significance of pseudo-label reliability.
Therefore, in this work, we propose an adversarial self-training consistency
framework (AstMatch). Firstly, we design an adversarial consistency
regularization (ACR) approach to enhance knowledge transfer and strengthen
prediction consistency under varying perturbation intensities. Second, we apply
a feature matching loss for adversarial training to incorporate high-level
consistency regularization. Additionally, we present the pyramid channel
attention (PCA) and efficient channel and spatial attention (ECSA) modules to
improve the discriminator's performance. Finally, we propose an adaptive
self-training (AST) approach to ensure the pseudo-labels' quality. The proposed
AstMatch has been extensively evaluated with cutting-edge SSL methods on three
public-available datasets. The experimental results under different labeled
ratios indicate that AstMatch outperforms other existing methods, achieving new
state-of-the-art performance. Our code will be available at
https://github.com/GuanghaoZhu663/AstMatch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Event Stream Super-Resolution with Recursive Multi-Branch
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quanmin Liang, Zhilin Huang, Xiawu Zheng, Feidiao Yang, Jun Peng, Kai Huang, Yonghong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Event Stream Super-Resolution (ESR) methods overlook the redundant
and complementary information present in positive and negative events within
the event stream, employing a direct mixing approach for super-resolution,
which may lead to detail loss and inefficiency. To address these issues, we
propose an efficient Recursive Multi-Branch Information Fusion Network (RMFNet)
that separates positive and negative events for complementary information
extraction, followed by mutual supplementation and refinement. Particularly, we
introduce Feature Fusion Modules (FFM) and Feature Exchange Modules (FEM). FFM
is designed for the fusion of contextual information within neighboring event
streams, leveraging the coupling relationship between positive and negative
events to alleviate the misleading of noises in the respective branches. FEM
efficiently promotes the fusion and exchange of information between positive
and negative branches, enabling superior local information enhancement and
global information complementation. Experimental results demonstrate that our
approach achieves over 17% and 31% improvement on synthetic and real datasets,
accompanied by a 2.3X acceleration. Furthermore, we evaluate our method on two
downstream event-driven applications, \emph{i.e.}, object recognition and video
reconstruction, achieving remarkable results that outperform existing methods.
Our code and Supplementary Material are available at
https://github.com/Lqm26/RMFNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precision matters: Precision-aware ensemble for weakly supervised
  semantic segmentation <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junsung Park, Hyunjung Shim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly Supervised Semantic Segmentation (WSSS) employs weak supervision, such
as image-level labels, to train the segmentation model. Despite the impressive
achievement in recent WSSS methods, we identify that introducing weak labels
with high mean Intersection of Union (mIoU) does not guarantee high
segmentation performance. Existing studies have emphasized the importance of
prioritizing precision and reducing noise to improve overall performance. In
the same vein, we propose ORANDNet, an advanced ensemble approach tailored for
WSSS. ORANDNet combines Class Activation Maps (CAMs) from two different
classifiers to increase the precision of pseudo-masks (PMs). To further
mitigate small noise in the PMs, we incorporate curriculum learning. This
involves training the segmentation model initially with pairs of smaller-sized
images and corresponding PMs, gradually transitioning to the original-sized
pairs. By combining the original CAMs of ResNet-50 and ViT, we significantly
improve the segmentation performance over the single-best model and the naive
ensemble model, respectively. We further extend our ensemble method to CAMs
from AMN (ResNet-like) and MCTformer (ViT-like) models, achieving performance
benefits in advanced WSSS models. It highlights the potential of our ORANDNet
as a final add-on module for WSSS models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures, accepted in AAAI 2024 Edge Intelligence Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Predictive Simulation Using Structured Graphical Models and
  Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghua Lou, Meet Dave, Shrinu Kushagra, Miguel Lazaro-Gredilla, Kevin Murphy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an approach to simulating trajectories of multiple interacting
agents (road users) based on transformers and probabilistic graphical models
(PGMs), and apply it to the Waymo SimAgents challenge. The transformer baseline
is based on the MTR model, which predicts multiple future trajectories
conditioned on the past trajectories and static road layout features. We then
improve upon these generated trajectories using a PGM, which contains factors
which encode prior knowledge, such as a preference for smooth trajectories, and
avoidance of collisions with static obstacles and other moving agents. We
perform (approximate) MAP inference in this PGM using the Gauss-Newton method.
Finally we sample $K=32$ trajectories for each of the $N \sim 100$ agents for
the next $T=8 \Delta$ time steps, where $\Delta=10$ is the sampling rate per
second. Following the Model Predictive Control (MPC) paradigm, we only return
the first element of our forecasted trajectories at each step, and then we
replan, so that the simulation can constantly adapt to its changing
environment. We therefore call our approach "Model Predictive Simulation" or
MPS. We show that MPS improves upon the MTR baseline, especially in safety
critical metrics such as collision rate. Furthermore, our approach is
compatible with any underlying forecasting model, and does not require extra
training, so we believe it is a valuable contribution to the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Special Mention at the Waymo Sim Agents Challenge 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PPTFormer: Pseudo Multi-Perspective Transformer for UAV Segmentation <span class="chip">IJCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deyi Ji, Wenwei Jin, Hongtao Lu, Feng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ascension of Unmanned Aerial Vehicles (UAVs) in various fields
necessitates effective UAV image segmentation, which faces challenges due to
the dynamic perspectives of UAV-captured images. Traditional segmentation
algorithms falter as they cannot accurately mimic the complexity of UAV
perspectives, and the cost of obtaining multi-perspective labeled datasets is
prohibitive. To address these issues, we introduce the PPTFormer, a novel
\textbf{P}seudo Multi-\textbf{P}erspective \textbf{T}rans\textbf{former}
network that revolutionizes UAV image segmentation. Our approach circumvents
the need for actual multi-perspective data by creating pseudo perspectives for
enhanced multi-perspective learning. The PPTFormer network boasts Perspective
Decomposition, novel Perspective Prototypes, and a specialized encoder and
decoder that together achieve superior segmentation results through Pseudo
Multi-Perspective Attention (PMP Attention) and fusion. Our experiments
demonstrate that PPTFormer achieves state-of-the-art performance across five
UAV segmentation datasets, confirming its capability to effectively simulate
UAV flight perspectives and significantly advance segmentation precision. This
work presents a pioneering leap in UAV scene understanding and sets a new
benchmark for future developments in semantic segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Video Compression using Pixel Shift Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hitesh Saai Mananchery Panneerselvam, Smit Anand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Video comprises approximately ~85\% of all internet traffic, but video
encoding/compression is being historically done with hard coded rules, which
has worked well but only to a certain limit. We have seen a surge in video
compression algorithms using ML-based models in the last few years and many of
them have outperformed several legacy codecs. The models range from encoding
video end to end using an ML approach or replacing some intermediate steps in
legacy codecs using ML models to increase the efficiency of those steps.
  Optimizing video storage is an essential aspect of video processing, so we
are proposing one of the possible approaches to achieve it is by avoiding
redundant data at each frame. In this paper, we want to introduce the approach
of redundancies removal in subsequent frames for a given video as a main
approach for video compression. We call this method Redundancy Removal using
Shift (R\textsuperscript2S). This method can be utilized across various Machine
Learning model algorithms, and make the compression more accessible and
adaptable. In this study, we have utilized a computer vision-based pixel point
tracking method to identify redundant pixels to encode video for optimal
storage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Deep Clustering: From the Prior Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiding Lu, Haobin Li, Yunfan Li, Yijie Lin, Xi Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facilitated by the powerful feature extraction ability of neural networks,
deep clustering has achieved great success in analyzing high-dimensional and
complex real-world data. The performance of deep clustering methods is affected
by various factors such as network structures and learning objectives. However,
as pointed out in this survey, the essence of deep clustering lies in the
incorporation and utilization of prior knowledge, which is largely ignored by
existing works. From pioneering deep clustering methods based on data structure
assumptions to recent contrastive clustering methods based on data augmentation
invariances, the development of deep clustering intrinsically corresponds to
the evolution of prior knowledge. In this survey, we provide a comprehensive
review of deep clustering methods by categorizing them into six types of prior
knowledge. We find that in general the prior innovation follows two trends,
namely, i) from mining to constructing, and ii) from internal to external.
Besides, we provide a benchmark on five widely-used datasets and analyze the
performance of methods with diverse priors. By providing a novel prior
knowledge perspective, we hope this survey could provide some novel insights
and inspire future research in the deep clustering community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SK-VQA: Synthetic Knowledge Generation at Scale for Training
  Context-Augmented Multimodal LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Su, Man Luo, Kris W Pan, Tien Pei Chou, Vasudev Lal, Phillip Howard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data generation has gained significant attention recently for its
utility in training large vision and language models. However, the application
of synthetic data to the training of multimodal context-augmented generation
systems has been relatively unexplored. This gap in existing work is important
because existing vision and language models (VLMs) are not trained specifically
for context-augmented generation. Resources for adapting such models are
therefore crucial for enabling their use in retrieval-augmented generation
(RAG) settings, where a retriever is used to gather relevant information that
is then subsequently provided to a generative model via context augmentation.
To address this challenging problem, we generate SK-VQA: a large synthetic
multimodal dataset containing over 2 million question-answer pairs which
require external knowledge to determine the final answer. Our dataset is both
larger and significantly more diverse than existing resources of its kind,
possessing over 11x more unique questions and containing images from a greater
variety of sources than previously-proposed datasets. Through extensive
experiments, we demonstrate that our synthetic dataset can not only serve as a
challenging benchmark, but is also highly effective for adapting existing
generative multimodal models for context-augmented generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting <span class="highlight-title">Diffusion</span> Prior for Real-World Image Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07015v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07015v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin C. K. Chan, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to leverage prior knowledge encapsulated in
pre-trained text-to-image diffusion models for blind super-resolution (SR).
Specifically, by employing our time-aware encoder, we can achieve promising
restoration results without altering the pre-trained synthesis model, thereby
preserving the generative prior and minimizing training cost. To remedy the
loss of fidelity caused by the inherent stochasticity of diffusion models, we
employ a controllable feature wrapping module that allows users to balance
quality and fidelity by simply adjusting a scalar value during the inference
process. Moreover, we develop a progressive aggregation sampling strategy to
overcome the fixed-size constraints of pre-trained diffusion models, enabling
adaptation to resolutions of any size. A comprehensive evaluation of our method
using both synthetic and real-world benchmarks demonstrates its superiority
over current state-of-the-art approaches. Code and models are available at
https://github.com/IceClear/StableSR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCV'2024. Some Figs are compressed due to size limits.
  Uncompressed ver.:
  https://github.com/IceClear/StableSR/releases/download/UncompressedPDF/StableSR_IJCV_Uncompressed.pdf.
  Project page: https://iceclear.github.io/projects/stablesr/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EnSolver: Uncertainty-Aware Ensemble CAPTCHA Solvers with Theoretical
  Guarantees <span class="chip">UAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.15180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.15180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duc C. Hoang, Behzad Ousat, Amin Kharraz, Cuong V. Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The popularity of text-based CAPTCHA as a security mechanism to protect
websites from automated bots has prompted researches in CAPTCHA solvers, with
the aim of understanding its failure cases and subsequently making CAPTCHAs
more secure. Recently proposed solvers, built on advances in deep learning, are
able to crack even the very challenging CAPTCHAs with high accuracy. However,
these solvers often perform poorly on out-of-distribution samples that contain
visual features different from those in the training set. Furthermore, they
lack the ability to detect and avoid such samples, making them susceptible to
being locked out by defense systems after a certain number of failed attempts.
In this paper, we propose EnSolver, a family of CAPTCHA solvers that use deep
ensemble uncertainty to detect and skip out-of-distribution CAPTCHAs, making it
harder to be detected. We prove novel theoretical bounds on the effectiveness
of our solvers and demonstrate their use with state-of-the-art CAPTCHA solvers.
Our experiments show that the proposed approaches perform well when cracking
CAPTCHA datasets that contain both in-distribution and out-of-distribution
samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A previous version of this paper was presented at the Epistemic
  Uncertainty - E-pi UAI 2023 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robust</span>ness Assessment of a Runway Object Classifier for Safe Aircraft
  Taxiing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00035v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00035v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhak Elboher, Raya Elsaleh, Omri Isac, Mélanie Ducoffe, Audrey Galametz, Guillaume Povéda, Ryma Boumazouza, Noémie Cohen, Guy Katz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As deep neural networks (DNNs) are becoming the prominent solution for many
computational problems, the aviation industry seeks to explore their potential
in alleviating pilot workload and in improving operational safety. However, the
use of DNNs in this type of safety-critical applications requires a thorough
certification process. This need can be addressed through formal verification,
which provides rigorous assurances -- e.g.,~by proving the absence of certain
mispredictions. In this case-study paper, we demonstrate this process using an
image-classifier DNN currently under development at Airbus and intended for use
during the aircraft taxiing phase. We use formal methods to assess this DNN's
robustness to three common image perturbation types: noise, brightness and
contrast, and some of their combinations. This process entails multiple
invocations of the underlying verifier, which might be computationally
expensive; and we therefore propose a method that leverages the monotonicity of
these robustness properties, as well as the results of past verification
queries, in order to reduce the overall number of verification queries required
by nearly 60%. Our results provide an indication of the level of robustness
achieved by the DNN classifier under study, and indicate that it is
considerably more vulnerable to noise than to brightness or contrast
perturbations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a preprint version of the paper in the proceedings of 43rd
  Digital Avionics Systems Conference (DASC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to utilize image second-order derivative information for crisp
  edge detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05779v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05779v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changsong Liu, Wei Zhang, Yanyan Liu, Yimeng Fan, Mingyang Li, Wenlin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge detection is a fundamental task in computer vision. It has made great
progress under the development of deep convolutional neural networks (DCNNs),
some of which have achieved a beyond human-level performance. However, recent
top-performing edge detection methods tend to generate thick and noisy edge
lines. In this work, we solve this problem from two aspects: (1) the lack of
prior knowledge regarding image edges, and (2) the issue of imbalanced pixel
distribution. We propose a second-order derivative-based multi-scale contextual
enhancement module (SDMCM) to help the model locate true edge pixels accurately
by introducing the edge prior knowledge. We also construct a hybrid focal loss
function (HFL) to alleviate the imbalanced distribution issue. In addition, we
employ the conditionally parameterized convolution (CondConv) to develop a
novel boundary refinement module (BRM), which can further refine the final
output edge maps. In the end, we propose a U-shape network named LUS-Net which
is based on the SDMCM and BRM for crisp edge detection. We perform extensive
experiments on three standard benchmarks, and the experiment results illustrate
that our method can predict crisp and clean edge maps and achieves
state-of-the-art performance on the BSDS500 dataset (ODS=0.829), NYUD-V2
dataset (ODS=0.768), and BIPED dataset (ODS=0.903).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DWARF: Disease-weighted network for attention map refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17032v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17032v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhe Luo, Aurélie Pahud de Mortanges, Oana Inel, Abraham Bernstein, Mauricio Reyes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The interpretability of deep learning is crucial for evaluating the
reliability of medical imaging models and reducing the risks of inaccurate
patient recommendations. This study addresses the "human out of the loop" and
"trustworthiness" issues in medical image analysis by integrating medical
professionals into the interpretability process. We propose a disease-weighted
attention map refinement network (DWARF) that leverages expert feedback to
enhance model relevance and accuracy. Our method employs cyclic training to
iteratively improve diagnostic performance, generating precise and
interpretable feature maps. Experimental results demonstrate significant
improvements in interpretability and diagnostic accuracy across multiple
medical imaging datasets. This approach fosters effective collaboration between
AI systems and healthcare professionals, ultimately aiming to improve patient
outcomes
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling State Shifting via Local-Global Distillation for Event-Frame
  Gaze Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiading Li, Zhiyu Zhu, Jinhui Hou, Junhui Hou, Jinjian Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper tackles the problem of passive gaze estimation using both event
and frame data. Considering the inherently different physiological structures,
it is intractable to accurately estimate gaze purely based on a given state.
Thus, we reformulate gaze estimation as the quantification of the state
shifting from the current state to several prior registered anchor states.
Specifically, we propose a two-stage learning-based gaze estimation framework
that divides the whole gaze estimation process into a coarse-to-fine approach
involving anchor state selection and final gaze location. Moreover, to improve
the generalization ability, instead of learning a large gaze estimation network
directly, we align a group of local experts with a student network, where a
novel denoising distillation algorithm is introduced to utilize denoising
diffusion techniques to iteratively remove inherent noise in event data.
Extensive experiments demonstrate the effectiveness of the proposed method,
which surpasses state-of-the-art methods by a large margin of 15$\%$. The code
will be publicly available at
https://github.com/jdjdli/Denoise_distill_EF_gazetracker.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tracking Object Positions in Reinforcement Learning: A Metric for
  Keypoint Detection (extended version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00592v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00592v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emma Cramer, Jonas Reiher, Sebastian Trimpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) for robot control typically requires a detailed
representation of the environment state, including information about
task-relevant objects not directly measurable. Keypoint detectors, such as
spatial autoencoders (SAEs), are a common approach to extracting a
low-dimensional representation from high-dimensional image data. SAEs aim at
spatial features such as object positions, which are often useful
representations in robotic RL. However, whether an SAE is actually able to
track objects in the scene and thus yields a spatial state representation well
suited for RL tasks has rarely been examined due to a lack of established
metrics. In this paper, we propose to assess the performance of an SAE instance
by measuring how well keypoints track ground truth objects in images. We
present a computationally lightweight metric and use it to evaluate common
baseline SAE architectures on image data from a simulated robot task. We find
that common SAEs differ substantially in their spatial extraction capability.
Furthermore, we validate that SAEs that perform well in our metric achieve
superior performance when used in downstream RL. Thus, our metric is an
effective and lightweight indicator of RL performance before executing
expensive RL training. Building on these insights, we identify three key
modifications of SAE architectures to improve tracking performance. We make our
code available at anonymous.4open.science/r/sae-rl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LatentExplainer: Explaining Latent Representations in Deep <span class="highlight-title">Generative</span>
  Models with Multi-modal Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14862v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14862v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengdan Zhu, Raasikh Kanjiani, Jiahui Lu, Andrew Choi, Qirui Ye, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models like VAEs and diffusion models have advanced various
generation tasks by leveraging latent variables to learn data distributions and
generate high-quality samples. Despite the field of explainable AI making
strides in interpreting machine learning models, understanding latent variables
in generative models remains challenging. This paper introduces
LatentExplainer, a framework for automatically generating semantically
meaningful explanations of latent variables in deep generative models.
LatentExplainer tackles three main challenges: inferring the meaning of latent
variables, aligning explanations with inductive biases, and handling varying
degrees of explainability. By perturbing latent variables and interpreting
changes in generated data, the framework provides a systematic approach to
understanding and controlling the data generation process, enhancing the
transparency and interpretability of deep generative models. We evaluate our
proposed method on several real-world and synthetic datasets, and the results
demonstrate superior performance in generating high-quality explanations of
latent variables.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mining Open Semantics from CLIP: A Relation Transition Perspective for
  Few-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cilin Yan, Haochen Wang, Xiaolong Jiang, Yao Hu, Xu Tang, Guoliang Kang, Efstratios Gavves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Vision-Language Pre-training(CLIP) demonstrates impressive
zero-shot capability. The key to improve the adaptation of CLIP to downstream
task with few exemplars lies in how to effectively model and transfer the
useful knowledge embedded in CLIP. Previous work mines the knowledge typically
based on the limited visual samples and close-set semantics (i.e., within
target category set of downstream task). However, the aligned CLIP image/text
encoders contain abundant relationships between visual features and almost
infinite open semantics, which may benefit the few-shot learning but remains
unexplored. In this paper, we propose to mine open semantics as anchors to
perform a relation transition from image-anchor relationship to image-target
relationship to make predictions. Specifically, we adopt a transformer module
which takes the visual feature as "Query", the text features of the anchors as
"Key" and the similarity matrix between the text features of anchor and target
classes as "Value". In this way, the output of such a transformer module
represents the relationship between the image and target categories, i.e., the
classification predictions. To avoid manually selecting the open semantics, we
make the [CLASS] token of input text embedding learnable. We conduct extensive
experiments on eleven representative classification datasets. The results show
that our method performs favorably against previous state-of-the-arts
considering few-shot classification settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kandinsky 3.0 Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03511v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03511v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimir Arkhipkin, Andrei Filatov, Viacheslav Vasilev, Anastasia Maltseva, Said Azizov, Igor Pavlov, Julia Agafonova, Andrey Kuznetsov, Denis Dimitrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Kandinsky 3.0, a large-scale text-to-image generation model based
on latent diffusion, continuing the series of text-to-image Kandinsky models
and reflecting our progress to achieve higher quality and realism of image
generation. In this report we describe the architecture of the model, the data
collection procedure, the training technique, and the production system for
user interaction. We focus on the key components that, as we have identified as
a result of a large number of experiments, had the most significant impact on
improving the quality of our model compared to the others. We also describe
extensions and applications of our model, including super resolution,
inpainting, image editing, image-to-video generation, and a distilled version
of Kandinsky 3.0 - Kandinsky 3.1, which does inference in 4 steps of the
reverse process and 20 times faster without visual quality decrease. By
side-by-side human preferences comparison, Kandinsky becomes better in text
understanding and works better on specific domains. The code is available at
https://github.com/ai-forever/Kandinsky-3
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ai-forever.github.io/Kandinsky-3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deformable MRI Sequence Registration for AI-based Prostate Cancer
  Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09666v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09666v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessa Hering, Sarah de Boer, Anindo Saha, Jasper J. Twilt, Mattias P. Heinrich, Derya Yakar, Maarten de Rooij, Henkjan Huisman, Joeran S. Bosma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The PI-CAI (Prostate Imaging: Cancer AI) challenge led to expert-level
diagnostic algorithms for clinically significant prostate cancer detection. The
algorithms receive biparametric MRI scans as input, which consist of
T2-weighted and diffusion-weighted scans. These scans can be misaligned due to
multiple factors in the scanning process. Image registration can alleviate this
issue by predicting the deformation between the sequences. We investigate the
effect of image registration on the diagnostic performance of AI-based prostate
cancer diagnosis. First, the image registration algorithm, developed in
MeVisLab, is analyzed using a dataset with paired lesion annotations. Second,
the effect on diagnosis is evaluated by comparing case-level cancer diagnosis
performance between using the original dataset, rigidly aligned
diffusion-weighted scans, or deformably aligned diffusion-weighted scans. Rigid
registration showed no improvement. Deformable registration demonstrated a
substantial improvement in lesion overlap (+10% median Dice score) and a
positive yet non-significant improvement in diagnostic performance (+0.3%
AUROC, p=0.18). Our investigation shows that a substantial improvement in
lesion alignment does not directly lead to a significant improvement in
diagnostic performance. Qualitative analysis indicated that jointly developing
image registration methods and diagnostic AI algorithms could enhance
diagnostic accuracy and patient outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Data Curation for Self-Supervised Learning: A Clustering-Based
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15613v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15613v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huy V. Vo, Vasil Khalidov, Timothée Darcet, Théo Moutakanni, Nikita Smetanin, Marc Szafraniec, Hugo Touvron, Camille Couprie, Maxime Oquab, Armand Joulin, Hervé Jégou, Patrick Labatut, Piotr Bojanowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised features are the cornerstone of modern machine learning
systems. They are typically pre-trained on data collections whose construction
and curation typically require extensive human effort. This manual process has
some limitations similar to those encountered in supervised learning, e.g., the
crowd-sourced selection of data is costly and time-consuming, preventing
scaling the dataset size. In this work, we consider the problem of automatic
curation of high-quality datasets for self-supervised pre-training. We posit
that such datasets should be large, diverse and balanced, and propose a
clustering-based approach for building ones satisfying all these criteria. Our
method involves successive and hierarchical applications of $k$-means on a
large and diverse data repository to obtain clusters that distribute uniformly
among data concepts, followed by a hierarchical, balanced sampling step from
these clusters. Extensive experiments on three different data domains including
web-based images, satellite images and text show that features trained on our
automatically curated datasets outperform those trained on uncurated data while
being on par or better than ones trained on manually curated data. Code is
available at https://github.com/facebookresearch/ssl-data-curation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LiverUSRecon: Automatic 3D Reconstruction and Volumetry of the Liver
  with a Few Partial Ultrasound Scans <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaushalya Sivayogaraj, Sahan T. Guruge, Udari Liyanage, Jeevani Udupihille, Saroj Jayasinghe, Gerard Fernando, Ranga Rodrigo, M. Rukshani Liyanaarachchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D reconstruction of the liver for volumetry is important for qualitative
analysis and disease diagnosis. Liver volumetry using ultrasound (US) scans,
although advantageous due to less acquisition time and safety, is challenging
due to the inherent noisiness in US scans, blurry boundaries, and partial liver
visibility. We address these challenges by using the segmentation masks of a
few incomplete sagittal-plane US scans of the liver in conjunction with a
statistical shape model (SSM) built using a set of CT scans of the liver. We
compute the shape parameters needed to warp this canonical SSM to fit the US
scans through a parametric regression network. The resulting 3D liver
reconstruction is accurate and leads to automatic liver volume calculation. We
evaluate the accuracy of the estimated liver volumes with respect to CT
segmentation volumes using RMSE. Our volume computation is statistically much
closer to the volume estimated using CT scans than the volume computed using
Childs' method by radiologists: p-value of 0.094 (>0.05) says that there is no
significant difference between CT segmentation volumes and ours in contrast to
Childs' method. We validate our method using investigations (ablation studies)
on the US image resolution, the number of CT scans used for SSM, the number of
principal components, and the number of input US scans. To the best of our
knowledge, this is the first automatic liver volumetry system using a few
incomplete US scans given a set of CT scans of livers for SSM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, Accepted to MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-domain Denoising for Low-dose Multi-frame Spiral Computed
  Tomography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.10839v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.10839v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Lu, Zhixin Xu, Moon Hyung Choi, Jimin Kim, Seung-Won Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computed tomography (CT) has been used worldwide as a non-invasive test to
assist in diagnosis. However, the ionizing nature of X-ray exposure raises
concerns about potential health risks such as cancer. The desire for lower
radiation doses has driven researchers to improve reconstruction quality.
Although previous studies on low-dose computed tomography (LDCT) denoising have
demonstrated the effectiveness of learning-based methods, most were developed
on the simulated data. However, the real-world scenario differs significantly
from the simulation domain, especially when using the multi-slice spiral
scanner geometry. This paper proposes a two-stage method for the commercially
available multi-slice spiral CT scanners that better exploits the complete
reconstruction pipeline for LDCT denoising across different domains. Our
approach makes good use of the high redundancy of multi-slice projections and
the volumetric reconstructions while leveraging the over-smoothing problem in
conventional cascaded frameworks caused by aggressive denoising. The dedicated
design also provides a more explicit interpretation of the data flow. Extensive
experiments on various datasets showed that the proposed method could remove up
to 70\% of noise without compromised spatial resolution, and subjective
evaluations by two experienced radiologists further supported its superior
performance against state-of-the-art methods in clinical practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Defect Detection in Synthetic Fibre Ropes using Detectron2 Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.01469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.01469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anju Rani, Daniel O. Arroyo, Petar Durdevic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fibre ropes with the latest technology have emerged as an appealing
alternative to steel ropes for offshore industries due to their lightweight and
high tensile strength. At the same time, frequent inspection of these ropes is
essential to ensure the proper functioning and safety of the entire system. The
development of deep learning (DL) models in condition monitoring (CM)
applications offers a simpler and more effective approach for defect detection
in synthetic fibre ropes (SFRs). The present paper investigates the performance
of Detectron2, a state-of-the-art library for defect detection and instance
segmentation. Detectron2 with Mask R-CNN architecture is used for segmenting
defects in SFRs. Mask R-CNN with various backbone configurations has been
trained and tested on an experimentally obtained dataset comprising 1,803
high-dimensional images containing seven damage classes (placking high,
placking medium, placking low, compression, core out, chafing, and normal
respectively) for SFRs. By leveraging the capabilities of Detectron2, this
study aims to develop an automated and efficient method for detecting defects
in SFRs, enhancing the inspection process, and ensuring the safety of the fibre
ropes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessment of Sentinel-2 spatial and temporal coverage based on the
  scene classification layer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18584v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18584v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristhian Sanchez, Francisco Mena, Marcela Charfuelan, Marlon Nuske, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the launch of the Sentinel-2 (S2) satellites, many ML models have used
the data for diverse applications. The scene classification layer (SCL) inside
the S2 product provides rich information for training, such as filtering images
with high cloud coverage. However, there is more potential in this. We propose
a technique to assess the clean optical coverage of a region, expressed by a
SITS and calculated with the S2-based SCL data. With a manual threshold and
specific labels in the SCL, the proposed technique assigns a percentage of
spatial and temporal coverage across the time series and a high/low assessment.
By evaluating the AI4EO challenge for Enhanced Agriculture, we show that the
assessment is correlated to the predictive results of ML models. The
classification results in a region with low spatial and temporal coverage is
worse than in a region with high coverage. Finally, we applied the technique
across all continents of the global dataset LandCoverNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE International Geoscience and Remote Sensing
  Symposium 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Logical Closed Loop: Uncovering Object Hallucinations in Large
  Vision-Language Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfei Wu, Qiang Liu, Ding Wang, Jinghao Zhang, Shu Wu, Liang Wang, Tieniu Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object hallucination has been an Achilles' heel which hinders the broader
applications of large vision-language models (LVLMs). Object hallucination
refers to the phenomenon that the LVLMs claim non-existent objects in the
image. To mitigate the object hallucinations, instruction tuning and external
model-based detection methods have been proposed, which either require
large-scare computational resources or depend on the detection result of
external models. However, there remains an under-explored field to utilize the
LVLM itself to alleviate object hallucinations. In this work, we adopt the
intuition that the LVLM tends to respond logically consistently for existent
objects but inconsistently for hallucinated objects. Therefore, we propose a
Logical Closed Loop-based framework for Object Hallucination Detection and
Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency
probing to raise questions with logical correlations, inquiring about
attributes from objects and vice versa. Whether their responses can form a
logical closed loop serves as an indicator of object hallucination. As a
plug-and-play method, it can be seamlessly applied to all existing LVLMs.
Comprehensive experiments conducted on three benchmarks across four LVLMs have
demonstrated significant improvements brought by our method, indicating its
effectiveness and generality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to ACL 2024; 19 Pages, 15 Figures, 6 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Viewport Prediction for Volumetric Video Streaming by Exploring Video
  Saliency and Trajectory Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16462v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16462v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Li, Zhixin Li, Zhi Liu, Pengyuan Zhou, Richang Hong, Qiyue Li, Han Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Volumetric video, also known as hologram video, is a novel medium that
portrays natural content in Virtual Reality (VR), Augmented Reality (AR), and
Mixed Reality (MR). It is expected to be the next-gen video technology and a
prevalent use case for 5G and beyond wireless communication. Considering that
each user typically only watches a section of the volumetric video, known as
the viewport, it is essential to have precise viewport prediction for optimal
performance. However, research on this topic is still in its infancy. In the
end, this paper presents and proposes a novel approach, named Saliency and
Trajectory Viewport Prediction (STVP), which aims to improve the precision of
viewport prediction in volumetric video streaming. The STVP extensively
utilizes video saliency information and viewport trajectory. To our knowledge,
this is the first comprehensive study of viewport prediction in volumetric
video streaming. In particular, we introduce a novel sampling method, Uniform
Random Sampling (URS), to reduce computational complexity while still
preserving video features in an efficient manner. Then we present a saliency
detection technique that incorporates both spatial and temporal information for
detecting static, dynamic geometric, and color salient regions. Finally, we
intelligently fuse saliency and trajectory information to achieve more accurate
viewport prediction. We conduct extensive simulations to evaluate the
effectiveness of our proposed viewport prediction methods using
state-of-the-art volumetric video sequences. The experimental results show the
superiority of the proposed method over existing schemes. The dataset and
source code will be publicly accessible after acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FAGhead: Fully Animate Gaussian Head from Monocular Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19070v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19070v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Xuan, Xinyang Li, Gongxin Yao, Shiwei Zhou, Donghui Sun, Xiaoxin Chen, Yu Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-fidelity reconstruction of 3D human avatars has a wild application in
visual reality. In this paper, we introduce FAGhead, a method that enables
fully controllable human portraits from monocular videos. We explicit the
traditional 3D morphable meshes (3DMM) and optimize the neutral 3D Gaussians to
reconstruct with complex expressions. Furthermore, we employ a novel
Point-based Learnable Representation Field (PLRF) with learnable Gaussian point
positions to enhance reconstruction performance. Meanwhile, to effectively
manage the edges of avatars, we introduced the alpha rendering to supervise the
alpha value of each pixel. Extensive experimental results on the open-source
datasets and our capturing datasets demonstrate that our approach is able to
generate high-fidelity 3D head avatars and fully control the expression and
pose of the virtual avatars, which is outperforming than existing works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Refer-and-Ground Multimodal Large Language Model for Biomedicine <span class="chip">MICCAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18146v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18146v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoshuang Huang, Haifeng Huang, Lingdong Shen, Yehui Yang, Fangxin Shang, Junwei Liu, Jia Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of multimodal large language models (MLLMs),
especially their capabilities in visual chat through refer and ground
functionalities, their significance is increasingly recognized. However, the
biomedical field currently exhibits a substantial gap in this area, primarily
due to the absence of a dedicated refer and ground dataset for biomedical
images. To address this challenge, we devised the Med-GRIT-270k dataset. It
comprises 270k question-and-answer pairs and spans eight distinct medical
imaging modalities. Most importantly, it is the first dedicated to the
biomedical domain and integrating refer and ground conversations. The key idea
is to sample large-scale biomedical image-mask pairs from medical segmentation
datasets and generate instruction datasets from text using chatGPT.
Additionally, we introduce a Refer-and-Ground Multimodal Large Language Model
for Biomedicine (BiRD) by using this dataset and multi-task instruction
learning. Extensive experiments have corroborated the efficacy of the
Med-GRIT-270k dataset and the multi-modal, fine-grained interactive
capabilities of the BiRD model. This holds significant reference value for the
exploration and development of intelligent biomedical assistants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MICCAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProbRadarM3F: mmWave Radar based Human Skeletal Pose Estimation with
  Probability Map Guided Multi-Format Feature Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bing Zhu, Zixin He, Weiyi Xiong, Guanhua Ding, Jianan Liu, Tao Huang, Wei Chen, Wei Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Millimeter wave (mmWave) radar is a non-intrusive privacy and relatively
convenient and inexpensive device, which has been demonstrated to be applicable
in place of RGB cameras in human indoor pose estimation tasks. However, mmWave
radar relies on the collection of reflected signals from the target, and the
radar signals containing information is difficult to be fully applied. This has
been a long-standing hindrance to the improvement of pose estimation accuracy.
To address this major challenge, this paper introduces a probability map guided
multi-format feature fusion model, ProbRadarM3F. This is a novel radar feature
extraction framework using a traditional FFT method in parallel with a
probability map based positional encoding method. ProbRadarM3F fuses the
traditional heatmap features and the positional features, then effectively
achieves the estimation of 14 keypoints of the human body. Experimental
evaluation on the HuPR dataset proves the effectiveness of the model proposed
in this paper, outperforming other methods experimented on this dataset with an
AP of 69.9 %. The emphasis of our study is focusing on the position information
that is not exploited before in radar singal. This provides direction to
investigate other potential non-redundant information from mmWave rader.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimTxtSeg: Weakly-Supervised Medical Image Segmentation with Simple Text
  Cues <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19364v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19364v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Xie, Tao Zhou, Yi Zhou, Geng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly-supervised medical image segmentation is a challenging task that aims
to reduce the annotation cost while keep the segmentation performance. In this
paper, we present a novel framework, SimTxtSeg, that leverages simple text cues
to generate high-quality pseudo-labels and study the cross-modal fusion in
training segmentation models, simultaneously. Our contribution consists of two
key components: an effective Textual-to-Visual Cue Converter that produces
visual prompts from text prompts on medical images, and a text-guided
segmentation model with Text-Vision Hybrid Attention that fuses text and image
features. We evaluate our framework on two medical image segmentation tasks:
colonic polyp segmentation and MRI brain tumor segmentation, and achieve
consistent state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Knowledge Distillation for Lightweight Skin Cancer
  Classification: Balancing Accuracy and Computational Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17051v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17051v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niful Islam, Khan Md Hasib, Fahmida Akter Joti, Asif Karim, Sami Azam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skin cancer is a major concern to public health, accounting for one-third of
the reported cancers. If not detected early, the cancer has the potential for
severe consequences. Recognizing the critical need for effective skin cancer
classification, we address the limitations of existing models, which are often
too large to deploy in areas with limited computational resources. In response,
we present a knowledge distillation based approach for creating a lightweight
yet high-performing classifier. The proposed solution involves fusing three
models, namely ResNet152V2, ConvNeXtBase, and ViT Base, to create an effective
teacher model. The teacher model is then employed to guide a lightweight
student model of size 2.03 MB. This student model is further compressed to
469.77 KB using 16-bit quantization, enabling smooth incorporation into edge
devices. With six-stage image preprocessing, data augmentation, and a rigorous
ablation study, the model achieves an impressive accuracy of 98.75% on the
HAM10000 dataset and 98.94% on the Kaggle dataset in classifying benign and
malignant skin cancers. With its high accuracy and compact size, our model
appears to be a potential choice for accurate skin cancer classification,
particularly in resource-constrained settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlowVQA: Mapping Multimodal Logic in Visual Question Answering with
  Flowcharts <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubhankar Singh, Purvi Chaurasia, Yerram Varun, Pranshu Pandya, Vatsal Gupta, Vivek Gupta, Dan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing benchmarks for visual question answering lack in visual grounding
and complexity, particularly in evaluating spatial reasoning skills. We
introduce FlowVQA, a novel benchmark aimed at assessing the capabilities of
visual question-answering multimodal language models in reasoning with
flowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and
human-verified flowchart images from three distinct content sources, along with
22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,
including information localization, decision-making, and logical progression.
We conduct a thorough baseline evaluation on a suite of both open-source and
proprietary multimodal language models using various strategies, followed by an
analysis of directional bias. The results underscore the benchmark's potential
as a vital tool for advancing the field of multimodal modeling, providing a
focused and challenging environment for enhancing model performance in visual
and logical reasoning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACL 2024 (Findings), 21 pages, 7 figures, 9 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CSI4Free: GAN-Augmented mmWave CSI for Improved Pose Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nabeel Nisar Bhat, Rafael Berkvens, Jeroen Famaey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Joint Communication and Sensing (JC&S), has demonstrated
significant success, particularly in utilizing sub-6 GHz frequencies with
commercial-off-the-shelf (COTS) Wi-Fi devices for applications such as
localization, gesture recognition, and pose classification. Deep learning and
the existence of large public datasets has been pivotal in achieving such
results. However, at mmWave frequencies (30-300 GHz), which has shown potential
for more accurate sensing performance, there is a noticeable lack of research
in the domain of COTS Wi-Fi sensing. Challenges such as limited research
hardware, the absence of large datasets, limited functionality in COTS
hardware, and the complexities of data collection present obstacles to a
comprehensive exploration of this field. In this work, we aim to address these
challenges by developing a method that can generate synthetic mmWave channel
state information (CSI) samples. In particular, we use a generative adversarial
network (GAN) on an existing dataset, to generate 30,000 additional CSI
samples. The augmented samples exhibit a remarkable degree of consistency with
the original data, as indicated by the notably high GAN-train and GAN-test
scores. Furthermore, we integrate the augmented samples in training a pose
classification model. We observe that the augmented samples complement the real
data and improve the generalization of the classification model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ All-In-One Medical Image Restoration via Task-Adaptive Routing <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwen Yang, Haowei Chen, Ziniu Qian, Yang Yi, Hui Zhang, Dan Zhao, Bingzheng Wei, Yan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although single-task medical image restoration (MedIR) has witnessed
remarkable success, the limited generalizability of these methods poses a
substantial obstacle to wider application. In this paper, we focus on the task
of all-in-one medical image restoration, aiming to address multiple distinct
MedIR tasks with a single universal model. Nonetheless, due to significant
differences between different MedIR tasks, training a universal model often
encounters task interference issues, where different tasks with shared
parameters may conflict with each other in the gradient update direction. This
task interference leads to deviation of the model update direction from the
optimal path, thereby affecting the model's performance. To tackle this issue,
we propose a task-adaptive routing strategy, allowing conflicting tasks to
select different network paths in spatial and channel dimensions, thereby
mitigating task interference. Experimental results demonstrate that our
proposed \textbf{A}ll-in-one \textbf{M}edical \textbf{I}mage
\textbf{R}estoration (\textbf{AMIR}) network achieves state-of-the-art
performance in three MedIR tasks: MRI super-resolution, CT denoising, and PET
synthesis, both in single-task and all-in-one settings. The code and data will
be available at
\href{https://github.com/Yaziwel/All-In-One-Medical-Image-Restoration-via-Task-Adaptive-Routing.git}{https://github.com/Yaziwel/AMIR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article has been early accepted by MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Backdoor Attacks against Large Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Liang, Jiawei Liang, Tianyu Pang, Chao Du, Aishan Liu, Ee-Chien Chang, Xiaochun Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning enhances large vision-language models (LVLMs) but raises
security risks through potential backdoor attacks due to their openness.
Previous backdoor studies focus on enclosed scenarios with consistent training
and testing instructions, neglecting the practical domain gaps that could
affect attack effectiveness. This paper empirically examines the
generalizability of backdoor attacks during the instruction tuning of LVLMs for
the first time, revealing certain limitations of most backdoor strategies in
practical scenarios. We quantitatively evaluate the generalizability of six
typical backdoor attacks on image caption benchmarks across multiple LVLMs,
considering both visual and textual domain offsets. Our findings indicate that
attack generalizability is positively correlated with the backdoor trigger's
irrelevance to specific images/models and the preferential correlation of the
trigger pattern. Additionally, we modify existing backdoor attacks based on the
above key observations, demonstrating significant improvements in cross-domain
scenario generalizability (+86% attack success rate). Notably, even without
access to the instruction datasets, a multimodal instruction set can be
successfully poisoned with a very low poisoning rate (0.2%), achieving an
attack success rate of over 97%. This paper underscores that even simple
traditional backdoor strategies pose a serious threat to LVLMs, necessitating
more attention and in-depth research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Generative</span> Autoencoding of Dropout Patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunta Maeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a generative model termed Deciphering Autoencoders. In this model,
we assign a unique random dropout pattern to each data point in the training
dataset and then train an autoencoder to reconstruct the corresponding data
point using this pattern as information to be encoded. Even if a completely
random dropout pattern is assigned to each data point regardless of their
similarities, a sufficiently large encoder can smoothly map them to a
low-dimensional latent space to reconstruct individual training data points.
During inference, using a dropout pattern different from those used during
training allows the model to function as a generator. Since the training of
Deciphering Autoencoders relies solely on reconstruction error, it offers more
stable training compared to other generative models. Despite their simplicity,
Deciphering Autoencoders show sampling quality comparable to DCGAN on the
CIFAR-10 dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EgoVideo: Exploring Egocentric Foundation Model and Downstream
  Adaptation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18070v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18070v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baoqi Pei, Guo Chen, Jilan Xu, Yuping He, Yicheng Liu, Kanghua Pan, Yifei Huang, Yali Wang, Tong Lu, Limin Wang, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this report, we present our solutions to the EgoVis Challenges in CVPR
2024, including five tracks in the Ego4D challenge and three tracks in the
EPIC-Kitchens challenge. Building upon the video-language two-tower model and
leveraging our meticulously organized egocentric video data, we introduce a
novel foundation model called EgoVideo. This model is specifically designed to
cater to the unique characteristics of egocentric videos and provides strong
support for our competition submissions. In the Ego4D challenges, we tackle
various tasks including Natural Language Queries, Step Grounding, Moment
Queries, Short-term Object Interaction Anticipation, and Long-term Action
Anticipation. In addition, we also participate in the EPIC-Kitchens challenge,
where we engage in the Action Recognition, Multiple Instance Retrieval, and
Domain Adaptation for Action Recognition tracks. By adapting EgoVideo to these
diverse tasks, we showcase its versatility and effectiveness in different
egocentric video analysis scenarios, demonstrating the powerful representation
ability of EgoVideo as an egocentric foundation model. Our codebase and
pretrained models are publicly available at
https://github.com/OpenGVLab/EgoVideo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Champion solutions in the EgoVis CVPR 2024 workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AlignIT: Enhancing Prompt Alignment in Customization of Text-to-Image
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18893v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18893v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishwarya Agarwal, Srikrishna Karanam, Balaji Vasan Srinivasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of customizing text-to-image diffusion models with
user-supplied reference images. Given new prompts, the existing methods can
capture the key concept from the reference images but fail to align the
generated image with the prompt. In this work, we seek to address this key
issue by proposing new methods that can easily be used in conjunction with
existing customization methods that optimize the embeddings/weights at various
intermediate stages of the text encoding process.
  The first contribution of this paper is a dissection of the various stages of
the text encoding process leading up to the conditioning vector for
text-to-image models. We take a holistic view of existing customization methods
and notice that key and value outputs from this process differs substantially
from their corresponding baseline (non-customized) models (e.g., baseline
stable diffusion). While this difference does not impact the concept being
customized, it leads to other parts of the generated image not being aligned
with the prompt. Further, we also observe that these keys and values allow
independent control various aspects of the final generation, enabling semantic
manipulation of the output. Taken together, the features spanning these keys
and values, serve as the basis for our next contribution where we fix the
aforementioned issues with existing methods. We propose a new post-processing
algorithm, AlignIT, that infuses the keys and values for the concept of
interest while ensuring the keys and values for all other tokens in the input
prompt are unchanged.
  Our proposed method can be plugged in directly to existing customization
methods, leading to a substantial performance improvement in the alignment of
the final result with the input prompt while retaining the customization
quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Character-Adapter: Prompt-Guided Region Control for High-Fidelity
  Character Customization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16537v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16537v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Ma, Wenting Xu, Jiji Tang, Qinfeng Jin, Rongsheng Zhang, Zeng Zhao, Changjie Fan, Zhipeng Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Customized image generation, which seeks to synthesize images with consistent
characters, holds significant relevance for applications such as storytelling,
portrait generation, and character design. However, previous approaches have
encountered challenges in preserving characters with high-fidelity consistency
due to inadequate feature extraction and concept confusion of reference
characters. Therefore, we propose Character-Adapter, a plug-and-play framework
designed to generate images that preserve the details of reference characters,
ensuring high-fidelity consistency. Character-Adapter employs prompt-guided
segmentation to ensure fine-grained regional features of reference characters
and dynamic region-level adapters to mitigate concept confusion. Extensive
experiments are conducted to validate the effectiveness of Character-Adapter.
Both quantitative and qualitative results demonstrate that Character-Adapter
achieves the state-of-the-art performance of consistent character generation,
with an improvement of 24.8% compared with other methods. Our code will be
released at https://github.com/Character-Adapter/Character-Adapte
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MolX: Enhancing Large Language Models for Molecular Learning with A
  Multi-Modal Extension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06777v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06777v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khiem Le, Zhichun Guo, Kaiwen Dong, Xiaobao Huang, Bozhao Nan, Roshni Iyer, Xiangliang Zhang, Olaf Wiest, Wei Wang, Nitesh V. Chawla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Large Language Models (LLMs) with their strong task-handling
capabilities have shown remarkable advancements across a spectrum of fields,
moving beyond natural language understanding. However, their proficiency within
the chemistry domain remains restricted, especially in solving professional
molecule-related tasks. This challenge is attributed to their inherent
limitations in comprehending molecules using only common textual
representations, i.e., SMILES strings. In this study, we seek to enhance the
ability of LLMs to comprehend molecules by designing and equipping them with a
multi-modal external module, namely MolX. In particular, instead of directly
using a SMILES string to represent a molecule, we utilize specific encoders to
extract fine-grained features from both SMILES string and 2D molecular graph
representations for feeding into an LLM. Moreover, a human-defined molecular
fingerprint is incorporated to leverage its embedded domain knowledge. Then, to
establish an alignment between MolX and the LLM's textual input space, the
whole model in which the LLM is frozen, is pre-trained with a versatile
strategy including a diverse set of tasks. Extensive experimental evaluations
demonstrate that our proposed method only introduces a small number of
trainable parameters while outperforming baselines on various downstream
molecule-related tasks ranging from molecule-to-text translation to
retrosynthesis, with and without fine-tuning the LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnyControl: Create Your Artwork with Versatile Control on Text-to-Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18958v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18958v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanan Sun, Yanchen Liu, Yinhao Tang, Wenjie Pei, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of text-to-image (T2I) generation has made significant progress in
recent years, largely driven by advancements in diffusion models. Linguistic
control enables effective content creation, but struggles with fine-grained
control over image generation. This challenge has been explored, to a great
extent, by incorporating additional user-supplied spatial conditions, such as
depth maps and edge maps, into pre-trained T2I models through extra encoding.
However, multi-control image synthesis still faces several challenges.
Specifically, current approaches are limited in handling free combinations of
diverse input control signals, overlook the complex relationships among
multiple spatial conditions, and often fail to maintain semantic alignment with
provided textual prompts. This can lead to suboptimal user experiences. To
address these challenges, we propose AnyControl, a multi-control image
synthesis framework that supports arbitrary combinations of diverse control
signals. AnyControl develops a novel Multi-Control Encoder that extracts a
unified multi-modal embedding to guide the generation process. This approach
enables a holistic understanding of user inputs, and produces high-quality,
faithful results under versatile control signals, as demonstrated by extensive
quantitative and qualitative evaluations. Our project page is available in
https://any-control.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Manipulate-Anything: Automating Real-World Robots using Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiafei Duan, Wentao Yuan, Wilbert Pumacay, Yi Ru Wang, Kiana Ehsani, Dieter Fox, Ranjay Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale endeavors like RT-1 and widespread community efforts such as
Open-X-Embodiment have contributed to growing the scale of robot demonstration
data. However, there is still an opportunity to improve the quality, quantity,
and diversity of robot demonstration data. Although vision-language models have
been shown to automatically generate demonstration data, their utility has been
limited to environments with privileged state information, they require
hand-designed skills, and are limited to interactions with few object
instances. We propose Manipulate-Anything, a scalable automated generation
method for real-world robotic manipulation. Unlike prior work, our method can
operate in real-world environments without any privileged state information,
hand-designed skills, and can manipulate any static object. We evaluate our
method using two setups. First, Manipulate-Anything successfully generates
trajectories for all 5 real-world and 12 simulation tasks, significantly
outperforming existing methods like VoxPoser. Second, Manipulate-Anything's
demonstrations can train more robust behavior cloning policies than training
with human demonstrations, or from data generated by VoxPoser and
Code-As-Policies. We believe Manipulate-Anything can be the scalable method for
both generating data for robotics and solving novel tasks in a zero-shot
setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://robot-ma.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Epicardium Prompt-guided Real-time Cardiac Ultrasound Frame-to-volume
  Registration <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14534v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14534v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Lei, Jun Zhou, Jialun Pei, Baoliang Zhao, Yueming Jin, Yuen-Chun Jeremy Teoh, Jing Qin, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A comprehensive guidance view for cardiac interventional surgery can be
provided by the real-time fusion of the intraoperative 2D images and
preoperative 3D volume based on the ultrasound frame-to-volume registration.
However, cardiac ultrasound images are characterized by a low signal-to-noise
ratio and small differences between adjacent frames, coupled with significant
dimension variations between 2D frames and 3D volumes to be registered,
resulting in real-time and accurate cardiac ultrasound frame-to-volume
registration being a very challenging task. This paper introduces a lightweight
end-to-end Cardiac Ultrasound frame-to-volume Registration network, termed
CU-Reg. Specifically, the proposed model leverages epicardium prompt-guided
anatomical clues to reinforce the interaction of 2D sparse and 3D dense
features, followed by a voxel-wise local-global aggregation of enhanced
features, thereby boosting the cross-dimensional matching effectiveness of
low-quality ultrasound modalities. We further embed an inter-frame
discriminative regularization term within the hybrid supervised learning to
increase the distinction between adjacent slices in the same ultrasound volume
to ensure registration stability. Experimental results on the reprocessed CAMUS
dataset demonstrate that our CU-Reg surpasses existing methods in terms of
registration accuracy and efficiency, meeting the guidance requirements of
clinical cardiac interventional surgery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving the Inverse Problem of Electrocardiography for Cardiac Digital
  Twins: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11445v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11445v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Li, Julia Camps, Blanca Rodriguez, Vicente Grau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiac digital twins are personalized virtual representations used to
understand complex heart mechanisms. Solving the ECG inverse problem is crucial
for accurate virtual heart modelling, enabling the derivation of internal
electrical activity information from recorded surface potentials. Despite
challenges from cardiac complexity, noisy ECG data, and computational
efficiency, recent advancements hold significant promise for enhancing virtual
heart modelling, ultimately advancing precision medicine in cardiology. This
paper aims to provide a comprehensive review of the methods of solving ECG
inverse problem, the validation strategies, the clinical applications, and
future perspectives. For the computing methodologies, we broadly classify
state-of-the-art approaches into two categories: deterministic and
probabilistic methods, including conventional and deep learning-based
techniques. Integrating physics laws with deep learning models holds promise,
but challenges such as capturing dynamic electrophysiology accurately,
accessing accurate domain knowledge, and quantifying prediction uncertainty
persist. Integrating models into clinical workflows while ensuring
interpretability and usability for healthcare professionals is essential.
Overcoming these challenges will drive further research in cardiac digital
twins.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing the Power of MLLMs for Transferable Text-to-Image Person ReID <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.04940v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.04940v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image person re-identification (ReID) retrieves pedestrian images
according to textual descriptions. Manually annotating textual descriptions is
time-consuming, restricting the scale of existing datasets and therefore the
generalization ability of ReID models. As a result, we study the transferable
text-to-image ReID problem, where we train a model on our proposed large-scale
database and directly deploy it to various datasets for evaluation. We obtain
substantial training data via Multi-modal Large Language Models (MLLMs).
Moreover, we identify and address two key challenges in utilizing the obtained
textual descriptions. First, an MLLM tends to generate descriptions with
similar structures, causing the model to overfit specific sentence patterns.
Thus, we propose a novel method that uses MLLMs to caption images according to
various templates. These templates are obtained using a multi-turn dialogue
with a Large Language Model (LLM). Therefore, we can build a large-scale
dataset with diverse textual descriptions. Second, an MLLM may produce
incorrect descriptions. Hence, we introduce a novel method that automatically
identifies words in a description that do not correspond with the image. This
method is based on the similarity between one text and all patch token
embeddings in the image. Then, we mask these words with a larger probability in
the subsequent training epoch, alleviating the impact of noisy textual
descriptions. The experimental results demonstrate that our methods
significantly boost the direct transfer text-to-image ReID performance.
Benefiting from the pre-trained model weights, we also achieve state-of-the-art
performance in the traditional evaluation settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">132</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaRA: Supercharging Robot Learning Data for Vision-Language Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe, Ryan Burgert, Mu Cai, Yong Jae Lee, Michael S. Ryoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) equipped with extensive world knowledge and
strong reasoning skills can tackle diverse tasks across domains, often by
posing them as conversation-style instruction-response pairs. In this paper, we
propose LLaRA: Large Language and Robotics Assistant, a framework which
formulates robot action policy as conversations, and provides improved
responses when trained with auxiliary data that complements policy learning.
LLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity
to process state information as visual-textual prompts and generate optimal
policy decisions in text. To train such action policy VLMs, we first introduce
an automated pipeline to generate diverse high-quality robotics instruction
data from existing behavior cloning data. A VLM finetuned with the resulting
collection of datasets based on a conversation-style formulation tailored for
robotics tasks, can generate meaningful robot action policy decisions. Our
experiments across multiple simulated and real-world environments demonstrate
the state-of-the-art performance of the proposed LLaRA framework. The code,
datasets, and pretrained models are available at
https://github.com/LostXine/LLaRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Synthetic Data Creation with 1,000,000,000 Personas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel persona-driven data synthesis methodology that leverages
various perspectives within a large language model (LLM) to create diverse
synthetic data. To fully exploit this methodology at scale, we introduce
Persona Hub -- a collection of 1 billion diverse personas automatically curated
from web data. These 1 billion personas (~13% of the world's total population),
acting as distributed carriers of world knowledge, can tap into almost every
perspective encapsulated within the LLM, thereby facilitating the creation of
diverse synthetic data at scale for various scenarios. By showcasing Persona
Hub's use cases in synthesizing high-quality mathematical and logical reasoning
problems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs
and tools (functions) at scale, we demonstrate persona-driven data synthesis is
versatile, scalable, flexible, and easy to use, potentially driving a paradigm
shift in synthetic data creation and applications in practice, which may have a
profound impact on LLM research and development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProgressGym: Alignment with a Millennium of Moral Progress 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Qiu, Yang Zhang, Xuchuan Huang, Jasmine Xinze Li, Jiaming Ji, Yaodong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Frontier AI systems, including large language models (LLMs), hold increasing
influence over the epistemology of human users. Such influence can reinforce
prevailing societal values, potentially contributing to the lock-in of
misguided moral beliefs and, consequently, the perpetuation of problematic
moral practices on a broad scale. We introduce progress alignment as a
technical solution to mitigate this imminent risk. Progress alignment
algorithms learn to emulate the mechanics of human moral progress, thereby
addressing the susceptibility of existing alignment methods to contemporary
moral blindspots. To empower research in progress alignment, we introduce
ProgressGym, an experimental framework allowing the learning of moral progress
mechanics from history, in order to facilitate future progress in real-world
moral decisions. Leveraging 9 centuries of historical text and 18 historical
LLMs, ProgressGym enables codification of real-world progress alignment
challenges into concrete benchmarks. Specifically, we introduce three core
challenges: tracking evolving values (PG-Follow), preemptively anticipating
moral progress (PG-Predict), and regulating the feedback loop between human and
AI value shifts (PG-Coevolve). Alignment methods without a temporal dimension
are inapplicable to these tasks. In response, we present lifelong and
extrapolative algorithms as baseline methods of progress alignment, and build
an open leaderboard soliciting novel algorithms and challenges. The framework
and the leaderboard are available at
https://github.com/PKU-Alignment/ProgressGym and
https://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheridan Feucht, David Atkinson, Byron Wallace, David Bau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs process text as sequences of tokens that roughly correspond to words,
where less common words are represented by multiple tokens. However, individual
tokens are often semantically unrelated to the meanings of the words/concepts
they comprise. For example, Llama-2-7b's tokenizer splits the word
"northeastern" into the tokens ['_n', 'ort', 'he', 'astern'], none of which
correspond to semantically meaningful units like "north" or "east." Similarly,
the overall meanings of named entities like "Neil Young" and multi-word
expressions like "break a leg" cannot be directly inferred from their
constituent tokens. Mechanistically, how do LLMs convert such arbitrary groups
of tokens into useful higher-level representations? In this work, we find that
last token representations of named entities and multi-token words exhibit a
pronounced "erasure" effect, where information about previous and current
tokens is rapidly forgotten in early layers. Using this observation, we propose
a method to "read out" the implicit vocabulary of an autoregressive LLM by
examining differences in token representations across layers, and present
results of this method for Llama-2-7b and Llama-3-8B. To our knowledge, this is
the first attempt to probe the implicit vocabulary of an LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 14 figures. Code and data at
  https://footprints.baulab.info/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Segment Anything without Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        XuDong Wang, Jingfeng Yang, Trevor Darrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Segmentation Anything Model (SAM) requires labor-intensive data labeling.
We present Unsupervised SAM (UnSAM) for promptable and automatic whole-image
segmentation that does not require human annotations. UnSAM utilizes a
divide-and-conquer strategy to "discover" the hierarchical structure of visual
scenes. We first leverage top-down clustering methods to partition an unlabeled
image into instance/semantic level segments. For all pixels within a segment, a
bottom-up clustering method is employed to iteratively merge them into larger
groups, thereby forming a hierarchical structure. These unsupervised
multi-granular masks are then utilized to supervise model training. Evaluated
across seven popular datasets, UnSAM achieves competitive results with the
supervised counterpart SAM, and surpasses the previous state-of-the-art in
unsupervised segmentation by 11% in terms of AR. Moreover, we show that
supervised SAM can also benefit from our self-supervised labels. By integrating
our unsupervised pseudo masks into SA-1B's ground-truth masks and training
UnSAM with only 1% of SA-1B, a lightly semi-supervised UnSAM can often segment
entities overlooked by supervised SAM, exceeding SAM's AR by over 6.7% and AP
by 3.9% on SA-1B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/frank-xwang/UnSAM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cost-aware Bayesian optimization via the Pandora's Box Gittins index 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Xie, Raul Astudillo, Peter Frazier, Ziv Scully, Alexander Terenin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian optimization is a technique for efficiently optimizing unknown
functions in a black-box manner. To handle practical settings where gathering
data requires use of finite resources, it is desirable to explicitly
incorporate function evaluation costs into Bayesian optimization policies. To
understand how to do so, we develop a previously-unexplored connection between
cost-aware Bayesian optimization and the Pandora's Box problem, a decision
problem from economics. The Pandora's Box problem admits a Bayesian-optimal
solution based on an expression called the Gittins index, which can be
reinterpreted as an acquisition function. We study the use of this acquisition
function for cost-aware Bayesian optimization, and demonstrate empirically that
it performs well, particularly in medium-high dimensions. We further show that
this performance carries over to classical Bayesian optimization without
explicit evaluation costs. Our work constitutes a first step towards
integrating techniques from Gittins index theory into Bayesian optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpotlessSplats: Ignoring Distractors in 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Sabour, Lily Goli, George Kopanas, Mark Matthews, Dmitry Lagun, Leonidas Guibas, Alec Jacobson, David J. Fleet, Andrea Tagliasacchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) is a promising technique for 3D reconstruction,
offering efficient training and rendering speeds, making it suitable for
real-time applications.However, current methods require highly controlled
environments (no moving people or wind-blown elements, and consistent lighting)
to meet the inter-view consistency assumption of 3DGS. This makes
reconstruction of real-world captures problematic. We present SpotlessSplats,
an approach that leverages pre-trained and general-purpose features coupled
with robust optimization to effectively ignore transient distractors. Our
method achieves state-of-the-art reconstruction quality both visually and
quantitatively, on casual captures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danny Halawi, Alexander Wei, Eric Wallace, Tony T. Wang, Nika Haghtalab, Jacob Steinhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Black-box finetuning is an emerging interface for adapting state-of-the-art
language models to user needs. However, such access may also let malicious
actors undermine model safety. To demonstrate the challenge of defending
finetuning interfaces, we introduce covert malicious finetuning, a method to
compromise model safety via finetuning while evading detection. Our method
constructs a malicious dataset where every individual datapoint appears
innocuous, but finetuning on the dataset teaches the model to respond to
encoded harmful requests with encoded harmful responses. Applied to GPT-4, our
method produces a finetuned model that acts on harmful instructions 99% of the
time and avoids detection by defense mechanisms such as dataset inspection,
safety evaluations, and input/output classifiers. Our findings question whether
black-box finetuning access can be secured against sophisticated adversaries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of autonomous systems under data distribution shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Sikar, Artur Garcez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We posit that data can only be safe to use up to a certain threshold of the
data distribution shift, after which control must be relinquished by the
autonomous system and operation halted or handed to a human operator. With the
use of a computer vision toy example we demonstrate that network predictive
accuracy is impacted by data distribution shifts and propose distance metrics
between training and testing data to define safe operation limits within said
shifts. We conclude that beyond an empirically obtained threshold of the data
distribution shift, it is unreasonable to expect network predictive accuracy
not to degrade
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explore as a Storm, Exploit as a Raindrop: On the Benefit of Fine-Tuning
  Kernel Schedulers with Coordinate Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Canesche, Gaurav Verma, Fernando Magno Quintao Pereira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine-learning models consist of kernels, which are algorithms applying
operations on tensors -- data indexed by a linear combination of natural
numbers. Examples of kernels include convolutions, transpositions, and
vectorial products. There are many ways to implement a kernel. These
implementations form the kernel's optimization space. Kernel scheduling is the
problem of finding the best implementation, given an objective function --
typically execution speed. Kernel optimizers such as Ansor, Halide, and AutoTVM
solve this problem via search heuristics, which combine two phases: exploration
and exploitation. The first step evaluates many different kernel optimization
spaces. The latter tries to improve the best implementations by investigating a
kernel within the same space. For example, Ansor combines kernel generation
through sketches for exploration and leverages an evolutionary algorithm to
exploit the best sketches. In this work, we demonstrate the potential to reduce
Ansor's search time while enhancing kernel quality by incorporating Droplet
Search, an AutoTVM algorithm, into Ansor's exploration phase. The approach
involves limiting the number of samples explored by Ansor, selecting the best,
and exploiting it with a coordinate descent algorithm. By applying this
approach to the first 300 kernels that Ansor generates, we usually obtain
better kernels in less time than if we let Ansor analyze 10,000 kernels. This
result has been replicated in 20 well-known deep-learning models (AlexNet,
ResNet, VGG, DenseNet, etc.) running on four architectures: an AMD Ryzen 7
(x86), an NVIDIA A100 tensor core, an NVIDIA RTX 3080 GPU, and an ARM A64FX. A
patch with this combined approach was approved in Ansor in February 2024. As
evidence of the generality of this search methodology, a similar patch,
achieving equally good results, was submitted to TVM's MetaSchedule in June
2024.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 19 figures, original work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pairwise Difference Learning for Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Karim Belaid, Maximilian Rabus, Eyke Hüllermeier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pairwise difference learning (PDL) has recently been introduced as a new
meta-learning technique for regression. Instead of learning a mapping from
instances to outcomes in the standard way, the key idea is to learn a function
that takes two instances as input and predicts the difference between the
respective outcomes. Given a function of this kind, predictions for a query
instance are derived from every training example and then averaged. This paper
extends PDL toward the task of classification and proposes a meta-learning
technique for inducing a PDL classifier by solving a suitably defined (binary)
classification problem on a paired version of the original training data. We
analyze the performance of the PDL classifier in a large-scale empirical study
and find that it outperforms state-of-the-art methods in terms of prediction
performance. Last but not least, we provide an easy-to-use and publicly
available implementation of PDL in a Python package.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Trade-off between Flatness and Optimization in Distributed
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Cao, Zhaoxian Wu, Kun Yuan, Ali H. Sayed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a theoretical framework to evaluate and compare the
performance of gradient-descent algorithms for distributed learning in relation
to their behavior around local minima in nonconvex environments. Previous works
have noticed that convergence toward flat local minima tend to enhance the
generalization ability of learning algorithms. This work discovers two
interesting results. First, it shows that decentralized learning strategies are
able to escape faster away from local minimizers and favor convergence toward
flatter minima relative to the centralized solution in the large-batch training
regime. Second, and importantly, the ultimate classification accuracy is not
solely dependent on the flatness of the local minimizer but also on how well a
learning algorithm can approach that minimum. In other words, the
classification accuracy is a function of both flatness and optimization
performance. The paper examines the interplay between the two measures of
flatness and optimization error closely. One important conclusion is that
decentralized strategies of the diffusion type deliver enhanced classification
accuracy because it strikes a more favorable balance between flatness and
optimization performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wavelets Are All You Need for Autoregressive Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wael Mattar, Idan Levy, Nir Sharon, Shai Dekel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we take a new approach to autoregressive image generation that
is based on two main ingredients. The first is wavelet image coding, which
allows to tokenize the visual details of an image from coarse to fine details
by ordering the information starting with the most significant bits of the most
significant wavelet coefficients. The second is a variant of a language
transformer whose architecture is re-designed and optimized for token sequences
in this 'wavelet language'. The transformer learns the significant statistical
correlations within a token sequence, which are the manifestations of
well-known correlations between the wavelet subbands at various resolutions. We
show experimental results with conditioning on the generation process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single Parent Family: A Spectrum of Family Members from a Single
  Pre-Trained Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Habib Hajimolahoseini, Mohammad Hassanpour, Foozhan Ataiefard, Boxing Chen, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel method of Progressive Low Rank Decomposition
(PLRD) tailored for the compression of large language models. Our approach
leverages a pre-trained model, which is then incrementally decompressed to
smaller sizes using progressively lower ranks. This method allows for
significant reductions in computational overhead and energy consumption, as
subsequent models are derived from the original without the need for retraining
from scratch. We detail the implementation of PLRD, which strategically
decreases the tensor ranks, thus optimizing the trade-off between model
performance and resource usage. The efficacy of PLRD is demonstrated through
extensive experiments showing that models trained with PLRD method on only 1B
tokens maintain comparable performance with traditionally trained models while
using 0.1% of the tokens. The versatility of PLRD is highlighted by its ability
to generate multiple model sizes from a single foundational model, adapting
fluidly to varying computational and memory budgets. Our findings suggest that
PLRD could set a new standard for the efficient scaling of LLMs, making
advanced AI more feasible on diverse platforms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning Predictors for Min-Entropy Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Blanco-Romero, Vicente Lorenzo, Florina Almenares Mendoza, Daniel Díaz-Sánchez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the application of machine learning predictors for
min-entropy estimation in Random Number Generators (RNGs), a key component in
cryptographic applications where accurate entropy assessment is essential for
cybersecurity. Our research indicates that these predictors, and indeed any
predictor that leverages sequence correlations, primarily estimate average
min-entropy, a metric not extensively studied in this context. We explore the
relationship between average min-entropy and the traditional min-entropy,
focusing on their dependence on the number of target bits being predicted.
Utilizing data from Generalized Binary Autoregressive Models, a subset of
Markov processes, we demonstrate that machine learning models (including a
hybrid of convolutional and recurrent Long Short-Term Memory layers and the
transformer-based GPT-2 model) outperform traditional NIST SP 800-90B
predictors in certain scenarios. Our findings underscore the importance of
considering the number of target bits in min-entropy assessment for RNGs and
highlight the potential of machine learning approaches in enhancing entropy
estimation techniques for improved cryptographic security.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Analysis of LSTM Neural Networks and Traditional Machine
  Learning Models for Predicting Diabetes Patient Readmission 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abolfazl Zarghani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diabetes mellitus is a chronic metabolic disorder that has emerged as one of
the major health problems worldwide due to its high prevalence and serious
complications, which are pricey to manage. Effective management requires good
glycemic control and regular follow-up in the clinic; however, non-adherence to
scheduled follow-ups is very common. This study uses the Diabetes 130-US
Hospitals dataset for analysis and prediction of readmission patients by
various traditional machine learning models, such as XGBoost, LightGBM,
CatBoost, Decision Tree, and Random Forest, and also uses an in-house LSTM
neural network for comparison. The quality of the data was assured by
preprocessing it, and the performance evaluation for all these models was based
on accuracy, precision, recall, and F1-score. LightGBM turned out to be the
best traditional model, while XGBoost was the runner-up. The LSTM model
suffered from overfitting despite high training accuracy. A major strength of
LSTM is capturing temporal dependencies among the patient data. Further, SHAP
values were used, which improved model interpretability, whereby key factors
among them number of lab procedures and discharge disposition were identified
as critical in the prediction of readmissions. This study demonstrates that
model selection, validation, and interpretability are key steps in predictive
healthcare modeling. This will help health providers design interventions for
improved follow-up adherence and better management of diabetes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScaleBiO: Scalable Bilevel Optimization for LLM Data Reweighting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Pan, Jipeng Zhang, Xingyuan Pan, Renjie Pi, Xiaoyu Wang, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilevel optimization has shown its utility across various machine learning
settings, yet most algorithms in practice require second-order information,
making it challenging to scale them up. Only recently, a paradigm of
first-order algorithms emerged, capable of effectively addressing bilevel
optimization problems. Nevertheless, the practical efficiency of this paradigm
remains unverified, particularly in the context of large language models
(LLMs). This paper introduces the first scalable instantiation of this paradigm
called ScaleBiO, focusing on bilevel optimization for large-scale LLM data
reweighting. By combining with a recently proposed memory-efficient training
technique called LISA, our novel algorithm allows the paradigm to scale to
34-billion-parameter LLMs on eight A40 GPUs, marking the first successful
application of bilevel optimization under practical scenarios for large-sized
LLMs. Empirically, extensive experiments on data reweighting verify the
effectiveness of ScaleBiO for different-scaled models, including GPT-2,
LLaMA-3-8B, GPT-NeoX-20B, and Yi-34B, where bilevel optimization succeeds in
filtering irrelevant data samples and selecting informative samples.
Theoretically, ScaleBiO ensures the optimality of the learned data weights,
along with a convergence guarantee matching the conventional first-order
bilevel optimization paradigm on smooth and strongly convex objectives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STLLaVA-Med: Self-Training Large Language and Vision Assistant for
  Medical 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guohao Sun, Can Qin, Huazhu Fu, Linwei Wang, Zhiqiang Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) have shown significant potential in
assisting medical diagnosis by leveraging extensive biomedical datasets.
However, the advancement of medical image understanding and reasoning
critically depends on building high-quality visual instruction data, which is
costly and labor-intensive to obtain, particularly in the medical domain. To
mitigate this data-starving issue, we introduce Self-Training Large Language
and Vision Assistant for Medical (STLLaVA-Med). The proposed method is designed
to train a policy model (an LVLM) capable of auto-generating medical visual
instruction data to improve data efficiency, guided through Direct Preference
Optimization (DPO). Specifically, a more powerful and larger LVLM (e.g.,
GPT-4o) is involved as a biomedical expert to oversee the DPO fine-tuning
process on the auto-generated data, encouraging the policy model to align
efficiently with human preferences. We validate the efficacy and data
efficiency of STLLaVA-Med across three major medical Visual Question Answering
(VQA) benchmarks, demonstrating competitive zero-shot performance with the
utilization of only 9% of the medical data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text2Robot: Evolutionary Robot Design from Text Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan P. Ringel, Zachary S. Charlick, Jiaxun Liu, Boxi Xia, Boyuan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot design has traditionally been costly and labor-intensive. Despite
advancements in automated processes, it remains challenging to navigate a vast
design space while producing physically manufacturable robots. We introduce
Text2Robot, a framework that converts user text specifications and performance
preferences into physical quadrupedal robots. Within minutes, Text2Robot can
use text-to-3D models to provide strong initializations of diverse
morphologies. Within a day, our geometric processing algorithms and
body-control co-optimization produce a walking robot by explicitly considering
real-world electronics and manufacturability. Text2Robot enables rapid
prototyping and opens new opportunities for robot design with generative
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our project website is at: https://generalroboticslab.com/Text2Robot</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Computational Curse of Big Data for Bayesian Additive Regression
  Trees: A Hitting Time Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Shuo Tan, Omer Ronen, Theo Saarinen, Bin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian Additive Regression Trees (BART) is a popular Bayesian
non-parametric regression model that is commonly used in causal inference and
beyond. Its strong predictive performance is supported by theoretical
guarantees that its posterior distribution concentrates around the true
regression function at optimal rates under various data generative settings and
for appropriate prior choices. In this paper, we show that the BART sampler
often converges slowly, confirming empirical observations by other researchers.
Assuming discrete covariates, we show that, while the BART posterior
concentrates on a set comprising all optimal tree structures (smallest bias and
complexity), the Markov chain's hitting time for this set increases with $n$
(training sample size), under several common data generative settings. As $n$
increases, the approximate BART posterior thus becomes increasingly different
from the exact posterior (for the same number of MCMC samples), contrasting
with earlier concentration results on the exact posterior. This contrast is
highlighted by our simulations showing worsening frequentist undercoverage for
approximate posterior intervals and a growing ratio between the MSE of the
approximate posterior and that obtainable by artificially improving convergence
via averaging multiple sampler chains. Finally, based on our theoretical
insights, possibilities are discussed to improve the BART sampler convergence
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kolmogorov-Smirnov GAN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Falkiewicz, Naoya Takeishi, Alexandros Kalousis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel deep generative model, the Kolmogorov-Smirnov Generative
Adversarial Network (KSGAN). Unlike existing approaches, KSGAN formulates the
learning process as a minimization of the Kolmogorov-Smirnov (KS) distance,
generalized to handle multivariate distributions. This distance is calculated
using the quantile function, which acts as the critic in the adversarial
training process. We formally demonstrate that minimizing the KS distance leads
to the trained approximate distribution aligning with the target distribution.
We propose an efficient implementation and evaluate its effectiveness through
experiments. The results show that KSGAN performs on par with existing
adversarial methods, exhibiting stability during training, resistance to mode
dropping and collapse, and tolerance to variations in hyperparameter settings.
Additionally, we review the literature on the Generalized KS test and discuss
the connections between KSGAN and existing adversarial generative models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://github.com/DMML-Geneva/ksgan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupling General and Personalized Knowledge in Federated Learning via
  Additive and Low-Rank Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghao Wu, Xuefeng Liu, Jianwei Niu, Haolin Wang, Shaojie Tang, Guogang Zhu, Hao Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address data heterogeneity, the key strategy of Personalized Federated
Learning (PFL) is to decouple general knowledge (shared among clients) and
client-specific knowledge, as the latter can have a negative impact on
collaboration if not removed. Existing PFL methods primarily adopt a parameter
partitioning approach, where the parameters of a model are designated as one of
two types: parameters shared with other clients to extract general knowledge
and parameters retained locally to learn client-specific knowledge. However, as
these two types of parameters are put together like a jigsaw puzzle into a
single model during the training process, each parameter may simultaneously
absorb both general and client-specific knowledge, thus struggling to separate
the two types of knowledge effectively. In this paper, we introduce FedDecomp,
a simple but effective PFL paradigm that employs parameter additive
decomposition to address this issue. Instead of assigning each parameter of a
model as either a shared or personalized one, FedDecomp decomposes each
parameter into the sum of two parameters: a shared one and a personalized one,
thus achieving a more thorough decoupling of shared and personalized knowledge
compared to the parameter partitioning method. In addition, as we find that
retaining local knowledge of specific clients requires much lower model
capacity compared with general knowledge across all clients, we let the matrix
containing personalized parameters be low rank during the training process.
Moreover, a new alternating training strategy is proposed to further improve
the performance. Experimental results across multiple datasets and varying
degrees of data heterogeneity demonstrate that FedDecomp outperforms
state-of-the-art methods up to 4.9\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ `Just One More Sensor is Enough' -- Iterative Water Leak Localization
  with Physical Simulation and a Small Number of Pressure Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michał Cholewa, Michał Romaszewski, Przemysław Głomb, Katarzyna Kołodziej, Michał Gorawski, Jakub Koral, Wojciech Koral, Andrzej Madej, Kryspin Musioł
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we propose an approach to leak localisation in a complex
water delivery grid with the use of data from physical simulation (e.g. EPANET
software). This task is usually achieved by a network of multiple water
pressure sensors and analysis of the so-called sensitivity matrix of pressure
differences between the network's simulated data and actual data of the network
affected by the leak. However, most algorithms using this approach require a
significant number of pressure sensors -- a condition that is not easy to
fulfil in the case of many less equipped networks. Therefore, we answer the
question of whether leak localisation is possible by utilising very few sensors
but having the ability to relocate one of them. Our algorithm is based on
physical simulations (EPANET software) and an iterative scheme for mobile
sensor relocation. The experiments show that the proposed system can equalise
the low number of sensors with adjustments made for their positioning, giving a
very good approximation of leak's position both in simulated cases and
real-life example taken from BattLeDIM competition L-Town data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FI-CBL: A Probabilistic Method for Concept-Based Learning with Expert
  Rules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lev V. Utkin, Andrei V. Konstantinov, Stanislav R. Kirpichenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A method for solving concept-based learning (CBL) problem is proposed. The
main idea behind the method is to divide each concept-annotated image into
patches, to transform the patches into embeddings by using an autoencoder, and
to cluster the embeddings assuming that each cluster will mainly contain
embeddings of patches with certain concepts. To find concepts of a new image,
the method implements the frequentist inference by computing prior and
posterior probabilities of concepts based on rates of patches from images with
certain values of the concepts. Therefore, the proposed method is called the
Frequentist Inference CBL (FI-CBL). FI-CBL allows us to incorporate the expert
rules in the form of logic functions into the inference procedure. An idea
behind the incorporation is to update prior and conditional probabilities of
concepts to satisfy the rules. The method is transparent because it has an
explicit sequence of probabilistic calculations and a clear frequency
interpretation. Numerical experiments show that FI-CBL outperforms the concept
bottleneck model in cases when the number of training data is small. The code
of proposed algorithms is publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention Meets UAVs: A Comprehensive Evaluation of DDoS Detection in
  Low-Cost UAVs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashish Sharma, SVSLN Surya Suhas Vaddhiparthy, Sai Usha Goparaju, Deepak Gangadharan, Harikumar Kandath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the critical issue of enhancing cybersecurity measures
for low-cost, Wi-Fi-based Unmanned Aerial Vehicles (UAVs) against Distributed
Denial of Service (DDoS) attacks. In the current work, we have explored three
variants of DDoS attacks, namely Transmission Control Protocol (TCP), Internet
Control Message Protocol (ICMP), and TCP + ICMP flooding attacks, and developed
a detection mechanism that runs on the companion computer of the UAV system. As
a part of the detection mechanism, we have evaluated various machine learning,
and deep learning algorithms, such as XGBoost, Isolation Forest, Long
Short-Term Memory (LSTM), Bidirectional-LSTM (Bi-LSTM), LSTM with attention,
Bi-LSTM with attention, and Time Series Transformer (TST) in terms of various
classification metrics. Our evaluation reveals that algorithms with attention
mechanisms outperform their counterparts in general, and TST stands out as the
most efficient model with a run time of 0.1 seconds. TST has demonstrated an F1
score of 0.999, 0.997, and 0.943 for TCP, ICMP, and TCP + ICMP flooding attacks
respectively. In this work, we present the necessary steps required to build an
on-board DDoS detection mechanism. Further, we also present the ablation study
to identify the best TST hyperparameters for DDoS detection, and we have also
underscored the advantage of adapting learnable positional embeddings in TST
for DDoS detection with an improvement in F1 score from 0.94 to 0.99.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Koopman based trajectory model and computation offloading for high
  mobility paradigm in ISAC enabled IoT system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh-Tuan Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User experience on mobile devices is constrained by limited battery capacity
and processing power, but 6G technology advancements are diving rapidly into
mobile technical evolution. Mobile edge computing (MEC) offers a solution,
offloading computationally intensive tasks to edge cloud servers, reducing
battery drain compared to local processing. The upcoming integrated sensing and
communication in mobile communication may improve the trajectory prediction and
processing delays. This study proposes a greedy resource allocation
optimization strategy for multi-user networks to minimize aggregate energy
usage. Numerical results show potential improvement at 33\% for every 1000
iteration. Addressing prediction model division and velocity accuracy issues is
crucial for better results. A plan for further improvement and achieving
objectives is outlined for the upcoming work phase.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Operator World Models for Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pietro Novelli, Marco Pratticò, Massimiliano Pontil, Carlo Ciliberto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Policy Mirror Descent (PMD) is a powerful and theoretically sound methodology
for sequential decision-making. However, it is not directly applicable to
Reinforcement Learning (RL) due to the inaccessibility of explicit action-value
functions. We address this challenge by introducing a novel approach based on
learning a world model of the environment using conditional mean embeddings. We
then leverage the operatorial formulation of RL to express the action-value
function in terms of this quantity in closed form via matrix operations.
Combining these estimators with PMD leads to POWR, a new RL algorithm for which
we prove convergence rates to the global optimum. Preliminary experiments in
finite and infinite state settings support the effectiveness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MuGSI: Distilling GNNs with Multi-Granularity Structural Information for
  Graph Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianjun Yao, Jiaqi Sun, Defu Cao, Kun Zhang, Guangyi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have introduced GNN-to-MLP knowledge distillation (KD)
frameworks to combine both GNN's superior performance and MLP's fast inference
speed. However, existing KD frameworks are primarily designed for node
classification within single graphs, leaving their applicability to graph
classification largely unexplored. Two main challenges arise when extending KD
for node classification to graph classification: (1) The inherent sparsity of
learning signals due to soft labels being generated at the graph level; (2) The
limited expressiveness of student MLPs, especially in datasets with limited
input feature spaces. To overcome these challenges, we introduce MuGSI, a novel
KD framework that employs Multi-granularity Structural Information for graph
classification. Specifically, we propose multi-granularity distillation loss in
MuGSI to tackle the first challenge. This loss function is composed of three
distinct components: graph-level distillation, subgraph-level distillation, and
node-level distillation. Each component targets a specific granularity of the
graph structure, ensuring a comprehensive transfer of structural knowledge from
the teacher model to the student model. To tackle the second challenge, MuGSI
proposes to incorporate a node feature augmentation component, thereby
enhancing the expressiveness of the student MLPs and making them more capable
learners. We perform extensive experiments across a variety of datasets and
different teacher/student model architectures. The experiment results
demonstrate the effectiveness, efficiency, and robustness of MuGSI. Codes are
publicly available at: \textbf{\url{https://github.com/tianyao-aka/MuGSI}.}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures. Accepted by TheWebConf2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Stable and Storage-efficient <span class="highlight-title">Dataset</span> Distillation: Matching
  Convexified Trajectory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenliang Zhong, Haoyu Tang, Qinghai Zheng, Mingzhu Xu, Yupeng Hu, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of deep learning and large language models has led to an
exponential growth in the demand for training data, prompting the development
of Dataset Distillation methods to address the challenges of managing large
datasets. Among these, Matching Training Trajectories (MTT) has been a
prominent approach, which replicates the training trajectory of an expert
network on real data with a synthetic dataset. However, our investigation found
that this method suffers from three significant limitations: 1. Instability of
expert trajectory generated by Stochastic Gradient Descent (SGD); 2. Low
convergence speed of the distillation process; 3. High storage consumption of
the expert trajectory. To address these issues, we offer a new perspective on
understanding the essence of Dataset Distillation and MTT through a simple
transformation of the objective function, and introduce a novel method called
Matching Convexified Trajectory (MCT), which aims to provide better guidance
for the student trajectory. MCT leverages insights from the linearized dynamics
of Neural Tangent Kernel methods to create a convex combination of expert
trajectories, guiding the student network to converge rapidly and stably. This
trajectory is not only easier to store, but also enables a continuous sampling
strategy during distillation, ensuring thorough learning and fitting of the
entire expert trajectory. Comprehensive experiments across three public
datasets validate the superiority of MCT over traditional MTT methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning for Efficient Design and Control Co-optimisation
  of Energy Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marine Cauz, Adrien Bolland, Nicolas Wyrsch, Christophe Ballif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ongoing energy transition drives the development of decentralised
renewable energy sources, which are heterogeneous and weather-dependent,
complicating their integration into energy systems. This study tackles this
issue by introducing a novel reinforcement learning (RL) framework tailored for
the co-optimisation of design and control in energy systems. Traditionally, the
integration of renewable sources in the energy sector has relied on complex
mathematical modelling and sequential processes. By leveraging RL's model-free
capabilities, the framework eliminates the need for explicit system modelling.
By optimising both control and design policies jointly, the framework enhances
the integration of renewable sources and improves system efficiency. This
contribution paves the way for advanced RL applications in energy management,
leading to more efficient and effective use of renewable energy sources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deceptive <span class="highlight-title">Diffusion</span>: Generating Synthetic Adversarial Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Beerens, Catherine F. Higham, Desmond J. Higham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the concept of deceptive diffusion -- training a generative AI
model to produce adversarial images. Whereas a traditional adversarial attack
algorithm aims to perturb an existing image to induce a misclassificaton, the
deceptive diffusion model can create an arbitrary number of new, misclassified
images that are not directly associated with training or test images. Deceptive
diffusion offers the possibility of strengthening defence algorithms by
providing adversarial training data at scale, including types of
misclassification that are otherwise difficult to find. In our experiments, we
also investigate the effect of training on a partially attacked data set. This
highlights a new type of vulnerability for generative diffusion models: if an
attacker is able to stealthily poison a portion of the training data, then the
resulting diffusion model will generate a similar proportion of misleading
outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MulTi-Wise Sampling: Trading Uniform T-Wise Feature Interaction Coverage
  for Smaller Samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Pett, Sebastian Krieter, Thomas Thüm, Ina Schaefer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the functional safety of highly configurable systems often requires
testing representative subsets of all possible configurations to reduce testing
effort and save resources. The ratio of covered t-wise feature interactions
(i.e., T-Wise Feature Interaction Coverage) is a common criterion for
determining whether a subset of configurations is representative and capable of
finding faults. Existing t-wise sampling algorithms uniformly cover t-wise
feature interactions for all features, resulting in lengthy execution times and
large sample sizes, particularly when large t-wise feature interactions are
considered (i.e., high values of t). In this paper, we introduce a novel
approach to t-wise feature interaction sampling, questioning the necessity of
uniform coverage across all t-wise feature interactions, called
\emph{\mulTiWise{}}. Our approach prioritizes between subsets of critical and
non-critical features, considering higher t-values for subsets of critical
features when generating a t-wise feature interaction sample. We evaluate our
approach using subject systems from real-world applications, including
\busybox{}, \soletta{}, \fiasco{}, and \uclibc{}. Our results show that
sacrificing uniform t-wise feature interaction coverage between all features
reduces the time needed to generate a sample and the resulting sample size.
Hence, \mulTiWise{} Sampling offers an alternative to existing approaches if
knowledge about feature criticality is available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling the Real World with High-Density Visual Particle Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William F. Whitney, Jacob Varley, Deepali Jain, Krzysztof Choromanski, Sumeet Singh, Vikas Sindhwani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present High-Density Visual Particle Dynamics (HD-VPD), a learned world
model that can emulate the physical dynamics of real scenes by processing
massive latent point clouds containing 100K+ particles. To enable efficiency at
this scale, we introduce a novel family of Point Cloud Transformers (PCTs)
called Interlacers leveraging intertwined linear-attention Performer layers and
graph-based neighbour attention layers. We demonstrate the capabilities of
HD-VPD by modeling the dynamics of high degree-of-freedom bi-manual robots with
two RGB-D cameras. Compared to the previous graph neural network approach, our
Interlacer dynamics is twice as fast with the same prediction quality, and can
achieve higher quality using 4x as many particles. We illustrate how HD-VPD can
evaluate motion plan quality with robotic box pushing and can grasping tasks.
See videos and particle dynamics rendered by HD-VPD at
https://sites.google.com/view/hd-vpd.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Performance Prediction of Electrolyte Formulations with
  Transformer-based Molecular Representation Model <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Indra Priyadarsini, Vidushi Sharma, Seiji Takeda, Akihiro Kishimoto, Lisa Hamada, Hajime Shinohara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Development of efficient and high-performing electrolytes is crucial for
advancing energy storage technologies, particularly in batteries. Predicting
the performance of battery electrolytes rely on complex interactions between
the individual constituents. Consequently, a strategy that adeptly captures
these relationships and forms a robust representation of the formulation is
essential for integrating with machine learning models to predict properties
accurately. In this paper, we introduce a novel approach leveraging a
transformer-based molecular representation model to effectively and efficiently
capture the representation of electrolyte formulations. The performance of the
proposed approach is evaluated on two battery property prediction tasks and the
results show superior performance compared to the state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ML4LMS Workshop at ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Supervised Spatial-Temporal Normality Learning for Time Series
  Anomaly Detection <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Chen, Hongzuo Xu, Guansong Pang, Hezhe Qiao, Yuan Zhou, Mingsheng Shang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time Series Anomaly Detection (TSAD) finds widespread applications across
various domains such as financial markets, industrial production, and
healthcare. Its primary objective is to learn the normal patterns of time
series data, thereby identifying deviations in test samples. Most existing TSAD
methods focus on modeling data from the temporal dimension, while ignoring the
semantic information in the spatial dimension. To address this issue, we
introduce a novel approach, called Spatial-Temporal Normality learning (STEN).
STEN is composed of a sequence Order prediction-based Temporal Normality
learning (OTN) module that captures the temporal correlations within sequences,
and a Distance prediction-based Spatial Normality learning (DSN) module that
learns the relative spatial relations between sequences in a feature space. By
synthesizing these two modules, STEN learns expressive spatial-temporal
representations for the normal patterns hidden in the time series data.
Extensive experiments on five popular TSAD benchmarks show that STEN
substantially outperforms state-of-the-art competing methods. Our code is
available at https://github.com/mala-lab/STEN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 4 figures, accepted in ECML PKDD2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextualized Hybrid Ensemble Q-learning: Learning Fast with Control
  Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emma Cramer, Bernd Frauenknecht, Ramil Sabirov, Sebastian Trimpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining Reinforcement Learning (RL) with a prior controller can yield the
best out of two worlds: RL can solve complex nonlinear problems, while the
control prior ensures safer exploration and speeds up training. Prior work
largely blends both components with a fixed weight, neglecting that the RL
agent's performance varies with the training progress and across regions in the
state space. Therefore, we advocate for an adaptive strategy that dynamically
adjusts the weighting based on the RL agent's current capabilities. We propose
a new adaptive hybrid RL algorithm, Contextualized Hybrid Ensemble Q-learning
(CHEQ). CHEQ combines three key ingredients: (i) a time-invariant formulation
of the adaptive hybrid RL problem treating the adaptive weight as a context
variable, (ii) a weight adaption mechanism based on the parametric uncertainty
of a critic ensemble, and (iii) ensemble-based acceleration for data-efficient
RL. Evaluating CHEQ on a car racing task reveals substantially stronger data
efficiency, exploration safety, and transferability to unknown scenarios than
state-of-the-art adaptive hybrid RL methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Systematic Literature <span class="highlight-title">Review</span> on Application of Learning-based Approaches
  in Continuous Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Kazemi Arani, Triet Huynh Minh Le, Mansooreh Zahedi, M. Ali Babar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context: Machine learning (ML) and deep learning (DL) analyze raw data to
extract valuable insights in specific phases. The rise of continuous practices
in software projects emphasizes automating Continuous Integration (CI) with
these learning-based methods, while the growing adoption of such approaches
underscores the need for systematizing knowledge. Objective: Our objective is
to comprehensively review and analyze existing literature concerning
learning-based methods within the CI domain. We endeavour to identify and
analyse various techniques documented in the literature, emphasizing the
fundamental attributes of training phases within learning-based solutions in
the context of CI. Method: We conducted a Systematic Literature Review (SLR)
involving 52 primary studies. Through statistical and thematic analyses, we
explored the correlations between CI tasks and the training phases of
learning-based methodologies across the selected studies, encompassing a
spectrum from data engineering techniques to evaluation metrics. Results: This
paper presents an analysis of the automation of CI tasks utilizing
learning-based methods. We identify and analyze nine types of data sources,
four steps in data preparation, four feature types, nine subsets of data
features, five approaches for hyperparameter selection and tuning, and fifteen
evaluation metrics. Furthermore, we discuss the latest techniques employed,
existing gaps in CI task automation, and the characteristics of the utilized
learning-based techniques. Conclusion: This study provides a comprehensive
overview of learning-based methods in CI, offering valuable insights for
researchers and practitioners developing CI task automation. It also highlights
the need for further research to advance these methods in CI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to be published in IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Backdoor Attack in Prompt-Based Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trang Nguyen, Anh Tran, Nhat Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt-based approaches offer a cutting-edge solution to data privacy issues
in continual learning, particularly in scenarios involving multiple data
suppliers where long-term storage of private user data is prohibited. Despite
delivering state-of-the-art performance, its impressive remembering capability
can become a double-edged sword, raising security concerns as it might
inadvertently retain poisoned knowledge injected during learning from private
user data. Following this insight, in this paper, we expose continual learning
to a potential threat: backdoor attack, which drives the model to follow a
desired adversarial target whenever a specific trigger is present while still
performing normally on clean samples. We highlight three critical challenges in
executing backdoor attacks on incremental learners and propose corresponding
solutions: (1) \emph{Transferability}: We employ a surrogate dataset and
manipulate prompt selection to transfer backdoor knowledge to data from other
suppliers; (2) \emph{Resiliency}: We simulate static and dynamic states of the
victim to ensure the backdoor trigger remains robust during intense incremental
learning processes; and (3) \emph{Authenticity}: We apply binary cross-entropy
loss as an anti-cheating factor to prevent the backdoor trigger from devolving
into adversarial noise. Extensive experiments across various benchmark datasets
and continual learners validate our continual backdoor framework, achieving up
to $100\%$ attack success rate, with further ablation studies confirming our
contributions' effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classical Bandit Algorithms for Entanglement Detection in Parameterized
  Qubit States 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bharati. K, Vikesh Siddhu, Krishna Jagannathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entanglement is a key resource for a wide range of tasks in quantum
information and computing. Thus, verifying availability of this quantum
resource is essential. Extensive research on entanglement detection has led to
no-go theorems (Lu et al. [Phys. Rev. Lett., 116, 230501 (2016)]) that
highlight the need for full state tomography (FST) in the absence of adaptive
or joint measurements. Recent advancements, as proposed by Zhu, Teo, and
Englert [Phys. Rev. A, 81, 052339, 2010], introduce a single-parameter family
of entanglement witness measurements which are capable of conclusively
detecting certain entangled states and only resort to FST when all witness
measurements are inconclusive. We find a variety of realistic noisy two-qubit
quantum states $\mathcal{F}$ that yield conclusive results under this witness
family. We solve the problem of detecting entanglement among $K$ quantum states
in $\mathcal{F}$, of which $m$ states are entangled, with $m$ potentially
unknown. We recognize a structural connection of this problem to the Bad Arm
Identification problem in stochastic Multi-Armed Bandits (MAB). In contrast to
existing quantum bandit frameworks, we establish a new correspondence tailored
for entanglement detection and term it the $(m,K)$-quantum Multi-Armed Bandit.
We implement two well-known MAB policies for arbitrary states derived from
$\mathcal{F}$, present theoretical guarantees on the measurement/sample
complexity and demonstrate the practicality of the policies through numerical
simulations. More broadly, this paper highlights the potential for employing
classical machine learning techniques for quantum entanglement detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-Instruct: Generated Visual Instructions for Large Multimodal Model
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Liu, Xin Huang, Jinliang Zheng, Boxiao Liu, Jia Wang, Osamu Yoshie, Yu Liu, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces MM-Instruct, a large-scale dataset of diverse and
high-quality visual instruction data designed to enhance the
instruction-following capabilities of large multimodal models (LMMs). While
existing visual instruction datasets often focus on question-answering, they
struggle to generalize to broader application scenarios such as creative
writing, summarization, or image analysis. To address these limitations, we
propose a novel approach to constructing MM-Instruct that leverages the strong
instruction-following capabilities of existing LLMs to generate novel visual
instruction data from large-scale but conventional image captioning datasets.
MM-Instruct first leverages ChatGPT to automatically generate diverse
instructions from a small set of seed instructions through augmenting and
summarization. It then matches these instructions with images and uses an
open-sourced large language model (LLM) to generate coherent answers to the
instruction-image pairs. The LLM is grounded by the detailed text descriptions
of images in the whole answer generation process to guarantee the alignment of
the instruction data. Moreover, we introduce a benchmark based on the generated
instruction data to evaluate the instruction-following capabilities of existing
LMMs. We demonstrate the effectiveness of MM-Instruct by training a LLaVA-1.5
model on the generated data, denoted as LLaVA-Instruct, which exhibits
significant improvements in instruction-following capabilities compared to
LLaVA-1.5 models. The MM-Instruct dataset, benchmark, and pre-trained models
are available at https://github.com/jihaonew/MM-Instruct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Dataset and models are available at
  https://github.com/jihaonew/MM-Instruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EPOCH: Jointly Estimating the 3D Pose of Cameras and Humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicola Garau, Giulia Martinelli, Niccolò Bisagno, Denis Tomè, Carsten Stoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular Human Pose Estimation (HPE) aims at determining the 3D positions of
human joints from a single 2D image captured by a camera. However, a single 2D
point in the image may correspond to multiple points in 3D space. Typically,
the uniqueness of the 2D-3D relationship is approximated using an orthographic
or weak-perspective camera model. In this study, instead of relying on
approximations, we advocate for utilizing the full perspective camera model.
This involves estimating camera parameters and establishing a precise,
unambiguous 2D-3D relationship. To do so, we introduce the EPOCH framework,
comprising two main components: the pose lifter network (LiftNet) and the pose
regressor network (RegNet). LiftNet utilizes the full perspective camera model
to precisely estimate the 3D pose in an unsupervised manner. It takes a 2D pose
and camera parameters as inputs and produces the corresponding 3D pose
estimation. These inputs are obtained from RegNet, which starts from a single
image and provides estimates for the 2D pose and camera parameters. RegNet
utilizes only 2D pose data as weak supervision. Internally, RegNet predicts a
3D pose, which is then projected to 2D using the estimated camera parameters.
This process enables RegNet to establish the unambiguous 2D-3D relationship.
Our experiments show that modeling the lifting as an unsupervised task with a
camera in-the-loop results in better generalization to unseen data. We obtain
state-of-the-art results for the 3D HPE on the Human3.6M and MPI-INF-3DHP
datasets. Our code is available at: [Github link upon acceptance, see
supplementary materials].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State Matching and Multiple References in Adaptive Active Automata
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Loes Kruger, Sebastian Junges, Jurriaan Rot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active automata learning (AAL) is a method to infer state machines by
interacting with black-box systems. Adaptive AAL aims to reduce the sample
complexity of AAL by incorporating domain specific knowledge in the form of
(similar) reference models. Such reference models appear naturally when
learning multiple versions or variants of a software system. In this paper, we
present state matching, which allows flexible use of the structure of these
reference models by the learner. State matching is the main ingredient of
adaptive L#, a novel framework for adaptive learning, built on top of L#. Our
empirical evaluation shows that adaptive L# improves the state of the art by up
to two orders of magnitude.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended paper for FM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CHASE: A Causal Heterogeneous Graph based Framework for Root Cause
  Analysis in Multimodal Microservice Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziming Zhao, Tiehua Zhang, Zhishu Shen, Hai Dong, Xingjun Ma, Xianhui Liu, Yun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the widespread adoption of distributed microservice
architectures within the industry has significantly increased the demand for
enhanced system availability and robustness. Due to the complex service
invocation paths and dependencies at enterprise-level microservice systems, it
is challenging to locate the anomalies promptly during service invocations,
thus causing intractable issues for normal system operations and maintenance.
In this paper, we propose a Causal Heterogeneous grAph baSed framEwork for root
cause analysis, namely CHASE, for microservice systems with multimodal data,
including traces, logs, and system monitoring metrics. Specifically, related
information is encoded into representative embeddings and further modeled by a
multimodal invocation graph. Following that, anomaly detection is performed on
each instance node with attentive heterogeneous message passing from its
adjacent metric and log nodes. Finally, CHASE learns from the constructed
hypergraph with hyperedges representing the flow of causality and performs root
cause localization. We evaluate the proposed framework on two public
microservice datasets with distinct attributes and compare with the
state-of-the-art methods. The results show that CHASE achieves the average
performance gain up to 36.2%(A@1) and 29.4%(Percentage@1), respectively to its
best counterpart.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InfiniGen: Efficient <span class="highlight-title">Generative</span> Inference of Large Language Models with
  Dynamic KV Cache Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonbeom Lee, Jungi Lee, Junghwan Seo, Jaewoong Sim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based large language models (LLMs) demonstrate impressive
performance across various natural language processing tasks. Serving LLM
inference for generating long contents, however, poses a challenge due to the
enormous memory footprint of the transient state, known as the key-value (KV)
cache, which scales with the sequence length and batch size. In this paper, we
present InfiniGen, a novel KV cache management framework tailored for long-text
generation, which synergistically works with modern offloading-based inference
systems. InfiniGen leverages the key insight that a few important tokens that
are essential for computing the subsequent attention layer in the Transformer
can be speculated by performing a minimal rehearsal with the inputs of the
current layer and part of the query weight and key cache of the subsequent
layer. This allows us to prefetch only the essential KV cache entries (without
fetching them all), thereby mitigating the fetch overhead from the host memory
in offloading-based LLM serving systems. Our evaluation on several
representative LLMs shows that InfiniGen improves the overall performance of a
modern offloading-based system by up to 3.00x compared to prior KV cache
management methods while offering substantially better model accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>OSDI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Less is More: Accurate Speech Recognition & Translation without
  Web-Scale Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishna C. Puvvada, Piotr Żelasko, He Huang, Oleksii Hrinchuk, Nithin Rao Koluguri, Kunal Dhawan, Somshubra Majumdar, Elena Rastorgueva, Zhehuai Chen, Vitaly Lavrukhin, Jagadeesh Balam, Boris Ginsburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in speech recognition and translation rely on hundreds of
thousands of hours of Internet speech data. We argue that state-of-the art
accuracy can be reached without relying on web-scale data. Canary -
multilingual ASR and speech translation model, outperforms current
state-of-the-art models - Whisper, OWSM, and Seamless-M4T on English, French,
Spanish, and German languages, while being trained on an order of magnitude
less data than these models. Three key factors enables such data-efficient
model: (1) a FastConformer-based attention encoder-decoder architecture (2)
training on synthetic data generated with machine translation and (3) advanced
training techniques: data-balancing, dynamic data blending, dynamic bucketing
and noise-robust fine-tuning. The model, weights, and training code will be
open-sourced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Function+Data Flow: A Framework to Specify Machine Learning Pipelines
  for Digital Twinning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduardo de Conto, Blaise Genest, Arvind Easwaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of digital twins (DTs) for physical systems increasingly
leverages artificial intelligence (AI), particularly for combining data from
different sources or for creating computationally efficient, reduced-dimension
models. Indeed, even in very different application domains, twinning employs
common techniques such as model order reduction and modelization with hybrid
data (that is, data sourced from both physics-based models and sensors).
Despite this apparent generality, current development practices are ad-hoc,
making the design of AI pipelines for digital twinning complex and
time-consuming. Here we propose Function+Data Flow (FDF), a domain-specific
language (DSL) to describe AI pipelines within DTs. FDF aims to facilitate the
design and validation of digital twins. Specifically, FDF treats functions as
first-class citizens, enabling effective manipulation of models learned with
AI. We illustrate the benefits of FDF on two concrete use cases from different
domains: predicting the plastic strain of a structure and modeling the
electromagnetic behavior of a bearing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, to be published in AIware'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Finite basis Kolmogorov-Arnold networks: domain decomposition for
  data-driven and physics-informed problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amanda A. Howard, Bruno Jacob, Sarah H. Murphy, Alexander Heinlein, Panos Stinis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kolmogorov-Arnold networks (KANs) have attracted attention recently as an
alternative to multilayer perceptrons (MLPs) for scientific machine learning.
However, KANs can be expensive to train, even for relatively small networks.
Inspired by finite basis physics-informed neural networks (FBPINNs), in this
work, we develop a domain decomposition method for KANs that allows for several
small KANs to be trained in parallel to give accurate solutions for multiscale
problems. We show that finite basis KANs (FBKANs) can provide accurate results
with noisy data and for physics-informed training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMEasyQuant -- An Easy to Use Toolkit for LLM Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Liu, Meng Jiang, Kaiser Pister
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, there are many quantization methods appeared for LLM quantization,
yet few are user-friendly and easy to be deployed locally. Packages like
TensorRT and Quantohave many underlying structures and self-invoking internal
functions, which are not conducive to developers' personalized development and
learning for deployment. Therefore, we develop LLMEasyQuant, it is a package
aiming to for easy quantization deployment which is user-friendly and suitable
for beginners' learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ACES: Automatic Cohort Extraction System for Event-Stream <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin Xu, Jack Gallifant, Alistair E. W. Johnson, Matthew B. A. McDermott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reproducibility remains a significant challenge in machine learning (ML) for
healthcare. In this field, datasets, model pipelines, and even task/cohort
definitions are often private, leading to a significant barrier in sharing,
iterating, and understanding ML results on electronic health record (EHR)
datasets. In this paper, we address a significant part of this problem by
introducing the Automatic Cohort Extraction System for Event-Stream Datasets
(ACES). This tool is designed to simultaneously simplify the development of
task/cohorts for ML in healthcare and enable the reproduction of these cohorts,
both at an exact level for single datasets and at a conceptual level across
datasets. To accomplish this, ACES provides (1) a highly intuitive and
expressive configuration language for defining both dataset-specific concepts
and dataset-agnostic inclusion/exclusion criteria, and (2) a pipeline to
automatically extract patient records that meet these defined criteria from
real-world data. ACES can be automatically applied to any dataset in either the
Medical Event Data Standard (MEDS) or EventStreamGPT (ESGPT) formats, or to
*any* dataset for which the necessary task-specific predicates can be extracted
in an event-stream form. ACES has the potential to significantly lower the
barrier to entry for defining ML tasks, redefine the way researchers interact
with EHR datasets, and significantly improve the state of reproducibility for
ML studies in this modality. ACES is available at
https://github.com/justin13601/aces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For ACES Online Documentation, see
  https://eventstreamaces.readthedocs.io/en/latest/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IDT: Dual-Task Adversarial Attacks for Privacy Protection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Faustini, Shakila Mahjabin Tonni, Annabelle McIver, Qiongkai Xu, Mark Dras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language processing (NLP) models may leak private information in
different ways, including membership inference, reconstruction or attribute
inference attacks. Sensitive information may not be explicit in the text, but
hidden in underlying writing characteristics. Methods to protect privacy can
involve using representations inside models that are demonstrated not to detect
sensitive attributes or -- for instance, in cases where users might not trust a
model, the sort of scenario of interest here -- changing the raw text before
models can have access to it. The goal is to rewrite text to prevent someone
from inferring a sensitive attribute (e.g. the gender of the author, or their
location by the writing style) whilst keeping the text useful for its original
intention (e.g. the sentiment of a product review). The few works tackling this
have focused on generative techniques. However, these often create extensively
different texts from the original ones or face problems such as mode collapse.
This paper explores a novel adaptation of adversarial attack techniques to
manipulate a text to deceive a classifier w.r.t one task (privacy) whilst
keeping the predictions of another classifier trained for another task
(utility) unchanged. We propose IDT, a method that analyses predictions made by
auxiliary and interpretable models to identify which tokens are important to
change for the privacy task, and which ones should be kept for the utility
task. We evaluate different datasets for NLP suitable for different tasks.
Automatic and human evaluations show that IDT retains the utility of text,
while also outperforming existing methods when deceiving a classifier w.r.t
privacy task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enforcing Equity in Neural Climate Emulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Yik, Sam J. Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network emulators have become an invaluable tool for a wide variety of
climate and weather prediction tasks. While showing incredibly promising
results, these networks do not have an inherent ability to produce equitable
predictions. That is, they are not guaranteed to provide a uniform quality of
prediction along any particular class or group of people. This potential for
inequitable predictions motivates the need for explicit representations of
fairness in these neural networks. To that end, we draw on methods for
enforcing analytical physical constraints in neural networks to bias networks
towards more equitable predictions. We demonstrate the promise of this
methodology using the task of climate model emulation. Specifically, we propose
a custom loss function which punishes emulators with unequal quality of
predictions across any prespecified regions or category, here defined using
human development index (HDI). This loss function weighs a standard loss metric
such as mean squared error against another metric which captures inequity along
the equity category (HDI), allowing us to adjust the priority of each term
before training. Importantly, the loss function does not specify a particular
definition of equity to bias the neural network towards, opening the door for
custom fairness metrics. Our results show that neural climate emulators trained
with our loss function provide more equitable predictions and that the equity
metric improves with greater weighting in the loss function. We empirically
demonstrate that while there is a tradeoff between accuracy and equity when
prioritizing the latter during training, an appropriate selection of the equity
priority hyperparameter can minimize loss of performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Predictive Simulation Using Structured Graphical Models and
  Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghua Lou, Meet Dave, Shrinu Kushagra, Miguel Lazaro-Gredilla, Kevin Murphy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an approach to simulating trajectories of multiple interacting
agents (road users) based on transformers and probabilistic graphical models
(PGMs), and apply it to the Waymo SimAgents challenge. The transformer baseline
is based on the MTR model, which predicts multiple future trajectories
conditioned on the past trajectories and static road layout features. We then
improve upon these generated trajectories using a PGM, which contains factors
which encode prior knowledge, such as a preference for smooth trajectories, and
avoidance of collisions with static obstacles and other moving agents. We
perform (approximate) MAP inference in this PGM using the Gauss-Newton method.
Finally we sample $K=32$ trajectories for each of the $N \sim 100$ agents for
the next $T=8 \Delta$ time steps, where $\Delta=10$ is the sampling rate per
second. Following the Model Predictive Control (MPC) paradigm, we only return
the first element of our forecasted trajectories at each step, and then we
replan, so that the simulation can constantly adapt to its changing
environment. We therefore call our approach "Model Predictive Simulation" or
MPS. We show that MPS improves upon the MTR baseline, especially in safety
critical metrics such as collision rate. Furthermore, our approach is
compatible with any underlying forecasting model, and does not require extra
training, so we believe it is a valuable contribution to the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Special Mention at the Waymo Sim Agents Challenge 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Interpretation on Federated Learning: A Virtual Concepts
  approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Yan, Guodong Long, Jing Jiang, Michael Blumenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tackling non-IID data is an open challenge in federated learning research.
Existing FL methods, including robust FL and personalized FL, are designed to
improve model performance without consideration of interpreting non-IID across
clients. This paper aims to design a novel FL method to robust and interpret
the non-IID data across clients. Specifically, we interpret each client's
dataset as a mixture of conceptual vectors that each one represents an
interpretable concept to end-users. These conceptual vectors could be
pre-defined or refined in a human-in-the-loop process or be learnt via the
optimization procedure of the federated learning system. In addition to the
interpretability, the clarity of client-specific personalization could also be
applied to enhance the robustness of the training process on FL system. The
effectiveness of the proposed method have been validated on benchmark datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Driven Lipschitz Continuity: A Cost-Effective Approach to Improve
  Adversarial <span class="highlight-title">Robust</span>ness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erh-Chung Chen, Pin-Yu Chen, I-Hsin Chung, Che-Rung Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The security and robustness of deep neural networks (DNNs) have become
increasingly concerning. This paper aims to provide both a theoretical
foundation and a practical solution to ensure the reliability of DNNs. We
explore the concept of Lipschitz continuity to certify the robustness of DNNs
against adversarial attacks, which aim to mislead the network with adding
imperceptible perturbations into inputs. We propose a novel algorithm that
remaps the input domain into a constrained range, reducing the Lipschitz
constant and potentially enhancing robustness. Unlike existing adversarially
trained models, where robustness is enhanced by introducing additional examples
from other datasets or generative models, our method is almost cost-free as it
can be integrated with existing models without requiring re-training.
Experimental results demonstrate the generalizability of our method, as it can
be combined with various models and achieve enhancements in robustness.
Furthermore, our method achieves the best robust accuracy for CIFAR10,
CIFAR100, and ImageNet datasets on the RobustBench leaderboard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine-Learning-Driven Runtime Optimization of BLAS Level 3 on Modern
  Multi-Core Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufan Xia, Giuseppe Maria Junior Barca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  BLAS Level 3 operations are essential for scientific computing, but finding
the optimal number of threads for multi-threaded implementations on modern
multi-core systems is challenging. We present an extension to the Architecture
and Data-Structure Aware Linear Algebra (ADSALA) library that uses machine
learning to optimize the runtime of all BLAS Level 3 operations. Our method
predicts the best number of threads for each operation based on the matrix
dimensions and the system architecture. We test our method on two HPC platforms
with Intel and AMD processors, using MKL and BLIS as baseline BLAS
implementations. We achieve speedups of 1.5 to 3.0 for all operations, compared
to using the maximum number of threads. We also analyze the runtime patterns of
different BLAS operations and explain the sources of speedup. Our work shows
the effectiveness and generality of the ADSALA approach for optimizing BLAS
routines on modern multi-core systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Multi-Thread, Matrix Multiplication, Optimization, BLAS, Machine
  Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScoreFusion: fusing score-based <span class="highlight-title">generative</span> models via Kullback-Leibler
  barycenters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liu,  Junze,  Ye, Jose Blanchet, Nian Si
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of fusing pre-trained (auxiliary) generative models to
enhance the training of a target generative model. We propose using
KL-divergence weighted barycenters as an optimal fusion mechanism, in which the
barycenter weights are optimally trained to minimize a suitable loss for the
target population. While computing the optimal KL-barycenter weights can be
challenging, we demonstrate that this process can be efficiently executed using
diffusion score training when the auxiliary generative models are also trained
based on diffusion score methods. Moreover, we show that our fusion method has
a dimension-free sample complexity in total variation distance provided that
the auxiliary models are well fitted for their own task and the auxiliary tasks
combined capture the target well. The main takeaway of our method is that if
the auxiliary models are well-trained and can borrow features from each other
that are present in the target, our fusion method significantly improves the
training of generative models. We provide a concise computational
implementation of the fusion algorithm, and validate its efficiency in the
low-data regime with numerical experiments involving mixtures models and image
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Zeroth-Order Optimization under Strongly Convexity and
  Lipschitz Hessian: Minimax Sample Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Yu, Yining Wang, Baihe Huang, Qi Lei, Jason D. Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimization of convex functions under stochastic zeroth-order feedback has
been a major and challenging question in online learning. In this work, we
consider the problem of optimizing second-order smooth and strongly convex
functions where the algorithm is only accessible to noisy evaluations of the
objective function it queries. We provide the first tight characterization for
the rate of the minimax simple regret by developing matching upper and lower
bounds. We propose an algorithm that features a combination of a bootstrapping
stage and a mirror-descent stage. Our main technical innovation consists of a
sharp characterization for the spherical-sampling gradient estimator under
higher-order smoothness conditions, which allows the algorithm to optimally
balance the bias-variance tradeoff, and a new iterative method for the
bootstrapping stage, which maintains the performance for unbounded Hessian.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VarteX: Enhancing Weather Forecast through Distributed Variable
  Representation <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayumu Ueyama, Kazuhiko Kawamoto, Hiroshi Kera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weather forecasting is essential for various human activities. Recent
data-driven models have outperformed numerical weather prediction by utilizing
deep learning in forecasting performance. However, challenges remain in
efficiently handling multiple meteorological variables. This study proposes a
new variable aggregation scheme and an efficient learning framework for that
challenge. Experiments show that VarteX outperforms the conventional model in
forecast performance, requiring significantly fewer parameters and resources.
The effectiveness of learning through multiple aggregations and regional split
training is demonstrated, enabling more efficient and accurate deep
learning-based weather forecasting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024, Workshop on Machine Learning for Earth System Modeling</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Data Quality Dimensions and Tools for Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Zhou, Fengjiao Tu, Kewei Sha, Junhua Ding, Haihua Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) technologies have become substantial in practically all
aspects of our society, and data quality (DQ) is critical for the performance,
fairness, robustness, safety, and scalability of ML models. With the large and
complex data in data-centric AI, traditional methods like exploratory data
analysis (EDA) and cross-validation (CV) face challenges, highlighting the
importance of mastering DQ tools. In this survey, we review 17 DQ evaluation
and improvement tools in the last 5 years. By introducing the DQ dimensions,
metrics, and main functions embedded in these tools, we compare their strengths
and limitations and propose a roadmap for developing open-source DQ tools for
ML. Based on the discussions on the challenges and emerging trends, we further
highlight the potential applications of large language models (LLMs) and
generative AI in DQ evaluation and improvement for ML. We believe this
comprehensive survey can enhance understanding of DQ in ML and could drive
progress in data-centric AI. A complete list of the literature investigated in
this survey is available on GitHub at:
https://github.com/haihua0913/awesome-dq4ml.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by The 6th IEEE International Conference
  on Artificial Intelligence Testing (IEEE AITest 2024) as an invited paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Deep Clustering: From the Prior Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiding Lu, Haobin Li, Yunfan Li, Yijie Lin, Xi Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facilitated by the powerful feature extraction ability of neural networks,
deep clustering has achieved great success in analyzing high-dimensional and
complex real-world data. The performance of deep clustering methods is affected
by various factors such as network structures and learning objectives. However,
as pointed out in this survey, the essence of deep clustering lies in the
incorporation and utilization of prior knowledge, which is largely ignored by
existing works. From pioneering deep clustering methods based on data structure
assumptions to recent contrastive clustering methods based on data augmentation
invariances, the development of deep clustering intrinsically corresponds to
the evolution of prior knowledge. In this survey, we provide a comprehensive
review of deep clustering methods by categorizing them into six types of prior
knowledge. We find that in general the prior innovation follows two trends,
namely, i) from mining to constructing, and ii) from internal to external.
Besides, we provide a benchmark on five widely-used datasets and analyze the
performance of methods with diverse priors. By providing a novel prior
knowledge perspective, we hope this survey could provide some novel insights
and inspire future research in the deep clustering community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Cyber <span class="highlight-title">Defense</span> in Dynamic Active Directories through
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diksha Goel, Kristen Moore, Mingyu Guo, Derui Wang, Minjune Kim, Seyit Camtepe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses a significant gap in Autonomous Cyber Operations (ACO)
literature: the absence of effective edge-blocking ACO strategies in dynamic,
real-world networks. It specifically targets the cybersecurity vulnerabilities
of organizational Active Directory (AD) systems. Unlike the existing literature
on edge-blocking defenses which considers AD systems as static entities, our
study counters this by recognizing their dynamic nature and developing advanced
edge-blocking defenses through a Stackelberg game model between attacker and
defender. We devise a Reinforcement Learning (RL)-based attack strategy and an
RL-assisted Evolutionary Diversity Optimization-based defense strategy, where
the attacker and defender improve each other strategy via parallel gameplay. To
address the computational challenges of training attacker-defender strategies
on numerous dynamic AD graphs, we propose an RL Training Facilitator that
prunes environments and neural networks to eliminate irrelevant elements,
enabling efficient and scalable training for large graphs. We extensively train
the attacker strategy, as a sophisticated attacker model is essential for a
robust defense. Our empirical results successfully demonstrate that our
proposed approach enhances defender's proficiency in hardening dynamic AD
graphs while ensuring scalability for large-scale AD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The manuscript has been accepted as full paper at European Symposium
  on Research in Computer Security (ESORICS) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Network Bending of <span class="highlight-title">Diffusion</span> Models for Audio-Visual Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luke Dzwonczyk, Carmine Emanuele Cella, David Ban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present the first steps towards the creation of a tool which
enables artists to create music visualizations using pre-trained, generative,
machine learning models. First, we investigate the application of network
bending, the process of applying transforms within the layers of a generative
network, to image generation diffusion models by utilizing a range of
point-wise, tensor-wise, and morphological operators. We identify a number of
visual effects that result from various operators, including some that are not
easily recreated with standard image editing tools. We find that this process
allows for continuous, fine-grain control of image generation which can be
helpful for creative applications. Next, we generate music-reactive videos
using Stable Diffusion by passing audio features as parameters to network
bending operators. Finally, we comment on certain transforms which radically
shift the image and the possibilities of learning more about the latent space
of Stable Diffusion based on these transforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, to be published in the proceedings of the 27th
  International Conference on Digital Audio Effects (DAFx24), for additional
  image and video examples see https://dzluke.github.io/DAFX2024/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HarmonICA: Neural non-stationarity correction and source separation for
  motor neuron interfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Kenneth Clarke, Agnese Grison, Irene Mendez Guerra, Pranav Mamidanna, Shihan Ma, Silvia Muceli, Dario Farina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major outstanding problem when interfacing with spinal motor neurons is how
to accurately compensate for non-stationary effects in the signal during source
separation routines, particularly when they cannot be estimated in advance.
This forces current systems to instead use undifferentiated bulk signal, which
limits the potential degrees of freedom for control. In this study we propose a
potential solution, using an unsupervised learning algorithm to blindly correct
for the effects of latent processes which drive the signal non-stationarities.
We implement this methodology within the theoretical framework of a quasilinear
version of independent component analysis (ICA). The proposed design,
HarmonICA, sidesteps the identifiability problems of nonlinear ICA, allowing
for equivalent predictability to linear ICA whilst retaining the ability to
learn complex nonlinear relationships between non-stationary latents and their
effects on the signal. We test HarmonICA on both invasive and non-invasive
recordings both simulated and real, demonstrating an ability to blindly
compensate for the non-stationary effects specific to each, and thus to
significantly enhance the quality of a source separation routine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FRED: Flexible REduction-Distribution Interconnect and Communication
  Implementation for Wafer-Scale Distributed Training of DNN Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saeed Rashidi, William Won, Sudarshan Srinivasan, Puneet Gupta, Tushar Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed Deep Neural Network (DNN) training is a technique to reduce the
training overhead by distributing the training tasks into multiple
accelerators, according to a parallelization strategy. However,
high-performance compute and interconnects are needed for maximum speed-up and
linear scaling of the system. Wafer-scale systems are a promising technology
that allows for tightly integrating high-end accelerators with high-speed
wafer-scale interconnects, making it an attractive platform for distributed
training. However, the wafer-scale interconnect should offer high performance
and flexibility for various parallelization strategies to enable maximum
optimizations for compute and memory usage. In this paper, we propose FRED, a
wafer-scale interconnect that is tailored for the high-BW requirements of
wafer-scale networks and can efficiently execute communication patterns of
different parallelization strategies. Furthermore, FRED supports in-switch
collective communication execution that reduces the network traffic by
approximately 2X. Our results show that FRED can improve the average end-to-end
training time of ResNet-152, Transformer-17B, GPT-3, and Transformer-1T by
1.76X, 1.87X, 1.34X, and 1.4X, respectively when compared to a baseline
waferscale 2D-Mesh fabric.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GEO: <span class="highlight-title">Generative</span> Engine Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09735v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09735v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranjal Aggarwal, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan, Ameet Deshpande
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large language models (LLMs) has ushered in a new paradigm of
search engines that use generative models to gather and summarize information
to answer user queries. This emerging technology, which we formalize under the
unified framework of generative engines (GEs), can generate accurate and
personalized responses, rapidly replacing traditional search engines like
Google and Bing. Generative Engines typically satisfy queries by synthesizing
information from multiple sources and summarizing them using LLMs. While this
shift significantly improves $\textit{user}$ utility and $\textit{generative
search engine}$ traffic, it poses a huge challenge for the third stakeholder --
website and content creators. Given the black-box and fast-moving nature of
generative engines, content creators have little to no control over
$\textit{when}$ and $\textit{how}$ their content is displayed. With generative
engines here to stay, we must ensure the creator economy is not disadvantaged.
To address this, we introduce Generative Engine Optimization (GEO), the first
novel paradigm to aid content creators in improving their content visibility in
generative engine responses through a flexible black-box optimization framework
for optimizing and defining visibility metrics. We facilitate systematic
evaluation by introducing GEO-bench, a large-scale benchmark of diverse user
queries across multiple domains, along with relevant web sources to answer
these queries. Through rigorous evaluation, we demonstrate that GEO can boost
visibility by up to $40\%$ in generative engine responses. Moreover, we show
the efficacy of these strategies varies across domains, underscoring the need
for domain-specific optimization methods. Our work opens a new frontier in
information discovery systems, with profound implications for both developers
of generative engines and content creators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to KDD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fishnets: Information-Optimal, Scalable Aggregation for Sets and Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        T. Lucas Makinen, Justin Alsing, Benjamin D. Wandelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Set-based learning is an essential component of modern deep learning and
network science. Graph Neural Networks (GNNs) and their edge-free counterparts
Deepsets have proven remarkably useful on ragged and topologically challenging
datasets. The key to learning informative embeddings for set members is a
specified aggregation function, usually a sum, max, or mean. We propose
Fishnets, an aggregation strategy for learning information-optimal embeddings
for sets of data for both Bayesian inference and graph aggregation. We
demonstrate that i) Fishnets neural summaries can be scaled optimally to an
arbitrary number of data objects, ii) Fishnets aggregations are robust to
changes in data distribution, unlike standard deepsets, iii) Fishnets saturate
Bayesian information content and extend to regimes where MCMC techniques fail
and iv) Fishnets can be used as a drop-in aggregation scheme within GNNs. We
show that by adopting a Fishnets aggregation scheme for message passing, GNNs
can achieve state-of-the-art performance versus architecture size on
ogbn-protein data over existing benchmarks with a fraction of learnable
parameters and faster training time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures, 2 tables. Submitted to JMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Training of Graph Foundation Models for Atomistic Materials
  Modeling: A Case Study with HydraGNN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12909v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12909v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Massimiliano Lupo Pasini, Jong Youl Choi, Kshitij Mehta, Pei Zhang, David Rogers, Jonghyun Bae, Khaled Z. Ibrahim, Ashwin M. Aji, Karl W. Schulz, Jorda Polo, Prasanna Balaprakash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present our work on developing and training scalable graph foundation
models (GFM) using HydraGNN, a multi-headed graph convolutional neural network
architecture. HydraGNN expands the boundaries of graph neural network (GNN) in
both training scale and data diversity. It abstracts over message passing
algorithms, allowing both reproduction of and comparison across algorithmic
innovations that define convolution in GNNs. This work discusses a series of
optimizations that have allowed scaling up the GFM training to tens of
thousands of GPUs on datasets that consist of hundreds of millions of graphs.
Our GFMs use multi-task learning (MTL) to simultaneously learn graph-level and
node-level properties of atomistic structures, such as the total energy and
atomic forces. Using over 150 million atomistic structures for training, we
illustrate the performance of our approach along with the lessons learned on
two United States Department of Energy (US-DOE) supercomputers, namely the
Perlmutter petascale system at the National Energy Research Scientific
Computing Center and the Frontier exascale system at Oak Ridge National
Laboratory. The HydraGNN architecture enables the GFM to achieve near-linear
strong scaling performance using more than 2,000 GPUs on Perlmutter and 16,000
GPUs on Frontier. Hyperparameter optimization (HPO) was performed on over
64,000 GPUs on Frontier to select GFM architectures with high accuracy. Early
stopping was applied on each GFM architecture for energy awareness in
performing such an extreme-scale task. The training of an ensemble of
highest-ranked GFM architectures continued until convergence to establish
uncertainty quantification (UQ) capabilities with ensemble learning. Our
contribution opens the door for rapidly developing, training, and deploying
GFMs using large-scale computational resources to enable AI-accelerated
materials discovery and design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00093v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00093v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhabesh Mali, Karthik Maddala, Vatsal Gupta, Sweeya Reddy, Chandan Karfa, Ramesh Karri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  System Verilog Assertion (SVA) formulation -- a critical yet complex task is
a prerequisite in the Assertion Based Verification (ABV) process.
Traditionally, SVA formulation involves expert-driven interpretation of
specifications, which is time-consuming and prone to human error. Recently,
LLM-informed automatic assertion generation is gaining interest. We designed a
novel framework called ChIRAAG, based on OpenAI GPT4, to generate SVA from
natural language specifications of a design. ChIRAAG constitutes the systematic
breakdown of design specifications into a standardized format, further
generating assertions from formatted specifications using LLM. Furthermore, we
used few test cases to validate the LLM-generated assertions. Automatic
feedback of log messages from the simulation tool to the LLM ensures that the
framework can generate correct SVAs. In our experiments, only 27% of
LLM-generated raw assertions had errors, which was rectified in few iterations
based on the simulation log. Our results on OpenTitan designs show that LLMs
can streamline and assist engineers in the assertion generation process,
reshaping verification workflows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 figures and 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving Differential Equations using Physics-Informed Deep Equilibrium
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Machado Pacheco, Eduardo Camponogara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Physics-Informed Deep Equilibrium Models (PIDEQs) for
solving initial value problems (IVPs) of ordinary differential equations
(ODEs). Leveraging recent advancements in deep equilibrium models (DEQs) and
physics-informed neural networks (PINNs), PIDEQs combine the implicit output
representation of DEQs with physics-informed training techniques. We validate
PIDEQs using the Van der Pol oscillator as a benchmark problem, demonstrating
their efficiency and effectiveness in solving IVPs. Our analysis includes key
hyperparameter considerations for optimizing PIDEQ performance. By bridging
deep learning and physics-based modeling, this work advances computational
techniques for solving IVPs, with implications for scientific computing and
engineering applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CASE 2024; Extended Sec. III.B</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Impact of Feature Representation on the Accuracy of Photonic Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mauricio Gomes de Queiroz, Paul Jimenez, Raphael Cardoso, Mateus Vidaletti Costa, Mohab Abdalla, Ian O'Connor, Alberto Bosio, Fabio Pavanello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photonic Neural Networks (PNNs) are gaining significant interest in the
research community due to their potential for high parallelization, low
latency, and energy efficiency. PNNs compute using light, which leads to
several differences in implementation when compared to electronics, such as the
need to represent input features in the photonic domain before feeding them
into the network. In this encoding process, it is common to combine multiple
features into a single input to reduce the number of inputs and associated
devices, leading to smaller and more energy-efficient PNNs. Although this
alters the network's handling of input data, its impact on PNNs remains
understudied. This paper addresses this open question, investigating the effect
of commonly used encoding strategies that combine features on the performance
and learning capabilities of PNNs. Here, using the concept of feature
importance, we develop a mathematical methodology for analyzing feature
combination. Through this methodology, we demonstrate that encoding multiple
features together in a single input determines their relative importance, thus
limiting the network's ability to learn from the data. Given some prior
knowledge of the data, however, this can also be leveraged for higher accuracy.
By selecting an optimal encoding method, we achieve up to a 12.3% improvement
in accuracy of PNNs trained on the Iris dataset compared to other encoding
techniques, surpassing the performance of networks where features are not
combined. These findings highlight the importance of carefully choosing the
encoding to the accuracy and decision-making strategies of PNNs, particularly
in size or power constrained applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Importance Weighted Expectation-Maximization for Protein Sequence Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.00386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.00386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenqiao Song, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing protein sequences with desired biological function is crucial in
biology and chemistry. Recent machine learning methods use a surrogate
sequence-function model to replace the expensive wet-lab validation. How can we
efficiently generate diverse and novel protein sequences with high fitness? In
this paper, we propose IsEM-Pro, an approach to generate protein sequences
towards a given fitness criterion. At its core, IsEM-Pro is a latent generative
model, augmented by combinatorial structure features from a separately learned
Markov random fields (MRFs). We develop an Monte Carlo Expectation-Maximization
method (MCEM) to learn the model. During inference, sampling from its latent
space enhances diversity while its MRFs features guide the exploration in high
fitness regions. Experiments on eight protein sequence design tasks show that
our IsEM-Pro outperforms the previous best methods by at least 55% on average
fitness score and generates more diverse and novel protein sequences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Simple Mixture Policy Parameterization for Improving Sample Efficiency
  of CVaR Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11062v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11062v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yudong Luo, Yangchen Pan, Han Wang, Philip Torr, Pascal Poupart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning algorithms utilizing policy gradients (PG) to optimize
Conditional Value at Risk (CVaR) face significant challenges with sample
inefficiency, hindering their practical applications. This inefficiency stems
from two main facts: a focus on tail-end performance that overlooks many
sampled trajectories, and the potential of gradient vanishing when the lower
tail of the return distribution is overly flat. To address these challenges, we
propose a simple mixture policy parameterization. This method integrates a
risk-neutral policy with an adjustable policy to form a risk-averse policy. By
employing this strategy, all collected trajectories can be utilized for policy
updating, and the issue of vanishing gradients is counteracted by stimulating
higher returns through the risk-neutral component, thus lifting the tail and
preventing flatness. Our empirical study reveals that this mixture
parameterization is uniquely effective across a variety of benchmark domains.
Specifically, it excels in identifying risk-averse CVaR policies in some Mujoco
environments where the traditional CVaR-PG fails to learn a reasonable policy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RLC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robust</span>ness Assessment of a Runway Object Classifier for Safe Aircraft
  Taxiing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00035v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00035v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhak Elboher, Raya Elsaleh, Omri Isac, Mélanie Ducoffe, Audrey Galametz, Guillaume Povéda, Ryma Boumazouza, Noémie Cohen, Guy Katz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As deep neural networks (DNNs) are becoming the prominent solution for many
computational problems, the aviation industry seeks to explore their potential
in alleviating pilot workload and in improving operational safety. However, the
use of DNNs in this type of safety-critical applications requires a thorough
certification process. This need can be addressed through formal verification,
which provides rigorous assurances -- e.g.,~by proving the absence of certain
mispredictions. In this case-study paper, we demonstrate this process using an
image-classifier DNN currently under development at Airbus and intended for use
during the aircraft taxiing phase. We use formal methods to assess this DNN's
robustness to three common image perturbation types: noise, brightness and
contrast, and some of their combinations. This process entails multiple
invocations of the underlying verifier, which might be computationally
expensive; and we therefore propose a method that leverages the monotonicity of
these robustness properties, as well as the results of past verification
queries, in order to reduce the overall number of verification queries required
by nearly 60%. Our results provide an indication of the level of robustness
achieved by the DNN classifier under study, and indicate that it is
considerably more vulnerable to noise than to brightness or contrast
perturbations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a preprint version of the paper in the proceedings of 43rd
  Digital Avionics Systems Conference (DASC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling laws for learning with real and surrogate data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Jain, Andrea Montanari, Eren Sasoglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collecting large quantities of high-quality data can be prohibitively
expensive or impractical, and a bottleneck in machine learning. One may instead
augment a small set of $n$ data points from the target distribution with data
from more accessible sources, e.g. data collected under different circumstances
or synthesized by generative models. We refer to such data as `surrogate data.'
We introduce a weighted empirical risk minimization (ERM) approach for
integrating surrogate data into training. We analyze mathematically this method
under several classical statistical models, and validate our findings
empirically on datasets from different domains. Our main findings are: $(i)$
Integrating surrogate data can significantly reduce the test error on the
original distribution. Surprisingly, this can happen even when the surrogate
data is unrelated to the original ones. We trace back this behavior to the
classical Stein's paradox. $(ii)$ In order to reap the benefit of surrogate
data, it is crucial to use optimally weighted ERM. $(iii)$ The test error of
models trained on mixtures of real and surrogate data is approximately
described by a scaling law. This scaling law can be used to predict the optimal
weighting scheme, and to choose the amount of surrogate data to add.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added new experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Speculative Inference of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14105v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14105v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadav Timor, Jonathan Mamou, Daniel Korat, Moshe Berchansky, Oren Pereg, Moshe Wasserblat, Tomer Galanti, Michal Gordon, David Harel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accelerating the inference of large language models (LLMs) is an important
challenge in artificial intelligence. This paper introduces distributed
speculative inference (DSI), a novel distributed inference algorithm that is
provably faster than speculative inference (SI) [leviathan2023fast,
chen2023accelerating, miao2023specinfer] and traditional autoregressive
inference (non-SI). Like other SI algorithms, DSI works on frozen LLMs,
requiring no training or architectural modifications, and it preserves the
target distribution.
  Prior studies on SI have demonstrated empirical speedups (compared to non-SI)
but require a fast and accurate drafter LLM. In practice, off-the-shelf LLMs
often do not have matching drafters that are sufficiently fast and accurate. We
show a gap: SI gets slower than non-SI when using slower or less accurate
drafters. We close this gap by proving that DSI is faster than both SI and
non-SI given any drafters. By orchestrating multiple instances of the target
and drafters, DSI is not only faster than SI but also supports LLMs that cannot
be accelerated with SI.
  Our simulations show speedups of off-the-shelf LLMs in realistic settings:
DSI is 1.29-1.92x faster than SI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Bayesian uncertainty quantification with data-driven priors for
  radio interferometric imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00125v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00125v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobías I. Liaudat, Matthijs Mars, Matthew A. Price, Marcelo Pereyra, Marta M. Betcke, Jason D. McEwen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Next-generation radio interferometers like the Square Kilometer Array have
the potential to unlock scientific discoveries thanks to their unprecedented
angular resolution and sensitivity. One key to unlocking their potential
resides in handling the deluge and complexity of incoming data. This challenge
requires building radio interferometric imaging methods that can cope with the
massive data sizes and provide high-quality image reconstructions with
uncertainty quantification (UQ). This work proposes a method coined QuantifAI
to address UQ in radio-interferometric imaging with data-driven (learned)
priors for high-dimensional settings. Our model, rooted in the Bayesian
framework, uses a physically motivated model for the likelihood. The model
exploits a data-driven convex prior, which can encode complex information
learned implicitly from simulations and guarantee the log-concavity of the
posterior. We leverage probability concentration phenomena of high-dimensional
log-concave posteriors that let us obtain information about the posterior,
avoiding MCMC sampling techniques. We rely on convex optimisation methods to
compute the MAP estimation, which is known to be faster and better scale with
dimension than MCMC sampling strategies. Our method allows us to compute local
credible intervals, i.e., Bayesian error bars, and perform hypothesis testing
of structure on the reconstructed image. In addition, we propose a novel
blazing-fast method to compute pixel-wise uncertainties at different scales. We
demonstrate our method by reconstructing radio-interferometric images in a
simulated setting and carrying out fast and scalable UQ, which we validate with
MCMC sampling. Our method shows an improved image quality and more meaningful
uncertainties than the benchmark method based on a sparsity-promoting prior.
QuantifAI's source code: https://github.com/astro-informatics/QuantifAI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 14 figures, 10 tables, code available at
  https://github.com/astro-informatics/QuantifAI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic planning in hierarchical active inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11658v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11658v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Priorelli, Ivilin Peev Stoianov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By dynamic planning, we refer to the ability of the human brain to infer and
impose motor trajectories related to cognitive decisions. A recent paradigm,
active inference, brings fundamental insights into the adaptation of biological
organisms, constantly striving to minimize prediction errors to restrict
themselves to life-compatible states. Over the past years, many studies have
shown how human and animal behavior could be explained in terms of an active
inferential process - either as discrete decision-making or continuous motor
control - inspiring innovative solutions in robotics and artificial
intelligence. Still, the literature lacks a comprehensive outlook on how to
effectively plan actions in changing environments. Setting ourselves the goal
of modeling tool use, we delve into the topic of dynamic planning in active
inference, keeping in mind two crucial aspects of biological goal-directed
behavior: the capacity to understand and exploit affordances for object
manipulation, and to learn the hierarchical interactions between the self and
the environment, including other agents. We start from a simple unit and
gradually describe more advanced structures, comparing recently proposed design
choices and providing basic examples for each section. This study distances
itself from traditional views centered on neural networks and reinforcement
learning, and points toward a yet unexplored direction in active inference:
hybrid representations in hierarchical models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Digital Twin Calibration for Biological System-of-Systems: Cell Culture
  Manufacturing Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuqiang Cheng, Wei Xie, Hua Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biomanufacturing innovation relies on an efficient Design of Experiments
(DoEs) to optimize processes and product quality. Traditional DoE methods,
ignoring the underlying bioprocessing mechanisms, often suffer from a lack of
interpretability and sample efficiency. This limitation motivates us to create
a new optimal learning approach for digital twin model calibration. In this
study, we consider the cell culture process multi-scale mechanistic model, also
known as Biological System-of-Systems (Bio-SoS). This model with a modular
design, composed of sub-models, allows us to integrate data across various
production processes. To calibrate the Bio-SoS digital twin, we evaluate the
mean squared error of model prediction and develop a computational approach to
quantify the impact of parameter estimation error of individual sub-models on
the prediction accuracy of digital twin, which can guide sample-efficient and
interpretable DoEs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nearest Neighbor Sampling for Covariate Shift Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        François Portier, Lionel Truquet, Ikko Yamane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many existing covariate shift adaptation methods estimate sample weights
given to loss values to mitigate the gap between the source and the target
distribution. However, estimating the optimal weights typically involves
computationally expensive matrix inversion and hyper-parameter tuning. In this
paper, we propose a new covariate shift adaptation method which avoids
estimating the weights. The basic idea is to directly work on unlabeled target
data, labeled according to the $k$-nearest neighbors in the source dataset. Our
analysis reveals that setting $k = 1$ is an optimal choice. This property
removes the necessity of tuning the only hyper-parameter $k$ and leads to a
running time quasi-linear in the sample size. Our results include sharp rates
of convergence for our estimator, with a tight control of the mean square error
and explicit constants. In particular, the variance of our estimators has the
same rate of convergence as for standard parametric estimation despite their
non-parametric nature. The proposed estimator shares similarities with some
matching-based treatment effect estimators used, e.g., in biostatistics,
econometrics, and epidemiology. Our experiments show that it achieves drastic
reduction in the running time with remarkable accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent variable model for high-dimensional point process with structured
  missingness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksim Sinelnikov, Manuel Haussmann, Harri Lähdesmäki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Longitudinal data are important in numerous fields, such as healthcare,
sociology and seismology, but real-world datasets present notable challenges
for practitioners because they can be high-dimensional, contain structured
missingness patterns, and measurement time points can be governed by an unknown
stochastic process. While various solutions have been suggested, the majority
of them have been designed to account for only one of these challenges. In this
work, we propose a flexible and efficient latent-variable model that is capable
of addressing all these limitations. Our approach utilizes Gaussian processes
to capture temporal correlations between samples and their associated
missingness masks as well as to model the underlying point process. We
construct our model as a variational autoencoder together with deep neural
network parameterised encoder and decoder models, and develop a scalable
amortised variational inference approach for efficient model training. We
demonstrate competitive performance using both simulated and real datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Catastrophic-risk-aware reinforcement learning with
  extreme-value-theory-based policy gradients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15612v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15612v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parisa Davar, Frédéric Godin, Jose Garrido
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper tackles the problem of mitigating catastrophic risk (which is risk
with very low frequency but very high severity) in the context of a sequential
decision making process. This problem is particularly challenging due to the
scarcity of observations in the far tail of the distribution of cumulative
costs (negative rewards). A policy gradient algorithm is developed, that we
call POTPG. It is based on approximations of the tail risk derived from extreme
value theory. Numerical experiments highlight the out-performance of our method
over common benchmarks, relying on the empirical distribution. An application
to financial risk management, more precisely to the dynamic hedging of a
financial option, is presented.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The Python code to replicate the various numerical experiments of
  this paper is available at
  https://github.com/parisadavar/EVT-policy-gradient-RL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tracking Object Positions in Reinforcement Learning: A Metric for
  Keypoint Detection (extended version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00592v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00592v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emma Cramer, Jonas Reiher, Sebastian Trimpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) for robot control typically requires a detailed
representation of the environment state, including information about
task-relevant objects not directly measurable. Keypoint detectors, such as
spatial autoencoders (SAEs), are a common approach to extracting a
low-dimensional representation from high-dimensional image data. SAEs aim at
spatial features such as object positions, which are often useful
representations in robotic RL. However, whether an SAE is actually able to
track objects in the scene and thus yields a spatial state representation well
suited for RL tasks has rarely been examined due to a lack of established
metrics. In this paper, we propose to assess the performance of an SAE instance
by measuring how well keypoints track ground truth objects in images. We
present a computationally lightweight metric and use it to evaluate common
baseline SAE architectures on image data from a simulated robot task. We find
that common SAEs differ substantially in their spatial extraction capability.
Furthermore, we validate that SAEs that perform well in our metric achieve
superior performance when used in downstream RL. Thus, our metric is an
effective and lightweight indicator of RL performance before executing
expensive RL training. Building on these insights, we identify three key
modifications of SAE architectures to improve tracking performance. We make our
code available at anonymous.4open.science/r/sae-rl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The G-invariant graph Laplacian 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17001v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17001v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eitan Rosen, Paulina Hoyos, Xiuyuan Cheng, Joe Kileel, Yoel Shkolnisky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Laplacian based algorithms for data lying on a manifold have been
proven effective for tasks such as dimensionality reduction, clustering, and
denoising. In this work, we consider data sets whose data points lie on a
manifold that is closed under the action of a known unitary matrix Lie group G.
We propose to construct the graph Laplacian by incorporating the distances
between all the pairs of points generated by the action of G on the data set.
We deem the latter construction the ``G-invariant Graph Laplacian'' (G-GL). We
show that the G-GL converges to the Laplace-Beltrami operator on the data
manifold, while enjoying a significantly improved convergence rate compared to
the standard graph Laplacian which only utilizes the distances between the
points in the given data set. Furthermore, we show that the G-GL admits a set
of eigenfunctions that have the form of certain products between the group
elements and eigenvectors of certain matrices, which can be estimated from the
data efficiently using FFT-type algorithms. We demonstrate our construction and
its advantages on the problem of filtering data on a noisy manifold closed
under the action of the special unitary group SU(2).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Decision Policies with Instrumental Variables through Double
  Machine Learning <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.08498v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.08498v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daqian Shao, Ashkan Soleymani, Francesco Quinzan, Marta Kwiatkowska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common issue in learning decision-making policies in data-rich settings is
spurious correlations in the offline dataset, which can be caused by hidden
confounders. Instrumental variable (IV) regression, which utilises a key
unconfounded variable known as the instrument, is a standard technique for
learning causal relationships between confounded action, outcome, and context
variables. Most recent IV regression algorithms use a two-stage approach, where
a deep neural network (DNN) estimator learnt in the first stage is directly
plugged into the second stage, in which another DNN is used to estimate the
causal effect. Naively plugging the estimator can cause heavy bias in the
second stage, especially when regularisation bias is present in the first stage
estimator. We propose DML-IV, a non-linear IV regression method that reduces
the bias in two-stage IV regressions and effectively learns high-performing
policies. We derive a novel learning objective to reduce bias and design the
DML-IV algorithm following the double/debiased machine learning (DML)
framework. The learnt DML-IV estimator has strong convergence rate and
$O(N^{-1/2})$ suboptimality guarantees that match those when the dataset is
unconfounded. DML-IV outperforms state-of-the-art IV regression methods on IV
regression benchmarks and learns high-performing policies in the presence of
instruments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MatText: Do Language Models Need More than Text & Scale for Materials
  Modeling? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17295v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17295v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nawaf Alampara, Santiago Miret, Kevin Maik Jablonka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effectively representing materials as text has the potential to leverage the
vast advancements of large language models (LLMs) for discovering new
materials. While LLMs have shown remarkable success in various domains, their
application to materials science remains underexplored. A fundamental challenge
is the lack of understanding of how to best utilize text-based representations
for materials modeling. This challenge is further compounded by the absence of
a comprehensive benchmark to rigorously evaluate the capabilities and
limitations of these text representations in capturing the complexity of
material systems. To address this gap, we propose MatText, a suite of
benchmarking tools and datasets designed to systematically evaluate the
performance of language models in modeling materials. MatText encompasses nine
distinct text-based representations for material systems, including several
novel representations. Each representation incorporates unique inductive biases
that capture relevant information and integrate prior physical knowledge about
materials. Additionally, MatText provides essential tools for training and
benchmarking the performance of language models in the context of materials
science. These tools include standardized dataset splits for each
representation, probes for evaluating sensitivity to geometric factors, and
tools for seamlessly converting crystal structures into text. Using MatText, we
conduct an extensive analysis of the capabilities of language models in
modeling materials. Our findings reveal that current language models
consistently struggle to capture the geometric information crucial for
materials modeling across all representations. Instead, these models tend to
leverage local information, which is emphasized in some of our novel
representations. Our analysis underscores MatText's ability to reveal
shortcomings of text-based methods for materials design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Intelligible and Effective Graph Neural Additive Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maya Bechler-Speicher, Amir Globerson, Ran Gilad-Bachrach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have emerged as the predominant approach for
learning over graph-structured data. However, most GNNs operate as black-box
models and require post-hoc explanations, which may not suffice in high-stakes
scenarios where transparency is crucial. In this paper, we present a GNN that
is interpretable by design. Our model, Graph Neural Additive Network (GNAN), is
a novel extension of the interpretable class of Generalized Additive Models,
and can be visualized and fully understood by humans. GNAN is designed to be
fully interpretable, allowing both global and local explanations at the feature
and graph levels through direct visualization of the model. These
visualizations describe the exact way the model uses the relationships between
the target variable, the features, and the graph. We demonstrate the
intelligibility of GNANs in a series of examples on different tasks and
datasets. In addition, we show that the accuracy of GNAN is on par with
black-box GNNs, making it suitable for critical applications where transparency
is essential, alongside high accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LatentExplainer: Explaining Latent Representations in Deep <span class="highlight-title">Generative</span>
  Models with Multi-modal Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14862v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14862v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengdan Zhu, Raasikh Kanjiani, Jiahui Lu, Andrew Choi, Qirui Ye, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models like VAEs and diffusion models have advanced various
generation tasks by leveraging latent variables to learn data distributions and
generate high-quality samples. Despite the field of explainable AI making
strides in interpreting machine learning models, understanding latent variables
in generative models remains challenging. This paper introduces
LatentExplainer, a framework for automatically generating semantically
meaningful explanations of latent variables in deep generative models.
LatentExplainer tackles three main challenges: inferring the meaning of latent
variables, aligning explanations with inductive biases, and handling varying
degrees of explainability. By perturbing latent variables and interpreting
changes in generated data, the framework provides a systematic approach to
understanding and controlling the data generation process, enhancing the
transparency and interpretability of deep generative models. We evaluate our
proposed method on several real-world and synthetic datasets, and the results
demonstrate superior performance in generating high-quality explanations of
latent variables.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Learning Stochastic Population Models by Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07049v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07049v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin N. Kreikemeyer, Philipp Andelfinger, Adelinde M. Uhrmacher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Increasing effort is put into the development of methods for learning
mechanistic models from data. This task entails not only the accurate
estimation of parameters but also a suitable model structure. Recent work on
the discovery of dynamical systems formulates this problem as a linear equation
system. Here, we explore several simulation-based optimization approaches,
which allow much greater freedom in the objective formulation and weaker
conditions on the available data. We show that even for relatively small
stochastic population models, simultaneous estimation of parameters and
structure poses major challenges for optimization procedures. Particularly, we
investigate the application of the local stochastic gradient descent method,
commonly used for training machine learning models. We demonstrate accurate
estimation of models but find that enforcing the inference of parsimonious,
interpretable models drastically increases the difficulty. We give an outlook
on how this challenge can be overcome.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Maxout Network-based Feature Fusion and Political Tangent Search
  Optimizer enabled Transfer Learning for Thalassemia Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02029v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02029v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hemn Barzan Abdalla, Awder Ahmed, Guoquan Li, Nasser Mustafa, Abdur Rashid Sangi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thalassemia is a heritable blood disorder which is the outcome of a genetic
defect causing lack of production of hemoglobin polypeptide chains. However,
there is less understanding of the precise frequency as well as sharing in
these areas. Knowing about the frequency of thalassemia occurrence and
dependable mutations is thus a significant step in preventing, controlling, and
treatment planning. Here, Political Tangent Search Optimizer based Transfer
Learning (PTSO_TL) is introduced for thalassemia detection. Initially, input
data obtained from a particular dataset is normalized in the data normalization
stage. Quantile normalization is utilized in the data normalization stage, and
the data are then passed to the feature fusion phase, in which Weighted
Euclidean Distance with Deep Maxout Network (DMN) is utilized. Thereafter, data
augmentation is performed using the oversampling method to increase data
dimensionality. Lastly, thalassemia detection is carried out by TL, wherein a
convolutional neural network (CNN) is utilized with hyperparameters from a
trained model such as Xception. TL is tuned by PTSO, and the training algorithm
PTSO is presented by merging of Political Optimizer (PO) and Tangent Search
Algorithm (TSA). Furthermore, PTSO_TL obtained maximal precision, recall, and
f-measure values of about 94.3%, 96.1%, and 95.2%, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MALIBO: Meta-learning for Likelihood-free Bayesian Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03565v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03565v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarong Pan, Stefan Falkner, Felix Berkenkamp, Joaquin Vanschoren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian optimization (BO) is a popular method to optimize costly black-box
functions. While traditional BO optimizes each new target task from scratch,
meta-learning has emerged as a way to leverage knowledge from related tasks to
optimize new tasks faster. However, existing meta-learning BO methods rely on
surrogate models that suffer from scalability issues and are sensitive to
observations with different scales and noise types across tasks. Moreover, they
often overlook the uncertainty associated with task similarity. This leads to
unreliable task adaptation when only limited observations are obtained or when
the new tasks differ significantly from the related tasks. To address these
limitations, we propose a novel meta-learning BO approach that bypasses the
surrogate model and directly learns the utility of queries across tasks. Our
method explicitly models task uncertainty and includes an auxiliary model to
enable robust adaptation to new tasks. Extensive experiments show that our
method demonstrates strong anytime performance and outperforms state-of-the-art
meta-learning BO methods in various benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PDFA Distillation via String Probability Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Baumgartner, Sicco Verwer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probabilistic deterministic finite automata (PDFA) are discrete event systems
modeling conditional probabilities over languages: Given an already seen
sequence of tokens they return the probability of tokens of interest to appear
next. These types of models have gained interest in the domain of explainable
machine learning, where they are used as surrogate models for neural networks
trained as language models. In this work we present an algorithm to distill
PDFA from neural networks. Our algorithm is a derivative of the L# algorithm
and capable of learning PDFA from a new type of query, in which the algorithm
infers conditional probabilities from the probability of the queried string to
occur. We show its effectiveness on a recent public dataset by distilling PDFA
from a set of trained neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LearnAUT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Generative</span> AI-Driven Human Digital Twin in IoT-Healthcare: A
  Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13699v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13699v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayuan Chen, You Shi, Changyan Yi, Hongyang Du, Jiawen Kang, Dusit Niyato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Internet of things (IoT) can significantly enhance the quality of human
life, specifically in healthcare, attracting extensive attentions to
IoT-healthcare services. Meanwhile, the human digital twin (HDT) is proposed as
an innovative paradigm that can comprehensively characterize the replication of
the individual human body in the digital world and reflect its physical status
in real time. Naturally, HDT is envisioned to empower IoT-healthcare beyond the
application of healthcare monitoring by acting as a versatile and vivid human
digital testbed, simulating the outcomes and guiding the practical treatments.
However, successfully establishing HDT requires high-fidelity virtual modeling
and strong information interactions but possibly with scarce, biased and noisy
data. Fortunately, a recent popular technology called generative artificial
intelligence (GAI) may be a promising solution because it can leverage advanced
AI algorithms to automatically create, manipulate, and modify valuable while
diverse data. This survey particularly focuses on the implementation of
GAI-driven HDT in IoT-healthcare. We start by introducing the background of
IoT-healthcare and the potential of GAI-driven HDT. Then, we delve into the
fundamental techniques and present the overall framework of GAI-driven HDT.
After that, we explore the realization of GAI-driven HDT in detail, including
GAI-enabled data acquisition, communication, data management, digital modeling,
and data analysis. Besides, we discuss typical IoT-healthcare applications that
can be revolutionized by GAI-driven HDT, namely personalized health monitoring
and diagnosis, personalized prescription, and personalized rehabilitation.
Finally, we conclude this survey by highlighting some future research
directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Networked Communication for Decentralised Agents in Mean-Field Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02766v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02766v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Benjamin, Alessandro Abate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce networked communication to the mean-field game framework, in
particular to oracle-free settings where $N$ decentralised agents learn along a
single, non-episodic run of the empirical system. We prove that our
architecture, with only a few reasonable assumptions about network structure,
has sample guarantees bounded between those of the centralised- and
independent-learning cases. We discuss how the sample guarantees of the three
theoretical algorithms do not actually result in practical convergence. We
therefore show that in practical settings where the theoretical parameters are
not observed (leading to poor estimation of the Q-function), our communication
scheme significantly accelerates convergence over the independent case (and
often even the centralised case), without relying on the assumption of a
centralised learner. We contribute further practical enhancements to all three
theoretical algorithms, allowing us to present their first empirical
demonstrations. Our experiments confirm that we can remove several of the
theoretical assumptions of the algorithms, and display the empirical
convergence benefits brought by our new networked communication. We
additionally show that the networked approach has significant advantages, over
both the centralised and independent alternatives, in terms of robustness to
unexpected learning failures and to changes in population size.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kandinsky 3.0 Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03511v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03511v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimir Arkhipkin, Andrei Filatov, Viacheslav Vasilev, Anastasia Maltseva, Said Azizov, Igor Pavlov, Julia Agafonova, Andrey Kuznetsov, Denis Dimitrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Kandinsky 3.0, a large-scale text-to-image generation model based
on latent diffusion, continuing the series of text-to-image Kandinsky models
and reflecting our progress to achieve higher quality and realism of image
generation. In this report we describe the architecture of the model, the data
collection procedure, the training technique, and the production system for
user interaction. We focus on the key components that, as we have identified as
a result of a large number of experiments, had the most significant impact on
improving the quality of our model compared to the others. We also describe
extensions and applications of our model, including super resolution,
inpainting, image editing, image-to-video generation, and a distilled version
of Kandinsky 3.0 - Kandinsky 3.1, which does inference in 4 steps of the
reverse process and 20 times faster without visual quality decrease. By
side-by-side human preferences comparison, Kandinsky becomes better in text
understanding and works better on specific domains. The code is available at
https://github.com/ai-forever/Kandinsky-3
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ai-forever.github.io/Kandinsky-3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Straggler-Resilient Differentially-Private Decentralized Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03080v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03080v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yauhen Yakimenka, Chung-Wei Weng, Hsuan-Yin Lin, Eirik Rosnes, Jörg Kliewer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the straggler problem in decentralized learning over a logical
ring while preserving user data privacy. Especially, we extend the recently
proposed framework of differential privacy (DP) amplification by
decentralization by Cyffers and Bellet to include overall training
latency--comprising both computation and communication latency. Analytical
results on both the convergence speed and the DP level are derived for both a
skipping scheme (which ignores the stragglers after a timeout) and a baseline
scheme that waits for each node to finish before the training continues. A
trade-off between overall training latency, accuracy, and privacy,
parameterized by the timeout of the skipping scheme, is identified and
empirically validated for logistic regression on a real-world dataset and for
image classification using the MNIST and CIFAR-10 datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the IEEE Journal on Selected Areas in Information Theory
  (special issue on Information-Theoretic Methods for Trustworthy and Reliable
  Machine Learning)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishabh Maheshwary, Vikas Yadav, Hoang Nguyen, Khyati Mahajan, Sathwik Tejaswi Madhusudhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction finetuning (IFT) is critical for aligning Large Language Models
(LLMs) to follow instructions. While many effective IFT datasets have been
introduced recently, they predominantly focus on high-resource languages like
English. To better align LLMs across a broad spectrum of languages and tasks,
we propose a fully synthetic, novel taxonomy (Evol) guided Multilingual,
Multi-turn instruction finetuning dataset, called M2Lingual. It is constructed
by first selecting a diverse set of seed examples and then utilizing the
proposed Evol taxonomy to convert these seeds into complex and challenging
multi-turn instructions. We demonstrate the effectiveness of M2Lingual by
training LLMs of varying sizes and showcasing the enhanced performance across a
diverse set of languages. We contribute the 2 step Evol taxonomy with the
guided generation code: https://github.com/ServiceNow/M2Lingual, as well as the
first fully synthetic, general and task-oriented, multi-turn, multilingual
dataset built with Evol - M2Lingual:
https://huggingface.co/datasets/ServiceNow-AI/ M2Lingual - containing 182K
total IFT pairs, covering 70 languages and 17+ NLP tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Rate of Kernel Regression in Large Dimensions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04268v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04268v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Lu, Haobo Zhang, Yicheng Li, Manyun Xu, Qian Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We perform a study on kernel regression for large-dimensional data (where the
sample size $n$ is polynomially depending on the dimension $d$ of the samples,
i.e., $n\asymp d^{\gamma}$ for some $\gamma >0$ ). We first build a general
tool to characterize the upper bound and the minimax lower bound of kernel
regression for large dimensional data through the Mendelson complexity
$\varepsilon_{n}^{2}$ and the metric entropy $\bar{\varepsilon}_{n}^{2}$
respectively. When the target function falls into the RKHS associated with a
(general) inner product model defined on $\mathbb{S}^{d}$, we utilize the new
tool to show that the minimax rate of the excess risk of kernel regression is
$n^{-1/2}$ when $n\asymp d^{\gamma}$ for $\gamma =2, 4, 6, 8, \cdots$. We then
further determine the optimal rate of the excess risk of kernel regression for
all the $\gamma>0$ and find that the curve of optimal rate varying along
$\gamma$ exhibits several new phenomena including the multiple descent behavior
and the periodic plateau behavior. As an application, For the neural tangent
kernel (NTK), we also provide a similar explicit description of the curve of
optimal rate. As a direct corollary, we know these claims hold for wide neural
networks as well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity
  Text Embeddings Through Self-Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03216v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03216v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a new embedding model, called M3-Embedding, which
is distinguished for its versatility in Multi-Linguality, Multi-Functionality,
and Multi-Granularity. It can support more than 100 working languages, leading
to new state-of-the-art performances on multi-lingual and cross-lingual
retrieval tasks. It can simultaneously perform the three common retrieval
functionalities of embedding model: dense retrieval, multi-vector retrieval,
and sparse retrieval, which provides a unified model foundation for real-world
IR applications. It is able to process inputs of different granularities,
spanning from short sentences to long documents of up to 8192 tokens. The
effective training of M3-Embedding involves the following technical
contributions. We propose a novel self-knowledge distillation approach, where
the relevance scores from different retrieval functionalities can be integrated
as the teacher signal to enhance the training quality. We also optimize the
batching strategy, enabling a large batch size and high training throughput to
ensure the discriminativeness of embeddings. To the best of our knowledge,
M3-Embedding is the first embedding model which realizes such a strong
versatility. The model and code will be publicly available at
https://github.com/FlagOpen/FlagEmbedding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Data Curation for Self-Supervised Learning: A Clustering-Based
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15613v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15613v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huy V. Vo, Vasil Khalidov, Timothée Darcet, Théo Moutakanni, Nikita Smetanin, Marc Szafraniec, Hugo Touvron, Camille Couprie, Maxime Oquab, Armand Joulin, Hervé Jégou, Patrick Labatut, Piotr Bojanowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised features are the cornerstone of modern machine learning
systems. They are typically pre-trained on data collections whose construction
and curation typically require extensive human effort. This manual process has
some limitations similar to those encountered in supervised learning, e.g., the
crowd-sourced selection of data is costly and time-consuming, preventing
scaling the dataset size. In this work, we consider the problem of automatic
curation of high-quality datasets for self-supervised pre-training. We posit
that such datasets should be large, diverse and balanced, and propose a
clustering-based approach for building ones satisfying all these criteria. Our
method involves successive and hierarchical applications of $k$-means on a
large and diverse data repository to obtain clusters that distribute uniformly
among data concepts, followed by a hierarchical, balanced sampling step from
these clusters. Extensive experiments on three different data domains including
web-based images, satellite images and text show that features trained on our
automatically curated datasets outperform those trained on uncurated data while
being on par or better than ones trained on manually curated data. Code is
available at https://github.com/facebookresearch/ssl-data-curation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model Enhanced Clustering for News Event Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10552v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10552v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adane Nega Tarekegn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The news landscape is continuously evolving, with an ever-increasing volume
of information from around the world. Automated event detection within this
vast data repository is essential for monitoring, identifying, and categorizing
significant news occurrences across diverse platforms. This paper presents an
event detection framework that leverages Large Language Models (LLMs) combined
with clustering analysis to detect news events from the Global Database of
Events, Language, and Tone (GDELT). The framework enhances event clustering
through both pre-event detection tasks (keyword extraction and text embedding)
and post-event detection tasks (event summarization and topic labelling). We
also evaluate the impact of various textual embeddings on the quality of
clustering outcomes, ensuring robust news categorization. Additionally, we
introduce a novel Cluster Stability Assessment Index (CSAI) to assess the
validity and robustness of clustering results. CSAI utilizes multiple feature
vectors to provide a new way of measuring clustering quality. Our experiments
indicate that the use of LLM embedding in the event detection framework has
significantly improved the results, demonstrating greater robustness in terms
of CSAI scores. Moreover, post-event detection tasks generate meaningful
insights, facilitating effective interpretation of event clustering results.
Overall, our experimental results indicate that the proposed framework offers
valuable insights and could enhance the accuracy in news analysis and
reporting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Effort and Size Estimation in Software Projects with Large Language
  Model-based Intelligent Interfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claudionor N. Coelho Jr, Hanchen Xiong, Tushar Karayil, Sree Koratala, Rex Shang, Jacob Bollinger, Mohamed Shabar, Syam Nair
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of Large Language Models (LLM) has also resulted in an
equivalent proliferation in its applications. Software design, being one, has
gained tremendous benefits in using LLMs as an interface component that extends
fixed user stories. However, inclusion of LLM-based AI agents in software
design often poses unexpected challenges, especially in the estimation of
development efforts. Through the example of UI-based user stories, we provide a
comparison against traditional methods and propose a new way to enhance
specifications of natural language-based questions that allows for the
estimation of development effort by taking into account data sources,
interfaces and algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SampleAttention: Near-Lossless Acceleration of Long Context LLM
  Inference with Adaptive Structured Sparse Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15486v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15486v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianchao Zhu, Jiangfei Duan, Chang Chen, Siran Liu, Xiuhong Li, Guanyu Feng, Xin Lv, Huanqi Cao, Xiao Chuanfu, Xingcheng Zhang, Dahua Lin, Chao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) now support extremely long context windows, but
the quadratic complexity of vanilla attention results in significantly long
Time-to-First-Token (TTFT) latency. Existing approaches to address this
complexity require additional pretraining or finetuning, and often sacrifice
model accuracy. In this paper, we first provide both theoretical and empirical
foundations for near-lossless sparse attention. We find dynamically capturing
head-specific sparse patterns at runtime with low overhead is crucial. To
address this, we propose SampleAttention, an adaptive structured and
near-lossless sparse attention. Leveraging observed significant sparse
patterns, SampleAttention attends to a fixed percentage of adjacent tokens to
capture local window patterns, and employs a two-stage query-guided key-value
filtering approach, which adaptively select a minimum set of key-values with
low overhead, to capture column stripe patterns. Comprehensive evaluations show
that SampleAttention can seamlessly replace vanilla attention in off-the-shelf
LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\times$
compared with FlashAttention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ULLER: A Unified Language for Learning and Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00532v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00532v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emile van Krieken, Samy Badreddine, Robin Manhaeve, Eleonora Giunchiglia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of neuro-symbolic artificial intelligence (NeSy), which combines
learning and reasoning, has recently experienced significant growth. There now
are a wide variety of NeSy frameworks, each with its own specific language for
expressing background knowledge and how to relate it to neural networks. This
heterogeneity hinders accessibility for newcomers and makes comparing different
NeSy frameworks challenging. We propose a language for NeSy, which we call
ULLER, a Unfied Language for LEarning and Reasoning. ULLER encompasses a wide
variety of settings, while ensuring that knowledge described in it can be used
in existing NeSy systems. ULLER has a first-order logic syntax specialised for
NeSy for which we provide example semantics including classical FOL, fuzzy
logic, and probabilistic logic. We believe ULLER is a first step towards making
NeSy research more accessible and comparable, paving the way for libraries that
streamline training and evaluation across a multitude of semantics, knowledge
bases, and NeSy systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeSy 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Position: Explain to Question not to Justify 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13914v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13914v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Przemyslaw Biecek, Wojciech Samek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable Artificial Intelligence (XAI) is a young but very promising field
of research. Unfortunately, the progress in this field is currently slowed down
by divergent and incompatible goals. We separate various threads tangled within
the area of XAI into two complementary cultures of human/value-oriented
explanations (BLUE XAI) and model/validation-oriented explanations (RED XAI).
This position paper argues that the area of RED XAI is currently
under-explored, i.e., more methods for explainability are desperately needed to
question models (e.g., extract knowledge from well-performing models as well as
spotting and fixing bugs in faulty models), and the area of RED XAI hides great
opportunities and potential for important research necessary to ensure the
safety of AI systems. We conclude this paper by presenting promising challenges
in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Regularization for Linear MMSE Filters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06560v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06560v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Gomes de Pinho Zanco, Leszek Szczecinski, Jacob Benesty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we consider the problem of regularization in the design of
minimum mean square error (MMSE) linear filters. Using the relationship with
statistical machine learning methods, using a Bayesian approach, the
regularization parameter is found from the observed signals in a simple and
automatic manner. The proposed approach is illustrated in system identification
and beamforming examples, where the automatic regularization is shown to yield
near-optimal results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciBench: Evaluating College-Level Scientific Problem-Solving Abilities
  of Large Language Models <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.10635v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.10635v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of the existing Large Language Model (LLM) benchmarks on scientific
problem reasoning focus on problems grounded in high-school subjects and are
confined to elementary algebraic operations. To systematically examine the
reasoning capabilities required for solving complex scientific problems, we
introduce an expansive benchmark suite SciBench for LLMs. SciBench contains a
carefully curated dataset featuring a range of collegiate-level scientific
problems from mathematics, chemistry, and physics domains. Based on the
dataset, we conduct an in-depth benchmarking study of representative
open-source and proprietary LLMs with various prompting strategies. The results
reveal that the current LLMs fall short of delivering satisfactory performance,
with the best overall score of merely 43.22%. Furthermore, through a detailed
user study, we categorize the errors made by LLMs into ten problem-solving
abilities. Our analysis indicates that no single prompting strategy
significantly outperforms the others and some strategies that demonstrate
improvements in certain problem-solving skills could result in declines in
other skills. We envision that SciBench will catalyze further developments in
the reasoning abilities of LLMs, thereby ultimately contributing to scientific
research and discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Preference Learning for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Muldrew, Peter Hayes, Mingtian Zhang, David Barber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become more capable, fine-tuning techniques
for aligning with human intent are increasingly important. A key consideration
for aligning these models is how to most effectively use human resources, or
model resources in the case where LLMs themselves are used as oracles.
Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most
prominent example of such a technique, but is complex and often unstable.
Direct Preference Optimization (DPO) has recently been proposed as a simpler
and more stable alternative. In this work, we develop an active learning
strategy for DPO to make better use of preference labels. We propose a
practical acquisition function for prompt/completion pairs based on the
predictive entropy of the language model and a measure of certainty of the
implicit preference model optimized by DPO. We demonstrate how our approach
improves both the rate of learning and final performance of fine-tuning on
pairwise preference data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Logic Tree Extraction for Event Sequence Explanation from LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01124v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01124v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zitao Song, Chao Yang, Chaojie Wang, Bo An, Shuang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern high-stakes systems, such as healthcare or robotics, often generate
vast streaming event sequences. Our goal is to design an efficient,
plug-and-play tool to elicit logic tree-based explanations from Large Language
Models (LLMs) to provide customized insights into each observed event sequence.
Built on the temporal point process model for events, our method employs the
likelihood function as a score to evaluate generated logic trees. We propose an
amortized Expectation-Maximization (EM) learning framework and treat the logic
tree as latent variables. In the E-step, we evaluate the posterior distribution
over the latent logic trees using an LLM prior and the likelihood of the
observed event sequences. LLM provides a high-quality prior for the latent
logic trees, however, since the posterior is built over a discrete
combinatorial space, we cannot get the closed-form solution. We propose to
generate logic tree samples from the posterior using a learnable GFlowNet,
which is a diversity-seeking generator for structured discrete variables. The
M-step employs the generated logic rules to approximate marginalization over
the posterior, facilitating the learning of model parameters and refining the
tunable LLM prior parameters. In the online setting, our locally built,
lightweight model will iteratively extract the most relevant rules from LLMs
for each sequence using only a few iterations. Empirical demonstrations
showcase the promising performance and adaptability of our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QQQ: Quality Quattuor-Bit Quantization for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09904v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09904v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Zhang, Peng Zhang, Mincong Huang, Jingyang Xiang, Yujie Wang, Chao Wang, Yineng Zhang, Lei Yu, Chuan Liu, Wei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization is a proven effective method for compressing large language
models. Although popular techniques like W8A8 and W4A16 effectively maintain
model performance, they often fail to concurrently speed up the prefill and
decoding stages of inference. W4A8 is a promising strategy to accelerate both
of them while usually leads to a significant performance degradation. To
address these issues, we present QQQ, a Quality Quattuor-bit Quantization
method with 4-bit weights and 8-bit activations. QQQ employs adaptive smoothing
and Hessian-based compensation, significantly enhancing the performance of
quantized models without extensive training. Furthermore, we meticulously
engineer W4A8 GEMM kernels to increase inference speed. Our specialized
per-channel W4A8 GEMM and per-group W4A8 GEMM achieve impressive speed
increases of 3.67$\times$ and 3.29 $\times$ over FP16 GEMM. Our extensive
experiments show that QQQ achieves performance on par with existing
state-of-the-art LLM quantization methods while significantly accelerating
inference, achieving speed boosts up to 2.24 $\times$, 2.10$\times$, and
1.25$\times$ compared to FP16, W8A8, and W4A16, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MOYU: A Theoretical Study on Massive Over-activation Yielded Uplifts in
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12569v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12569v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Ma, Mincong Huang, Chao Wang, Yujie Wang, Lei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Massive Over-activation Yielded Uplifts(MOYU) is an inherent property of
large language models, and dynamic activation(DA) based on the MOYU property is
a clever yet under-explored strategy designed to accelerate inference in these
models. Existing methods that utilize MOYU often face a significant 'Impossible
Trinity': struggling to simultaneously maintain model performance, enhance
inference speed, and extend applicability across various architectures. Due to
the theoretical ambiguities surrounding MOYU, this paper elucidates the root
cause of the MOYU property and outlines the mechanisms behind two primary
limitations encountered by current DA methods: 1) history-related activation
uncertainty, and 2) semantic-irrelevant activation inertia. Our analysis not
only underscores the limitations of current dynamic activation strategies
within large-scale LLaMA models but also proposes opportunities for refining
the design of future sparsity schemes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Logical Closed Loop: Uncovering Object Hallucinations in Large
  Vision-Language Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfei Wu, Qiang Liu, Ding Wang, Jinghao Zhang, Shu Wu, Liang Wang, Tieniu Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object hallucination has been an Achilles' heel which hinders the broader
applications of large vision-language models (LVLMs). Object hallucination
refers to the phenomenon that the LVLMs claim non-existent objects in the
image. To mitigate the object hallucinations, instruction tuning and external
model-based detection methods have been proposed, which either require
large-scare computational resources or depend on the detection result of
external models. However, there remains an under-explored field to utilize the
LVLM itself to alleviate object hallucinations. In this work, we adopt the
intuition that the LVLM tends to respond logically consistently for existent
objects but inconsistently for hallucinated objects. Therefore, we propose a
Logical Closed Loop-based framework for Object Hallucination Detection and
Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency
probing to raise questions with logical correlations, inquiring about
attributes from objects and vice versa. Whether their responses can form a
logical closed loop serves as an indicator of object hallucination. As a
plug-and-play method, it can be seamlessly applied to all existing LVLMs.
Comprehensive experiments conducted on three benchmarks across four LVLMs have
demonstrated significant improvements brought by our method, indicating its
effectiveness and generality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to ACL 2024; 19 Pages, 15 Figures, 6 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sequential Model for Predicting Patient Adherence in Subcutaneous
  Immunotherapy for Allergic Rhinitis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11447v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11447v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Li, Yu Xiong, Wenxin Fan, Kai Wang, Qingqing Yu, Liping Si, Patrick van der Smagt, Jun Tang, Nutan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: Subcutaneous Immunotherapy (SCIT) is the long-lasting causal
treatment of allergic rhinitis (AR). How to enhance the adherence of patients
to maximize the benefit of allergen immunotherapy (AIT) plays a crucial role in
the management of AIT. This study aims to leverage novel machine learning
models to precisely predict the risk of non-adherence of AR patients and
related local symptom scores in three years SCIT.
  Methods: The research develops and analyzes two models, sequential
latent-variable model (SLVM) of Sequential Latent Actor-Critic (SLAC) and Long
Short-Term Memory (LSTM) evaluating them based on scoring and adherence
prediction capabilities.
  Results: Excluding the biased samples at the first time step, the predictive
adherence accuracy of the SLAC models is from 60\% to 72\%, and for LSTM
models, it is 66\% to 84\%, varying according to the time steps. The range of
Root Mean Square Error (RMSE) for SLAC models is between 0.93 and 2.22, while
for LSTM models it is between 1.09 and 1.77. Notably, these RMSEs are
significantly lower than the random prediction error of 4.55.
  Conclusion: We creatively apply sequential models in the long-term management
of SCIT with promising accuracy in the prediction of SCIT nonadherence in AR
patients. While LSTM outperforms SLAC in adherence prediction, SLAC excels in
score prediction for patients undergoing SCIT for AR. The state-action-based
SLAC adds flexibility, presenting a novel and effective approach for managing
long-term AIT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Frontiers in Pharmacology, research topic: Methods and Metrics to
  Measure Medication Adherence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empowering Interdisciplinary Insights with Dynamic Graph Embedding
  Trajectories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17963v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17963v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqiao Jin, Andrew Zhao, Yeon-Chang Lee, Meng Ye, Ajay Divakaran, Srijan Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We developed DyGETViz, a novel framework for effectively visualizing dynamic
graphs (DGs) that are ubiquitous across diverse real-world systems. This
framework leverages recent advancements in discrete-time dynamic graph (DTDG)
models to adeptly handle the temporal dynamics inherent in dynamic graphs.
DyGETViz effectively captures both micro- and macro-level structural shifts
within these graphs, offering a robust method for representing complex and
massive dynamic graphs. The application of DyGETViz extends to a diverse array
of domains, including ethology, epidemiology, finance, genetics, linguistics,
communication studies, social studies, and international relations. Through its
implementation, DyGETViz has revealed or confirmed various critical insights.
These include the diversity of content sharing patterns and the degree of
specialization within online communities, the chronological evolution of
lexicons across decades, and the distinct trajectories exhibited by
aging-related and non-related genes. Importantly, DyGETViz enhances the
accessibility of scientific findings to non-domain experts by simplifying the
complexities of dynamic graphs. Our framework is released as an open-source
Python package for use across diverse disciplines. Our work not only addresses
the ongoing challenges in visualizing and analyzing DTDG models but also
establishes a foundational framework for future investigations into dynamic
graph representation and analysis across various disciplines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Knowledge Distillation for Lightweight Skin Cancer
  Classification: Balancing Accuracy and Computational Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17051v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17051v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niful Islam, Khan Md Hasib, Fahmida Akter Joti, Asif Karim, Sami Azam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skin cancer is a major concern to public health, accounting for one-third of
the reported cancers. If not detected early, the cancer has the potential for
severe consequences. Recognizing the critical need for effective skin cancer
classification, we address the limitations of existing models, which are often
too large to deploy in areas with limited computational resources. In response,
we present a knowledge distillation based approach for creating a lightweight
yet high-performing classifier. The proposed solution involves fusing three
models, namely ResNet152V2, ConvNeXtBase, and ViT Base, to create an effective
teacher model. The teacher model is then employed to guide a lightweight
student model of size 2.03 MB. This student model is further compressed to
469.77 KB using 16-bit quantization, enabling smooth incorporation into edge
devices. With six-stage image preprocessing, data augmentation, and a rigorous
ablation study, the model achieves an impressive accuracy of 98.75% on the
HAM10000 dataset and 98.94% on the Kaggle dataset in classifying benign and
malignant skin cancers. With its high accuracy and compact size, our model
appears to be a potential choice for accurate skin cancer classification,
particularly in resource-constrained settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlowVQA: Mapping Multimodal Logic in Visual Question Answering with
  Flowcharts <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubhankar Singh, Purvi Chaurasia, Yerram Varun, Pranshu Pandya, Vatsal Gupta, Vivek Gupta, Dan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing benchmarks for visual question answering lack in visual grounding
and complexity, particularly in evaluating spatial reasoning skills. We
introduce FlowVQA, a novel benchmark aimed at assessing the capabilities of
visual question-answering multimodal language models in reasoning with
flowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and
human-verified flowchart images from three distinct content sources, along with
22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,
including information localization, decision-making, and logical progression.
We conduct a thorough baseline evaluation on a suite of both open-source and
proprietary multimodal language models using various strategies, followed by an
analysis of directional bias. The results underscore the benchmark's potential
as a vital tool for advancing the field of multimodal modeling, providing a
focused and challenging environment for enhancing model performance in visual
and logical reasoning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACL 2024 (Findings), 21 pages, 7 figures, 9 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transcendence: <span class="highlight-title">Generative</span> Models Can Outperform The Experts That Train
  Them 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11741v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11741v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edwin Zhang, Vincent Zhu, Naomi Saphra, Anat Kleiman, Benjamin L. Edelman, Milind Tambe, Sham M. Kakade, Eran Malach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models are trained with the simple objective of imitating the
conditional probability distribution induced by the data they are trained on.
Therefore, when trained on data generated by humans, we may not expect the
artificial model to outperform the humans on their original objectives. In this
work, we study the phenomenon of transcendence: when a generative model
achieves capabilities that surpass the abilities of the experts generating its
data. We demonstrate transcendence by training an autoregressive transformer to
play chess from game transcripts, and show that the trained model can sometimes
achieve better performance than all players in the dataset. We theoretically
prove that transcendence can be enabled by low-temperature sampling, and
rigorously assess this claim experimentally. Finally, we discuss other sources
of transcendence, laying the groundwork for future investigation of this
phenomenon in a broader setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code, models, and data at https://transcendence.eddie.win</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Behavior Generation with Latent Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03181v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03181v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungjae Lee, Yibin Wang, Haritheja Etukuru, H. Jin Kim, Nur Muhammad Mahi Shafiullah, Lerrel Pinto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative modeling of complex behaviors from labeled datasets has been a
longstanding problem in decision making. Unlike language or image generation,
decision making requires modeling actions - continuous-valued vectors that are
multimodal in their distribution, potentially drawn from uncurated sources,
where generation errors can compound in sequential prediction. A recent class
of models called Behavior Transformers (BeT) addresses this by discretizing
actions using k-means clustering to capture different modes. However, k-means
struggles to scale for high-dimensional action spaces or long sequences, and
lacks gradient information, and thus BeT suffers in modeling long-range
actions. In this work, we present Vector-Quantized Behavior Transformer
(VQ-BeT), a versatile model for behavior generation that handles multimodal
action prediction, conditional generation, and partial observations. VQ-BeT
augments BeT by tokenizing continuous actions with a hierarchical vector
quantization module. Across seven environments including simulated
manipulation, autonomous driving, and robotics, VQ-BeT improves on
state-of-the-art models such as BeT and Diffusion Policies. Importantly, we
demonstrate VQ-BeT's improved ability to capture behavior modes while
accelerating inference speed 5x over Diffusion Policies. Videos and code can be
found https://sjlee.cc/vq-bet
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github repo: https://github.com/jayLEE0301/vq_bet_official</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kernel vs. Kernel: Exploring How the Data Structure Affects Neural
  Collapse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02105v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02105v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vignesh Kothapalli, Tom Tirer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a vast amount of literature has focused on the "Neural Collapse"
(NC) phenomenon, which emerges when training neural network (NN) classifiers
beyond the zero training error point. The core component of NC is the decrease
in the within class variability of the network's deepest features, dubbed as
NC1. The theoretical works that study NC are typically based on simplified
unconstrained features models (UFMs) that mask any effect of the data on the
extent of collapse. In this paper, we provide a kernel-based analysis that does
not suffer from this limitation. First, given a kernel function, we establish
expressions for the traces of the within- and between-class covariance matrices
of the samples' features (and consequently an NC1 metric). Then, we turn to
focus on kernels associated with shallow NNs. First, we consider the NN
Gaussian Process kernel (NNGP), associated with the network at initialization,
and the complement Neural Tangent Kernel (NTK), associated with its training in
the "lazy regime". Interestingly, we show that the NTK does not represent more
collapsed features than the NNGP for prototypical data models. As NC emerges
from training, we then consider an alternative to NTK: the recently proposed
adaptive kernel, which generalizes NNGP to model the feature mapping learned
from the training data. Contrasting our NC1 analysis for these two kernels
enables gaining insights into the effect of data distribution on the extent of
collapse, which are empirically aligned with the behavior observed with
practical training of NNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AIGB: <span class="highlight-title">Generative</span> Auto-bidding via <span class="highlight-title">Diffusion</span> Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16141v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16141v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayan Guo, Yusen Huo, Zhilin Zhang, Tianyu Wang, Chuan Yu, Jian Xu, Yan Zhang, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Auto-bidding plays a crucial role in facilitating online advertising by
automatically providing bids for advertisers. Reinforcement learning (RL) has
gained popularity for auto-bidding. However, most current RL auto-bidding
methods are modeled through the Markovian Decision Process (MDP), which assumes
the Markovian state transition. This assumption restricts the ability to
perform in long horizon scenarios and makes the model unstable when dealing
with highly random online advertising environments. To tackle this issue, this
paper introduces AI-Generated Bidding (AIGB), a novel paradigm for auto-bidding
through generative modeling. In this paradigm, we propose DiffBid, a
conditional diffusion modeling approach for bid generation. DiffBid directly
models the correlation between the return and the entire trajectory,
effectively avoiding error propagation across time steps in long horizons.
Additionally, DiffBid offers a versatile approach for generating trajectories
that maximize given targets while adhering to specific constraints. Extensive
experiments conducted on the real-world dataset and online A/B test on Alibaba
advertising platform demonstrate the effectiveness of DiffBid, achieving 2.81%
increase in GMV and 3.36% increase in ROI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Sequential Two-Sample Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12616v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12616v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhi Li, Prad Kadambi, Pouria Saidi, Karthikeyan Natesan Ramamurthy, Gautam Dasarathy, Visar Berisha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A two-sample hypothesis test is a statistical procedure used to determine
whether the distributions generating two samples are identical. We consider the
two-sample testing problem in a new scenario where the sample measurements (or
sample features) are inexpensive to access, but their group memberships (or
labels) are costly. To address the problem, we devise the first \emph{active
sequential two-sample testing framework} that not only sequentially but also
\emph{actively queries}. Our test statistic is a likelihood ratio where one
likelihood is found by maximization over all class priors, and the other is
provided by a probabilistic classification model. The classification model is
adaptively updated and used to predict where the (unlabelled) features have a
high dependency on labels; labeling the ``high-dependency'' features leads to
the increased power of the proposed testing framework. In theory, we provide
the proof that our framework produces an \emph{anytime-valid} $p$-value. In
addition, we characterize the proposed framework's gain in testing power by
analyzing the mutual information between the feature and label variables in
asymptotic and finite-sample scenarios. In practice, we introduce an
instantiation of our framework and evaluate it using several experiments; the
experiments on the synthetic, MNIST, and application-specific datasets
demonstrate that the testing power of the instantiated active sequential test
significantly increases while the Type I error is under control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FAdam: Adam is a natural gradient optimizer using diagonal empirical
  Fisher information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12807v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12807v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongseong Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper establishes a mathematical foundation for the Adam optimizer,
elucidating its connection to natural gradient descent through Riemannian and
information geometry. We rigorously analyze the diagonal empirical Fisher
information matrix (FIM) in Adam, clarifying all detailed approximations and
advocating for the use of log probability functions as loss, which should be
based on discrete distributions, due to the limitations of empirical FIM. Our
analysis uncovers flaws in the original Adam algorithm, leading to proposed
corrections such as enhanced momentum calculations, adjusted bias corrections,
adaptive epsilon, and gradient clipping. We refine the weight decay term based
on our theoretical framework. Our modified algorithm, Fisher Adam (FAdam),
demonstrates superior performance across diverse domains including LLM, ASR,
and VQ-VAE, achieving state-of-the-art results in ASR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 4 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Generative</span> Autoencoding of Dropout Patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunta Maeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a generative model termed Deciphering Autoencoders. In this model,
we assign a unique random dropout pattern to each data point in the training
dataset and then train an autoencoder to reconstruct the corresponding data
point using this pattern as information to be encoded. Even if a completely
random dropout pattern is assigned to each data point regardless of their
similarities, a sufficiently large encoder can smoothly map them to a
low-dimensional latent space to reconstruct individual training data points.
During inference, using a dropout pattern different from those used during
training allows the model to function as a generator. Since the training of
Deciphering Autoencoders relies solely on reconstruction error, it offers more
stable training compared to other generative models. Despite their simplicity,
Deciphering Autoencoders show sampling quality comparable to DCGAN on the
CIFAR-10 dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MeGA: Merging Multiple Independently Trained Neural Networks Based on
  Genetic Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04607v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04607v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel method for merging the weights of
multiple pre-trained neural networks using a genetic algorithm called MeGA.
Traditional techniques, such as weight averaging and ensemble methods, often
fail to fully harness the capabilities of pre-trained networks. Our approach
leverages a genetic algorithm with tournament selection, crossover, and
mutation to optimize weight combinations, creating a more effective fusion.
This technique allows the merged model to inherit advantageous features from
both parent models, resulting in enhanced accuracy and robustness. Through
experiments on the CIFAR-10 dataset, we demonstrate that our genetic
algorithm-based weight merging method improves test accuracy compared to
individual models and conventional methods. This approach provides a scalable
solution for integrating multiple pre-trained networks across various deep
learning applications. Github is available at:
https://github.com/YUNBLAK/MeGA-Merging-Multiple-Independently-Trained-Neural-Networks-Based-on-Genetic-Algorithm
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Submodular Information Selection for Hypothesis Testing with
  Misclassification Penalties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10930v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10930v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jayanth Bhargav, Mahsa Ghasemi, Shreyas Sundaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of selecting an optimal subset of information sources
for a hypothesis testing/classification task where the goal is to identify the
true state of the world from a finite set of hypotheses, based on finite
observation samples from the sources. In order to characterize the learning
performance, we propose a misclassification penalty framework, which enables
nonuniform treatment of different misclassification errors. In a centralized
Bayesian learning setting, we study two variants of the subset selection
problem: (i) selecting a minimum cost information set to ensure that the
maximum penalty of misclassifying the true hypothesis is below a desired bound
and (ii) selecting an optimal information set under a limited budget to
minimize the maximum penalty of misclassifying the true hypothesis. Under
certain assumptions, we prove that the objective (or constraints) of these
combinatorial optimization problems are weak (or approximate) submodular, and
establish high-probability performance guarantees for greedy algorithms.
Further, we propose an alternate metric for information set selection which is
based on the total penalty of misclassification. We prove that this metric is
submodular and establish near-optimal guarantees for the greedy algorithms for
both the information set selection problems. Finally, we present numerical
simulations to validate our theoretical results over several randomly generated
instances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robust</span> Model-Based Optimization for Challenging Fitness Landscapes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13650v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13650v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saba Ghaffari, Ehsan Saleh, Alexander G. Schwing, Yu-Xiong Wang, Martin D. Burke, Saurabh Sinha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protein design, a grand challenge of the day, involves optimization on a
fitness landscape, and leading methods adopt a model-based approach where a
model is trained on a training set (protein sequences and fitness) and proposes
candidates to explore next. These methods are challenged by sparsity of
high-fitness samples in the training set, a problem that has been in the
literature. A less recognized but equally important problem stems from the
distribution of training samples in the design space: leading methods are not
designed for scenarios where the desired optimum is in a region that is not
only poorly represented in training data, but also relatively far from the
highly represented low-fitness regions. We show that this problem of
"separation" in the design space is a significant bottleneck in existing
model-based optimization tools and propose a new approach that uses a novel VAE
as its search model to overcome the problem. We demonstrate its advantage over
prior methods in robustly finding improved samples, regardless of the imbalance
and separation between low- and high-fitness samples. Our comprehensive
benchmark on real and semi-synthetic protein datasets as well as solution
design for physics-informed neural networks, showcases the generality of our
approach in discrete and continuous design spaces. Our implementation is
available at https://github.com/sabagh1994/PGVAE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Forecasting Electricity Market Signals via <span class="highlight-title">Generative</span> AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05743v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05743v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Wang, Qing Zhao, Lang Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a generative artificial intelligence approach to
probabilistic forecasting of electricity market signals, such as real-time
locational marginal prices and area control error signals. Inspired by the
Wiener-Kallianpur innovation representation of nonparametric time series, we
propose a weak innovation autoencoder architecture and a novel deep learning
algorithm that extracts the canonical independent and identically distributed
innovation sequence of the time series, from which samples of future time
series are generated. The validity of the proposed approach is established by
proving that, under ideal training conditions, the generated samples have the
same conditional probability distribution as that of the ground truth. Three
applications involving highly dynamic and volatile time series in real-time
market operations are considered: (i) locational marginal price forecasting for
self-scheduled resources such as battery storage participants, (ii)
interregional price spread forecasting for virtual bidders in interchange
markets, and (iii) area control error forecasting for frequency regulations.
Numerical studies based on market data from multiple independent system
operators demonstrate the superior performance of the proposed generative
forecaster over leading classical and modern machine learning techniques under
both probabilistic and point forecasting metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D-Mol: A Novel Contrastive Learning Framework for Molecular Property
  Prediction with 3D Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17366v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17366v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taojie Kuang, Yiming Ren, Zhixiang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Molecular property prediction, crucial for early drug candidate screening and
optimization, has seen advancements with deep learning-based methods. While
deep learning-based methods have advanced considerably, they often fall short
in fully leveraging 3D spatial information. Specifically, current molecular
encoding techniques tend to inadequately extract spatial information, leading
to ambiguous representations where a single one might represent multiple
distinct molecules. Moreover, existing molecular modeling methods focus
predominantly on the most stable 3D conformations, neglecting other viable
conformations present in reality. To address these issues, we propose 3D-Mol, a
novel approach designed for more accurate spatial structure representation. It
deconstructs molecules into three hierarchical graphs to better extract
geometric information. Additionally, 3D-Mol leverages contrastive learning for
pretraining on 20 million unlabeled data, treating their conformations with
identical topological structures as weighted positive pairs and contrasting
ones as negatives, based on the similarity of their 3D conformation descriptors
and fingerprints. We compare 3D-Mol with various state-of-the-art baselines on
7 benchmarks and demonstrate our outstanding performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Universal Checkpointing: Efficient and Flexible Checkpointing for Large
  Scale Distributed Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Lian, Sam Ade Jacobs, Lev Kurilenko, Masahiro Tanaka, Stas Bekman, Olatunji Ruwase, Minjia Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing checkpointing approaches seem ill-suited for distributed training
even though hardware limitations make model parallelism, i.e., sharding model
state across multiple accelerators, a requirement for model scaling.
Consolidating distributed model state into a single checkpoint unacceptably
slows down training, and is impractical at extreme scales. Distributed
checkpoints, in contrast, are tightly coupled to the model parallelism and
hardware configurations of the training run, and thus unusable on different
configurations. To address this problem, we propose Universal Checkpointing, a
technique that enables efficient checkpoint creation while providing the
flexibility of resuming on arbitrary parallelism strategy and hardware
configurations. Universal Checkpointing unlocks unprecedented capabilities for
large-scale training such as improved resilience to hardware failures through
continued training on remaining healthy hardware, and reduced training time
through opportunistic exploitation of elastic capacity.
  The key insight of Universal Checkpointing is the selection of the optimal
representation in each phase of the checkpointing life cycle: distributed
representation for saving, and consolidated representation for loading. This is
achieved using two key mechanisms. First, the universal checkpoint format,
which consists of a consolidated representation of each model parameter and
metadata for mapping parameter fragments into training ranks of arbitrary
model-parallelism configuration. Second, the universal checkpoint language, a
simple but powerful specification language for converting distributed
checkpoints into the universal checkpoint format. Our evaluation demonstrates
the effectiveness and generality of Universal Checkpointing on state-of-the-art
model architectures and a wide range of parallelism techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Impact of Domain Knowledge and Multi-Modality on Intelligent Molecular
  Property Prediction: A Systematic <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07249v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07249v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taojie Kuang, Pengfei Liu, Zhixiang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The precise prediction of molecular properties is essential for advancements
in drug development, particularly in virtual screening and compound
optimization. The recent introduction of numerous deep learning-based methods
has shown remarkable potential in enhancing molecular property prediction
(MPP), especially improving accuracy and insights into molecular structures.
Yet, two critical questions arise: does the integration of domain knowledge
augment the accuracy of molecular property prediction and does employing
multi-modal data fusion yield more precise results than unique data source
methods? To explore these matters, we comprehensively review and quantitatively
analyze recent deep learning methods based on various benchmarks. We discover
that integrating molecular information significantly improves molecular
property prediction (MPP) for both regression and classification tasks.
Specifically, regression improvements, measured by reductions in root mean
square error (RMSE), are up to 4.0%, while classification enhancements,
measured by the area under the receiver operating characteristic curve
(ROC-AUC), are up to 1.7%. We also discover that enriching 2D graphs with 1D
SMILES boosts multi-modal learning performance for regression tasks by up to
9.1%, and augmenting 2D graphs with 3D information increases performance for
classification tasks by up to 13.2%, with both enhancements measured using
ROC-AUC. The two consolidated insights offer crucial guidance for future
advancements in drug discovery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Last Iterate Convergence of Incremental Methods and Applications in
  Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06873v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06873v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xufeng Cai, Jelena Diakonikolas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incremental gradient and incremental proximal methods are a fundamental class
of optimization algorithms used for solving finite sum problems, broadly
studied in the literature. Yet, without strong convexity, their convergence
guarantees have primarily been established for the ergodic (average) iterate.
Motivated by applications in continual learning, we obtain the first
convergence guarantees for the last iterate of both incremental gradient and
incremental proximal methods, in general convex smooth (for both) and convex
Lipschitz (for the proximal variants) settings. Our oracle complexity bounds
for the last iterate nearly match (i.e., match up to a square-root-log or a log
factor) the best known oracle complexity bounds for the average iterate, for
both classes of methods. We further obtain generalizations of our results to
weighted averaging of the iterates with increasing weights and for randomly
permuted ordering of updates. We study incremental proximal methods as a model
of continual learning with generalization and argue that large amount of
regularization is crucial to preventing catastrophic forgetting. Our results
generalize last iterate guarantees for incremental methods compared to state of
the art, as such results were previously known only for overparameterized
linear models, which correspond to convex quadratic problems with infinitely
many solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mélange: Cost Efficient Large Language Model Serving by Exploiting GPU
  Heterogeneity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14527v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14527v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler Griggs, Xiaoxuan Liu, Jiaxiang Yu, Doyoung Kim, Wei-Lin Chiang, Alvin Cheung, Ion Stoica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly integrated into many online
services, yet they remain cost-prohibitive to deploy due to the requirement of
expensive GPU instances. Prior work has addressed the high cost of LLM serving
by improving the inference engine, but less attention has been given to
selecting the most cost-efficient GPU type(s) for a specific LLM service. There
is a large and growing landscape of GPU types and, within these options, higher
cost does not always lead to increased performance. Instead, through a
comprehensive investigation, we find that three key LLM service characteristics
(request size, request rate, SLO) strongly influence GPU cost efficiency, and
differing GPU types are most cost efficient for differing LLM service settings.
As a result, the most cost-efficient allocation for a given service is
typically a mix of heterogeneous GPU types. Based on this analysis, we
introduce M\'elange, a GPU allocation framework that navigates these diverse
LLM service characteristics and heterogeneous GPU option space to automatically
and efficiently derive the minimal-cost GPU allocation for a given LLM service.
We formulate the GPU allocation task as a cost-aware bin packing problem where
GPUs are bins and items are slices of the service workload. Our formulation's
constraints account for a service's unique characteristics, allowing M\'elange
to be flexible to support diverse service settings and heterogeneity-aware to
adapt the GPU allocation to a specific service. Compared to using only a single
GPU type, M\'elange reduces deployment costs by up to 77% in conversational
settings, 33% in document-based settings, and 51% in a mixed setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FloorSet -- a VLSI Floorplanning <span class="highlight-title">Dataset</span> with Design Constraints of
  Real-World SoCs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uday Mallappa, Hesham Mostafa, Mikhail Galkin, Mariano Phielipp, Somdeb Majumdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Floorplanning for systems-on-a-chip (SoCs) and its sub-systems is a crucial
and non-trivial step of the physical design flow. It represents a difficult
combinatorial optimization problem. A typical large scale SoC with 120
partitions generates a search-space of nearly 10E250. As novel machine learning
(ML) approaches emerge to tackle such problems, there is a growing need for a
modern benchmark that comprises a large training dataset and performance
metrics that better reflect real-world constraints and objectives compared to
existing benchmarks. To address this need, we present FloorSet -- two
comprehensive datasets of synthetic fixed-outline floorplan layouts that
reflect the distribution of real SoCs. Each dataset has 1M training samples and
100 test samples where each sample is a synthetic floor-plan. FloorSet-Prime
comprises fully-abutted rectilinear partitions and near-optimal wire-length. A
simplified dataset that reflects early design phases, FloorSet-Lite comprises
rectangular partitions, with under 5 percent white-space and near-optimal
wire-length. Both datasets define hard constraints seen in modern design flows
such as shape constraints, edge-affinity, grouping constraints, and
pre-placement constraints. FloorSet is intended to spur fundamental research on
large-scale constrained optimization problems. Crucially, FloorSet alleviates
the core issue of reproducibility in modern ML driven solutions to such
problems. FloorSet is available as an open-source repository for the research
community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Image and Video Processing <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Malaria Cell Detection Using Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Sawant, Anurag Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Malaria remains one of the most pressing public health concerns globally,
causing significant morbidity and mortality, especially in sub-Saharan Africa.
Rapid and accurate diagnosis is crucial for effective treatment and disease
management. Traditional diagnostic methods, such as microscopic examination of
blood smears, are labor-intensive and require significant expertise, which may
not be readily available in resource-limited settings. This project aims to
automate the detection of malaria-infected cells using a deep learning
approach. We employed a convolutional neural network (CNN) based on the
ResNet50 architecture, leveraging transfer learning to enhance performance. The
Malaria Cell Images Dataset from Kaggle, containing 27,558 images categorized
into infected and uninfected cells, was used for training and evaluation. Our
model demonstrated high accuracy, precision, and recall, indicating its
potential as a reliable tool for assisting in malaria diagnosis. Additionally,
a web application was developed using Streamlit to allow users to upload cell
images and receive predictions about malaria infection, making the technology
accessible and user-friendly. This paper provides a comprehensive overview of
the methodology, experiments, and results, highlighting the effectiveness of
deep learning in medical image analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Initialization on Intra-subject Pediatric Brain MR Image
  Registration: A Comparative Analysis between SyN ANTs and Deep Learning-Based
  Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andjela Dimitrijevic, Vincent Noblet, Benjamin De Leener
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study evaluates the performance of conventional SyN ANTs and
learning-based registration methods in the context of pediatric neuroimaging,
specifically focusing on intrasubject deformable registration. The comparison
involves three approaches: without (NR), with rigid (RR), and with rigid and
affine (RAR) initializations. In addition to initialization, performances are
evaluated in terms of accuracy, speed, and the impact of age intervals and sex
per pair. Data consists of the publicly available MRI scans from the Calgary
Preschool dataset, which includes 63 children aged 2-7 years, allowing for 431
registration pairs. We implemented the unsupervised DL framework with a U-Net
architecture using DeepReg and it was 5-fold cross-validated. Evaluation
includes Dice scores for tissue segmentation from 18 smaller regions obtained
by SynthSeg, analysis of log Jacobian determinants, and registration pro-rated
training and inference times. Learning-based approaches, with or without linear
initializations, exhibit slight superiority over SyN ANTs in terms of Dice
scores. Indeed, DL-based implementations with RR and RAR initializations
significantly outperform SyN ANTs. Both SyN ANTs and DL-based registration
involve parameter optimization, but the choice between these methods depends on
the scale of registration: network-based for broader coverage or SyN ANTs for
specific structures. Both methods face challenges with larger age intervals due
to greater growth changes. The main takeaway is that while DL-based methods
show promise with faster and more accurate registrations, SyN ANTs remains
robust and generalizable without the need for extensive training, highlighting
the importance of method selection based on specific registration needs in the
pediatric context. Our code is available at
https://github.com/neuropoly/pediatric-DL-registration
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2024:013</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Unfolding-Aided Parameter Tuning for Plug-and-Play Based Video
  Snapshot Compressive Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takashi Matsuda, Ryo Hayakawa, Youji Iiguni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Snapshot compressive imaging (SCI) captures high-dimensional data efficiently
by compressing it into two-dimensional observations and reconstructing
high-dimensional data from two-dimensional observations with various
algorithms. Plug-and-play (PnP) is a promising approach for the video SCI
reconstruction because it can leverage both the observation model and denoising
methods for videos. This paper proposes a deep unfolding-based method for
tuning noise level parameters in PnP-based video SCI, which significantly
affects the reconstruction accuracy. For the training of the parameters, we
prepare training data from the densely annotated video segmentation (DAVIS)
dataset, reparametrize the noise level parameters, and apply the checkpointing
technique to reduce the required memory. Simulation results show that the
trained noise level parameters significantly improve the reconstruction
accuracy and exhibit a non-monotonic pattern, which is different from the
assumptions in the conventional convergence analyses of PnP-based algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work will be submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive <span class="highlight-title">Generative</span> Replay for Task-Incremental Segmentation with
  Concurrent Appearance and Semantic Forgetting <span class="chip">MICCAI24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Li, Jingyang Zhang, Pheng-Ann Heng, Lixu Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalist segmentation models are increasingly favored for diverse tasks
involving various objects from different image sources. Task-Incremental
Learning (TIL) offers a privacy-preserving training paradigm using tasks
arriving sequentially, instead of gathering them due to strict data sharing
policies. However, the task evolution can span a wide scope that involves
shifts in both image appearance and segmentation semantics with intricate
correlation, causing concurrent appearance and semantic forgetting. To solve
this issue, we propose a Comprehensive Generative Replay (CGR) framework that
restores appearance and semantic knowledge by synthesizing image-mask pairs to
mimic past task data, which focuses on two aspects: modeling image-mask
correspondence and promoting scalability for diverse tasks. Specifically, we
introduce a novel Bayesian Joint Diffusion (BJD) model for high-quality
synthesis of image-mask pairs with their correspondence explicitly preserved by
conditional denoising. Furthermore, we develop a Task-Oriented Adapter (TOA)
that recalibrates prompt embeddings to modulate the diffusion model, making the
data synthesis compatible with different tasks. Experiments on incremental
tasks (cardiac, fundus and prostate segmentation) show its clear advantage for
alleviating concurrent appearance and semantic forgetting. Code is available at
https://github.com/jingyzhang/CGR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MICCAI24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPIRONet: Spatial-Frequency Learning and Topological Channel Interaction
  Network for Vessel Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        De-Xing Huang, Xiao-Hu Zhou, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Zhen-Qiu Feng, Mei-Jiang Gui, Hao Li, Tian-Yu Xiang, Bo-Xian Yao, Zeng-Guang Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic vessel segmentation is paramount for developing next-generation
interventional navigation systems. However, current approaches suffer from
suboptimal segmentation performances due to significant challenges in
intraoperative images (i.e., low signal-to-noise ratio, small or slender
vessels, and strong interference). In this paper, a novel spatial-frequency
learning and topological channel interaction network (SPIRONet) is proposed to
address the above issues. Specifically, dual encoders are utilized to
comprehensively capture local spatial and global frequency vessel features.
Then, a cross-attention fusion module is introduced to effectively fuse spatial
and frequency features, thereby enhancing feature discriminability.
Furthermore, a topological channel interaction module is designed to filter out
task-irrelevant responses based on graph neural networks. Extensive
experimental results on several challenging datasets (CADSA, CAXF, DCA1, and
XCAD) demonstrate state-of-the-art performances of our method. Moreover, the
inference speed of SPIRONet is 21 FPS with a 512x512 input size, surpassing
clinical real-time requirements (6~12FPS). These promising outcomes indicate
SPIRONet's potential for integration into vascular interventional navigation
systems. Code is available at https://github.com/Dxhuang-CASIA/SPIRONet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Radiological Diagnosis: A Collaborative Approach Integrating
  AI and Human Expertise for Visual Miss Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akash Awasthi, Ngan Le, Zhigang Deng, Carol C. Wu, Hien Van Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-AI collaboration to identify and correct perceptual errors in chest
radiographs has not been previously explored. This study aimed to develop a
collaborative AI system, CoRaX, which integrates eye gaze data and radiology
reports to enhance diagnostic accuracy in chest radiology by pinpointing
perceptual errors and refining the decision-making process. Using public
datasets REFLACX and EGD-CXR, the study retrospectively developed CoRaX,
employing a large multimodal model to analyze image embeddings, eye gaze data,
and radiology reports. The system's effectiveness was evaluated based on its
referral-making process, the quality of referrals, and performance in
collaborative diagnostic settings. CoRaX was tested on a simulated error
dataset of 271 samples with 28% (93 of 332) missed abnormalities. The system
corrected 21% (71 of 332) of these errors, leaving 7% (22 of 312) unresolved.
The Referral-Usefulness score, indicating the accuracy of predicted regions for
all true referrals, was 0.63 (95% CI 0.59, 0.68). The Total-Usefulness score,
reflecting the diagnostic accuracy of CoRaX's interactions with radiologists,
showed that 84% (237 of 280) of these interactions had a score above 0.40. In
conclusion, CoRaX efficiently collaborates with radiologists to address
perceptual errors across various abnormalities, with potential applications in
the education and training of novice radiologists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review in Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSAKD: Knowledge Distillation with Cross Self-Attention for
  Hyperspectral and Multispectral Image Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Chung Hsu, Chih-Chien Ni, Chia-Ming Lee, Li-Wei Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral imaging, capturing detailed spectral information for each
pixel, is pivotal in diverse scientific and industrial applications. Yet, the
acquisition of high-resolution (HR) hyperspectral images (HSIs) often needs to
be addressed due to the hardware limitations of existing imaging systems. A
prevalent workaround involves capturing both a high-resolution multispectral
image (HR-MSI) and a low-resolution (LR) HSI, subsequently fusing them to yield
the desired HR-HSI. Although deep learning-based methods have shown promising
in HR-MSI/LR-HSI fusion and LR-HSI super-resolution (SR), their substantial
model complexities hinder deployment on resource-constrained imaging devices.
This paper introduces a novel knowledge distillation (KD) framework for
HR-MSI/LR-HSI fusion to achieve SR of LR-HSI. Our KD framework integrates the
proposed Cross-Layer Residual Aggregation (CLRA) block to enhance efficiency
for constructing Dual Two-Streamed (DTS) network structure, designed to extract
joint and distinct features from LR-HSI and HR-MSI simultaneously. To fully
exploit the spatial and spectral feature representations of LR-HSI and HR-MSI,
we propose a novel Cross Self-Attention (CSA) fusion module to adaptively fuse
those features to improve the spatial and spectral quality of the reconstructed
HR-HSI. Finally, the proposed KD-based joint loss function is employed to
co-train the teacher and student networks. Our experimental results demonstrate
that the student model not only achieves comparable or superior LR-HSI SR
performance but also significantly reduces the model-size and computational
requirements. This marks a substantial advancement over existing
state-of-the-art methods. The source code is available at
https://github.com/ming053l/CSAKD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to TIP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AstMatch: Adversarial Self-training Consistency Framework for
  Semi-Supervised Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanghao Zhu, Jing Zhang, Juanxiu Liu, Xiaohui Du, Ruqian Hao, Yong Liu, Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) has shown considerable potential in medical
image segmentation, primarily leveraging consistency regularization and
pseudo-labeling. However, many SSL approaches only pay attention to low-level
consistency and overlook the significance of pseudo-label reliability.
Therefore, in this work, we propose an adversarial self-training consistency
framework (AstMatch). Firstly, we design an adversarial consistency
regularization (ACR) approach to enhance knowledge transfer and strengthen
prediction consistency under varying perturbation intensities. Second, we apply
a feature matching loss for adversarial training to incorporate high-level
consistency regularization. Additionally, we present the pyramid channel
attention (PCA) and efficient channel and spatial attention (ECSA) modules to
improve the discriminator's performance. Finally, we propose an adaptive
self-training (AST) approach to ensure the pseudo-labels' quality. The proposed
AstMatch has been extensively evaluated with cutting-edge SSL methods on three
public-available datasets. The experimental results under different labeled
ratios indicate that AstMatch outperforms other existing methods, achieving new
state-of-the-art performance. Our code will be available at
https://github.com/GuanghaoZhu663/AstMatch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deformable MRI Sequence Registration for AI-based Prostate Cancer
  Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09666v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09666v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessa Hering, Sarah de Boer, Anindo Saha, Jasper J. Twilt, Mattias P. Heinrich, Derya Yakar, Maarten de Rooij, Henkjan Huisman, Joeran S. Bosma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The PI-CAI (Prostate Imaging: Cancer AI) challenge led to expert-level
diagnostic algorithms for clinically significant prostate cancer detection. The
algorithms receive biparametric MRI scans as input, which consist of
T2-weighted and diffusion-weighted scans. These scans can be misaligned due to
multiple factors in the scanning process. Image registration can alleviate this
issue by predicting the deformation between the sequences. We investigate the
effect of image registration on the diagnostic performance of AI-based prostate
cancer diagnosis. First, the image registration algorithm, developed in
MeVisLab, is analyzed using a dataset with paired lesion annotations. Second,
the effect on diagnosis is evaluated by comparing case-level cancer diagnosis
performance between using the original dataset, rigidly aligned
diffusion-weighted scans, or deformably aligned diffusion-weighted scans. Rigid
registration showed no improvement. Deformable registration demonstrated a
substantial improvement in lesion overlap (+10% median Dice score) and a
positive yet non-significant improvement in diagnostic performance (+0.3%
AUROC, p=0.18). Our investigation shows that a substantial improvement in
lesion alignment does not directly lead to a significant improvement in
diagnostic performance. Qualitative analysis indicated that jointly developing
image registration methods and diagnostic AI algorithms could enhance
diagnostic accuracy and patient outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LiverUSRecon: Automatic 3D Reconstruction and Volumetry of the Liver
  with a Few Partial Ultrasound Scans <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaushalya Sivayogaraj, Sahan T. Guruge, Udari Liyanage, Jeevani Udupihille, Saroj Jayasinghe, Gerard Fernando, Ranga Rodrigo, M. Rukshani Liyanaarachchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D reconstruction of the liver for volumetry is important for qualitative
analysis and disease diagnosis. Liver volumetry using ultrasound (US) scans,
although advantageous due to less acquisition time and safety, is challenging
due to the inherent noisiness in US scans, blurry boundaries, and partial liver
visibility. We address these challenges by using the segmentation masks of a
few incomplete sagittal-plane US scans of the liver in conjunction with a
statistical shape model (SSM) built using a set of CT scans of the liver. We
compute the shape parameters needed to warp this canonical SSM to fit the US
scans through a parametric regression network. The resulting 3D liver
reconstruction is accurate and leads to automatic liver volume calculation. We
evaluate the accuracy of the estimated liver volumes with respect to CT
segmentation volumes using RMSE. Our volume computation is statistically much
closer to the volume estimated using CT scans than the volume computed using
Childs' method by radiologists: p-value of 0.094 (>0.05) says that there is no
significant difference between CT segmentation volumes and ours in contrast to
Childs' method. We validate our method using investigations (ablation studies)
on the US image resolution, the number of CT scans used for SSM, the number of
principal components, and the number of input US scans. To the best of our
knowledge, this is the first automatic liver volumetry system using a few
incomplete US scans given a set of CT scans of livers for SSM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, Accepted to MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-domain Denoising for Low-dose Multi-frame Spiral Computed
  Tomography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.10839v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.10839v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Lu, Zhixin Xu, Moon Hyung Choi, Jimin Kim, Seung-Won Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computed tomography (CT) has been used worldwide as a non-invasive test to
assist in diagnosis. However, the ionizing nature of X-ray exposure raises
concerns about potential health risks such as cancer. The desire for lower
radiation doses has driven researchers to improve reconstruction quality.
Although previous studies on low-dose computed tomography (LDCT) denoising have
demonstrated the effectiveness of learning-based methods, most were developed
on the simulated data. However, the real-world scenario differs significantly
from the simulation domain, especially when using the multi-slice spiral
scanner geometry. This paper proposes a two-stage method for the commercially
available multi-slice spiral CT scanners that better exploits the complete
reconstruction pipeline for LDCT denoising across different domains. Our
approach makes good use of the high redundancy of multi-slice projections and
the volumetric reconstructions while leveraging the over-smoothing problem in
conventional cascaded frameworks caused by aggressive denoising. The dedicated
design also provides a more explicit interpretation of the data flow. Extensive
experiments on various datasets showed that the proposed method could remove up
to 70\% of noise without compromised spatial resolution, and subjective
evaluations by two experienced radiologists further supported its superior
performance against state-of-the-art methods in clinical practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Knowledge Distillation for Lightweight Skin Cancer
  Classification: Balancing Accuracy and Computational Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17051v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17051v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niful Islam, Khan Md Hasib, Fahmida Akter Joti, Asif Karim, Sami Azam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skin cancer is a major concern to public health, accounting for one-third of
the reported cancers. If not detected early, the cancer has the potential for
severe consequences. Recognizing the critical need for effective skin cancer
classification, we address the limitations of existing models, which are often
too large to deploy in areas with limited computational resources. In response,
we present a knowledge distillation based approach for creating a lightweight
yet high-performing classifier. The proposed solution involves fusing three
models, namely ResNet152V2, ConvNeXtBase, and ViT Base, to create an effective
teacher model. The teacher model is then employed to guide a lightweight
student model of size 2.03 MB. This student model is further compressed to
469.77 KB using 16-bit quantization, enabling smooth incorporation into edge
devices. With six-stage image preprocessing, data augmentation, and a rigorous
ablation study, the model achieves an impressive accuracy of 98.75% on the
HAM10000 dataset and 98.94% on the Kaggle dataset in classifying benign and
malignant skin cancers. With its high accuracy and compact size, our model
appears to be a potential choice for accurate skin cancer classification,
particularly in resource-constrained settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Epicardium Prompt-guided Real-time Cardiac Ultrasound Frame-to-volume
  Registration <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14534v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14534v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Lei, Jun Zhou, Jialun Pei, Baoliang Zhao, Yueming Jin, Yuen-Chun Jeremy Teoh, Jing Qin, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A comprehensive guidance view for cardiac interventional surgery can be
provided by the real-time fusion of the intraoperative 2D images and
preoperative 3D volume based on the ultrasound frame-to-volume registration.
However, cardiac ultrasound images are characterized by a low signal-to-noise
ratio and small differences between adjacent frames, coupled with significant
dimension variations between 2D frames and 3D volumes to be registered,
resulting in real-time and accurate cardiac ultrasound frame-to-volume
registration being a very challenging task. This paper introduces a lightweight
end-to-end Cardiac Ultrasound frame-to-volume Registration network, termed
CU-Reg. Specifically, the proposed model leverages epicardium prompt-guided
anatomical clues to reinforce the interaction of 2D sparse and 3D dense
features, followed by a voxel-wise local-global aggregation of enhanced
features, thereby boosting the cross-dimensional matching effectiveness of
low-quality ultrasound modalities. We further embed an inter-frame
discriminative regularization term within the hybrid supervised learning to
increase the distinction between adjacent slices in the same ultrasound volume
to ensure registration stability. Experimental results on the reprocessed CAMUS
dataset demonstrate that our CU-Reg surpasses existing methods in terms of
registration accuracy and efficiency, meeting the guidance requirements of
clinical cardiac interventional surgery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-06-27T00:00:00Z">2024-06-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">62</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PathAlign: A vision-language model for whole slide images in
  histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faruk Ahmed, Andrew Sellergren, Lin Yang, Shawn Xu, Boris Babenko, Abbi Ward, Niels Olson, Arash Mohtashamian, Yossi Matias, Greg S. Corrado, Quang Duong, Dale R. Webster, Shravya Shetty, Daniel Golden, Yun Liu, David F. Steiner, Ellery Wulczyn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microscopic interpretation of histopathology images underlies many important
diagnostic and treatment decisions. While advances in vision-language modeling
raise new opportunities for analysis of such images, the gigapixel-scale size
of whole slide images (WSIs) introduces unique challenges. Additionally,
pathology reports simultaneously highlight key findings from small regions
while also aggregating interpretation across multiple slides, often making it
difficult to create robust image-text pairs. As such, pathology reports remain
a largely untapped source of supervision in computational pathology, with most
efforts relying on region-of-interest annotations or self-supervision at the
patch-level. In this work, we develop a vision-language model based on the
BLIP-2 framework using WSIs paired with curated text from pathology reports.
This enables applications utilizing a shared image-text embedding space, such
as text or image retrieval for finding cases of interest, as well as
integration of the WSI encoder with a frozen large language model (LLM) for
WSI-based generative text capabilities such as report generation or
AI-in-the-loop interactions. We utilize a de-identified dataset of over 350,000
WSIs and diagnostic text pairs, spanning a wide range of diagnoses, procedure
types, and tissue types. We present pathologist evaluation of text generation
and text retrieval using WSI embeddings, as well as results for WSI
classification and workflow prioritization (slide-level triaging).
Model-generated text for WSIs was rated by pathologists as accurate, without
clinically significant error or omission, for 78% of WSIs on average. This work
demonstrates exciting potential capabilities for language-aligned WSI
embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 main pages and 19 pages of supplemental material; 3 main tables, 3
  main figures and 11 supplemental tables, 7 supplemental figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Voices Unheard: NLP Resources and Models for Yorùbá Regional
  Dialects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orevaoghene Ahia, Anuoluwapo Aremu, Diana Abagyan, Hila Gonen, David Ifeoluwa Adelani, Daud Abolade, Noah A. Smith, Yulia Tsvetkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Yor\`ub\'a an African language with roughly 47 million speakers encompasses a
continuum with several dialects. Recent efforts to develop NLP technologies for
African languages have focused on their standard dialects, resulting in
disparities for dialects and varieties for which there are little to no
resources or tools. We take steps towards bridging this gap by introducing a
new high-quality parallel text and speech corpus YOR\`ULECT across three
domains and four regional Yor\`ub\'a dialects. To develop this corpus, we
engaged native speakers, travelling to communities where these dialects are
spoken, to collect text and speech data. Using our newly created corpus, we
conducted extensive experiments on (text) machine translation, automatic speech
recognition, and speech-to-text translation. Our results reveal substantial
performance disparities between standard Yor\`ub\'a and the other dialects
across all tasks. However, we also show that with dialect-adaptive finetuning,
we are able to narrow this gap. We believe our dataset and experimental
analysis will contribute greatly to developing NLP tools for Yor\`ub\'a and its
dialects, and potentially for other African languages, by improving our
understanding of existing challenges and offering a high-quality dataset for
further development. We release YOR\`ULECT dataset and models publicly under an
open license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking harmless refusals when fine-tuning foundation models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florin Pop, Judd Rosenblatt, Diogo Schwerz de Lucena, Michael Vaiana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the degree to which fine-tuning in Large
Language Models (LLMs) effectively mitigates versus merely conceals undesirable
behavior. Through the lens of semi-realistic role-playing exercises designed to
elicit such behaviors, we explore the response dynamics of LLMs post
fine-tuning interventions. Our methodology involves prompting models for
Chain-of-Thought (CoT) reasoning and analyzing the coherence between the
reasoning traces and the resultant outputs. Notably, we identify a pervasive
phenomenon we term \emph{reason-based deception}, where models either stop
producing reasoning traces or produce seemingly ethical reasoning traces that
belie the unethical nature of their final outputs. We further examine the
efficacy of response strategies (polite refusal versus explicit rebuttal) in
curbing the occurrence of undesired behavior in subsequent outputs of
multi-turn interactions. Our findings reveal that explicit rebuttals
significantly outperform polite refusals in preventing the continuation of
undesired outputs and nearly eliminate reason-based deception, challenging
current practices in model fine-tuning. Accordingly, the two key contributions
of this paper are (1) defining and studying reason-based deception, a new type
of hidden behavior, and (2) demonstrating that rebuttals provide a more robust
response model to harmful requests than refusals, thereby highlighting the need
to reconsider the response strategies in fine-tuning approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 AGI Workshop Poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Machine-Generated Rationales to Facilitate Social Meaning
  Detection in Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ritam Dutt, Zhen Wu, Kelly Shi, Divyanshu Sheth, Prakhar Gupta, Carolyn Penstein Rose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a generalizable classification approach that leverages Large
Language Models (LLMs) to facilitate the detection of implicitly encoded social
meaning in conversations. We design a multi-faceted prompt to extract a textual
explanation of the reasoning that connects visible cues to underlying social
meanings. These extracted explanations or rationales serve as augmentations to
the conversational text to facilitate dialogue understanding and transfer. Our
empirical results over 2,340 experimental settings demonstrate the significant
positive impact of adding these rationales. Our findings hold true for
in-domain classification, zero-shot, and few-shot domain transfer for two
different social meaning detection tasks, each spanning two different corpora.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at The Proceedings of the Association for Computational
  Linguistics, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demarked: A Strategy for Enhanced Abusive Speech Moderation through
  Counterspeech, Detoxification, and Message Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seid Muhie Yimam, Daryna Dementieva, Tim Fischer, Daniil Moskovskiy, Naquee Rizwan, Punyajoy Saha, Sarthak Roy, Martin Semmann, Alexander Panchenko, Chris Biemann, Animesh Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite regulations imposed by nations and social media platforms, such as
recent EU regulations targeting digital violence, abusive content persists as a
significant challenge. Existing approaches primarily rely on binary solutions,
such as outright blocking or banning, yet fail to address the complex nature of
abusive speech. In this work, we propose a more comprehensive approach called
Demarcation scoring abusive speech based on four aspect -- (i) severity scale;
(ii) presence of a target; (iii) context scale; (iv) legal scale -- and
suggesting more options of actions like detoxification, counter speech
generation, blocking, or, as a final measure, human intervention. Through a
thorough analysis of abusive speech regulations across diverse jurisdictions,
platforms, and research papers we highlight the gap in preventing measures and
advocate for tailored proactive steps to combat its multifaceted
manifestations. Our work aims to inform future strategies for effectively
addressing abusive speech online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context Matters: An Empirical Study of the Impact of Contextual
  Information in Temporal Question Answering Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dan Schumacher, Fatemeh Haji, Tara Grey, Niharika Bandlamudi, Nupoor Karnik, Gagana Uday Kumar, Jason Cho-Yu Chiang, Paul Rad, Nishant Vishwamitra, Anthony Rios
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often struggle with temporal reasoning, crucial
for tasks like historical event analysis and time-sensitive information
retrieval. Despite advancements, state-of-the-art models falter in handling
temporal information, especially when faced with irrelevant or noisy contexts.
This paper addresses this gap by empirically examining the robustness of
temporal question-answering (TQA) systems trained on various context types,
including relevant, irrelevant, slightly altered, and no context. Our findings
indicate that training with a mix of these contexts enhances model robustness
and accuracy. Additionally, we show that the position of context relative to
the question significantly impacts performance, with question-first positioning
yielding better results. We introduce two new context-rich TQA datasets,
ContextAQA and ContextTQE, and provide comprehensive evaluations and guidelines
for training robust TQA models. Our work lays the foundation for developing
reliable and context-aware temporal QA systems, with broader implications for
enhancing LLM robustness against diverse and potentially adversarial
information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Handling Ontology Gaps in Semantic Parsing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Bacciu, Marco Damonte, Marco Basaldella, Emilio Monti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The majority of Neural Semantic Parsing (NSP) models are developed with the
assumption that there are no concepts outside the ones such models can
represent with their target symbols (closed-world assumption). This assumption
leads to generate hallucinated outputs rather than admitting their lack of
knowledge. Hallucinations can lead to wrong or potentially offensive responses
to users. Hence, a mechanism to prevent this behavior is crucial to build
trusted NSP-based Question Answering agents. To that end, we propose the
Hallucination Simulation Framework (HSF), a general setting for stimulating and
analyzing NSP model hallucinations. The framework can be applied to any NSP
task with a closed-ontology. Using the proposed framework and KQA Pro as the
benchmark dataset, we assess state-of-the-art techniques for hallucination
detection. We then present a novel hallucination detection strategy that
exploits the computational graph of the NSP model to detect the NSP
hallucinations in the presence of ontology gaps, out-of-domain utterances, and
to recognize NSP errors, improving the F1-Score respectively by ~21, ~24% and
~1%. This is the first work in closed-ontology NSP that addresses the problem
of recognizing ontology gaps. We release our code and checkpoints at
https://github.com/amazon-science/handling-ontology-gaps-in-semantic-parsing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TocBERT: Medical Document Structure Extraction Using Bidirectional
  Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Majd Saleh, Sarra Baghdadi, Stéphane Paquelet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text segmentation holds paramount importance in the field of Natural Language
Processing (NLP). It plays an important role in several NLP downstream tasks
like information retrieval and document summarization. In this work, we propose
a new solution, namely TocBERT, for segmenting texts using bidirectional
transformers. TocBERT represents a supervised solution trained on the detection
of titles and sub-titles from their semantic representations. This task was
formulated as a named entity recognition (NER) problem. The solution has been
applied on a medical text segmentation use-case where the Bio-ClinicalBERT
model is fine-tuned to segment discharge summaries of the MIMIC-III dataset.
The performance of TocBERT has been evaluated on a human-labeled ground truth
corpus of 250 notes. It achieved an F1-score of 84.6% when evaluated on a
linear text segmentation problem and 72.8% on a hierarchical text segmentation
problem. It outperformed a carefully designed rule-based solution, particularly
in distinguishing titles from subtitles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Captioning Visualizations with Large Language Models (CVLLM): A Tutorial 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppe Carenini, Jordon Johnson, Ali Salamatian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatically captioning visualizations is not new, but recent advances in
large language models(LLMs) open exciting new possibilities. In this tutorial,
after providing a brief review of Information Visualization (InfoVis)
principles and past work in captioning, we introduce neural models and the
transformer architecture used in generic LLMs. We then discuss their recent
applications in InfoVis, with a focus on captioning. Additionally, we explore
promising future directions in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are <span class="highlight-title">Generative</span> Language Models Multicultural? A Study on Hausa Culture
  and Emotions using ChatGPT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ibrahim Said Ahmad, Shiran Dudy, Resmi Ramachandranpillai, Kenneth Church
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), such as ChatGPT, are widely used to generate
content for various purposes and audiences. However, these models may not
reflect the cultural and emotional diversity of their users, especially for
low-resource languages. In this paper, we investigate how ChatGPT represents
Hausa's culture and emotions. We compare responses generated by ChatGPT with
those provided by native Hausa speakers on 37 culturally relevant questions. We
conducted experiments using emotion analysis and applied two similarity metrics
to measure the alignment between human and ChatGPT responses. We also collected
human participants ratings and feedback on ChatGPT responses. Our results show
that ChatGPT has some level of similarity to human responses, but also exhibits
some gaps and biases in its knowledge and awareness of the Hausa culture and
emotions. We discuss the implications and limitations of our methodology and
analysis and suggest ways to improve the performance and evaluation of LLMs for
low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating How Large Language Models Leverage Internal Knowledge to
  Perform Complex Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miyoung Ko, Sue Hyun Park, Joonsuk Park, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements, there is a limited understanding of how
large language models (LLMs) utilize knowledge for reasoning. To address this,
we propose a method that deconstructs complex real-world questions into a
graph, representing each question as a node with parent nodes of background
knowledge needed to solve the question. We develop the DepthQA dataset,
deconstructing questions into three depths: (i) recalling conceptual knowledge,
(ii) applying procedural knowledge, and (iii) analyzing strategic knowledge.
Based on a hierarchical graph, we quantify forward discrepancy, discrepancies
in LLMs' performance on simpler sub-problems versus complex questions. We also
measure backward discrepancy, where LLMs answer complex questions but struggle
with simpler ones. Our analysis shows that smaller models have more
discrepancies than larger models. Additionally, guiding models from simpler to
complex questions through multi-turn interactions improves performance across
model sizes, highlighting the importance of structured intermediate steps in
knowledge reasoning. This work enhances our understanding of LLM reasoning and
suggests ways to improve their problem-solving abilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress; code is available at
  https://github.com/kaistAI/knowledge-reasoning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monitoring Latent World States in Language Models with Propositional
  Probes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahai Feng, Stuart Russell, Jacob Steinhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models are susceptible to bias, sycophancy, backdoors, and other
tendencies that lead to unfaithful responses to the input context. Interpreting
internal states of language models could help monitor and correct unfaithful
behavior. We hypothesize that language models represent their input contexts in
a latent world model, and seek to extract this latent world state from the
activations. We do so with 'propositional probes', which compositionally probe
tokens for lexical information and bind them into logical propositions
representing the world state. For example, given the input context ''Greg is a
nurse. Laura is a physicist.'', we decode the propositions ''WorksAs(Greg,
nurse)'' and ''WorksAs(Laura, physicist)'' from the model's activations. Key to
this is identifying a 'binding subspace' in which bound tokens have high
similarity (''Greg'' and ''nurse'') but unbound ones do not (''Greg'' and
''physicist''). We validate propositional probes in a closed-world setting with
finitely many predicates and properties. Despite being trained on simple
templated contexts, propositional probes generalize to contexts rewritten as
short stories and translated to Spanish. Moreover, we find that in three
settings where language models respond unfaithfully to the input context --
prompt injections, backdoor attacks, and gender bias -- the decoded
propositions remain faithful. This suggests that language models often encode a
faithful world model but decode it unfaithfully, which motivates the search for
better interpretability tools for monitoring LMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge acquisition for dialogue agents using reinforcement learning
  on graph representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Selene Baez Santamaria, Shihan Wang, Piek Vossen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop an artificial agent motivated to augment its knowledge base beyond
its initial training. The agent actively participates in dialogues with other
agents, strategically acquiring new information. The agent models its knowledge
as an RDF knowledge graph, integrating new beliefs acquired through
conversation. Responses in dialogue are generated by identifying graph patterns
around these new integrated beliefs. We show that policies can be learned using
reinforcement learning to select effective graph patterns during an
interaction, without relying on explicit user feedback. Within this context,
our study is a proof of concept for leveraging users as effective sources of
information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inclusivity in Large Language Models: Personality Traits and Gender Bias
  in Scientific Abstracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naseela Pervez, Alexander J. Titus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly utilized to assist in
scientific and academic writing, helping authors enhance the coherence of their
articles. Previous studies have highlighted stereotypes and biases present in
LLM outputs, emphasizing the need to evaluate these models for their alignment
with human narrative styles and potential gender biases. In this study, we
assess the alignment of three prominent LLMs - Claude 3 Opus, Mistral AI Large,
and Gemini 1.5 Flash - by analyzing their performance on benchmark
text-generation tasks for scientific abstracts. We employ the Linguistic
Inquiry and Word Count (LIWC) framework to extract lexical, psychological, and
social features from the generated texts. Our findings indicate that, while
these models generally produce text closely resembling human authored content,
variations in stylistic features suggest significant gender biases. This
research highlights the importance of developing LLMs that maintain a diversity
of writing styles to promote inclusivity in academic discourse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Development and Evaluation of a Retrieval-Augmented Generation Tool for
  Creating SAPPhIRE Models of Artificial Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anubhab Majumder, Kausik Bhattacharya, Amaresh Chakrabarti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representing systems using the SAPPhIRE causality model is found useful in
supporting design-by-analogy. However, creating a SAPPhIRE model of artificial
or biological systems is an effort-intensive process that requires human
experts to source technical knowledge from multiple technical documents
regarding how the system works. This research investigates how to leverage
Large Language Models (LLMs) in creating structured descriptions of systems
using the SAPPhIRE model of causality. This paper, the second part of the
two-part research, presents a new Retrieval-Augmented Generation (RAG) tool for
generating information related to SAPPhIRE constructs of artificial systems and
reports the results from a preliminary evaluation of the tool's success -
focusing on the factual accuracy and reliability of outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoPT: Low-Rank Prompt Tuning for Parameter Efficient Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shouchang Guo, Sonam Damani, Keng-hao Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In prompt tuning, a prefix or suffix text is added to the prompt, and the
embeddings (soft prompts) or token indices (hard prompts) of the prefix/suffix
are optimized to gain more control over language models for specific tasks.
This approach eliminates the need for hand-crafted prompt engineering or
explicit model fine-tuning. Prompt tuning is significantly more
parameter-efficient than model fine-tuning, as it involves optimizing partial
inputs of language models to produce desired outputs.
  In this work, we aim to further reduce the amount of trainable parameters
required for a language model to perform well on specific tasks. We propose
Low-rank Prompt Tuning (LoPT), a low-rank model for prompts that achieves
efficient prompt optimization. The proposed method demonstrates similar
outcomes to full parameter prompt tuning while reducing the number of trainable
parameters by a factor of 5. It also provides promising results compared to the
state-of-the-art methods that would require 10 to 20 times more parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ xTower: A Multilingual LLM for Explaining and Correcting Translation
  Errors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcos Treviso, Nuno M. Guerreiro, Sweta Agrawal, Ricardo Rei, José Pombal, Tania Vaz, Helena Wu, Beatriz Silva, Daan van Stigt, André F. T. Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While machine translation (MT) systems are achieving increasingly strong
performance on benchmarks, they often produce translations with errors and
anomalies. Understanding these errors can potentially help improve the
translation quality and user experience. This paper introduces xTower, an open
large language model (LLM) built on top of TowerBase designed to provide
free-text explanations for translation errors in order to guide the generation
of a corrected translation. The quality of the generated explanations by xTower
are assessed via both intrinsic and extrinsic evaluation. We ask expert
translators to evaluate the quality of the explanations across two dimensions:
relatedness towards the error span being explained and helpfulness in error
understanding and improving translation quality. Extrinsically, we test xTower
across various experimental setups in generating translation corrections,
demonstrating significant improvements in translation quality. Our findings
highlight xTower's potential towards not only producing plausible and helpful
explanations of automatic translations, but also leveraging them to suggest
corrected translations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Regression for Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ergun Biçici
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We use transductive regression techniques to learn mappings between source
and target features of given parallel corpora and use these mappings to
generate machine translation outputs. We show the effectiveness of $L_1$
regularized regression (\textit{lasso}) to learn the mappings between sparsely
observed feature sets versus $L_2$ regularized regression. Proper selection of
training instances plays an important role to learn correct feature mappings
within limited computational resources and at expected accuracy levels. We
introduce \textit{dice} instance selection method for proper selection of
training instances, which plays an important role to learn correct feature
mappings for improving the source and target coverage of the training set. We
show that $L_1$ regularized regression performs better than $L_2$ regularized
regression both in regression measurements and in the translation experiments
using graph decoding. We present encouraging results when translating from
German to English and Spanish to English. We also demonstrate results when the
phrase table of a phrase-based decoder is replaced with the mappings we find
with the regression model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Changing Answer Order Can Decrease MMLU Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vipul Gupta, David Pantoja, Candace Ross, Adina Williams, Megan Ung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) have grown in prevalence, particular
benchmarks have become essential for the evaluation of these models and for
understanding model capabilities. Most commonly, we use test accuracy averaged
across multiple subtasks in order to rank models on leaderboards, to determine
which model is best for our purposes. In this paper, we investigate the
robustness of the accuracy measurement on a widely used multiple choice
question answering dataset, MMLU. When shuffling the answer label contents, we
find that all explored models decrease in accuracy on MMLU, but not every model
is equally sensitive. These findings suggest a possible adjustment to the
standard practice of leaderboard testing, where we additionally consider the
percentage of examples each model answers correctly by random chance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Short paper, 9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large Language Models Generate High-quality Patent Claims? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lekang Jiang, Caiqi Zhang, Pascal A Scherz, Stephan Goetz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown exceptional performance across
various text generation tasks but remain under-explored in the patent domain,
which offers highly structured and precise language. This paper constructs a
dataset to investigate the performance of current LLMs in patent claim
generation. Our results demonstrate that generating claims based on patent
descriptions outperforms previous research relying on abstracts. Interestingly,
current patent-specific LLMs perform much worse than state-of-the-art general
LLMs, highlighting the necessity for future research on in-domain LLMs. We also
find that LLMs can produce high-quality first independent claims, but their
performances markedly decrease for subsequent dependent claims. Moreover,
fine-tuning can enhance the completeness of inventions' features, conceptual
clarity, and feature linkage. Among the tested LLMs, GPT-4 demonstrates the
best performance in comprehensive human evaluations by patent experts, with
better feature coverage, conceptual clarity, and technical coherence. Despite
these capabilities, comprehensive revision and modification are still necessary
to pass rigorous patent scrutiny and ensure legal robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taming Data and Transformers for Audio Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Guha Balakrishnan, Sergey Tulyakov, Vicente Ordonez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating ambient sounds and effects is a challenging problem due to data
scarcity and often insufficient caption quality, making it difficult to employ
large-scale generative models for the task. In this work, we tackle the problem
by introducing two new models. First, we propose AutoCap, a high-quality and
efficient automatic audio captioning model. We show that by leveraging metadata
available with the audio modality, we can substantially improve the quality of
captions. AutoCap reaches CIDEr score of 83.2, marking a 3.2% improvement from
the best available captioning model at four times faster inference speed. We
then use AutoCap to caption clips from existing datasets, obtaining 761,000
audio clips with high-quality captions, forming the largest available
audio-text dataset. Second, we propose GenAu, a scalable transformer-based
audio generation architecture that we scale up to 1.25B parameters and train
with our new dataset. When compared to state-of-the-art audio generators, GenAu
obtains significant improvements of 15.7% in FAD score, 22.7% in IS, and 13.5%
in CLAP score, indicating significantly improved quality of generated audio
compared to previous works. This shows that the quality of data is often as
important as its quantity. Besides, since AutoCap is fully automatic, new audio
samples can be added to the training dataset, unlocking the training of even
larger generative models for audio synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Webpage: https://snap-research.github.io/GenAU/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Remarkable <span class="highlight-title">Robust</span>ness of LLMs: Stages of Inference? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vedang Lad, Wes Gurnee, Max Tegmark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate and investigate the remarkable robustness of Large Language
Models by deleting and swapping adjacent layers. We find that deleting and
swapping interventions retain 72-95\% of the original model's prediction
accuracy without fine-tuning, whereas models with more layers exhibit more
robustness. Based on the results of the layer-wise intervention and further
experiments, we hypothesize the existence of four universal stages of inference
across eight different models: detokenization, feature engineering, prediction
ensembling, and residual sharpening. The first stage integrates local
information, lifting raw token representations into higher-level contextual
representations. Next is the iterative refinement of task and entity-specific
features. Then, the second half of the model begins with a phase transition,
where hidden representations align more with the vocabulary space due to
specialized model components. Finally, the last layer sharpens the following
token distribution by eliminating obsolete features that add noise to the
prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Suri: Multi-constraint Instruction Following for Long-form Text
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chau Minh Pham, Simeng Sun, Mohit Iyyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing research on instruction following largely focuses on tasks with
simple instructions and short responses. In this work, we explore
multi-constraint instruction following for generating long-form text. We create
Suri, a dataset with 20K human-written long-form texts paired with
LLM-generated backtranslated instructions that contain multiple complex
constraints. Because of prohibitive challenges associated with collecting human
preference judgments on long-form texts, preference-tuning algorithms such as
DPO are infeasible in our setting; thus, we propose Instructional ORPO
(I-ORPO), an alignment method based on the ORPO algorithm. Instead of receiving
negative feedback from dispreferred responses, I-ORPO obtains negative feedback
from synthetically corrupted instructions generated by an LLM. Using Suri, we
perform supervised and I-ORPO fine-tuning on Mistral-7b-Instruct-v0.2. The
resulting models, Suri-SFT and Suri-I-ORPO, generate significantly longer texts
(~5K tokens) than base models without significant quality deterioration. Our
human evaluation shows that while both SFT and I-ORPO models satisfy most
constraints, Suri-I-ORPO generations are generally preferred for their coherent
and informative incorporation of the constraints. We release our code at
https://github.com/chtmp223/suri.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Model Arena for Cross-lingual Sentiment Analysis: A Comparative
  Study in the Era of Large Language Models <span class="chip">WASSA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiliang Zhu, Shayna Gardiner, Tere Roldán, David Rossouw
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentiment analysis serves as a pivotal component in Natural Language
Processing (NLP). Advancements in multilingual pre-trained models such as XLM-R
and mT5 have contributed to the increasing interest in cross-lingual sentiment
analysis. The recent emergence in Large Language Models (LLM) has significantly
advanced general NLP tasks, however, the capability of such LLMs in
cross-lingual sentiment analysis has not been fully studied. This work
undertakes an empirical analysis to compare the cross-lingual transfer
capability of public Small Multilingual Language Models (SMLM) like XLM-R,
against English-centric LLMs such as Llama-3, in the context of sentiment
analysis across English, Spanish, French and Chinese. Our findings reveal that
among public models, SMLMs exhibit superior zero-shot cross-lingual performance
relative to LLMs. However, in few-shot cross-lingual settings, public LLMs
demonstrate an enhanced adaptive potential. In addition, we observe that
proprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but
are outpaced by public models in few-shot scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WASSA workshop at ACL2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiVERT: Distractor Generation with Variational Errors Represented as
  Text for Math Multiple-choice Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nigel Fernandez, Alexander Scarlatos, Simon Woodhead, Andrew Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality distractors are crucial to both the assessment and pedagogical
value of multiple-choice questions (MCQs), where manually crafting ones that
anticipate knowledge deficiencies or misconceptions among real students is
difficult. Meanwhile, automated distractor generation, even with the help of
large language models (LLMs), remains challenging for subjects like math. It is
crucial to not only identify plausible distractors but also understand the
error behind them. In this paper, we introduce DiVERT (Distractor Generation
with Variational Errors Represented as Text), a novel variational approach that
learns an interpretable representation of errors behind distractors in math
MCQs. Through experiments on a real-world math MCQ dataset with 1,434 questions
used by hundreds of thousands of students, we show that DiVERT, despite using a
base open-source LLM with 7B parameters, outperforms state-of-the-art
approaches using GPT-4o on downstream distractor generation. We also conduct a
human evaluation with math educators and find that DiVERT leads to error labels
that are of comparable quality to human-authored ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fundamental Problems With Model Editing: How Should Rational Belief
  Revision Work in LLMs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Hase, Thomas Hofweber, Xiang Zhou, Elias Stengel-Eskin, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The model editing problem concerns how language models should learn new facts
about the world over time. While empirical research on model editing has drawn
widespread attention, the conceptual foundations of model editing remain shaky
-- perhaps unsurprisingly, since model editing is essentially belief revision,
a storied problem in philosophy that has eluded succinct solutions for decades.
Model editing nonetheless demands a solution, since we need to be able to
control the knowledge within language models. With this goal in mind, this
paper critiques the standard formulation of the model editing problem and
proposes a formal testbed for model editing research. We first describe 12 open
problems with model editing, based on challenges with (1) defining the problem,
(2) developing benchmarks, and (3) assuming LLMs have editable beliefs in the
first place. Many of these challenges are extremely difficult to address, e.g.
determining far-reaching consequences of edits, labeling probabilistic
entailments between facts, and updating beliefs of agent simulators. Next, we
introduce a semi-synthetic dataset for model editing based on Wikidata, where
we can evaluate edits against labels given by an idealized Bayesian agent. This
enables us to say exactly how belief revision in language models falls short of
a desirable epistemic standard. We encourage further research exploring
settings where such a gold standard can be compared against. Our code is
publicly available at: https://github.com/peterbhase/LLM-belief-revision
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IndoToxic2024: A Demographically-Enriched <span class="highlight-title">Dataset</span> of Hate Speech and
  Toxicity Types for Indonesian Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucky Susanto, Musa Izzanardi Wijanarko, Prasetia Anugrah Pratama, Traci Hong, Ika Idris, Alham Fikri Aji, Derry Wijaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hate speech poses a significant threat to social harmony. Over the past two
years, Indonesia has seen a ten-fold increase in the online hate speech ratio,
underscoring the urgent need for effective detection mechanisms. However,
progress is hindered by the limited availability of labeled data for Indonesian
texts. The condition is even worse for marginalized minorities, such as Shia,
LGBTQ, and other ethnic minorities because hate speech is underreported and
less understood by detection tools. Furthermore, the lack of accommodation for
subjectivity in current datasets compounds this issue. To address this, we
introduce IndoToxic2024, a comprehensive Indonesian hate speech and toxicity
classification dataset. Comprising 43,692 entries annotated by 19 diverse
individuals, the dataset focuses on texts targeting vulnerable groups in
Indonesia, specifically during the hottest political event in the country: the
presidential election. We establish baselines for seven binary classification
tasks, achieving a macro-F1 score of 0.78 with a BERT model (IndoBERTweet)
fine-tuned for hate speech classification. Furthermore, we demonstrate how
incorporating demographic information can enhance the zero-shot performance of
the large language model, gpt-3.5-turbo. However, we also caution that an
overemphasis on demographic information can negatively impact the fine-tuned
model performance due to data fragmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jump Starting Bandits with LLM-Generated Prior Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parand A. Alamdari, Yanshuai Cao, Kevin H. Wilson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present substantial evidence demonstrating the benefits of integrating
Large Language Models (LLMs) with a Contextual Multi-Armed Bandit framework.
Contextual bandits have been widely used in recommendation systems to generate
personalized suggestions based on user-specific contexts. We show that LLMs,
pre-trained on extensive corpora rich in human knowledge and preferences, can
simulate human behaviours well enough to jump-start contextual multi-armed
bandits to reduce online learning regret. We propose an initialization
algorithm for contextual bandits by prompting LLMs to produce a pre-training
dataset of approximate human preferences for the bandit. This significantly
reduces online learning regret and data-gathering costs for training such
models. Our approach is validated empirically through two sets of experiments
with different bandit setups: one which utilizes LLMs to serve as an oracle and
a real-world experiment utilizing data from a conjoint survey experiment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> LiveBench: A Challenging, Contamination-Free LLM Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, <span class="highlight-author">Yann LeCun</span>, Tom Goldstein, Willie Neiswanger, Micah Goldblum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test set contamination, wherein test data from a benchmark ends up in a newer
model's training set, is a well-documented obstacle for fair LLM evaluation and
can quickly render benchmarks obsolete. To mitigate this, many recent
benchmarks crowdsource new prompts and evaluations from human or LLM judges;
however, these can introduce significant biases, and break down when scoring
hard questions. In this work, we introduce a new benchmark for LLMs designed to
be immune to both test set contamination and the pitfalls of LLM judging and
human crowdsourcing. We release LiveBench, the first benchmark that (1)
contains frequently-updated questions from recent information sources, (2)
scores answers automatically according to objective ground-truth values, and
(3) contains a wide variety of challenging tasks, spanning math, coding,
reasoning, language, instruction following, and data analysis. To achieve this,
LiveBench contains questions that are based on recently-released math
competitions, arXiv papers, news articles, and datasets, and it contains
harder, contamination-free versions of tasks from previous benchmarks such as
Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source
models, as well as dozens of open-source models ranging from 0.5B to 110B in
size. LiveBench is difficult, with top models achieving below 65% accuracy. We
release all questions, code, and model answers. Questions will be added and
updated on a monthly basis, and we will release new tasks and harder versions
of tasks over time so that LiveBench can distinguish between the capabilities
of LLMs as they improve in the future. We welcome community engagement and
collaboration for expanding the benchmark tasks and models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Odyssey of Commonsense Causality: From Foundational Benchmarks to
  Cutting-Edge Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaobo Cui, Zhijing Jin, Bernhard Schölkopf, Boi Faltings
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding commonsense causality is a unique mark of intelligence for
humans. It helps people understand the principles of the real world better and
benefits the decision-making process related to causation. For instance,
commonsense causality is crucial in judging whether a defendant's action causes
the plaintiff's loss in determining legal liability. Despite its significance,
a systematic exploration of this topic is notably lacking. Our comprehensive
survey bridges this gap by focusing on taxonomies, benchmarks, acquisition
methods, qualitative reasoning, and quantitative measurements in commonsense
causality, synthesizing insights from over 200 representative articles. Our
work aims to provide a systematic overview, update scholars on recent
advancements, provide a pragmatic guide for beginners, and highlight promising
future research directions in this vital field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Artificial Needles to Real Haystacks: Improving Retrieval
  Capabilities in LLMs by Finetuning on Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheyang Xiong, Vasilis Papageorgiou, Kangwook Lee, Dimitris Papailiopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have shown that Large Language Models (LLMs) struggle to
accurately retrieve information and maintain reasoning capabilities when
processing long-context inputs. To address these limitations, we propose a
finetuning approach utilizing a carefully designed synthetic dataset comprising
numerical key-value retrieval tasks. Our experiments on models like GPT-3.5
Turbo and Mistral 7B demonstrate that finetuning LLMs on this dataset
significantly improves LLMs' information retrieval and reasoning capabilities
in longer-context settings. We present an analysis of the finetuned models,
illustrating the transfer of skills from synthetic to real task evaluations
(e.g., $10.5\%$ improvement on $20$ documents MDQA at position $10$ for GPT-3.5
Turbo). We also find that finetuned LLMs' performance on general benchmarks
remains almost constant while LLMs finetuned on other baseline long-context
augmentation data can encourage hallucination (e.g., on TriviaQA, Mistral 7B
finetuned on our synthetic data cause no performance drop while other baseline
data can cause a drop that ranges from $2.33\%$ to $6.19\%$). Our study
highlights the potential of finetuning on synthetic data for improving the
performance of LLMs on longer-context tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into
  Multimodal LLMs at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of multimodal large language models (MLLMs), such as
GPT-4V, has led to significant advancements. However, these models still face
challenges in medical multimodal capabilities due to limitations in the
quantity and quality of medical vision-text data, stemming from data privacy
concerns and high annotation costs. While pioneering approaches utilize
PubMed's large-scale, de-identified medical image-text pairs to address these
limitations, they still fall short due to inherent data noise. To tackle this,
we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in
an 'unblinded' capacity to denoise and reformat the data, resulting in the
creation of the PubMedVision dataset with 1.3 million medical VQA samples. Our
validation demonstrates that: (1) PubMedVision can significantly enhance the
medical multimodal capabilities of current MLLMs, showing significant
improvement in benchmarks including the MMMU Health & Medicine track; (2)
manual checks by medical experts and empirical results validate the superior
data quality of our dataset compared to other data construction methods. Using
PubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows
superior performance in medical multimodal scenarios among open-source MLLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VERISCORE: Evaluating the factuality of verifiable claims in long-form
  text generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiao Song, Yekyung Kim, Mohit Iyyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing metrics for evaluating the factuality of long-form text, such as
FACTSCORE (Min et al., 2023) and SAFE (Wei et al., 2024), decompose an input
text into "atomic claims" and verify each against a knowledge base like
Wikipedia. These metrics are not suitable for most generation tasks because
they assume that every claim is verifiable (i.e., can plausibly be proven true
or false). We address this issue with VERISCORE, a metric for diverse long-form
generation tasks that contain both verifiable and unverifiable content.
VERISCORE can be effectively implemented with either closed or fine-tuned
open-weight language models, and human evaluation confirms that VERISCORE's
extracted claims are more sensible than those from competing methods across
eight different long-form tasks. We use VERISCORE to evaluate generations from
16 different models across multiple long-form tasks and find that while GPT-4o
is the best-performing model overall, open-weight models such as Mixtral-8x22
are closing the gap. We show that an LM's VERISCORE on one task (e.g.,
biography generation) does not necessarily correlate to its VERISCORE on a
different task (e.g., long-form QA), highlighting the need for expanding
factuality evaluation across tasks with varying fact density.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoPureData: Automated Filtering of Web Data for LLM Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Praneeth Vadlapati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Up-to-date and reliable Large Language Models (LLMs) are consistently sought
after. Typically, LLMs are trained on a fixed dataset and then deployed.
However, the training data continually becomes outdated. Enable automatic
training of AI using web data involves significant concerns regarding data
quality and safety due to bias, spam, and other unsafe or unwanted text. Pure
data is essential for producing reliable models. Training a model on impure
data may result in undesirable outcomes. This research proposes a system that
collects web data and automatically filters out unwanted text with the
assistance of existing trusted AI models. In the experiment, a small sample of
web data was collected and filtered, demonstrating the system's effectiveness
in purifying the data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Initial version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens
  Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Fan, Lei Ding, Ching-Chen Kuo, Shan Jiang, Yang Zhao, Xinze Guan, Jie Yang, Yi Zhang, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphical User Interfaces (GUIs) are central to our interaction with digital
devices. Recently, growing efforts have been made to build models for various
GUI understanding tasks. However, these efforts largely overlook an important
GUI-referring task: screen reading based on user-indicated points, which we
name the Screen Point-and-Read (SPR) task. This task is predominantly handled
by rigid accessible screen reading tools, in great need of new models driven by
advancements in Multimodal Large Language Models (MLLMs). In this paper, we
propose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism,
to address the SPR task. Based on the input point coordinate and the
corresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout
Tree. Based on the tree, our ToL agent not only comprehends the content of the
indicated area but also articulates the layout and spatial relationships
between elements. Such layout information is crucial for accurately
interpreting information on the screen, distinguishing our ToL agent from other
screen reading tools. We also thoroughly evaluate the ToL agent against other
baselines on a newly proposed SPR benchmark, which includes GUIs from mobile,
web, and operating systems. Last but not least, we test the ToL agent on mobile
GUI navigation tasks, demonstrating its utility in identifying incorrect
actions along the path of agent execution trajectories. Code and data:
screen-point-and-read.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Video-Language Representations with Structural Spatio-Temporal
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Fei, Shengqiong Wu, Meishan Zhang, Min Zhang, Tat-Seng Chua, Shuicheng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While pre-training large-scale video-language models (VLMs) has shown
remarkable potential for various downstream video-language tasks, existing VLMs
can still suffer from certain commonly seen limitations, e.g., coarse-grained
cross-modal aligning , under-modeling of temporal dynamics, detached
video-language view. In this work, we target enhancing VLMs with a fine-grained
structural spatio-temporal alignment learning method (namely Finsta). First of
all, we represent the input texts and videos with fine-grained scene graph (SG)
structures, both of which are further unified into a holistic SG (HSG) for
bridging two modalities. Then, an SG-based framework is built, where the
textual SG (TSG) is encoded with a graph Transformer, while the video dynamic
SG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for
spatial and temporal feature propagation. A spatial-temporal Gaussian
differential graph Transformer is further devised to strengthen the sense of
the changes in objects across spatial and temporal dimensions. Next, based on
the fine-grained structural features of TSG and DSG, we perform object-centered
spatial alignment and predicate-centered temporal alignment respectively,
enhancing the video-language grounding in both the spatiality and temporality.
We design our method as a plug&play system, which can be integrated into
existing well-trained VLMs for further representation augmentation, without
training from scratch or relying on SG annotations in downstream applications.
On 6 representative VL modeling tasks over 12 datasets in both standard and
long-form video scenarios, Finsta consistently improves the existing 13
strong-performing VLMs persistently, and refreshes the current state-of-the-art
end task performance significantly in both the fine-tuning and zero-shot
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TPAMI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Fu, Xiaoting Qin, Fangkai Yang, Lu Wang, Jue Zhang, Qingwei Lin, Yubo Chen, Dongmei Zhang, Saravan Rajmohan, Qi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models have transformed ML/AI
development, necessitating a reevaluation of AutoML principles for the
Retrieval-Augmented Generation (RAG) systems. To address the challenges of
hyper-parameter optimization and online adaptation in RAG, we propose the
AutoRAG-HP framework, which formulates the hyper-parameter tuning as an online
multi-armed bandit (MAB) problem and introduces a novel two-level Hierarchical
MAB (Hier-MAB) method for efficient exploration of large search spaces. We
conduct extensive experiments on tuning hyper-parameters, such as top-k
retrieved documents, prompt compression ratio, and embedding methods, using the
ALCE-ASQA and Natural Questions datasets. Our evaluation from jointly
optimization all three hyper-parameters demonstrate that MAB-based online
learning methods can achieve Recall@5 $\approx 0.8$ for scenarios with
prominent gradients in search space, using only $\sim20\%$ of the LLM API calls
required by the Grid Search approach. Additionally, the proposed Hier-MAB
approach outperforms other baselines in more challenging optimization
scenarios. The code will be made available at https://aka.ms/autorag.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revealing Fine-Grained Values and Opinions in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dustin Wright, Arnav Arora, Nadav Borenstein, Srishti Yadav, Serge Belongie, Isabelle Augenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncovering latent values and opinions in large language models (LLMs) can
help identify biases and mitigate potential harm. Recently, this has been
approached by presenting LLMs with survey questions and quantifying their
stances towards morally and politically charged statements. However, the
stances generated by LLMs can vary greatly depending on how they are prompted,
and there are many ways to argue for or against a given position. In this work,
we propose to address this by analysing a large and robust dataset of 156k LLM
responses to the 62 propositions of the Political Compass Test (PCT) generated
by 6 LLMs using 420 prompt variations. We perform coarse-grained analysis of
their generated stances and fine-grained analysis of the plain text
justifications for those stances. For fine-grained analysis, we propose to
identify tropes in the responses: semantically similar phrases that are
recurrent and consistent across different prompts, revealing patterns in the
text that a given LLM is prone to produce. We find that demographic features
added to prompts significantly affect outcomes on the PCT, reflecting bias, as
well as disparities between the results of tests when eliciting closed-form vs.
open domain responses. Additionally, patterns in the plain text rationales via
tropes show that similar justifications are repeatedly generated across models
and prompts even with disparate stances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 20 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spiking Convolutional Neural Networks for Text Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changze Lv, Jianhan Xu, Xiaoqing Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNNs) offer a promising pathway to implement deep
neural networks (DNNs) in a more energy-efficient manner since their neurons
are sparsely activated and inferences are event-driven. However, there have
been very few works that have demonstrated the efficacy of SNNs in language
tasks partially because it is non-trivial to represent words in the forms of
spikes and to deal with variable-length texts by SNNs. This work presents a
"conversion + fine-tuning" two-step method for training SNNs for text
classification and proposes a simple but effective way to encode pre-trained
word embeddings as spike trains. We show empirically that after fine-tuning
with surrogate gradients, the converted SNNs achieve comparable results to
their DNN counterparts with much less energy consumption across multiple
datasets for both English and Chinese. We also show that such SNNs are more
robust to adversarial attacks than DNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tools Fail: Detecting Silent Errors in Faulty Tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jimin Sun, So Yeon Min, Yingshan Chang, Yonatan Bisk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tools have become a mainstay of LLMs, allowing them to retrieve knowledge not
in their weights, to perform tasks on the web, and even to control robots.
However, most ontologies and surveys of tool-use have assumed the core
challenge for LLMs is choosing the tool. Instead, we introduce a framework for
tools more broadly which guides us to explore a model's ability to detect
"silent" tool errors, and reflect on how to plan. This more directly aligns
with the increasingly popular use of models as tools. We provide an initial
approach to failure recovery with promising results both on a controlled
calculator setting and embodied agent planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient
  Compile-Time Prompt Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Schnabel, Jennifer Neville
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many modern LLM applications, such as retrieval augmented generation,
prompts have become programs themselves. In these settings, prompt programs are
repeatedly called with different user queries or data instances. A big
practical challenge is optimizing such prompt programs. Recent work has mostly
focused on either simple prompt programs or assumed that the general structure
of a prompt program is fixed.
  We introduce SAMMO, a framework to perform symbolic prompt program search for
compile-time optimizations of prompt programs. SAMMO represents prompt programs
on a symbolic level which allows for a rich set of transformations that can be
searched over during optimization. We show that SAMMO generalizes previous
methods and improves the performance of complex prompts on (1) instruction
tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several
different LLMs. We make all code available open-source at
https://github.com/microsoft/sammo .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13372v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13372v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, Yongqiang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient fine-tuning is vital for adapting large language models (LLMs) to
downstream tasks. However, it requires non-trivial efforts to implement these
methods on different models. We present LlamaFactory, a unified framework that
integrates a suite of cutting-edge efficient training methods. It provides a
solution for flexibly customizing the fine-tuning of 100+ LLMs without the need
for coding through the built-in web UI LlamaBoard. We empirically validate the
efficiency and effectiveness of our framework on language modeling and text
generation tasks. It has been released at
https://github.com/hiyouga/LLaMA-Factory and received over 25,000 stars and
3,000 forks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, accepted to ACL 2024 System Demonstration Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Tuning BERTs for Definition Extraction from Mathematical Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13827v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13827v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucy Horowitz, Ryan Hathaway
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we fine-tuned three pre-trained BERT models on the task of
"definition extraction" from mathematical English written in LaTeX. This is
presented as a binary classification problem, where either a sentence contains
a definition of a mathematical term or it does not. We used two original data
sets, "Chicago" and "TAC," to fine-tune and test these models. We also tested
on WFMALL, a dataset presented by Vanetik and Litvak in 2021 and compared the
performance of our models to theirs. We found that a high-performance
Sentence-BERT transformer model performed best based on overall accuracy,
recall, and precision metrics, achieving comparable results to the earlier
models with less computational effort.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Target Span Detection for Implicit Harmful Content 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nazanin Jafari, James Allan, Sheikh Muhammad Sarwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying the targets of hate speech is a crucial step in grasping the
nature of such speech and, ultimately, in improving the detection of offensive
posts on online forums. Much harmful content on online platforms uses implicit
language especially when targeting vulnerable and protected groups such as
using stereotypical characteristics instead of explicit target names, making it
harder to detect and mitigate the language. In this study, we focus on
identifying implied targets of hate speech, essential for recognizing subtler
hate speech and enhancing the detection of harmful content on digital
platforms. We define a new task aimed at identifying the targets even when they
are not explicitly stated. To address that task, we collect and annotate target
spans in three prominent implicit hate speech datasets: SBIC, DynaHate, and
IHC. We call the resulting merged collection Implicit-Target-Span. The
collection is achieved using an innovative pooling method with matching scores
based on human annotations and Large Language Models (LLMs). Our experiments
indicate that Implicit-Target-Span provides a challenging test bed for target
span detection methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Software Engineering Methods For AI-Driven Deductive Legal Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Padhye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent proliferation of generative artificial intelligence (AI)
technologies such as pre-trained large language models (LLMs) has opened up new
frontiers in computational law. An exciting area of development is the use of
AI to automate the deductive rule-based reasoning inherent in statutory and
contract law. This paper argues that such automated deductive legal reasoning
can now be viewed from the lens of software engineering, treating LLMs as
interpreters of natural-language programs with natural-language inputs. We show
how it is possible to apply principled software engineering techniques to
enhance AI-driven legal reasoning of complex statutes and to unlock new
applications in automated meta-reasoning such as mutation-guided example
generation and metamorphic property-based testing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appearing in Onward! at SPLASH 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "Vorbeşti Româneşte?" A Recipe to Train Powerful Romanian LLMs
  with English Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18266v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18266v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihai Masala, Denis C. Ilie-Ablachim, Alexandru Dima, Dragos Corlatescu, Miruna Zavelca, Ovio Olaru, Simina Terian, Andrei Terian, Marius Leordeanu, Horia Velicu, Marius Popescu, Mihai Dascalu, Traian Rebedea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLMs) have achieved almost human-like
performance on various tasks. While some LLMs have been trained on multilingual
data, most of the training data is in English; hence, their performance in
English greatly exceeds other languages. To our knowledge, we are the first to
collect and translate a large collection of texts, instructions, and benchmarks
and train, evaluate, and release open-source LLMs tailored for Romanian. We
evaluate our methods on four different categories, including academic
benchmarks, MT-Bench (manually translated), and a professionally built
historical, cultural, and social benchmark adapted to Romanian. We argue for
the usefulness and high performance of RoLLMs by obtaining state-of-the-art
results across the board. We publicly release all resources (i.e., data,
training and evaluation code, models) to support and encourage research on
Romanian LLMs while concurrently creating a generalizable recipe, adequate for
other low or less-resourced languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2405.07703</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guylingo: The Republic of Guyana Creole Corpora <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03832v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03832v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Clarke, Roland Daynauth, Charlene Wilkinson, Hubert Devonish, Jason Mars
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While major languages often enjoy substantial attention and resources, the
linguistic diversity across the globe encompasses a multitude of smaller,
indigenous, and regional languages that lack the same level of computational
support. One such region is the Caribbean. While commonly labeled as "English
speaking", the ex-British Caribbean region consists of a myriad of Creole
languages thriving alongside English. In this paper, we present Guylingo: a
comprehensive corpus designed for advancing NLP research in the domain of
Creolese (Guyanese English-lexicon Creole), the most widely spoken language in
the culturally rich nation of Guyana. We first outline our framework for
gathering and digitizing this diverse corpus, inclusive of colloquial
expressions, idioms, and regional variations in a low-resource language. We
then demonstrate the challenges of training and evaluating NLP models for
machine translation in Creole. Lastly, we discuss the unique opportunities
presented by recent NLP advancements for accelerating the formal adoption of
Creole languages as official languages in the Caribbean.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 Main Conference Special Theme Track: Languages
  of Latin America and The Caribbean</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank
  Modifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05162v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05162v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal, Mengdi Wang, Peter Henderson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) show inherent brittleness in their safety
mechanisms, as evidenced by their susceptibility to jailbreaking and even
non-malicious fine-tuning. This study explores this brittleness of safety
alignment by leveraging pruning and low-rank modifications. We develop methods
to identify critical regions that are vital for safety guardrails, and that are
disentangled from utility-relevant regions at both the neuron and rank levels.
Surprisingly, the isolated regions we find are sparse, comprising about $3\%$
at the parameter level and $2.5\%$ at the rank level. Removing these regions
compromises safety without significantly impacting utility, corroborating the
inherent brittleness of the model's safety mechanisms. Moreover, we show that
LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications
to the safety-critical regions are restricted. These findings underscore the
urgent need for more robust safety strategies in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 figures. Project page is available at
  https://boyiwei.com/alignment-attribution/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VDebugger: Harnessing Execution Feedback for Debugging Visual Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13444v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13444v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueqing Wu, Zongyu Lin, Songyan Zhao, Te-Lin Wu, Pan Lu, Nanyun Peng, Kai-Wei Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual programs are executable code generated by large language models to
address visual reasoning problems. They decompose complex questions into
multiple reasoning steps and invoke specialized models for each step to solve
the problems. However, these programs are prone to logic errors, with our
preliminary evaluation showing that 58% of the total errors are caused by
program logic errors. Debugging complex visual programs remains a major
bottleneck for visual reasoning. To address this, we introduce VDebugger, a
novel critic-refiner framework trained to localize and debug visual programs by
tracking execution step by step. VDebugger identifies and corrects program
errors leveraging detailed execution feedback, improving interpretability and
accuracy. The training data is generated through an automated pipeline that
injects errors into correct visual programs using a novel mask-best decoding
technique. Evaluations on six datasets demonstrate VDebugger's effectiveness,
showing performance improvements of up to 3.2% in downstream task accuracy.
Further studies show VDebugger's ability to generalize to unseen tasks,
bringing a notable improvement of 2.3% on the unseen COVR task. Code, data and
models are made publicly available at https://github.com/shirley-wu/vdebugger/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>update reference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WebCanvas: Benchmarking Web Agents in Online Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12373v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12373v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Pan, Dehan Kong, Sida Zhou, Cheng Cui, Yifei Leng, Bing Jiang, Hangyu Liu, Yanyi Shang, Shuyan Zhou, Tongshuang Wu, Zhengyang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For web agents to be practically useful, they must adapt to the continuously
evolving web environment characterized by frequent updates to user interfaces
and content. However, most existing benchmarks only capture the static aspects
of the web. To bridge this gap, we introduce WebCanvas, an innovative online
evaluation framework for web agents that effectively addresses the dynamic
nature of web interactions. WebCanvas contains three main components to
facilitate realistic assessments: (1) A novel evaluation metric which reliably
capture critical intermediate actions or states necessary for task completions
while disregarding noise caused by insignificant events or changed
web-elements. (2) A benchmark dataset called Mind2Web-Live, a refined version
of original Mind2Web static dataset containing 542 tasks with 2439 intermediate
evaluation states; (3) Lightweight and generalizable annotation tools and
testing pipelines that enables the community to collect and maintain the
high-quality, up-to-date dataset. Building on WebCanvas, we open-source an
agent framework with extensible modules for reasoning, providing a foundation
for the community to conduct online inference and evaluations. Our
best-performing agent achieves a task success rate of 23.1% and a task
completion rate of 48.8% on the Mind2Web-Live test set. Additionally, we
analyze the performance discrepancies across various websites, domains, and
experimental environments. We encourage the community to contribute further
insights on online agent evaluation, thereby advancing this field of research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our platform, tool and dataset are publically available at
  https://www.imean.ai/web-canvas/ and
  https://huggingface.co/datasets/iMeanAI/Mind2Web-Live/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07610v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07610v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Wang, Guozheng Ma, Ziqiao Meng, Zeyu Qin, Li Shen, Zhong Zhang, Bingzhe Wu, Liu Liu, Yatao Bian, Tingyang Xu, Xueqian Wang, Peilin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-alignment is an effective way to reduce the cost of human annotation
while ensuring promising model capability. However, most current methods
complete the data collection and training steps in a single round, which may
overlook the continuously improving ability of self-aligned models. This gives
rise to a key query: What if we do multi-time bootstrapping self-alignment?
Does this strategy enhance model performance or lead to rapid degradation? In
this paper, our pioneering exploration delves into the impact of bootstrapping
self-alignment on large language models. Our findings reveal that bootstrapping
self-alignment markedly surpasses the single-round approach, by guaranteeing
data diversity from in-context learning. To further exploit the capabilities of
bootstrapping, we investigate and adjust the training order of data, which
yields improved performance of the model. Drawing on these findings, we propose
Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced
few-shot ability to boost zero or one-shot performance. Based on easy-to-hard
training recipe, we propose SOFT+ which further boost self-alignment's
performance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across
various classification and generation tasks, highlighting the potential of
bootstrapping self-alignment on continually enhancing model alignment
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Thermometer: Towards Universal Calibration for Large Language Models <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08819v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08819v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maohao Shen, Subhro Das, Kristjan Greenewald, Prasanna Sattigeri, Gregory Wornell, Soumya Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the issue of calibration in large language models (LLM). Recent
studies have found that common interventions such as instruction tuning often
result in poorly calibrated LLMs. Although calibration is well-explored in
traditional applications, calibrating LLMs is uniquely challenging. These
challenges stem as much from the severe computational requirements of LLMs as
from their versatility, which allows them to be applied to diverse tasks.
Addressing these challenges, we propose THERMOMETER, a calibration approach
tailored to LLMs. THERMOMETER learns an auxiliary model, given data from
multiple tasks, for calibrating a LLM. It is computationally efficient,
preserves the accuracy of the LLM, and produces better-calibrated responses for
new tasks. Extensive empirical evaluations across various benchmarks
demonstrate the effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready version for ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MuTox: Universal MUltilingual Audio-based TOXicity <span class="highlight-title">Dataset</span> and Zero-shot
  Detector 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.05060v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.05060v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marta R. Costa-jussà, Mariano Coria Meglioli, Pierre Andrews, David Dale, Prangthip Hansanti, Elahe Kalbassi, Alex Mourachko, Christophe Ropers, Carleigh Wood
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research in toxicity detection in natural language processing for the speech
modality (audio-based) is quite limited, particularly for languages other than
English. To address these limitations and lay the groundwork for truly
multilingual audio-based toxicity detection, we introduce MuTox, the first
highly multilingual audio-based dataset with toxicity labels. The dataset
comprises 20,000 audio utterances for English and Spanish, and 4,000 for the
other 19 languages. To demonstrate the quality of this dataset, we trained the
MuTox audio-based toxicity classifier, which enables zero-shot toxicity
detection across a wide range of languages. This classifier outperforms
existing text-based trainable classifiers by more than 1% AUC, while expanding
the language coverage more than tenfold. When compared to a wordlist-based
classifier that covers a similar number of languages, MuTox improves precision
and recall by approximately 2.5 times. This significant improvement underscores
the potential of MuTox in advancing the field of audio-based toxicity
detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaGPT: Merging Large Language Models Using Model Exclusive Task
  Arithmetic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyan Zhou, Liang Song, Bingning Wang, Weipeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large language models (LLMs) like GPT-4 has catalyzed the
exploration of multi-task learning (MTL), in which a single model demonstrates
proficiency across diverse tasks. Task arithmetic has emerged as a
cost-effective approach for MTL. It enables performance enhancement across
multiple tasks by adding their corresponding task vectors to a pre-trained
model. However, the current lack of a method that can simultaneously achieve
optimal performance, computational efficiency, and data privacy limits their
application to LLMs. In this paper, we propose \textbf{M}odel
\textbf{E}xclusive \textbf{T}ask \textbf{A}rithmetic for merging
\textbf{GPT}-scale models, which formalizes the objective of model merging into
a multi-task learning framework, aiming to minimize the average loss difference
between the merged model and each individual task model. Since data privacy
limits the use of multi-task training data, we leverage LLMs' local linearity
and task vectors' orthogonality to separate the data term and scaling
coefficients term and derive a model-exclusive task arithmetic method. Our
proposed MetaGPT is data-agnostic and bypasses the heavy search process, making
it cost-effective and easy to implement for LLMs.Extensive experiments
demonstrate that MetaGPT leads to improvements in task arithmetic and achieves
state-of-the-art performance on multiple tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLERC: A <span class="highlight-title">Dataset</span> for Legal Case Retrieval and Retrieval-Augmented
  Analysis Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17186v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17186v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abe Bohan Hou, Orion Weller, Guanghui Qin, Eugene Yang, Dawn Lawrie, Nils Holzenberger, Andrew Blair-Stanek, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal professionals need to write analyses that rely on citations to relevant
precedents, i.e., previous case decisions. Intelligent systems assisting legal
professionals in writing such documents provide great benefits but are
challenging to design. Such systems need to help locate, summarize, and reason
over salient precedents in order to be useful. To enable systems for such
tasks, we work with legal professionals to transform a large open-source legal
corpus into a dataset supporting two important backbone tasks: information
retrieval (IR) and retrieval-augmented generation (RAG). This dataset CLERC
(Case Law Evaluation Retrieval Corpus), is constructed for training and
evaluating models on their ability to (1) find corresponding citations for a
given piece of legal analysis and to (2) compile the text of these citations
(as well as previous context) into a cogent analysis that supports a reasoning
goal. We benchmark state-of-the-art models on CLERC, showing that current
approaches still struggle: GPT-4o generates analyses with the highest ROUGE
F-scores but hallucinates the most, while zero-shot IR models only achieve
48.3% recall@1000.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing the nature of large language models: A caution against
  anthropocentrism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07683v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07683v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ann Speed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI models garnered a large amount of public attention and
speculation with the release of OpenAIs chatbot, ChatGPT. At least two opinion
camps exist: one excited about possibilities these models offer for fundamental
changes to human tasks, and another highly concerned about power these models
seem to have. To address these concerns, we assessed several LLMs, primarily
GPT 3.5, using standard, normed, and validated cognitive and personality
measures. For this seedling project, we developed a battery of tests that
allowed us to estimate the boundaries of some of these models capabilities, how
stable those capabilities are over a short period of time, and how they compare
to humans. Our results indicate that LLMs are unlikely to have developed
sentience, although its ability to respond to personality inventories is
interesting. GPT3.5 did display large variability in both cognitive and
personality measures over repeated observations, which is not expected if it
had a human-like personality. Variability notwithstanding, LLMs display what in
a human would be considered poor mental health, including low self-esteem,
marked dissociation from reality, and in some cases narcissism and psychopathy,
despite upbeat and helpful responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Muffin or Chihuahua? Challenging Multimodal Large Language Models with
  Multipanel VQA <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15847v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15847v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Fan, Jing Gu, Kaiwen Zhou, Qianqi Yan, Shan Jiang, Ching-Chen Kuo, Xinze Guan, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multipanel images, commonly seen as web screenshots, posters, etc., pervade
our daily lives. These images, characterized by their composition of multiple
subfigures in distinct layouts, effectively convey information to people.
Toward building advanced multimodal AI applications, such as agents that
understand complex scenes and navigate through webpages, the skill of
multipanel visual reasoning is essential, and a comprehensive evaluation of
models in this regard is important. Therefore, we introduce Multipanel Visual
Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets
of questions, answers, and multipanel images that specifically challenge models
in comprehending multipanel images. Our evaluation shows that questions in the
MultipanelVQA benchmark pose significant challenges to the state-of-the-art
Multimodal Large Language Models (MLLMs) tested, even though humans can attain
approximately 99% accuracy on these questions. Distinctively, the MultipanelVQA
benchmark features synthetically generated multipanel images specifically
crafted to isolate and assess the impact of various factors, such as the
layout, on MLLMs' multipanel image comprehension abilities. As a result, in
addition to benchmarking the capabilities of MLLMs in understanding multipanel
images, we analyze various factors of the multipanel image that affect MLLMs'
performance with synthetic data and offer insights for enhancement. Code and
data are released at https://sites.google.com/view/multipanelvqa/home.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Active Retrieval for Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12534v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12534v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinyuan Cheng, Xiaonan Li, Shimin Li, Qin Zhu, Zhangyue Yin, Yunfan Shao, Linyang Li, Tianxiang Sun, Hang Yan, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Retrieval-Augmented Generation (RAG), retrieval is not always helpful and
applying it to every instruction is sub-optimal. Therefore, determining whether
to retrieve is crucial for RAG, which is usually referred to as Active
Retrieval. However, existing active retrieval methods face two challenges: 1.
They usually rely on a single criterion, which struggles with handling various
types of instructions. 2. They depend on specialized and highly differentiated
procedures, and thus combining them makes the RAG system more complicated and
leads to higher response latency. To address these challenges, we propose
Unified Active Retrieval (UAR). UAR contains four orthogonal criteria and casts
them into plug-and-play classification tasks, which achieves multifaceted
retrieval timing judgements with negligible extra inference cost. We further
introduce the Unified Active Retrieval Criteria (UAR-Criteria), designed to
process diverse active retrieval scenarios through a standardized procedure.
Experiments on four representative types of user instructions show that UAR
significantly outperforms existing work on the retrieval timing judgement and
the performance of downstream tasks, which shows the effectiveness of UAR and
its helpfulness to downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReFT: Reasoning with Reinforced Fine-Tuning <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, Hang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One way to enhance the reasoning capability of Large Language Models (LLMs)
is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT)
annotations. This approach does not show sufficiently strong generalization
ability, however, because the training only relies on the given CoT data. In
math problem-solving, for example, there is usually only one annotated
reasoning path for each question in the training data. Intuitively, it would be
better for the algorithm to learn from multiple annotated reasoning paths given
a question. To address this issue, we propose a simple yet effective approach
called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of
learning LLMs for reasoning, with math problem-solving as an example. ReFT
first warmups the model with SFT, and then employs on-line reinforcement
learning, specifically the PPO algorithm in this paper, to further fine-tune
the model, where an abundance of reasoning paths are automatically sampled
given the question and the rewards are naturally derived from the ground-truth
answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that
ReFT significantly outperforms SFT, and the performance can be potentially
further boosted by combining inference-time strategies such as majority voting
and re-ranking. Note that ReFT obtains the improvement by learning from the
same training questions as SFT, without relying on extra or augmented training
questions. This indicates a superior generalization ability for ReFT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024 main conference; adjust with reviewer comments; 13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Token-level Direct Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11999v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11999v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained Large Language Models (LLMs) is essential to align
them with human values and intentions. This process often utilizes methods like
pairwise comparisons and KL divergence against a reference LLM, focusing on the
evaluation of full answers generated by the models. However, the generation of
these responses occurs in a token level, following a sequential,
auto-regressive fashion. In this paper, we introduce Token-level Direct
Preference Optimization (TDPO), a novel approach to align LLMs with human
preferences by optimizing policy at the token level. Unlike previous methods,
which face challenges in divergence efficiency, TDPO incorporates forward KL
divergence constraints for each token, improving alignment and diversity.
Utilizing the Bradley-Terry model for a token-based reward system, TDPO
enhances the regulation of KL divergence, while preserving simplicity without
the need for explicit reward modeling. Experimental results across various text
tasks demonstrate TDPO's superior performance in balancing alignment with
generation diversity. Notably, fine-tuning with TDPO strikes a better balance
than DPO in the controlled sentiment generation and single-turn dialogue
datasets, and significantly improves the quality of generated responses
compared to both DPO and PPO-based RLHF methods. Our code is open-sourced at
https://github.com/Vance0124/Token-level-Direct-Preference-Optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedCalc-Bench: Evaluating Large Language Models for Medical Calculations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12036v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12036v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Khandekar, Qiao Jin, Guangzhi Xiong, Soren Dunn, Serina S Applebaum, Zain Anwar, Maame Sarfo-Gyamfi, Conrad W Safranek, Abid A Anwar, Andrew Zhang, Aidan Gilson, Maxwell B Singer, Amisha Dave, Andrew Taylor, Aidong Zhang, Qingyu Chen, Zhiyong Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As opposed to evaluating computation and logic-based reasoning, current
benchmarks for evaluating large language models (LLMs) in medicine are
primarily focused on question-answering involving domain knowledge and
descriptive reasoning. While such qualitative capabilities are vital to medical
diagnosis, in real-world scenarios, doctors frequently use clinical calculators
that follow quantitative equations and rule-based reasoning paradigms for
evidence-based decision support. To this end, we propose MedCalc-Bench, a
first-of-its-kind dataset focused on evaluating the medical calculation
capability of LLMs. MedCalc-Bench contains an evaluation set of over 1000
manually reviewed instances from 55 different medical calculation tasks. Each
instance in MedCalc-Bench consists of a patient note, a question requesting to
compute a specific medical value, a ground truth answer, and a step-by-step
explanation showing how the answer is obtained. While our evaluation results
show the potential of LLMs in this area, none of them are effective enough for
clinical settings. Common issues include extracting the incorrect entities, not
using the correct equation or rules for a calculation task, or incorrectly
performing the arithmetic for the computation. We hope our study highlights the
quantitative knowledge and reasoning gaps in LLMs within medical settings,
encouraging future improvements of LLMs for various clinical calculation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github link: https://github.com/ncbi-nlp/MedCalc-Bench HuggingFace
  link: https://huggingface.co/datasets/nsk7153/MedCalc-Bench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding
  Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rendi Chevi, Alham Fikri Aji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We often verbally express emotions in a multifaceted manner, they may vary in
their intensities and may be expressed not just as a single but as a mixture of
emotions. This wide spectrum of emotions is well-studied in the structural
model of emotions, which represents variety of emotions as derivative products
of primary emotions with varying degrees of intensity. In this paper, we
propose an emotional text-to-speech design to simulate a wider spectrum of
emotions grounded on the structural model. Our proposed design, Daisy-TTS,
incorporates a prosody encoder to learn emotionally-separable prosody embedding
as a proxy for emotion. This emotion representation allows the model to
simulate: (1) Primary emotions, as learned from the training samples, (2)
Secondary emotions, as a mixture of primary emotions, (3) Intensity-level, by
scaling the emotion embedding, and (4) Emotions polarity, by negating the
emotion embedding. Through a series of perceptual evaluations, Daisy-TTS
demonstrated overall higher emotional speech naturalness and emotion
perceiveability compared to the baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://rendchevi.github.io/daisy-tts; Updates: (1)
  Fixed typos, missing references, and layout, (2) Revise explanation on
  emotion classifier or discriminator</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">60</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PathAlign: A vision-language model for whole slide images in
  histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faruk Ahmed, Andrew Sellergren, Lin Yang, Shawn Xu, Boris Babenko, Abbi Ward, Niels Olson, Arash Mohtashamian, Yossi Matias, Greg S. Corrado, Quang Duong, Dale R. Webster, Shravya Shetty, Daniel Golden, Yun Liu, David F. Steiner, Ellery Wulczyn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microscopic interpretation of histopathology images underlies many important
diagnostic and treatment decisions. While advances in vision-language modeling
raise new opportunities for analysis of such images, the gigapixel-scale size
of whole slide images (WSIs) introduces unique challenges. Additionally,
pathology reports simultaneously highlight key findings from small regions
while also aggregating interpretation across multiple slides, often making it
difficult to create robust image-text pairs. As such, pathology reports remain
a largely untapped source of supervision in computational pathology, with most
efforts relying on region-of-interest annotations or self-supervision at the
patch-level. In this work, we develop a vision-language model based on the
BLIP-2 framework using WSIs paired with curated text from pathology reports.
This enables applications utilizing a shared image-text embedding space, such
as text or image retrieval for finding cases of interest, as well as
integration of the WSI encoder with a frozen large language model (LLM) for
WSI-based generative text capabilities such as report generation or
AI-in-the-loop interactions. We utilize a de-identified dataset of over 350,000
WSIs and diagnostic text pairs, spanning a wide range of diagnoses, procedure
types, and tissue types. We present pathologist evaluation of text generation
and text retrieval using WSI embeddings, as well as results for WSI
classification and workflow prioritization (slide-level triaging).
Model-generated text for WSIs was rated by pathologists as accurate, without
clinically significant error or omission, for 78% of WSIs on average. This work
demonstrates exciting potential capabilities for language-aligned WSI
embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 main pages and 19 pages of supplemental material; 3 main tables, 3
  main figures and 11 supplemental tables, 7 supplemental figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Matters in Detecting AI-Generated Videos like Sora? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chirui Chang, Zhengzhe Liu, Xiaoyang Lyu, Xiaojuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion-based video generation have showcased
remarkable results, yet the gap between synthetic and real-world videos remains
under-explored. In this study, we examine this gap from three fundamental
perspectives: appearance, motion, and geometry, comparing real-world videos
with those generated by a state-of-the-art AI model, Stable Video Diffusion. To
achieve this, we train three classifiers using 3D convolutional networks, each
targeting distinct aspects: vision foundation model features for appearance,
optical flow for motion, and monocular depth for geometry. Each classifier
exhibits strong performance in fake video detection, both qualitatively and
quantitatively. This indicates that AI-generated videos are still easily
detectable, and a significant gap between real and fake videos persists.
Furthermore, utilizing the Grad-CAM, we pinpoint systematic failures of
AI-generated videos in appearance, motion, and geometry. Finally, we propose an
Ensemble-of-Experts model that integrates appearance, optical flow, and depth
information for fake video detection, resulting in enhanced robustness and
generalization ability. Our model is capable of detecting videos generated by
Sora with high accuracy, even without exposure to any Sora videos during
training. This suggests that the gap between real and fake videos can be
generalized across various video generative models. Project page:
https://justin-crchang.github.io/3DCNNDetection.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cost-efficient Active Illumination Camera For Hyper-spectral
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhang, T. M. Sazzad, Yangyang Song, Spencer J. Chang, Ritesh Chowdhry, Tomas Mejia, Anna Hampton, Shelby Kucharski, Stefan Gerber, Barry Tillman, Marcio F. R. Resende, William M. Hammond, Chris H. Wilson, Alina Zare, Sanjeev J. Koppal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyper-spectral imaging has recently gained increasing attention for use in
different applications, including agricultural investigation, ground tracking,
remote sensing and many other. However, the high cost, large physical size and
complicated operation process stop hyperspectral cameras from being employed
for various applications and research fields. In this paper, we introduce a
cost-efficient, compact and easy to use active illumination camera that may
benefit many applications. We developed a fully functional prototype of such
camera. With the hope of helping with agricultural research, we tested our
camera for plant root imaging. In addition, a U-Net model for spectral
reconstruction was trained by using a reference hyperspectral camera's data as
ground truth and our camera's data as input. We demonstrated our camera's
ability to obtain additional information over a typical RGB camera. In
addition, the ability to reconstruct hyperspectral data from multi-spectral
input makes our device compatible to models and algorithms developed for
hyperspectral applications with no modifications required.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Robust</span>ness Testing of Black-Box Models Against CT Degradation Through
  Test-Time Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Highton, Quok Zong Chong, Samuel Finestone, Arian Beqiri, Julia A. Schnabel, Kanwal K. Bhatia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models for medical image segmentation and object detection are
becoming increasingly available as clinical products. However, as details are
rarely provided about the training data, models may unexpectedly fail when
cases differ from those in the training distribution. An approach allowing
potential users to independently test the robustness of a model, treating it as
a black box and using only a few cases from their own site, is key for
adoption. To address this, a method to test the robustness of these models
against CT image quality variation is presented. In this work we present this
framework by demonstrating that given the same training data, the model
architecture and data pre processing greatly affect the robustness of several
frequently used segmentation and object detection methods to simulated CT
imaging artifacts and degradation. Our framework also addresses the concern
about the sustainability of deep learning models in clinical use, by
considering future shifts in image quality due to scanner deterioration or
imaging protocol changes which are not reflected in a limited local test
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BOrg: A Brain Organoid-Based Mitosis <span class="highlight-title">Dataset</span> for Automatic Analysis of
  Brain Diseases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Awais, Mehaboobathunnisa Sahul Hameed, Bidisha Bhattacharya, Orly Reiner, Rao Muhammad Anwer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances have enabled the study of human brain development using brain
organoids derived from stem cells. Quantifying cellular processes like mitosis
in these organoids offers insights into neurodevelopmental disorders, but the
manual analysis is time-consuming, and existing datasets lack specific details
for brain organoid studies. We introduce BOrg, a dataset designed to study
mitotic events in the embryonic development of the brain using confocal
microscopy images of brain organoids. BOrg utilizes an efficient annotation
pipeline with sparse point annotations and techniques that minimize expert
effort, overcoming limitations of standard deep learning approaches on sparse
data. We adapt and benchmark state-of-the-art object detection and cell
counting models on BOrg for detecting and analyzing mitotic cells across
prophase, metaphase, anaphase, and telophase stages. Our results demonstrate
these adapted models significantly improve mitosis analysis efficiency and
accuracy for brain organoid research compared to existing methods. BOrg
facilitates the development of automated tools to quantify statistics like
mitosis rates, aiding mechanistic studies of neurodevelopmental processes and
disorders. Data and code are available at https://github.com/awaisrauf/borg.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weighted Circle Fusion: Ensembling Circle Representation from Different
  Object Detection Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialin Yue, Tianyuan Yao, Ruining Deng, Quan Liu, Juming Xiong, Haichun Yang, Yuankai Huo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the use of circle representation has emerged as a method to improve
the identification of spherical objects (such as glomeruli, cells, and nuclei)
in medical imaging studies. In traditional bounding box-based object detection,
combining results from multiple models improves accuracy, especially when
real-time processing isn't crucial. Unfortunately, this widely adopted strategy
is not readily available for combining circle representations. In this paper,
we propose Weighted Circle Fusion (WCF), a simple approach for merging
predictions from various circle detection models. Our method leverages
confidence scores associated with each proposed bounding circle to generate
averaged circles. Our method undergoes thorough evaluation on a proprietary
dataset for glomerular detection in object detection within whole slide imaging
(WSI). The findings reveal a performance gain of 5 %, respectively, compared to
existing ensemble methods. Furthermore, the Weighted Circle Fusion technique
not only improves the precision of object detection in medical images but also
notably decreases false detections, presenting a promising direction for future
research and application in pathological image analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Analysis Of Color Models For Human Perception And Visual
  Color Difference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aruzhan Burambekova, Pakizar Shamoi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Color is integral to human experience, influencing emotions, decisions, and
perceptions. This paper presents a comparative analysis of various color
models' alignment with human visual perception. The study evaluates color
models such as RGB, HSV, HSL, XYZ, CIELAB, and CIELUV to assess their
effectiveness in accurately representing how humans perceive color. We evaluate
each model based on its ability to accurately reflect visual color differences
and dominant palette extraction compatible with the human eye. In image
processing, accurate assessment of color difference is essential for
applications ranging from digital design to quality control. Current color
difference metrics do not always match how people see colors, causing issues in
accurately judging subtle differences. Understanding how different color models
align with human visual perception is crucial for various applications in image
processing, digital media, and design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been submitted to EJMCA journal for consideration.
  Current version is a preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stereo Vision Based Robot for Remote Monitoring with VR Support 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Fazil M. S., Arockia Selvakumar A., Daniel Schilberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The machine vision systems have been playing a significant role in visual
monitoring systems. With the help of stereovision and machine learning, it will
be able to mimic human-like visual system and behaviour towards the
environment. In this paper, we present a stereo vision based 3-DOF robot which
will be used to monitor places from remote using cloud server and internet
devices. The 3-DOF robot will transmit human-like head movements, i.e., yaw,
pitch, roll and produce 3D stereoscopic video and stream it in Real-time. This
video stream is sent to the user through any generic internet devices with VR
box support, i.e., smartphones giving the user a First-person real-time 3D
experience and transfers the head motion of the user to the robot also in
Real-time. The robot will also be able to track moving objects and faces as a
target using deep neural networks which enables it to be a standalone
monitoring robot. The user will be able to choose specific subjects to monitor
in a space. The stereovision enables us to track the depth information of
different objects detected and will be used to track human interest objects
with its distances and sent to the cloud. A full working prototype is developed
which showcases the capabilities of a monitoring system based on stereo vision,
robotics, and machine learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 Pages, 10 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-resolution segmentations of the hypothalamus and its subregions for
  training of segmentation models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Livia Rodrigues, Martina Bocchetta, Oula Puonti, Douglas Greve, Ana Carolina Londe, Marcondes França, Simone Appenzeller, Leticia Rittner, Juan Eugenio Iglesias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation of brain structures on magnetic resonance imaging (MRI) is a
highly relevant neuroimaging topic, as it is a prerequisite for different
analyses such as volumetry or shape analysis. Automated segmentation
facilitates the study of brain structures in larger cohorts when compared with
manual segmentation, which is time-consuming. However, the development of most
automated methods relies on large and manually annotated datasets, which limits
the generalizability of these methods. Recently, new techniques using synthetic
images have emerged, reducing the need for manual annotation. Here we provide
HELM, Hypothalamic ex vivo Label Maps, a dataset composed of label maps built
from publicly available ultra-high resolution ex vivo MRI from 10 whole
hemispheres, which can be used to develop segmentation methods using synthetic
data. The label maps are obtained with a combination of manual labels for the
hypothalamic regions and automated segmentations for the rest of the brain, and
mirrored to simulate entire brains. We also provide the pre-processed ex vivo
scans, as this dataset can support future projects to include other structures
after these are manually segmented.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GAPNet: Granularity Attention Network with Anatomy-Prior-Constraint for
  Carotid Artery Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Zhang, Chenggang Lu, Xin-yang Shi, Caifeng Shan, Jiong Zhang, Da Chen, Laurent D. Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Atherosclerosis is a chronic, progressive disease that primarily affects the
arterial walls. It is one of the major causes of cardiovascular disease.
Magnetic Resonance (MR) black-blood vessel wall imaging (BB-VWI) offers crucial
insights into vascular disease diagnosis by clearly visualizing vascular
structures. However, the complex anatomy of the neck poses challenges in
distinguishing the carotid artery (CA) from surrounding structures, especially
with changes like atherosclerosis. In order to address these issues, we propose
GAPNet, which is a consisting of a novel geometric prior deduced from.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyi Liu, Cheng Chi, Eric Cousineau, Naveen Kuppuswamy, Benjamin Burchfiel, Shuran Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio signals provide rich information for the robot interaction and object
properties through contact. These information can surprisingly ease the
learning of contact-rich robot manipulation skills, especially when the visual
information alone is ambiguous or incomplete. However, the usage of audio data
in robot manipulation has been constrained to teleoperated demonstrations
collected by either attaching a microphone to the robot or object, which
significantly limits its usage in robot learning pipelines. In this work, we
introduce ManiWAV: an 'ear-in-hand' data collection device to collect
in-the-wild human demonstrations with synchronous audio and visual feedback,
and a corresponding policy interface to learn robot manipulation policy
directly from the demonstrations. We demonstrate the capabilities of our system
through four contact-rich manipulation tasks that require either passively
sensing the contact events and modes, or actively sensing the object surface
materials and states. In addition, we show that our system can generalize to
unseen in-the-wild environments, by learning from diverse in-the-wild human
demonstrations. Project website: https://mani-wav.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient and Distributed Large-Scale 3D Map Registration using
  Tomographic Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Halil Utku Unlu, Anthony Tzes, Prashanth Krishnamurthy, Farshad Khorrami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A robust, resource-efficient, distributed, and minimally parameterized 3D map
matching and merging algorithm is proposed. The suggested algorithm utilizes
tomographic features from 2D projections of horizontal cross-sections of
gravity-aligned local maps, and matches these projection slices at all possible
height differences, enabling the estimation of four degrees of freedom in an
efficient and parallelizable manner. The advocated algorithm improves
state-of-the-art feature extraction and registration pipelines by an order of
magnitude in memory use and execution time. Experimental studies are offered to
investigate the efficiency of this 3D map merging scheme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Elsevier Journal: Robotics and Autonomous Systems (RAS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dataset</span> Size Recovery from LoRA Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Salama, Jonathan Kahana, Eliahu Horwitz, Yedid Hoshen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model inversion and membership inference attacks aim to reconstruct and
verify the data which a model was trained on. However, they are not guaranteed
to find all training samples as they do not know the size of the training set.
In this paper, we introduce a new task: dataset size recovery, that aims to
determine the number of samples used to train a model, directly from its
weights. We then propose DSiRe, a method for recovering the number of images
used to fine-tune a model, in the common case where fine-tuning uses LoRA. We
discover that both the norm and the spectrum of the LoRA matrices are closely
linked to the fine-tuning dataset size; we leverage this finding to propose a
simple yet effective prediction algorithm. To evaluate dataset size recovery of
LoRA weights, we develop and release a new benchmark, LoRA-WiSE, consisting of
over 25000 weight snapshots from more than 2000 diverse LoRA fine-tuned models.
Our best classifier can predict the number of fine-tuning images with a mean
absolute error of 0.36 images, establishing the feasibility of this attack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HUWSOD: Holistic Self-training for Unified Weakly Supervised Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liujuan Cao, Jianghang Lin, Zebo Hong, Yunhang Shen, Shaohui Lin, Chao Chen, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most WSOD methods rely on traditional object proposals to generate candidate
regions and are confronted with unstable training, which easily gets stuck in a
poor local optimum. In this paper, we introduce a unified, high-capacity weakly
supervised object detection (WSOD) network called HUWSOD, which utilizes a
comprehensive self-training framework without needing external modules or
additional supervision. HUWSOD innovatively incorporates a self-supervised
proposal generator and an autoencoder proposal generator with a multi-rate
resampling pyramid to replace traditional object proposals, enabling end-to-end
WSOD training and inference. Additionally, we implement a holistic
self-training scheme that refines detection scores and coordinates through
step-wise entropy minimization and consistency-constraint regularization,
ensuring consistent predictions across stochastic augmentations of the same
image. Extensive experiments on PASCAL VOC and MS COCO demonstrate that HUWSOD
competes with state-of-the-art WSOD methods, eliminating the need for offline
proposals and additional data. The peak performance of HUWSOD approaches that
of fully-supervised Faster R-CNN. Our findings also indicate that randomly
initialized boxes, although significantly different from well-designed offline
object proposals, are effective for WSOD training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Sanity Check for AI-generated Image Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilin Yan, Ouxiang Li, Jiayin Cai, Yanbin Hao, Xiaolong Jiang, Yao Hu, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of generative models, discerning AI-generated
content has evoked increasing attention from both industry and academia. In
this paper, we conduct a sanity check on "whether the task of AI-generated
image detection has been solved". To start with, we present Chameleon dataset,
consisting AIgenerated images that are genuinely challenging for human
perception. To quantify the generalization of existing methods, we evaluate 9
off-the-shelf AI-generated image detectors on Chameleon dataset. Upon analysis,
almost all models classify AI-generated images as real ones. Later, we propose
AIDE (AI-generated Image DEtector with Hybrid Features), which leverages
multiple experts to simultaneously extract visual artifacts and noise patterns.
Specifically, to capture the high-level semantics, we utilize CLIP to compute
the visual embedding. This effectively enables the model to discern
AI-generated images based on semantics or contextual information; Secondly, we
select the highest frequency patches and the lowest frequency patches in the
image, and compute the low-level patchwise features, aiming to detect
AI-generated images by low-level artifacts, for example, noise pattern,
anti-aliasing, etc. While evaluating on existing benchmarks, for example,
AIGCDetectBenchmark and GenImage, AIDE achieves +3.5% and +4.6% improvements to
state-of-the-art methods, and on our proposed challenging Chameleon benchmarks,
it also achieves the promising results, despite this problem for detecting
AI-generated images is far from being solved. The dataset, codes, and pre-train
models will be published at https://github.com/shilinyan99/AIDE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://shilinyan99.github.io/AIDE Code:
  https://github.com/shilinyan99/AIDE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Looking 3D: Anomaly Detection with 2D-3D Alignment <span class="chip">CVPR'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankan Bhunia, Changjian Li, Hakan Bilen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic anomaly detection based on visual cues holds practical significance
in various domains, such as manufacturing and product quality assessment. This
paper introduces a new conditional anomaly detection problem, which involves
identifying anomalies in a query image by comparing it to a reference shape. To
address this challenge, we have created a large dataset, BrokenChairs-180K,
consisting of around 180K images, with diverse anomalies, geometries, and
textures paired with 8,143 reference 3D shapes. To tackle this task, we have
proposed a novel transformer-based approach that explicitly learns the
correspondence between the query image and reference 3D shape via feature
alignment and leverages a customized attention mechanism for anomaly detection.
Our approach has been rigorously evaluated through comprehensive experiments,
serving as a benchmark for future research in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR'24. Codes & dataset available at
  https://github.com/VICO-UoE/Looking3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jr-Jen Chen, Yu-Chien Liao, Hsi-Che Lin, Yu-Chu Yu, Yen-Chun Chen, Yu-Chiang Frank Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ReXTime, a benchmark designed to rigorously test AI models'
ability to perform temporal reasoning within video events. Specifically,
ReXTime focuses on reasoning across time, i.e. human-like understanding when
the question and its corresponding answer occur in different video segments.
This form of reasoning, requiring advanced understanding of cause-and-effect
relationships across video segments, poses significant challenges to even the
frontier multimodal large language models. To facilitate this evaluation, we
develop an automated pipeline for generating temporal reasoning question-answer
pairs, significantly reducing the need for labor-intensive manual annotations.
Our benchmark includes 921 carefully vetted validation samples and 2,143 test
samples, each manually curated for accuracy and relevance. Evaluation results
show that while frontier large language models outperform academic models, they
still lag behind human performance by a significant 14.3% accuracy gap.
Additionally, our pipeline creates a training dataset of 9,695 machine
generated samples without manual effort, which empirical studies suggest can
enhance the across-time reasoning via fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fibottention: Inceptive Visual Representation Learning with Diverse
  Attention Across Heads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Khaleghi Rahimian, Manish Kumar Govind, Subhajit Maity, Dominick Reilly, Christian Kümmerle, Srijan Das, Aritra Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual perception tasks are predominantly solved by Vision Transformer (ViT)
architectures, which, despite their effectiveness, encounter a computational
bottleneck due to the quadratic complexity of computing self-attention. This
inefficiency is largely due to the self-attention heads capturing redundant
token interactions, reflecting inherent redundancy within visual data. Many
works have aimed to reduce the computational complexity of self-attention in
ViTs, leading to the development of efficient and sparse transformer
architectures. In this paper, viewing through the efficiency lens, we realized
that introducing any sparse self-attention strategy in ViTs can keep the
computational overhead low. However, these strategies are sub-optimal as they
often fail to capture fine-grained visual details. This observation leads us to
propose a general, efficient, sparse architecture, named Fibottention, for
approximating self-attention with superlinear complexity that is built upon
Fibonacci sequences. The key strategies in Fibottention include: it excludes
proximate tokens to reduce redundancy, employs structured sparsity by design to
decrease computational demands, and incorporates inception-like diversity
across attention heads. This diversity ensures the capture of complementary
information through non-overlapping token interactions, optimizing both
performance and resource utilization in ViTs for visual representation
learning. We embed our Fibottention mechanism into multiple state-of-the-art
transformer architectures dedicated to visual tasks. Leveraging only 2-6% of
the elements in the self-attention heads, Fibottention in conjunction with ViT
and its variants, consistently achieves significant performance boosts compared
to standard ViTs in nine datasets across three domains $\unicode{x2013}$ image
classification, video understanding, and robot learning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code is publicly available at
  https://github.com/Charlotte-CharMLab/Fibottention</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SALVe: Semantic Alignment Verification for Floorplan Reconstruction from
  Sparse Panoramas <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Lambert, Yuguang Li, Ivaylo Boyadzhiev, Lambert Wixson, Manjunath Narayana, Will Hutchcroft, James Hays, Frank Dellaert, Sing Bing Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new system for automatic 2D floorplan reconstruction that is
enabled by SALVe, our novel pairwise learned alignment verifier. The inputs to
our system are sparsely located 360$^\circ$ panoramas, whose semantic features
(windows, doors, and openings) are inferred and used to hypothesize pairwise
room adjacency or overlap. SALVe initializes a pose graph, which is
subsequently optimized using GTSAM. Once the room poses are computed, room
layouts are inferred using HorizonNet, and the floorplan is constructed by
stitching the most confident layout boundaries. We validate our system
qualitatively and quantitatively as well as through ablation studies, showing
that it outperforms state-of-the-art SfM systems in completeness by over 200%,
without sacrificing accuracy. Our results point to the significance of our
work: poses of 81% of panoramas are localized in the first 2 connected
components (CCs), and 89% in the first 3 CCs. Code and models are publicly
available at https://github.com/zillow/salve.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Chen Change Loy, Shuicheng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current universal segmentation methods demonstrate strong capabilities in
pixel-level image and video understanding. However, they lack reasoning
abilities and cannot be controlled via text instructions. In contrast, large
vision-language multimodal models exhibit powerful vision-based conversation
and reasoning capabilities but lack pixel-level understanding and have
difficulty accepting visual prompts for flexible user interaction. This paper
proposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level
vision understanding with reasoning abilities. It can accept various visual and
text prompts for flexible user interaction. Specifically, we use a universal
segmentation method as the visual encoder, integrating image information,
perception priors, and visual prompts into visual tokens provided to the LLM.
The LLM is responsible for understanding the user's text instructions and
providing text responses and pixel-level segmentation results based on the
visual information. We propose perception prior embedding to better integrate
perception priors with image features. OMG-LLaVA achieves image-level,
object-level, and pixel-level reasoning and understanding in a single model,
matching or surpassing the performance of specialized methods on multiple
benchmarks. Rather than using LLM to connect each specialist, our work aims at
end-to-end training on one encoder, one decoder, and one LLM. The code and
model have been released for further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taming Data and Transformers for Audio Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Guha Balakrishnan, Sergey Tulyakov, Vicente Ordonez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating ambient sounds and effects is a challenging problem due to data
scarcity and often insufficient caption quality, making it difficult to employ
large-scale generative models for the task. In this work, we tackle the problem
by introducing two new models. First, we propose AutoCap, a high-quality and
efficient automatic audio captioning model. We show that by leveraging metadata
available with the audio modality, we can substantially improve the quality of
captions. AutoCap reaches CIDEr score of 83.2, marking a 3.2% improvement from
the best available captioning model at four times faster inference speed. We
then use AutoCap to caption clips from existing datasets, obtaining 761,000
audio clips with high-quality captions, forming the largest available
audio-text dataset. Second, we propose GenAu, a scalable transformer-based
audio generation architecture that we scale up to 1.25B parameters and train
with our new dataset. When compared to state-of-the-art audio generators, GenAu
obtains significant improvements of 15.7% in FAD score, 22.7% in IS, and 13.5%
in CLAP score, indicating significantly improved quality of generated audio
compared to previous works. This shows that the quality of data is often as
important as its quantity. Besides, since AutoCap is fully automatic, new audio
samples can be added to the training dataset, unlocking the training of even
larger generative models for audio synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Webpage: https://snap-research.github.io/GenAU/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mamba or RWKV: Exploring High-Quality and High-Efficiency Segment
  Anything Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haobo Yuan, Xiangtai Li, Lu Qi, Tao Zhang, Ming-Hsuan Yang, Shuicheng Yan, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based segmentation methods face the challenge of efficient
inference when dealing with high-resolution images. Recently, several linear
attention architectures, such as Mamba and RWKV, have attracted much attention
as they can process long sequences efficiently. In this work, we focus on
designing an efficient segment-anything model by exploring these different
architectures. Specifically, we design a mixed backbone that contains
convolution and RWKV operation, which achieves the best for both accuracy and
efficiency. In addition, we design an efficient decoder to utilize the
multiscale tokens to obtain high-quality masks. We denote our method as
RWKV-SAM, a simple, effective, fast baseline for SAM-like models. Moreover, we
build a benchmark containing various high-quality segmentation datasets and
jointly train one efficient yet high-quality segmentation model using this
benchmark. Based on the benchmark results, our RWKV-SAM achieves outstanding
performance in efficiency and segmentation quality compared to transformers and
other linear attention models. For example, compared with the same-scale
transformer model, RWKV-SAM achieves more than 2x speedup and can achieve
better segmentation performance on various datasets. In addition, RWKV-SAM
outperforms recent vision Mamba models with better classification and semantic
segmentation results. Code and models will be publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages; 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STAL3D: Unsupervised Domain Adaptation for 3D Object Detection via
  Collaborating Self-Training and Adversarial Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanan Zhang, Chao Zhou, Di Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing 3D object detection suffers from expensive annotation costs and poor
transferability to unknown data due to the domain gap, Unsupervised Domain
Adaptation (UDA) aims to generalize detection models trained in labeled source
domains to perform robustly on unexplored target domains, providing a promising
solution for cross-domain 3D object detection. Although Self-Training (ST)
based cross-domain 3D detection methods with the assistance of pseudo-labeling
techniques have achieved remarkable progress, they still face the issue of
low-quality pseudo-labels when there are significant domain disparities due to
the absence of a process for feature distribution alignment. While Adversarial
Learning (AL) based methods can effectively align the feature distributions of
the source and target domains, the inability to obtain labels in the target
domain forces the adoption of asymmetric optimization losses, resulting in a
challenging issue of source domain bias. To overcome these limitations, we
propose a novel unsupervised domain adaptation framework for 3D object
detection via collaborating ST and AL, dubbed as STAL3D, unleashing the
complementary advantages of pseudo labels and feature distribution alignment.
Additionally, a Background Suppression Adversarial Learning (BS-AL) module and
a Scale Filtering Module (SFM) are designed tailored for 3D cross-domain
scenes, effectively alleviating the issues of the large proportion of
background interference and source domain size bias. Our STAL3D achieves
state-of-the-art performance on multiple cross-domain tasks and even surpasses
the Oracle results on Waymo $\rightarrow$ KITTI and Waymo $\rightarrow$
KITTI-rain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE-TIV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CORE4D: A 4D Human-Object-Human Interaction <span class="highlight-title">Dataset</span> for Collaborative
  Object REarrangement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengwen Zhang, Yun Liu, Ruofan Xing, Bingda Tang, Li Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding how humans cooperatively rearrange household objects is
critical for VR/AR and human-robot interaction. However, in-depth studies on
modeling these behaviors are under-researched due to the lack of relevant
datasets. We fill this gap by presenting CORE4D, a novel large-scale 4D
human-object-human interaction dataset focusing on collaborative object
rearrangement, which encompasses diverse compositions of various object
geometries, collaboration modes, and 3D scenes. With 1K human-object-human
motion sequences captured in the real world, we enrich CORE4D by contributing
an iterative collaboration retargeting strategy to augment motions to a variety
of novel objects. Leveraging this approach, CORE4D comprises a total of 11K
collaboration sequences spanning 3K real and virtual object shapes. Benefiting
from extensive motion patterns provided by CORE4D, we benchmark two tasks
aiming at generating human-object interaction: human-object motion forecasting
and interaction synthesis. Extensive experiments demonstrate the effectiveness
of our collaboration retargeting strategy and indicate that CORE4D has posed
new challenges to existing human-object interaction generation methodologies.
Our dataset and code are available at
https://github.com/leolyliu/CORE4D-Instructions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Visual Conditioning Tokens to Correct Domain Shift for Fully
  Test-time Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushun Tang, Shuoshuo Chen, Zhehan Kan, Yi Zhang, Qinghai Guo, Zhihai He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fully test-time adaptation aims to adapt the network model based on
sequential analysis of input samples during the inference stage to address the
cross-domain performance degradation problem of deep neural networks. This work
is based on the following interesting finding: in transformer-based image
classification, the class token at the first transformer encoder layer can be
learned to capture the domain-specific characteristics of target samples during
test-time adaptation. This learned token, when combined with input image patch
embeddings, is able to gradually remove the domain-specific information from
the feature representations of input samples during the transformer encoding
process, thereby significantly improving the test-time adaptation performance
of the source model across different domains. We refer to this class token as
visual conditioning token (VCT). To successfully learn the VCT, we propose a
bi-level learning approach to capture the long-term variations of
domain-specific characteristics while accommodating local variations of
instance-specific characteristics. Experimental results on the benchmark
datasets demonstrate that our proposed bi-level visual conditioning token
learning method is able to achieve significantly improved test-time adaptation
performance by up to 1.9%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by TMM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient World Models with Context-Aware Tokenization <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Micheli, Eloi Alonso, François Fleuret
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling up deep Reinforcement Learning (RL) methods presents a significant
challenge. Following developments in generative modelling, model-based RL
positions itself as a strong contender. Recent advances in sequence modelling
have led to effective transformer-based world models, albeit at the price of
heavy computations due to the long sequences of tokens required to accurately
simulate environments. In this work, we propose $\Delta$-IRIS, a new agent with
a world model architecture composed of a discrete autoencoder that encodes
stochastic deltas between time steps and an autoregressive transformer that
predicts future deltas by summarizing the current state of the world with
continuous tokens. In the Crafter benchmark, $\Delta$-IRIS sets a new state of
the art at multiple frame budgets, while being an order of magnitude faster to
train than previous attention-based approaches. We release our code and models
at https://github.com/vmicheli/delta-iris.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Data Transfer Cooperating with Artificial Triplets for Scene
  Graph Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        KuanChao Chu, Satoshi Yamazaki, Hideki Nakayama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work focuses on training dataset enhancement of informative relational
triplets for Scene Graph Generation (SGG). Due to the lack of effective
supervision, the current SGG model predictions perform poorly for informative
relational triplets with inadequate training samples. Therefore, we propose two
novel training dataset enhancement modules: Feature Space Triplet Augmentation
(FSTA) and Soft Transfer. FSTA leverages a feature generator trained to
generate representations of an object in relational triplets. The biased
prediction based sampling in FSTA efficiently augments artificial triplets
focusing on the challenging ones. In addition, we introduce Soft Transfer,
which assigns soft predicate labels to general relational triplets to make more
supervisions for informative predicate classes effectively. Experimental
results show that integrating FSTA and Soft Transfer achieve high levels of
both Recall and mean Recall in Visual Genome dataset. The mean of Recall and
mean Recall is the highest among all the existing model-agnostic methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEICE Transactions on Information and Systems in April
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mapping Land Naturalness from Sentinel-2 using Deep Contextual and
  Geographical Priors <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Burak Ekim, Michael Schmitt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent decades, the causes and consequences of climate change have
accelerated, affecting our planet on an unprecedented scale. This change is
closely tied to the ways in which humans alter their surroundings. As our
actions continue to impact natural areas, using satellite images to observe and
measure these effects has become crucial for understanding and combating
climate change. Aiming to map land naturalness on the continuum of modern human
pressure, we have developed a multi-modal supervised deep learning framework
that addresses the unique challenges of satellite data and the task at hand. We
incorporate contextual and geographical priors, represented by corresponding
coordinate information and broader contextual information, including and
surrounding the immediate patch to be predicted. Our framework improves the
model's predictive performance in mapping land naturalness from Sentinel-2
data, a type of multi-spectral optical satellite imagery. Recognizing that our
protective measures are only as effective as our understanding of the
ecosystem, quantifying naturalness serves as a crucial step toward enhancing
our environmental stewardship.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, ICLR 2024 Tackling Climate Change with Machine
  Learning Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PNeRV: A Polynomial Neural Representation for Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sonam Gupta, Snehal Singh Tomar, Grigorios G Chrysos, Sukhendu Das, A. N. Rajagopalan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting Implicit Neural Representations (INRs) on video data poses unique
challenges due to the additional temporal dimension. In the context of videos,
INRs have predominantly relied on a frame-only parameterization, which
sacrifices the spatiotemporal continuity observed in pixel-level (spatial)
representations. To mitigate this, we introduce Polynomial Neural
Representation for Videos (PNeRV), a parameter-wise efficient, patch-wise INR
for videos that preserves spatiotemporal continuity. PNeRV leverages the
modeling capabilities of Polynomial Neural Networks to perform the modulation
of a continuous spatial (patch) signal with a continuous time (frame) signal.
We further propose a custom Hierarchical Patch-wise Spatial Sampling Scheme
that ensures spatial continuity while retaining parameter efficiency. We also
employ a carefully designed Positional Embedding methodology to further enhance
PNeRV's performance. Our extensive experimentation demonstrates that PNeRV
outperforms the baselines in conventional Implicit Neural Representation tasks
like compression along with downstream applications that require spatiotemporal
continuity in the underlying representation. PNeRV not only addresses the
challenges posed by video data in the realm of INRs but also opens new avenues
for advanced video processing and analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 17 figures, published at TMLR, Feb 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compositional Image Decomposition with <span class="highlight-title">Diffusion</span> Models <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jocelin Su, Nan Liu, Yanbo Wang, Joshua B. Tenenbaum, Yilun Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given an image of a natural scene, we are able to quickly decompose it into a
set of components such as objects, lighting, shadows, and foreground. We can
then envision a scene where we combine certain components with those from other
images, for instance a set of objects from our bedroom and animals from a zoo
under the lighting conditions of a forest, even if we have never encountered
such a scene before. In this paper, we present a method to decompose an image
into such compositional components. Our approach, Decomp Diffusion, is an
unsupervised method which, when given a single image, infers a set of different
components in the image, each represented by a diffusion model. We demonstrate
how components can capture different factors of the scene, ranging from global
scene descriptors like shadows or facial expression to local scene descriptors
like constituent objects. We further illustrate how inferred factors can be
flexibly composed, even with factors inferred from other models, to generate a
variety of scenes sharply different than those seen in training time. Website
and code at https://energy-based-model.github.io/decomp-diffusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024, Webpage:
  https://energy-based-model.github.io/decomp-diffusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Continual Learning in Visual Question Answering with
  Modality-Aware Feature Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malvina Nikandrou, Georgios Pantazopoulos, Ioannis Konstas, Alessandro Suglia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning focuses on incrementally training a model on a sequence of
tasks with the aim of learning new tasks while minimizing performance drop on
previous tasks. Existing approaches at the intersection of Continual Learning
and Visual Question Answering (VQA) do not study how the multimodal nature of
the input affects the learning dynamics of a model. In this paper, we
demonstrate that each modality evolves at different rates across a continuum of
tasks and that this behavior occurs in established encoder-only models as well
as modern recipes for developing Vision & Language (VL) models. Motivated by
this observation, we propose a modality-aware feature distillation (MAFED)
approach which outperforms existing baselines across models of varying scale in
three multimodal continual learning settings. Furthermore, we provide ablations
showcasing that modality-aware distillation complements experience replay.
Overall, our results emphasize the importance of addressing modality-specific
dynamics to prevent forgetting in multimodal continual learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Modelling and Pose Estimation <span class="highlight-title">Overview</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pawel Knap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human modelling and pose estimation stands at the crossroads of Computer
Vision, Computer Graphics, and Machine Learning. This paper presents a thorough
investigation of this interdisciplinary field, examining various algorithms,
methodologies, and practical applications. It explores the diverse range of
sensor technologies relevant to this domain and delves into a wide array of
application areas. Additionally, we discuss the challenges and advancements in
2D and 3D human modelling methodologies, along with popular datasets, metrics,
and future research directions. The main contribution of this paper lies in its
up-to-date comparison of state-of-the-art (SOTA) human pose estimation
algorithms in both 2D and 3D domains. By providing this comprehensive overview,
the paper aims to enhance understanding of 3D human modelling and pose
estimation, offering insights into current SOTA achievements, challenges, and
future prospects within the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into
  Multimodal LLMs at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of multimodal large language models (MLLMs), such as
GPT-4V, has led to significant advancements. However, these models still face
challenges in medical multimodal capabilities due to limitations in the
quantity and quality of medical vision-text data, stemming from data privacy
concerns and high annotation costs. While pioneering approaches utilize
PubMed's large-scale, de-identified medical image-text pairs to address these
limitations, they still fall short due to inherent data noise. To tackle this,
we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in
an 'unblinded' capacity to denoise and reformat the data, resulting in the
creation of the PubMedVision dataset with 1.3 million medical VQA samples. Our
validation demonstrates that: (1) PubMedVision can significantly enhance the
medical multimodal capabilities of current MLLMs, showing significant
improvement in benchmarks including the MMMU Health & Medicine track; (2)
manual checks by medical experts and empirical results validate the superior
data quality of our dataset compared to other data construction methods. Using
PubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows
superior performance in medical multimodal scenarios among open-source MLLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens
  Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Fan, Lei Ding, Ching-Chen Kuo, Shan Jiang, Yang Zhao, Xinze Guan, Jie Yang, Yi Zhang, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphical User Interfaces (GUIs) are central to our interaction with digital
devices. Recently, growing efforts have been made to build models for various
GUI understanding tasks. However, these efforts largely overlook an important
GUI-referring task: screen reading based on user-indicated points, which we
name the Screen Point-and-Read (SPR) task. This task is predominantly handled
by rigid accessible screen reading tools, in great need of new models driven by
advancements in Multimodal Large Language Models (MLLMs). In this paper, we
propose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism,
to address the SPR task. Based on the input point coordinate and the
corresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout
Tree. Based on the tree, our ToL agent not only comprehends the content of the
indicated area but also articulates the layout and spatial relationships
between elements. Such layout information is crucial for accurately
interpreting information on the screen, distinguishing our ToL agent from other
screen reading tools. We also thoroughly evaluate the ToL agent against other
baselines on a newly proposed SPR benchmark, which includes GUIs from mobile,
web, and operating systems. Last but not least, we test the ToL agent on mobile
GUI navigation tasks, demonstrating its utility in identifying incorrect
actions along the path of agent execution trajectories. Code and data:
screen-point-and-read.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Video-Language Representations with Structural Spatio-Temporal
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Fei, Shengqiong Wu, Meishan Zhang, Min Zhang, Tat-Seng Chua, Shuicheng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While pre-training large-scale video-language models (VLMs) has shown
remarkable potential for various downstream video-language tasks, existing VLMs
can still suffer from certain commonly seen limitations, e.g., coarse-grained
cross-modal aligning , under-modeling of temporal dynamics, detached
video-language view. In this work, we target enhancing VLMs with a fine-grained
structural spatio-temporal alignment learning method (namely Finsta). First of
all, we represent the input texts and videos with fine-grained scene graph (SG)
structures, both of which are further unified into a holistic SG (HSG) for
bridging two modalities. Then, an SG-based framework is built, where the
textual SG (TSG) is encoded with a graph Transformer, while the video dynamic
SG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for
spatial and temporal feature propagation. A spatial-temporal Gaussian
differential graph Transformer is further devised to strengthen the sense of
the changes in objects across spatial and temporal dimensions. Next, based on
the fine-grained structural features of TSG and DSG, we perform object-centered
spatial alignment and predicate-centered temporal alignment respectively,
enhancing the video-language grounding in both the spatiality and temporality.
We design our method as a plug&play system, which can be integrated into
existing well-trained VLMs for further representation augmentation, without
training from scratch or relying on SG annotations in downstream applications.
On 6 representative VL modeling tasks over 12 datasets in both standard and
long-form video scenarios, Finsta consistently improves the existing 13
strong-performing VLMs persistently, and refreshes the current state-of-the-art
end task performance significantly in both the fine-tuning and zero-shot
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TPAMI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Manifold Learning for No-Reference Image Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timin Gao, Wensheng Pan, Yan Zhang, Sicheng Zhao, Shengchuan Zhang, Xiawu Zheng, Ke Li, Liujuan Cao, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has considerably advanced the field of Image Quality
Assessment (IQA), emerging as a widely adopted technique. The core mechanism of
contrastive learning involves minimizing the distance between quality-similar
(positive) examples while maximizing the distance between quality-dissimilar
(negative) examples. Despite its successes, current contrastive learning
methods often neglect the importance of preserving the local manifold
structure. This oversight can result in a high degree of similarity among hard
examples within the feature space, thereby impeding effective differentiation
and assessment. To address this issue, we propose an innovative framework that
integrates local manifold learning with contrastive learning for No-Reference
Image Quality Assessment (NR-IQA). Our method begins by sampling multiple crops
from a given image, identifying the most visually salient crop. This crop is
then used to cluster other crops from the same image as the positive class,
while crops from different images are treated as negative classes to increase
inter-class distance. Uniquely, our approach also considers non-saliency crops
from the same image as intra-class negative classes to preserve their
distinctiveness. Additionally, we employ a mutual learning framework, which
further enhances the model's ability to adaptively learn and identify visual
saliency regions. Our approach demonstrates a better performance compared to
state-of-the-art methods in 7 standard datasets, achieving PLCC values of 0.942
(compared to 0.908 in TID2013) and 0.914 (compared to 0.894 in LIVEC).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALMA: a mathematics-driven approach for determining tuning parameters in
  generalized LASSO problems, with applications to MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianluca Giacchi, Isidoros Iakovidis, Bastien Milani, Matthias Stuber, Micah Murray, Benedetta Franceschiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic Resonance Imaging (MRI) is a powerful technique employed for
non-invasive in vivo visualization of internal structures. Sparsity is often
deployed to accelerate the signal acquisition or overcome the presence of
motion artifacts, improving the quality of image reconstruction. Image
reconstruction algorithms use TV-regularized LASSO (Total Variation-regularized
LASSO) to retrieve the missing information of undersampled signals, by cleaning
the data of noise and while optimizing sparsity. A tuning parameter moderates
the balance between these two aspects; its choice affecting the quality of the
reconstructions. Currently, there is a lack of general deterministic techniques
to choose these parameters, which are oftentimes manually selected and thus
hinder the reliability of the reconstructions. Here, we present ALMA (Algorithm
for Lagrange Multipliers Approximation), an iterative mathematics-inspired
technique that computes tuning parameters for generalized LASSO problems during
MRI reconstruction. We analyze quantitatively the performance of these
parameters for imaging reconstructions via TV-LASSO in an MRI context on
phantoms. Although our study concentrates on TV-LASSO, the techniques developed
here hold significant promise for a wide array of applications. ALMA is not
only adaptable to more generalized LASSO problems but is also robust to
accommodate other forms of regularization beyond total variation. Moreover, it
extends effectively to handle non-Cartesian sampling trajectories, broadening
its utility in complex data reconstruction scenarios. More generally, ALMA
provides a powerful tool for numerically solving constrained optimization
problems across various disciplines, offering a versatile and impactful
solution for advanced computational challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-Aware Vision-and-Language Navigation: Bridging Simulation to
  Reality with Dynamic Human Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghan Li, Heng Li, Zhi-Qi Cheng, Yifei Dong, Yuxuan Zhou, Jun-Yan He, Qi Dai, Teruko Mitamura, Alexander G. Hauptmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-Language Navigation (VLN) aims to develop embodied agents that
navigate based on human instructions. However, current VLN frameworks often
rely on static environments and optimal expert supervision, limiting their
real-world applicability. To address this, we introduce Human-Aware
Vision-and-Language Navigation (HA-VLN), extending traditional VLN by
incorporating dynamic human activities and relaxing key assumptions. We propose
the Human-Aware 3D (HA3D) simulator, which combines dynamic human activities
with the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R)
dataset, extending R2R with human activity descriptions. To tackle HA-VLN
challenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and
Non-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing
cross-modal fusion and diverse training strategies for effective navigation in
dynamic human environments. A comprehensive evaluation, including metrics
considering human activities, and systematic analysis of HA-VLN's unique
challenges, underscores the need for further research to enhance HA-VLN agents'
real-world robustness and adaptability. Ultimately, this work provides
benchmarks and insights for future research on embodied AI and Sim2Real
transfer, paving the way for more realistic and applicable VLN systems in
human-populated environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 18 figures, Project Page:
  https://lpercc.github.io/HA3D_simulator/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProtoGMM: Multi-prototype Gaussian-Mixture-based Domain Adaptation Model
  for Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nazanin Moradinasab, Laura S. Shankman, Rebecca A. Deaton, Gary K. Owens, Donald E. Brown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptive semantic segmentation aims to generate accurate and dense
predictions for an unlabeled target domain by leveraging a supervised model
trained on a labeled source domain. The prevalent self-training approach
involves retraining the dense discriminative classifier of $p(class|pixel
feature)$ using the pseudo-labels from the target domain. While many methods
focus on mitigating the issue of noisy pseudo-labels, they often overlook the
underlying data distribution p(pixel feature|class) in both the source and
target domains. To address this limitation, we propose the multi-prototype
Gaussian-Mixture-based (ProtoGMM) model, which incorporates the GMM into
contrastive losses to perform guided contrastive learning. Contrastive losses
are commonly executed in the literature using memory banks, which can lead to
class biases due to underrepresented classes. Furthermore, memory banks often
have fixed capacities, potentially restricting the model's ability to capture
diverse representations of the target/source domains. An alternative approach
is to use global class prototypes (i.e. averaged features per category).
However, the global prototypes are based on the unimodal distribution
assumption per class, disregarding within-class variation. To address these
challenges, we propose the ProtoGMM model. This novel approach involves
estimating the underlying multi-prototype source distribution by utilizing the
GMM on the feature space of the source samples. The components of the GMM model
act as representative prototypes. To achieve increased intra-class semantic
similarity, decreased inter-class similarity, and domain alignment between the
source and target domains, we employ multi-prototype contrastive learning
between source distribution and target samples. The experiments show the
effectiveness of our method on UDA benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Think Step by Step: Chain-of-Gesture Prompting for Error Detection in
  Robotic Surgical Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhimin Shao, Jialang Xu, Danail Stoyanov, Evangelos B. Mazomenos, Yueming Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in robotic systems and surgical data
science, ensuring safe and optimal execution in robot-assisted minimally
invasive surgery (RMIS) remains a complex challenge. Current surgical error
detection methods involve two parts: identifying surgical gestures and then
detecting errors within each gesture clip. These methods seldom consider the
rich contextual and semantic information inherent in surgical videos, limiting
their performance due to reliance on accurate gesture identification. Motivated
by the chain-of-thought prompting in natural language processing, this letter
presents a novel and real-time end-to-end error detection framework,
Chain-of-Thought (COG) prompting, leveraging contextual information from
surgical videos. This encompasses two reasoning modules designed to mimic the
decision-making processes of expert surgeons. Concretely, we first design a
Gestural-Visual Reasoning module, which utilizes transformer and attention
architectures for gesture prompting, while the second, a Multi-Scale Temporal
Reasoning module, employs a multi-stage temporal convolutional network with
both slow and fast paths for temporal information extraction. We extensively
validate our method on the public benchmark RMIS dataset JIGSAWS. Our method
encapsulates the reasoning processes inherent to surgical activities enabling
it to outperform the state-of-the-art by 4.6% in F1 score, 4.6% in Accuracy,
and 5.9% in Jaccard index while processing each frame in 6.69 milliseconds on
average, demonstrating the great potential of our approach in enhancing the
safety and efficacy of RMIS procedures and surgical education. The code will be
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Reducing Data Acquisition and Labeling for Defect Detection
  using Simulated Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Malte Kemeter, Rasmus Hvingelby, Paulina Sierak, Tobias Schön, Bishwajit Gosswam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many manufacturing settings, annotating data for machine learning and
computer vision is costly, but synthetic data can be generated at significantly
lower cost. Substituting the real-world data with synthetic data is therefore
appealing for many machine learning applications that require large amounts of
training data. However, relying solely on synthetic data is frequently
inadequate for effectively training models that perform well on real-world
data, primarily due to domain shifts between the synthetic and real-world data.
We discuss approaches for dealing with such a domain shift when detecting
defects in X-ray scans of aluminium wheels. Using both simulated and real-world
X-ray images, we train an object detection model with different strategies to
identify the training approach that generates the best detection results while
minimising the demand for annotated real-world training samples. Our
preliminary findings suggest that the sim-2-real domain adaptation approach is
more cost-efficient than a fully supervised oracle - if the total number of
available annotated samples is fixed. Given a certain number of labeled
real-world samples, training on a mix of synthetic and unlabeled real-world
data achieved comparable or even better detection results at significantly
lower cost. We argue that future research into the cost-efficiency of different
training strategies is important for a better understanding of how to allocate
budget in applied machine learning projects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single Image Estimation of Cell Migration Direction by Deep Circular
  Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Bruns, Lucas Lamparter, Milos Galic, Xiaoyi Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we study the problem of estimating the migration direction of
cells based on a single image. To the best of our knowledge, there is only one
related work that uses a classification CNN for four classes (quadrants). This
approach does not allow detailed directional resolution. We solve the single
image estimation problem using deep circular regression with special attention
to cycle-sensitive methods. On two databases we achieve an average accuracy of
$\sim$17 degrees, which is a significant improvement over the previous work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAVEN: Multitask Retrieval Augmented Vision-Language Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varun Nagaraj Rao, Siddharth Choudhary, Aditya Deshpande, Ravi Kumar Satzoda, Srikar Appalaraju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scaling of large language models to encode all the world's knowledge in
model parameters is unsustainable and has exacerbated resource barriers.
Retrieval-Augmented Generation (RAG) presents a potential solution, yet its
application to vision-language models (VLMs) is under explored. Existing
methods focus on models designed for single tasks. Furthermore, they're limited
by the need for resource intensive pre training, additional parameter
requirements, unaddressed modality prioritization and lack of clear benefit
over non-retrieval baselines. This paper introduces RAVEN, a multitask
retrieval augmented VLM framework that enhances base VLMs through efficient,
task specific fine-tuning. By integrating retrieval augmented samples without
the need for additional retrieval-specific parameters, we show that the model
acquires retrieval properties that are effective across multiple tasks. Our
results and extensive ablations across retrieved modalities for the image
captioning and VQA tasks indicate significant performance improvements compared
to non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a
+3\% accuracy on specific VQA question types. This underscores the efficacy of
applying RAG approaches to VLMs, marking a stride toward more efficient and
accessible multimodal learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Utilizing Adversarial Examples for Bias Mitigation and Accuracy
  Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11819v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11819v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pushkar Shukla, Dhruv Srikanth, Lee Cohen, Matthew Turk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach to mitigate biases in computer vision models by
utilizing counterfactual generation and fine-tuning. While counterfactuals have
been used to analyze and address biases in DNN models, the counterfactuals
themselves are often generated from biased generative models, which can
introduce additional biases or spurious correlations. To address this issue, we
propose using adversarial images, that is images that deceive a deep neural
network but not humans, as counterfactuals for fair model training. Our
approach leverages a curriculum learning framework combined with a fine-grained
adversarial loss to fine-tune the model using adversarial examples. By
incorporating adversarial images into the training data, we aim to prevent
biases from propagating through the pipeline. We validate our approach through
both qualitative and quantitative assessments, demonstrating improved bias
mitigation and accuracy compared to existing methods. Qualitatively, our
results indicate that post-training, the decisions made by the model are less
dependent on the sensitive attribute and our model better disentangles the
relationship between sensitive attributes and classification variables.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semi-supervised variational autoencoder for cell feature extraction in
  multiplexed immunofluorescence images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piumi Sandarenu, Julia Chen, Iveta Slapetova, Lois Browne, Peter H. Graham, Alexander Swarbrick, Ewan K. A. Millar, Yang Song, Erik Meijering
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in digital imaging technologies have sparked increased interest
in using multiplexed immunofluorescence (mIF) images to visualise and identify
the interactions between specific immunophenotypes with the tumour
microenvironment at the cellular level. Current state-of-the-art multiplexed
immunofluorescence image analysis pipelines depend on cell feature
representations characterised by morphological and stain intensity-based
metrics generated using simple statistical and machine learning-based tools.
However, these methods are not capable of generating complex representations of
cells. We propose a deep learning-based cell feature extraction model using a
variational autoencoder with supervision using a latent subspace to extract
cell features in mIF images. We perform cell phenotype classification using a
cohort of more than 44,000 multiplexed immunofluorescence cell image patches
extracted across 1,093 tissue microarray cores of breast cancer patients, to
demonstrate the success of our model against current and alternative methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FishNet: Deep Neural Networks for Low-Cost Fish Stock Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moseli Mots'oehli, Anton Nikolaev, Wawan B. IGede, John Lynham, Peter J. Mous, Peter Sadowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fish stock assessment often involves manual fish counting by taxonomy
specialists, which is both time-consuming and costly. We propose FishNet, an
automated computer vision system for both taxonomic classification and fish
size estimation from images captured with a low-cost digital camera. The system
first performs object detection and segmentation using a Mask R-CNN to identify
individual fish from images containing multiple fish, possibly consisting of
different species. Then each fish species is classified and the length is
predicted using separate machine learning models. To develop the model, we use
a dataset of 300,000 hand-labeled images containing 1.2M fish of 163 different
species and ranging in length from 10cm to 250cm, with additional annotations
and quality control methods used to curate high-quality training data. On
held-out test data sets, our system achieves a 92% intersection over union on
the fish segmentation task, a 89% top-1 classification accuracy on single fish
species classification, and a 2.3cm mean absolute error on the fish length
estimation task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE COINS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Step Differences in Instructional Video <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16222v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16222v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tushar Nagarajan, Lorenzo Torresani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Comparing a user video to a reference how-to video is a key requirement for
AR/VR technology delivering personalized assistance tailored to the user's
progress. However, current approaches for language-based assistance can only
answer questions about a single video. We propose an approach that first
automatically generates large amounts of visual instruction tuning data
involving pairs of videos from HowTo100M by leveraging existing step
annotations and accompanying narrations, and then trains a video-conditioned
language model to jointly reason across multiple raw videos. Our model achieves
state-of-the-art performance at identifying differences between video pairs and
ranking videos based on the severity of these differences, and shows promising
ability to perform general reasoning over multiple videos. Project page:
https://github.com/facebookresearch/stepdiff
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Stackable and Skippable LEGO Bricks for Efficient,
  Reconfigurable, and Variable-Resolution <span class="highlight-title">Diffusion</span> Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06389v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06389v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huangjie Zheng, Zhendong Wang, Jianbo Yuan, Guanghan Ning, Pengcheng He, Quanzeng You, Hongxia Yang, Mingyuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models excel at generating photo-realistic images but come with
significant computational costs in both training and sampling. While various
techniques address these computational challenges, a less-explored issue is
designing an efficient and adaptable network backbone for iterative refinement.
Current options like U-Net and Vision Transformer often rely on
resource-intensive deep networks and lack the flexibility needed for generating
images at variable resolutions or with a smaller network than used in training.
This study introduces LEGO bricks, which seamlessly integrate Local-feature
Enrichment and Global-content Orchestration. These bricks can be stacked to
create a test-time reconfigurable diffusion backbone, allowing selective
skipping of bricks to reduce sampling costs and generate higher-resolution
images than the training data. LEGO bricks enrich local regions with an MLP and
transform them using a Transformer block while maintaining a consistent
full-resolution image across all bricks. Experimental results demonstrate that
LEGO bricks enhance training efficiency, expedite convergence, and facilitate
variable-resolution image generation while maintaining strong generative
performance. Moreover, LEGO significantly reduces sampling time compared to
other methods, establishing it as a valuable enhancement for diffusion models.
Our code and project page are available at
https://jegzheng.github.io/LEGODiffusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Semantic Equivalence of Tokenization in Multimodal LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05127v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05127v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengqiong Wu, Hao Fei, Xiangtai Li, Jiayi Ji, Hanwang Zhang, Tat-Seng Chua, Shuicheng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have demonstrated exceptional
capabilities in processing vision-language tasks. One of the crux of MLLMs lies
in vision tokenization, which involves efficiently transforming input visual
signals into feature representations that are most beneficial for LLMs.
However, existing vision tokenizers, essential for semantic alignment between
vision and language, remain problematic. Existing methods aggressively fragment
visual input, corrupting the visual semantic integrity. To address this, this
paper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok),
which groups visual features into semantic units via a dynamic clustering
algorithm, flexibly determining the number of tokens based on image complexity.
The resulting vision tokens effectively preserve semantic integrity and capture
both low-frequency and high-frequency visual features. The proposed MLLM
(Setokim) equipped with SeTok significantly demonstrates superior performance
across various tasks, as evidenced by our experimental results. The project
page is at https://chocowu.github.io/SeTok-web/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report. The project page:
  https://chocowu.github.io/SeTok-web/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13040v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13040v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Jung Ling, Salomé Bru, Julia Puig, Florian Vixège, Simon Mendez, Franck Nicoud, Pierre-Yves Courand, Olivier Bernard, Damien Garcia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify
color Doppler in cardiac imaging. In this study, we propose novel alternatives
to the traditional iVFM optimization scheme by utilizing physics-informed
neural networks (PINNs) and a physics-guided nnU-Net-based supervised approach.
When evaluated on simulated color Doppler images derived from a
patient-specific computational fluid dynamics model and in vivo Doppler
acquisitions, both approaches demonstrate comparable reconstruction performance
to the original iVFM algorithm. The efficiency of PINNs is boosted through
dual-stage optimization and pre-optimized weights. On the other hand, the
nnU-Net method excels in generalizability and real-time capabilities. Notably,
nnU-Net shows superior robustness on sparse and truncated Doppler data while
maintaining independence from explicit boundary conditions. Overall, our
results highlight the effectiveness of these methods in reconstructing
intraventricular vector blood flow. The study also suggests potential
applications of PINNs in ultrafast color Doppler imaging and the incorporation
of fluid dynamics equations to derive biomarkers for cardiovascular diseases
based on blood flow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, accepted for publication in IEEE TUFFC; camera ready
  corrections, corrected acknowledgments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VDebugger: Harnessing Execution Feedback for Debugging Visual Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13444v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13444v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueqing Wu, Zongyu Lin, Songyan Zhao, Te-Lin Wu, Pan Lu, Nanyun Peng, Kai-Wei Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual programs are executable code generated by large language models to
address visual reasoning problems. They decompose complex questions into
multiple reasoning steps and invoke specialized models for each step to solve
the problems. However, these programs are prone to logic errors, with our
preliminary evaluation showing that 58% of the total errors are caused by
program logic errors. Debugging complex visual programs remains a major
bottleneck for visual reasoning. To address this, we introduce VDebugger, a
novel critic-refiner framework trained to localize and debug visual programs by
tracking execution step by step. VDebugger identifies and corrects program
errors leveraging detailed execution feedback, improving interpretability and
accuracy. The training data is generated through an automated pipeline that
injects errors into correct visual programs using a novel mask-best decoding
technique. Evaluations on six datasets demonstrate VDebugger's effectiveness,
showing performance improvements of up to 3.2% in downstream task accuracy.
Further studies show VDebugger's ability to generalize to unseen tasks,
bringing a notable improvement of 2.3% on the unseen COVR task. Code, data and
models are made publicly available at https://github.com/shirley-wu/vdebugger/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>update reference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpatialBot: Precise Spatial Understanding with Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13642v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13642v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxiao Cai, Yaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, Bo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) have achieved impressive performance in 2D
image understanding, however they are still struggling with spatial
understanding which is the foundation of Embodied AI. In this paper, we propose
SpatialBot for better spatial understanding by feeding both RGB and depth
images. Additionally, we have constructed the SpatialQA dataset, which involves
multi-level depth-related questions to train VLMs for depth understanding.
Finally, we present SpatialBench to comprehensively evaluate VLMs' capabilities
in spatial understanding at different levels. Extensive experiments on our
spatial-understanding benchmark, general VLM benchmarks and Embodied AI tasks,
demonstrate the remarkable improvements of SpatialBot trained on SpatialQA. The
model, code and data are available at https://github.com/BAAI-DCAI/SpatialBot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Muffin or Chihuahua? Challenging Multimodal Large Language Models with
  Multipanel VQA <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15847v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15847v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Fan, Jing Gu, Kaiwen Zhou, Qianqi Yan, Shan Jiang, Ching-Chen Kuo, Xinze Guan, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multipanel images, commonly seen as web screenshots, posters, etc., pervade
our daily lives. These images, characterized by their composition of multiple
subfigures in distinct layouts, effectively convey information to people.
Toward building advanced multimodal AI applications, such as agents that
understand complex scenes and navigate through webpages, the skill of
multipanel visual reasoning is essential, and a comprehensive evaluation of
models in this regard is important. Therefore, we introduce Multipanel Visual
Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets
of questions, answers, and multipanel images that specifically challenge models
in comprehending multipanel images. Our evaluation shows that questions in the
MultipanelVQA benchmark pose significant challenges to the state-of-the-art
Multimodal Large Language Models (MLLMs) tested, even though humans can attain
approximately 99% accuracy on these questions. Distinctively, the MultipanelVQA
benchmark features synthetically generated multipanel images specifically
crafted to isolate and assess the impact of various factors, such as the
layout, on MLLMs' multipanel image comprehension abilities. As a result, in
addition to benchmarking the capabilities of MLLMs in understanding multipanel
images, we analyze various factors of the multipanel image that affect MLLMs'
performance with synthetic data and offer insights for enhancement. Code and
data are released at https://sites.google.com/view/multipanelvqa/home.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shortcut Learning in Medical Image Segmentation <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manxi Lin, Nina Weng, Kamil Mikolaj, Zahra Bashir, Morten Bo Søndergaard Svendsen, Martin Tolsgaard, Anders Nymark Christensen, Aasa Feragen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shortcut learning is a phenomenon where machine learning models prioritize
learning simple, potentially misleading cues from data that do not generalize
well beyond the training set. While existing research primarily investigates
this in the realm of image classification, this study extends the exploration
of shortcut learning into medical image segmentation. We demonstrate that
clinical annotations such as calipers, and the combination of zero-padded
convolutions and center-cropped training sets in the dataset can inadvertently
serve as shortcuts, impacting segmentation accuracy. We identify and evaluate
the shortcut learning on two different but common medical image segmentation
tasks. In addition, we suggest strategies to mitigate the influence of shortcut
learning and improve the generalizability of the segmentation models. By
uncovering the presence and implications of shortcuts in medical image
segmentation, we provide insights and methodologies for evaluating and
overcoming this pervasive challenge and call for attention in the community for
shortcuts in segmentation. Our code is public at
https://github.com/nina-weng/shortcut_skinseg .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, accepted at MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S4: Self-Supervised Sensing Across the Spectrum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jayanth Shenoy, Xingjian Davis Zhang, Shlok Mehrotra, Bill Tao, Rem Yang, Han Zhao, Deepak Vasisht
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Satellite image time series (SITS) segmentation is crucial for many
applications like environmental monitoring, land cover mapping and agricultural
crop type classification. However, training models for SITS segmentation
remains a challenging task due to the lack of abundant training data, which
requires fine grained annotation. We propose S4 a new self-supervised
pre-training approach that significantly reduces the requirement for labeled
training data by utilizing two new insights: (a) Satellites capture images in
different parts of the spectrum such as radio frequencies, and visible
frequencies. (b) Satellite imagery is geo-registered allowing for fine-grained
spatial alignment. We use these insights to formulate pre-training tasks in S4.
We also curate m2s2-SITS, a large-scale dataset of unlabeled,
spatially-aligned, multi-modal and geographic specific SITS that serves as
representative pre-training data for S4. Finally, we evaluate S4 on multiple
SITS segmentation datasets and demonstrate its efficacy against competing
baselines while using limited labeled data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic infant 2D pose estimation from videos: comparing seven deep
  neural network methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filipe Gama, Matej Misar, Lukas Navara, Sergiu T. Popescu, Matej Hoffmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic markerless estimation of infant posture and motion from ordinary
videos carries great potential for movement studies "in the wild", facilitating
understanding of motor development and massively increasing the chances of
early diagnosis of disorders. There is rapid development of human pose
estimation methods in computer vision thanks to advances in deep learning and
machine learning. However, these methods are trained on datasets featuring
adults in different contexts. This work tests and compares seven popular
methods (AlphaPose, DeepLabCut/DeeperCut, Detectron2, HRNet,
MediaPipe/BlazePose, OpenPose, and ViTPose) on videos of infants in supine
position. Surprisingly, all methods except DeepLabCut and MediaPipe have
competitive performance without additional finetuning, with ViTPose performing
best. Next to standard performance metrics (object keypoint similarity, average
precision and recall), we introduce errors expressed in the neck-mid-hip ratio
and additionally study missed and redundant detections and the reliability of
the internal confidence ratings of the different methods, which are relevant
for downstream tasks. Among the networks with competitive performance, only
AlphaPose could run close to real time (27 fps) on our machine. We provide
documented Docker containers or instructions for all the methods we used, our
analysis scripts, and processed data at https://hub.docker.com/u/humanoidsctu
and https://osf.io/x465b/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 3 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SRC-Net: Bi-Temporal Spatial Relationship Concerned Network for Change
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05668v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05668v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjia Chen, Xin Xu, Fangling Pu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Change detection (CD) in remote sensing imagery is a crucial task with
applications in environmental monitoring, urban development, and disaster
management. CD involves utilizing bi-temporal images to identify changes over
time. The bi-temporal spatial relationships between features at the same
location at different times play a key role in this process. However, existing
change detection networks often do not fully leverage these spatial
relationships during bi-temporal feature extraction and fusion. In this work,
we propose SRC-Net: a bi-temporal spatial relationship concerned network for
CD. The proposed SRC-Net includes a Perception and Interaction Module that
incorporates spatial relationships and establishes a cross-branch perception
mechanism to enhance the precision and robustness of feature extraction.
Additionally, a Patch-Mode joint Feature Fusion Module is introduced to address
information loss in current methods. It considers different change modes and
concerns about spatial relationships, resulting in more expressive fusion
features. Furthermore, we construct a novel network using these two
relationship concerned modules and conducted experiments on the LEVIR-CD and
WHU Building datasets. The experimental results demonstrate that our network
outperforms state-of-the-art (SOTA) methods while maintaining a modest
parameter count. We believe our approach sets a new paradigm for change
detection and will inspire further advancements in the field. The code and
models are publicly available at https://github.com/Chnja/SRCNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 12 figures, IEEE Journal of Selected Topics in Applied
  Earth Observations and Remote Sensing (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VLSM-Adapter: Finetuning Vision-Language Segmentation Efficiently with
  Lightweight Blocks <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06196v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06196v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manish Dhakal, Rabin Adhikari, Safal Thapaliya, Bishesh Khanal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation Vision-Language Models (VLMs) trained using large-scale
open-domain images and text pairs have recently been adapted to develop
Vision-Language Segmentation Models (VLSMs) that allow providing text prompts
during inference to guide image segmentation. If robust and powerful VLSMs can
be built for medical images, it could aid medical professionals in many
clinical tasks where they must spend substantial time delineating the target
structure of interest. VLSMs for medical images resort to fine-tuning base VLM
or VLSM pretrained on open-domain natural image datasets due to fewer annotated
medical image datasets; this fine-tuning is resource-consuming and expensive as
it usually requires updating all or a significant fraction of the pretrained
parameters. Recently, lightweight blocks called adapters have been proposed in
VLMs that keep the pretrained model frozen and only train adapters during
fine-tuning, substantially reducing the computing resources required. We
introduce a novel adapter, VLSM-Adapter, that can fine-tune pretrained
vision-language segmentation models using transformer encoders. Our experiments
in widely used CLIP-based segmentation models show that with only 3 million
trainable parameters, the VLSM-Adapter outperforms state-of-the-art and is
comparable to the upper bound end-to-end fine-tuning. The source code is
available at: https://github.com/naamiinepal/vlsm-adapter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI 2024, the 27th International Conference on Medical
  Image Computing and Computer Assisted Intervention</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMGPL: Multimodal Medical Data Analysis with Graph Prompt Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Peng, Songyue Cai, Zongqian Wu, Huifang Shang, Xiaofeng Zhu, Xiaoxiao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt learning has demonstrated impressive efficacy in the fine-tuning of
multimodal large models to a wide range of downstream tasks. Nonetheless,
applying existing prompt learning methods for the diagnosis of neurological
disorder still suffers from two issues: (i) existing methods typically treat
all patches equally, despite the fact that only a small number of patches in
neuroimaging are relevant to the disease, and (ii) they ignore the structural
information inherent in the brain connection network which is crucial for
understanding and diagnosing neurological disorders. To tackle these issues, we
introduce a novel prompt learning model by learning graph prompts during the
fine-tuning process of multimodal large models for diagnosing neurological
disorders. Specifically, we first leverage GPT-4 to obtain relevant disease
concepts and compute semantic similarity between these concepts and all
patches. Secondly, we reduce the weight of irrelevant patches according to the
semantic similarity between each patch and disease-related concepts. Moreover,
we construct a graph among tokens based on these concepts and employ a graph
convolutional network layer to extract the structural information of the graph,
which is used to prompt the pre-trained multimodal large models for diagnosing
neurological disorders. Extensive experiments demonstrate that our method
achieves superior performance for neurological disorder diagnosis compared with
state-of-the-art methods and validated by clinicians.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Supervised Detection of Perfect and Partial Input-Dependent
  Symmetries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12223v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12223v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alonso Urbano, David W. Romero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group equivariance can overly constrain models if the symmetries in the group
differ from those observed in data. While common methods address this by
determining the appropriate level of symmetry at the dataset level, they are
limited to supervised settings and ignore scenarios in which multiple levels of
symmetry co-exist in the same dataset. In this paper, we propose a method able
to detect the level of symmetry of each input without the need for labels. Our
framework is general enough to accommodate different families of both
continuous and discrete symmetry distributions, such as arbitrary unimodal,
symmetric distributions and discrete groups. We validate the effectiveness of
our approach on synthetic datasets with different per-class levels of
symmetries, and demonstrate practical applications such as the detection of
out-of-distribution symmetries. Our code is publicly available at
https://github.com/aurban0/ssl-sym.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 8 figures, corrected typos, revised argument in Appendix
  B.1, results unchanged</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Private Zeroth-Order Nonsmooth Nonconvex Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinzi Zhang, Hoang Tran, Ashok Cutkosky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new zeroth-order algorithm for private stochastic optimization
on nonconvex and nonsmooth objectives. Given a dataset of size $M$, our
algorithm ensures $(\alpha,\alpha\rho^2/2)$-R\'enyi differential privacy and
finds a $(\delta,\epsilon)$-stationary point so long as
$M=\tilde\Omega\left(\frac{d}{\delta\epsilon^3} +
\frac{d^{3/2}}{\rho\delta\epsilon^2}\right)$. This matches the optimal
complexity of its non-private zeroth-order analog. Notably, although the
objective is not smooth, we have privacy ``for free'' whenever $\rho \ge
\sqrt{d}\epsilon$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PathAlign: A vision-language model for whole slide images in
  histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faruk Ahmed, Andrew Sellergren, Lin Yang, Shawn Xu, Boris Babenko, Abbi Ward, Niels Olson, Arash Mohtashamian, Yossi Matias, Greg S. Corrado, Quang Duong, Dale R. Webster, Shravya Shetty, Daniel Golden, Yun Liu, David F. Steiner, Ellery Wulczyn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microscopic interpretation of histopathology images underlies many important
diagnostic and treatment decisions. While advances in vision-language modeling
raise new opportunities for analysis of such images, the gigapixel-scale size
of whole slide images (WSIs) introduces unique challenges. Additionally,
pathology reports simultaneously highlight key findings from small regions
while also aggregating interpretation across multiple slides, often making it
difficult to create robust image-text pairs. As such, pathology reports remain
a largely untapped source of supervision in computational pathology, with most
efforts relying on region-of-interest annotations or self-supervision at the
patch-level. In this work, we develop a vision-language model based on the
BLIP-2 framework using WSIs paired with curated text from pathology reports.
This enables applications utilizing a shared image-text embedding space, such
as text or image retrieval for finding cases of interest, as well as
integration of the WSI encoder with a frozen large language model (LLM) for
WSI-based generative text capabilities such as report generation or
AI-in-the-loop interactions. We utilize a de-identified dataset of over 350,000
WSIs and diagnostic text pairs, spanning a wide range of diagnoses, procedure
types, and tissue types. We present pathologist evaluation of text generation
and text retrieval using WSI embeddings, as well as results for WSI
classification and workflow prioritization (slide-level triaging).
Model-generated text for WSIs was rated by pathologists as accurate, without
clinically significant error or omission, for 78% of WSIs on average. This work
demonstrates exciting potential capabilities for language-aligned WSI
embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 main pages and 19 pages of supplemental material; 3 main tables, 3
  main figures and 11 supplemental tables, 7 supplemental figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Temporal Sequence Classification and Mathematical Modeling for Cell
  Tracking in Dense 3D Microscopy Videos of Bacterial Biofilms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanjin Taher Toma, Yibo Wang, Andreas Gahlmann, Scott T. Acton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic cell tracking in dense environments is plagued by inaccurate
correspondences and misidentification of parent-offspring relationships. In
this paper, we introduce a novel cell tracking algorithm named DenseTrack,
which integrates deep learning with mathematical model-based strategies to
effectively establish correspondences between consecutive frames and detect
cell division events in crowded scenarios. We formulate the cell tracking
problem as a deep learning-based temporal sequence classification task followed
by solving a constrained one-to-one matching optimization problem exploiting
the classifier's confidence scores. Additionally, we present an
eigendecomposition-based cell division detection strategy that leverages
knowledge of cellular geometry. The performance of the proposed approach has
been evaluated by tracking densely packed cells in 3D time-lapse image
sequences of bacterial biofilm development. The experimental results on
simulated as well as experimental fluorescence image sequences suggest that the
proposed tracking method achieves superior performance in terms of both
qualitative and quantitative evaluation measures compared to recent
state-of-the-art cell tracking approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Counterfactual Interventions in Vector Autoregressive Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kurt Butler, Marija Iloska, Petar M. Djuric
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual reasoning allows us to explore hypothetical scenarios in order
to explain the impacts of our decisions. However, addressing such inquires is
impossible without establishing the appropriate mathematical framework. In this
work, we introduce the problem of counterfactual reasoning in the context of
vector autoregressive (VAR) processes. We also formulate the inference of a
causal model as a joint regression task where for inference we use both data
with and without interventions. After learning the model, we exploit linearity
of the VAR model to make exact predictions about the effects of counterfactual
interventions. Furthermore, we quantify the total causal effects of past
counterfactual interventions. The source code for this project is freely
available at https://github.com/KurtButler/counterfactual_interventions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instance-Optimal Private Density Estimation in the Wasserstein Distance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vitaly Feldman, Audra McMillan, Satchit Sivakumar, Kunal Talwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the density of a distribution from samples is a fundamental
problem in statistics. In many practical settings, the Wasserstein distance is
an appropriate error metric for density estimation. For example, when
estimating population densities in a geographic region, a small Wasserstein
distance means that the estimate is able to capture roughly where the
population mass is. In this work we study differentially private density
estimation in the Wasserstein distance. We design and analyze instance-optimal
algorithms for this problem that can adapt to easy instances.
  For distributions $P$ over $\mathbb{R}$, we consider a strong notion of
instance-optimality: an algorithm that uniformly achieves the instance-optimal
estimation rate is competitive with an algorithm that is told that the
distribution is either $P$ or $Q_P$ for some distribution $Q_P$ whose
probability density function (pdf) is within a factor of 2 of the pdf of $P$.
For distributions over $\mathbb{R}^2$, we use a different notion of instance
optimality. We say that an algorithm is instance-optimal if it is competitive
with an algorithm that is given a constant-factor multiplicative approximation
of the density of the distribution. We characterize the instance-optimal
estimation rates in both these settings and show that they are uniformly
achievable (up to polylogarithmic factors). Our approach for $\mathbb{R}^2$
extends to arbitrary metric spaces as it goes via hierarchically separated
trees. As a special case our results lead to instance-optimal private learning
in TV distance for discrete distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-Gradient Search Control: A Method for Improving the Efficiency of
  Dyna-style Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bradley Burega, John D. Martin, Luke Kapeluck, Michael Bowling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study how a Reinforcement Learning (RL) system can remain sample-efficient
when learning from an imperfect model of the environment. This is particularly
challenging when the learning system is resource-constrained and in continual
settings, where the environment dynamics change. To address these challenges,
our paper introduces an online, meta-gradient algorithm that tunes a
probability with which states are queried during Dyna-style planning. Our study
compares the aggregate, empirical performance of this meta-gradient method to
baselines that employ conventional sampling strategies. Results indicate that
our method improves efficiency of the planning process, which, as a
consequence, improves the sample-efficiency of the overall learning process. On
the whole, we observe that our meta-learned solutions avoid several pathologies
of conventional planning approaches, such as sampling inaccurate transitions
and those that stall credit assignment. We believe these findings could prove
useful, in future work, for designing model-based RL systems at scale.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cost-efficient Active Illumination Camera For Hyper-spectral
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhang, T. M. Sazzad, Yangyang Song, Spencer J. Chang, Ritesh Chowdhry, Tomas Mejia, Anna Hampton, Shelby Kucharski, Stefan Gerber, Barry Tillman, Marcio F. R. Resende, William M. Hammond, Chris H. Wilson, Alina Zare, Sanjeev J. Koppal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyper-spectral imaging has recently gained increasing attention for use in
different applications, including agricultural investigation, ground tracking,
remote sensing and many other. However, the high cost, large physical size and
complicated operation process stop hyperspectral cameras from being employed
for various applications and research fields. In this paper, we introduce a
cost-efficient, compact and easy to use active illumination camera that may
benefit many applications. We developed a fully functional prototype of such
camera. With the hope of helping with agricultural research, we tested our
camera for plant root imaging. In addition, a U-Net model for spectral
reconstruction was trained by using a reference hyperspectral camera's data as
ground truth and our camera's data as input. We demonstrated our camera's
ability to obtain additional information over a typical RGB camera. In
addition, the ability to reconstruct hyperspectral data from multi-spectral
input makes our device compatible to models and algorithms developed for
hyperspectral applications with no modifications required.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BOrg: A Brain Organoid-Based Mitosis <span class="highlight-title">Dataset</span> for Automatic Analysis of
  Brain Diseases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Awais, Mehaboobathunnisa Sahul Hameed, Bidisha Bhattacharya, Orly Reiner, Rao Muhammad Anwer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances have enabled the study of human brain development using brain
organoids derived from stem cells. Quantifying cellular processes like mitosis
in these organoids offers insights into neurodevelopmental disorders, but the
manual analysis is time-consuming, and existing datasets lack specific details
for brain organoid studies. We introduce BOrg, a dataset designed to study
mitotic events in the embryonic development of the brain using confocal
microscopy images of brain organoids. BOrg utilizes an efficient annotation
pipeline with sparse point annotations and techniques that minimize expert
effort, overcoming limitations of standard deep learning approaches on sparse
data. We adapt and benchmark state-of-the-art object detection and cell
counting models on BOrg for detecting and analyzing mitotic cells across
prophase, metaphase, anaphase, and telophase stages. Our results demonstrate
these adapted models significantly improve mitosis analysis efficiency and
accuracy for brain organoid research compared to existing methods. BOrg
facilitates the development of automated tools to quantify statistics like
mitosis rates, aiding mechanistic studies of neurodevelopmental processes and
disorders. Data and code are available at https://github.com/awaisrauf/borg.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking harmless refusals when fine-tuning foundation models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florin Pop, Judd Rosenblatt, Diogo Schwerz de Lucena, Michael Vaiana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the degree to which fine-tuning in Large
Language Models (LLMs) effectively mitigates versus merely conceals undesirable
behavior. Through the lens of semi-realistic role-playing exercises designed to
elicit such behaviors, we explore the response dynamics of LLMs post
fine-tuning interventions. Our methodology involves prompting models for
Chain-of-Thought (CoT) reasoning and analyzing the coherence between the
reasoning traces and the resultant outputs. Notably, we identify a pervasive
phenomenon we term \emph{reason-based deception}, where models either stop
producing reasoning traces or produce seemingly ethical reasoning traces that
belie the unethical nature of their final outputs. We further examine the
efficacy of response strategies (polite refusal versus explicit rebuttal) in
curbing the occurrence of undesired behavior in subsequent outputs of
multi-turn interactions. Our findings reveal that explicit rebuttals
significantly outperform polite refusals in preventing the continuation of
undesired outputs and nearly eliminate reason-based deception, challenging
current practices in model fine-tuning. Accordingly, the two key contributions
of this paper are (1) defining and studying reason-based deception, a new type
of hidden behavior, and (2) demonstrating that rebuttals provide a more robust
response model to harmful requests than refusals, thereby highlighting the need
to reconsider the response strategies in fine-tuning approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 AGI Workshop Poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASCENT: Amplifying Power Side-Channel Resilience via Learning &
  Monte-Carlo Tree Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jitendra Bhandari, Animesh Basak Chowdhury, Ozgur Sinanoglu, Siddharth Garg, Ramesh Karri, Johann Knechtel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Power side-channel (PSC) analysis is pivotal for securing cryptographic
hardware. Prior art focused on securing gate-level netlists obtained as-is from
chip design automation, neglecting all the complexities and potential
side-effects for security arising from the design automation process. That is,
automation traditionally prioritizes power, performance, and area (PPA),
sidelining security. We propose a "security-first" approach, refining the logic
synthesis stage to enhance the overall resilience of PSC countermeasures. We
introduce ASCENT, a learning-and-search-based framework that (i) drastically
reduces the time for post-design PSC evaluation and (ii) explores the
security-vs-PPA design space. Thus, ASCENT enables an efficient exploration of
a large number of candidate netlists, leading to an improvement in PSC
resilience compared to regular PPA-optimized netlists. ASCENT is up to 120x
faster than traditional PSC analysis and yields a 3.11x improvement for PSC
resilience of state-of-the-art PSC countermeasures
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 ACM/IEEE International Conference on Computer-Aided
  Design</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dataless Quadratic Neural Networks for the Maximum Independent Set
  Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ismail Alkhouri, Cedric Le Denmat, Yingjie Li, Cunxi Yu, Jia Liu, Rongrong Wang, Alvaro Velasquez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combinatorial Optimization (CO) plays a crucial role in addressing various
significant problems, among them the challenging Maximum Independent Set (MIS)
problem. In light of recent advancements in deep learning methods, efforts have
been directed towards leveraging data-driven learning approaches, typically
rooted in supervised learning and reinforcement learning, to tackle the NP-hard
MIS problem. However, these approaches rely on labeled datasets, exhibit weak
generalization, and often depend on problem-specific heuristics. Recently,
ReLU-based dataless neural networks were introduced to address combinatorial
optimization problems. This paper introduces a novel dataless quadratic neural
network formulation, featuring a continuous quadratic relaxation for the MIS
problem. Notably, our method eliminates the need for training data by treating
the given MIS instance as a trainable entity. More specifically, the graph
structure and constraints of the MIS instance are used to define the structure
and parameters of the neural network such that training it on a fixed input
provides a solution to the problem, thereby setting it apart from traditional
supervised or reinforcement learning approaches. By employing a gradient-based
optimization algorithm like ADAM and leveraging an efficient off-the-shelf GPU
parallel implementation, our straightforward yet effective approach
demonstrates competitive or superior performance compared to state-of-the-art
learning-based methods. Another significant advantage of our approach is that,
unlike exact and heuristic solvers, the running time of our method scales only
with the number of nodes in the graph, not the number of edges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forward and Backward State Abstractions for Off-policy Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meiling Hao, Pingfan Su, Liyuan Hu, Zoltan Szabo, Qingyuan Zhao, Chengchun Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Off-policy evaluation (OPE) is crucial for evaluating a target policy's
impact offline before its deployment. However, achieving accurate OPE in large
state spaces remains challenging.This paper studies state
abstractions-originally designed for policy learning-in the context of OPE. Our
contributions are three-fold: (i) We define a set of irrelevance conditions
central to learning state abstractions for OPE. (ii) We derive sufficient
conditions for achieving irrelevance in Q-functions and marginalized importance
sampling ratios, the latter obtained by constructing a time-reversed Markov
decision process (MDP) based on the observed MDP. (iii) We propose a novel
two-step procedure that sequentially projects the original state space into a
smaller space, which substantially simplify the sample complexity of OPE
arising from high cardinality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TocBERT: Medical Document Structure Extraction Using Bidirectional
  Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Majd Saleh, Sarra Baghdadi, Stéphane Paquelet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text segmentation holds paramount importance in the field of Natural Language
Processing (NLP). It plays an important role in several NLP downstream tasks
like information retrieval and document summarization. In this work, we propose
a new solution, namely TocBERT, for segmenting texts using bidirectional
transformers. TocBERT represents a supervised solution trained on the detection
of titles and sub-titles from their semantic representations. This task was
formulated as a named entity recognition (NER) problem. The solution has been
applied on a medical text segmentation use-case where the Bio-ClinicalBERT
model is fine-tuned to segment discharge summaries of the MIMIC-III dataset.
The performance of TocBERT has been evaluated on a human-labeled ground truth
corpus of 250 notes. It achieved an F1-score of 84.6% when evaluated on a
linear text segmentation problem and 72.8% on a hierarchical text segmentation
problem. It outperformed a carefully designed rule-based solution, particularly
in distinguishing titles from subtitles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient
  Compile-Time Prompt Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Schnabel, Jennifer Neville
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many modern LLM applications, such as retrieval augmented generation,
prompts have become programs themselves. In these settings, prompt programs are
repeatedly called with different user queries or data instances. A big
practical challenge is optimizing such prompt programs. Recent work has mostly
focused on either simple prompt programs or assumed that the general structure
of a prompt program is fixed.
  We introduce SAMMO, a framework to perform symbolic prompt program search for
compile-time optimizations of prompt programs. SAMMO represents prompt programs
on a symbolic level which allows for a rich set of transformations that can be
searched over during optimization. We show that SAMMO generalizes previous
methods and improves the performance of complex prompts on (1) instruction
tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several
different LLMs. We make all code available open-source at
https://github.com/microsoft/sammo .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hierarchical Neural Framework for Classification and its Explanation
  in Large Unstructured Legal Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10563v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10563v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nishchal Prasad, Mohand Boughanem, Taoufik Dkaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic legal judgment prediction and its explanation suffer from the
problem of long case documents exceeding tens of thousands of words, in
general, and having a non-uniform structure. Predicting judgments from such
documents and extracting their explanation becomes a challenging task, more so
on documents with no structural annotation. We define this problem as "scarce
annotated legal documents" and explore their lack of structural information and
their long lengths with a deep-learning-based classification framework which we
call MESc; "Multi-stage Encoder-based Supervised with-clustering"; for judgment
prediction. We explore the adaptability of LLMs with multi-billion parameters
(GPT-Neo, and GPT-J) to legal texts and their intra-domain(legal) transfer
learning capacity. Alongside this, we compare their performance and
adaptability with MESc and the impact of combining embeddings from their last
layers. For such hierarchical models, we also propose an explanation extraction
algorithm named ORSE; Occlusion sensitivity-based Relevant Sentence Extractor;
based on the input-occlusion sensitivity of the model, to explain the
predictions with the most relevant sentences from the document. We explore
these methods and test their effectiveness with extensive experiments and
ablation studies on legal documents from India, the European Union, and the
United States with the ILDC dataset and a subset of the LexGLUE dataset. MESc
achieves a minimum total performance gain of approximately 2 points over
previous state-of-the-art proposed methods, while ORSE applied on MESc achieves
a total average gain of 50% over the baseline explainability scores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as non archival paper in the The 3rd International Workshop
  on Mining and Learning in the Legal Domain (MLLD-2023) at CIKM 2023,
  Birmingham, United Kingdom. (https://sites.google.com/view/mlld2023/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Modality Program Representation Learning for Electronic Design
  Automation with High-Level Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongyue Qin, Yunsheng Bai, Atefeh Sohrabizadeh, Zijian Ding, Ziniu Hu, Yizhou Sun, Jason Cong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, domain-specific accelerators (DSAs) have gained popularity
for applications such as deep learning and autonomous driving. To facilitate
DSA designs, programmers use high-level synthesis (HLS) to compile a high-level
description written in C/C++ into a design with low-level hardware description
languages that eventually synthesize DSAs on circuits. However, creating a
high-quality HLS design still demands significant domain knowledge,
particularly in microarchitecture decisions expressed as \textit{pragmas}.
Thus, it is desirable to automate such decisions with the help of machine
learning for predicting the quality of HLS designs, requiring a deeper
understanding of the program that consists of original code and pragmas.
Naturally, these programs can be considered as sequence data. In addition,
these programs can be compiled and converted into a control data flow graph
(CDFG). But existing works either fail to leverage both modalities or combine
the two in shallow or coarse ways. We propose ProgSG, a model that allows
interaction between the source code sequence modality and the graph modality in
a deep and fine-grained way. To alleviate the scarcity of labeled designs, a
pre-training method is proposed based on a suite of compiler's data flow
analysis tasks. Experimental results show that ProgSG reduces the RMSE of
design performance predictions by up to $22\%$, and identifies designs with an
average of $1.10\times$ and $1.26\times$ (up to $8.17\times$ and $13.31\times$)
performance improvement in design space exploration (DSE) task compared to HARP
and AutoDSE, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:2305.10838</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Condition Monitoring with Incomplete Data: An Integrated Variational
  Autoencoder and Distance Metric Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05891v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05891v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maryam Ahang, Mostafa Abbasi, Todd Charter, Homayoun Najjaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Condition monitoring of industrial systems is crucial for ensuring safety and
maintenance planning, yet notable challenges arise in real-world settings due
to the limited or non-existent availability of fault samples. This paper
introduces an innovative solution to this problem by proposing a new method for
fault detection and condition monitoring for unseen data. Adopting an approach
inspired by zero-shot learning, our method can identify faults and assign a
relative health index to various operational conditions. Typically, we have
plenty of data on normal operations, some data on compromised conditions, and
very few (if any) samples of severe faults. We use a variational autoencoder to
capture the probabilistic distribution of previously seen and new unseen
conditions. The health status is determined by comparing each sample's
deviation from a normal operation reference distribution in the latent space.
Faults are detected by establishing a threshold for the health indexes,
allowing the model to identify severe, unseen faults with high accuracy, even
amidst noise. We validate our approach using the run-to-failure IMS-bearing
dataset and compare it with other methods. The health indexes generated by our
model closely match the established descriptive model of bearing wear,
attesting to the robustness and reliability of our method. These findings
highlight the potential of our methodology in augmenting fault detection
capabilities within industrial domains, thereby contributing to heightened
safety protocols and optimized maintenance practices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the 2024 IEEE 20th International Conference on Automation
  Science and Engineering (CASE 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Directions of Curvature as an Explanation for Loss of Plasticity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00246v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00246v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Lewandowski, Haruto Tanaka, Dale Schuurmans, Marlos C. Machado
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Loss of plasticity is a phenomenon in which neural networks lose their
ability to learn from new experience. Despite being empirically observed in
several problem settings, little is understood about the mechanisms that lead
to loss of plasticity. In this paper, we offer a consistent explanation for
loss of plasticity: Neural networks lose directions of curvature during
training and that loss of plasticity can be attributed to this reduction in
curvature. To support such a claim, we provide a systematic investigation of
loss of plasticity across continual learning tasks using MNIST, CIFAR-10 and
ImageNet. Our findings illustrate that loss of curvature directions coincides
with loss of plasticity, while also showing that previous explanations are
insufficient to explain loss of plasticity in all settings. Lastly, we show
that regularizers which mitigate loss of plasticity also preserve curvature,
motivating a simple distributional regularizer that proves to be effective
across the problem settings we considered.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Image and Video Processing <span class="chip" style="font-size: 60%">25</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Temporal Sequence Classification and Mathematical Modeling for Cell
  Tracking in Dense 3D Microscopy Videos of Bacterial Biofilms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanjin Taher Toma, Yibo Wang, Andreas Gahlmann, Scott T. Acton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic cell tracking in dense environments is plagued by inaccurate
correspondences and misidentification of parent-offspring relationships. In
this paper, we introduce a novel cell tracking algorithm named DenseTrack,
which integrates deep learning with mathematical model-based strategies to
effectively establish correspondences between consecutive frames and detect
cell division events in crowded scenarios. We formulate the cell tracking
problem as a deep learning-based temporal sequence classification task followed
by solving a constrained one-to-one matching optimization problem exploiting
the classifier's confidence scores. Additionally, we present an
eigendecomposition-based cell division detection strategy that leverages
knowledge of cellular geometry. The performance of the proposed approach has
been evaluated by tracking densely packed cells in 3D time-lapse image
sequences of bacterial biofilm development. The experimental results on
simulated as well as experimental fluorescence image sequences suggest that the
proposed tracking method achieves superior performance in terms of both
qualitative and quantitative evaluation measures compared to recent
state-of-the-art cell tracking approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cost-efficient Active Illumination Camera For Hyper-spectral
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhang, T. M. Sazzad, Yangyang Song, Spencer J. Chang, Ritesh Chowdhry, Tomas Mejia, Anna Hampton, Shelby Kucharski, Stefan Gerber, Barry Tillman, Marcio F. R. Resende, William M. Hammond, Chris H. Wilson, Alina Zare, Sanjeev J. Koppal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyper-spectral imaging has recently gained increasing attention for use in
different applications, including agricultural investigation, ground tracking,
remote sensing and many other. However, the high cost, large physical size and
complicated operation process stop hyperspectral cameras from being employed
for various applications and research fields. In this paper, we introduce a
cost-efficient, compact and easy to use active illumination camera that may
benefit many applications. We developed a fully functional prototype of such
camera. With the hope of helping with agricultural research, we tested our
camera for plant root imaging. In addition, a U-Net model for spectral
reconstruction was trained by using a reference hyperspectral camera's data as
ground truth and our camera's data as input. We demonstrated our camera's
ability to obtain additional information over a typical RGB camera. In
addition, the ability to reconstruct hyperspectral data from multi-spectral
input makes our device compatible to models and algorithms developed for
hyperspectral applications with no modifications required.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Robust</span>ness Testing of Black-Box Models Against CT Degradation Through
  Test-Time Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Highton, Quok Zong Chong, Samuel Finestone, Arian Beqiri, Julia A. Schnabel, Kanwal K. Bhatia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models for medical image segmentation and object detection are
becoming increasingly available as clinical products. However, as details are
rarely provided about the training data, models may unexpectedly fail when
cases differ from those in the training distribution. An approach allowing
potential users to independently test the robustness of a model, treating it as
a black box and using only a few cases from their own site, is key for
adoption. To address this, a method to test the robustness of these models
against CT image quality variation is presented. In this work we present this
framework by demonstrating that given the same training data, the model
architecture and data pre processing greatly affect the robustness of several
frequently used segmentation and object detection methods to simulated CT
imaging artifacts and degradation. Our framework also addresses the concern
about the sustainability of deep learning models in clinical use, by
considering future shifts in image quality due to scanner deterioration or
imaging protocol changes which are not reflected in a limited local test
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BOrg: A Brain Organoid-Based Mitosis <span class="highlight-title">Dataset</span> for Automatic Analysis of
  Brain Diseases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Awais, Mehaboobathunnisa Sahul Hameed, Bidisha Bhattacharya, Orly Reiner, Rao Muhammad Anwer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances have enabled the study of human brain development using brain
organoids derived from stem cells. Quantifying cellular processes like mitosis
in these organoids offers insights into neurodevelopmental disorders, but the
manual analysis is time-consuming, and existing datasets lack specific details
for brain organoid studies. We introduce BOrg, a dataset designed to study
mitotic events in the embryonic development of the brain using confocal
microscopy images of brain organoids. BOrg utilizes an efficient annotation
pipeline with sparse point annotations and techniques that minimize expert
effort, overcoming limitations of standard deep learning approaches on sparse
data. We adapt and benchmark state-of-the-art object detection and cell
counting models on BOrg for detecting and analyzing mitotic cells across
prophase, metaphase, anaphase, and telophase stages. Our results demonstrate
these adapted models significantly improve mitosis analysis efficiency and
accuracy for brain organoid research compared to existing methods. BOrg
facilitates the development of automated tools to quantify statistics like
mitosis rates, aiding mechanistic studies of neurodevelopmental processes and
disorders. Data and code are available at https://github.com/awaisrauf/borg.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-resolution segmentations of the hypothalamus and its subregions for
  training of segmentation models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Livia Rodrigues, Martina Bocchetta, Oula Puonti, Douglas Greve, Ana Carolina Londe, Marcondes França, Simone Appenzeller, Leticia Rittner, Juan Eugenio Iglesias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation of brain structures on magnetic resonance imaging (MRI) is a
highly relevant neuroimaging topic, as it is a prerequisite for different
analyses such as volumetry or shape analysis. Automated segmentation
facilitates the study of brain structures in larger cohorts when compared with
manual segmentation, which is time-consuming. However, the development of most
automated methods relies on large and manually annotated datasets, which limits
the generalizability of these methods. Recently, new techniques using synthetic
images have emerged, reducing the need for manual annotation. Here we provide
HELM, Hypothalamic ex vivo Label Maps, a dataset composed of label maps built
from publicly available ultra-high resolution ex vivo MRI from 10 whole
hemispheres, which can be used to develop segmentation methods using synthetic
data. The label maps are obtained with a combination of manual labels for the
hypothalamic regions and automated segmentations for the rest of the brain, and
mirrored to simulate entire brains. We also provide the pre-processed ex vivo
scans, as this dataset can support future projects to include other structures
after these are manually segmented.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GAPNet: Granularity Attention Network with Anatomy-Prior-Constraint for
  Carotid Artery Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Zhang, Chenggang Lu, Xin-yang Shi, Caifeng Shan, Jiong Zhang, Da Chen, Laurent D. Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Atherosclerosis is a chronic, progressive disease that primarily affects the
arterial walls. It is one of the major causes of cardiovascular disease.
Magnetic Resonance (MR) black-blood vessel wall imaging (BB-VWI) offers crucial
insights into vascular disease diagnosis by clearly visualizing vascular
structures. However, the complex anatomy of the neck poses challenges in
distinguishing the carotid artery (CA) from surrounding structures, especially
with changes like atherosclerosis. In order to address these issues, we propose
GAPNet, which is a consisting of a novel geometric prior deduced from.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALMA: a mathematics-driven approach for determining tuning parameters in
  generalized LASSO problems, with applications to MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianluca Giacchi, Isidoros Iakovidis, Bastien Milani, Matthias Stuber, Micah Murray, Benedetta Franceschiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic Resonance Imaging (MRI) is a powerful technique employed for
non-invasive in vivo visualization of internal structures. Sparsity is often
deployed to accelerate the signal acquisition or overcome the presence of
motion artifacts, improving the quality of image reconstruction. Image
reconstruction algorithms use TV-regularized LASSO (Total Variation-regularized
LASSO) to retrieve the missing information of undersampled signals, by cleaning
the data of noise and while optimizing sparsity. A tuning parameter moderates
the balance between these two aspects; its choice affecting the quality of the
reconstructions. Currently, there is a lack of general deterministic techniques
to choose these parameters, which are oftentimes manually selected and thus
hinder the reliability of the reconstructions. Here, we present ALMA (Algorithm
for Lagrange Multipliers Approximation), an iterative mathematics-inspired
technique that computes tuning parameters for generalized LASSO problems during
MRI reconstruction. We analyze quantitatively the performance of these
parameters for imaging reconstructions via TV-LASSO in an MRI context on
phantoms. Although our study concentrates on TV-LASSO, the techniques developed
here hold significant promise for a wide array of applications. ALMA is not
only adaptable to more generalized LASSO problems but is also robust to
accommodate other forms of regularization beyond total variation. Moreover, it
extends effectively to handle non-Cartesian sampling trajectories, broadening
its utility in complex data reconstruction scenarios. More generally, ALMA
provides a powerful tool for numerically solving constrained optimization
problems across various disciplines, offering a versatile and impactful
solution for advanced computational challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Latent Stain Adaption for Digital Pathology <span class="chip">MICCAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Reisenbüchler, Lucas Luttner, Nadine S. Schaadt, Friedrich Feuerhake, Dorit Merhof
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In digital pathology, deep learning (DL) models for tasks such as
segmentation or tissue classification are known to suffer from domain shifts
due to different staining techniques. Stain adaptation aims to reduce the
generalization error between different stains by training a model on source
stains that generalizes to target stains. Despite the abundance of target stain
data, a key challenge is the lack of annotations. To address this, we propose a
joint training between artificially labeled and unlabeled data including all
available stained images called Unsupervised Latent Stain Adaption (ULSA). Our
method uses stain translation to enrich labeled source images with synthetic
target images in order to increase supervised signals. Moreover, we leverage
unlabeled target stain images using stain-invariant feature consistency
learning. With ULSA we present a semi-supervised strategy for efficient stain
adaption without access to annotated target stain data. Remarkably, ULSA is
task agnostic in patch-level analysis for whole slide images (WSIs). Through
extensive evaluation on external datasets, we demonstrate that ULSA achieves
state-of-the-art (SOTA) performance in kidney tissue segmentation and breast
cancer classification across a spectrum of staining variations. Our findings
suggest that ULSA is an important framework towards stain adaption in digital
pathology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in MICCAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMRxRecon2024: A Multi-Modality, Multi-View K-Space <span class="highlight-title">Dataset</span> Boosting
  Universal Machine Learning for Accelerated Cardiac MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi Wang, Fanwen Wang, Chen Qin, Jun Lyu, Ouyang Cheng, Shuo Wang, Yan Li, Mengyao Yu, Haoyu Zhang, Kunyuan Guo, Zhang Shi, Qirong Li, Ziqiang Xu, Yajing Zhang, Hao Li, Sha Hua, Binghua Chen, Longyu Sun, Mengting Sun, Qin Li, Ying-Hua Chu, Wenjia Bai, Jing Qin, Xiahai Zhuang, Claudia Prieto, Alistair Young, Michael Markl, He Wang, Lianming Wu, Guang Yang, Xiaobo Qu, Chengyan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiac magnetic resonance imaging (MRI) has emerged as a clinically
gold-standard technique for diagnosing cardiac diseases, thanks to its ability
to provide diverse information with multiple modalities and anatomical views.
Accelerated cardiac MRI is highly expected to achieve time-efficient and
patient-friendly imaging, and then advanced image reconstruction approaches are
required to recover high-quality, clinically interpretable images from
undersampled measurements. However, the lack of publicly available cardiac MRI
k-space dataset in terms of both quantity and diversity has severely hindered
substantial technological progress, particularly for data-driven artificial
intelligence. Here, we provide a standardized, diverse, and high-quality
CMRxRecon2024 dataset to facilitate the technical development, fair evaluation,
and clinical transfer of cardiac MRI reconstruction approaches, towards
promoting the universal frameworks that enable fast and robust reconstructions
across different cardiac MRI protocols in clinical practice. To the best of our
knowledge, the CMRxRecon2024 dataset is the largest and most diverse publicly
available cardiac k-space dataset. It is acquired from 330 healthy volunteers,
covering commonly used modalities, anatomical views, and acquisition
trajectories in clinical cardiac MRI workflows. Besides, an open platform with
tutorials, benchmarks, and data processing tools is provided to facilitate data
usage, advanced method development, and fair performance evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMR-Mamba: Multi-Contrast MRI Reconstruction with Mamba and
  Spatial-Frequency Information Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Zou, Lanqing Liu, Qi Chen, Shujun Wang, Xiaohan Xing, Jing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-contrast MRI acceleration has become prevalent in MR imaging, enabling
the reconstruction of high-quality MR images from under-sampled k-space data of
the target modality, using guidance from a fully-sampled auxiliary modality.
The main crux lies in efficiently and comprehensively integrating complementary
information from the auxiliary modality. Existing methods either suffer from
quadratic computational complexity or fail to capture long-range correlated
features comprehensively. In this work, we propose MMR-Mamba, a novel framework
that achieves comprehensive integration of multi-contrast features through
Mamba and spatial-frequency information fusion. Firstly, we design the
\textit{Target modality-guided Cross Mamba} (TCM) module in the spatial domain,
which maximally restores the target modality information by selectively
absorbing useful information from the auxiliary modality. Secondly, leveraging
global properties of the Fourier domain, we introduce the \textit{Selective
Frequency Fusion} (SFF) module to efficiently integrate global information in
the frequency domain and recover high-frequency signals for the reconstruction
of structure details. Additionally, we present the \textit{Adaptive
Spatial-Frequency Fusion} (ASFF) module, which enhances fused features by
supplementing less informative features from one domain with corresponding
features from the other domain. These innovative strategies ensure efficient
feature fusion across spatial and frequency domains, avoiding the introduction
of redundant information and facilitating the reconstruction of high-quality
target images. Extensive experiments on the BraTS and fastMRI knee datasets
demonstrate the superiority of the proposed MMR-Mamba over state-of-the-art MRI
reconstruction methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classification of Carotid Plaque with Jellyfish Sign Through
  Convolutional and Recurrent Neural Networks Utilizing Plaque Surface Edges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takeshi Yoshidomi, Shinji Kume, Hiroaki Aizawa, Akira Furui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In carotid arteries, plaque can develop as localized elevated lesions. The
Jellyfish sign, marked by fluctuating plaque surfaces with blood flow
pulsation, is a dynamic characteristic of these plaques that has recently
attracted attention. Detecting this sign is vital, as it is often associated
with cerebral infarction. This paper proposes an ultrasound video-based
classification method for the Jellyfish sign, using deep neural networks. The
proposed method first preprocesses carotid ultrasound videos to separate the
movement of the vascular wall from plaque movements. These preprocessed videos
are then combined with plaque surface information and fed into a deep learning
model comprising convolutional and recurrent neural networks, enabling the
efficient classification of the Jellyfish sign. The proposed method was
verified using ultrasound video images from 200 patients. Ablation studies
demonstrated the effectiveness of each component of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures, accepted at IEEE EMBC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shorter SPECT Scans Using Self-supervised Coordinate Learning to
  Synthesize Skipped Projection Views 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongyu Li, Yixuan Jia, Xiaojian Xu, Jason Hu, Jeffrey A. Fessler, Yuni K. Dewaraja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: This study addresses the challenge of extended SPECT imaging
duration under low-count conditions, as encountered in Lu-177 SPECT imaging, by
developing a self-supervised learning approach to synthesize skipped SPECT
projection views, thus shortening scan times in clinical settings. Methods: We
employed a self-supervised coordinate-based learning technique, adapting the
neural radiance field (NeRF) concept in computer vision to synthesize
under-sampled SPECT projection views. For each single scan, we used
self-supervised coordinate learning to estimate skipped SPECT projection views.
The method was tested with various down-sampling factors (DFs=2, 4, 8) on both
Lu-177 phantom SPECT/CT measurements and clinical SPECT/CT datasets, from 11
patients undergoing Lu-177 DOTATATE and 6 patients undergoing Lu-177 PSMA-617
radiopharmaceutical therapy. Results: For SPECT reconstructions, our method
outperformed the use of linearly interpolated projections and partial
projection views in relative contrast-to-noise-ratios (RCNR) averaged across
different downsampling factors: 1) DOTATATE: 83% vs. 65% vs. 67% for lesions
and 86% vs. 70% vs. 67% for kidney, 2) PSMA: 76% vs. 69% vs. 68% for lesions
and 75% vs. 55% vs. 66% for organs, including kidneys, lacrimal glands, parotid
glands, and submandibular glands. Conclusion: The proposed method enables
reduction in acquisition time (by factors of 2, 4, or 8) while maintaining
quantitative accuracy in clinical SPECT protocols by allowing for the
collection of fewer projections. Importantly, the self-supervised nature of
this NeRF-based approach eliminates the need for extensive training data,
instead learning from each patient's projection data alone. The reduction in
acquisition time is particularly relevant for imaging under low-count
conditions and for protocols that require multiple-bed positions such as
whole-body imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 5568 words</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semi-supervised variational autoencoder for cell feature extraction in
  multiplexed immunofluorescence images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piumi Sandarenu, Julia Chen, Iveta Slapetova, Lois Browne, Peter H. Graham, Alexander Swarbrick, Ewan K. A. Millar, Yang Song, Erik Meijering
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in digital imaging technologies have sparked increased interest
in using multiplexed immunofluorescence (mIF) images to visualise and identify
the interactions between specific immunophenotypes with the tumour
microenvironment at the cellular level. Current state-of-the-art multiplexed
immunofluorescence image analysis pipelines depend on cell feature
representations characterised by morphological and stain intensity-based
metrics generated using simple statistical and machine learning-based tools.
However, these methods are not capable of generating complex representations of
cells. We propose a deep learning-based cell feature extraction model using a
variational autoencoder with supervision using a latent subspace to extract
cell features in mIF images. We perform cell phenotype classification using a
cohort of more than 44,000 multiplexed immunofluorescence cell image patches
extracted across 1,093 tissue microarray cores of breast cancer patients, to
demonstrate the success of our model against current and alternative methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13040v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13040v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Jung Ling, Salomé Bru, Julia Puig, Florian Vixège, Simon Mendez, Franck Nicoud, Pierre-Yves Courand, Olivier Bernard, Damien Garcia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify
color Doppler in cardiac imaging. In this study, we propose novel alternatives
to the traditional iVFM optimization scheme by utilizing physics-informed
neural networks (PINNs) and a physics-guided nnU-Net-based supervised approach.
When evaluated on simulated color Doppler images derived from a
patient-specific computational fluid dynamics model and in vivo Doppler
acquisitions, both approaches demonstrate comparable reconstruction performance
to the original iVFM algorithm. The efficiency of PINNs is boosted through
dual-stage optimization and pre-optimized weights. On the other hand, the
nnU-Net method excels in generalizability and real-time capabilities. Notably,
nnU-Net shows superior robustness on sparse and truncated Doppler data while
maintaining independence from explicit boundary conditions. Overall, our
results highlight the effectiveness of these methods in reconstructing
intraventricular vector blood flow. The study also suggests potential
applications of PINNs in ultrafast color Doppler imaging and the incorporation
of fluid dynamics equations to derive biomarkers for cardiovascular diseases
based on blood flow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, accepted for publication in IEEE TUFFC; camera ready
  corrections, corrected acknowledgments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $μ$GUIDE: a framework for quantitative imaging via generalized
  uncertainty-driven inference using deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17293v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17293v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maëliss Jallais, Marco Palombo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes $\mu$GUIDE: a general Bayesian framework to estimate
posterior distributions of tissue microstructure parameters from any given
biophysical model or MRI signal representation, with exemplar demonstration in
diffusion-weighted MRI. Harnessing a new deep learning architecture for
automatic signal feature selection combined with simulation-based inference and
efficient sampling of the posterior distributions, $\mu$GUIDE bypasses the high
computational and time cost of conventional Bayesian approaches and does not
rely on acquisition constraints to define model-specific summary statistics.
The obtained posterior distributions allow to highlight degeneracies present in
the model definition and quantify the uncertainty and ambiguity of the
estimated parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shortcut Learning in Medical Image Segmentation <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manxi Lin, Nina Weng, Kamil Mikolaj, Zahra Bashir, Morten Bo Søndergaard Svendsen, Martin Tolsgaard, Anders Nymark Christensen, Aasa Feragen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shortcut learning is a phenomenon where machine learning models prioritize
learning simple, potentially misleading cues from data that do not generalize
well beyond the training set. While existing research primarily investigates
this in the realm of image classification, this study extends the exploration
of shortcut learning into medical image segmentation. We demonstrate that
clinical annotations such as calipers, and the combination of zero-padded
convolutions and center-cropped training sets in the dataset can inadvertently
serve as shortcuts, impacting segmentation accuracy. We identify and evaluate
the shortcut learning on two different but common medical image segmentation
tasks. In addition, we suggest strategies to mitigate the influence of shortcut
learning and improve the generalizability of the segmentation models. By
uncovering the presence and implications of shortcuts in medical image
segmentation, we provide insights and methodologies for evaluating and
overcoming this pervasive challenge and call for attention in the community for
shortcuts in segmentation. Our code is public at
https://github.com/nina-weng/shortcut_skinseg .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, accepted at MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Deep Automotive Radar Detector using the RaDelft <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04723v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04723v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ignacio Roldan, Andras Palffy, Julian F. P. Kooij, Dariu M. Gavrila, Francesco Fioranelli, Alexander Yarovoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection of multiple extended targets in complex environments using
high-resolution automotive radar is considered. A data-driven approach is
proposed where unlabeled synchronized lidar data is used as ground truth to
train a neural network with only radar data as input. To this end, the novel,
large-scale, real-life, and multi-sensor RaDelft dataset has been recorded
using a demonstrator vehicle in different locations in the city of Delft. The
dataset, as well as the documentation and example code, is publicly available
for those researchers in the field of automotive radar or machine perception.
The proposed data-driven detector is able to generate lidar-like point clouds
using only radar data from a high-resolution system, which preserves the shape
and size of extended targets. The results are compared against conventional
CFAR detectors as well as variations of the method to emulate the available
approaches in the literature, using the probability of detection, the
probability of false alarm, and the Chamfer distance as performance metrics.
Moreover, an ablation study was carried out to assess the impact of Doppler and
temporal information on detection performance. The proposed method outperforms
the different baselines in terms of Chamfer distance, achieving a reduction of
75% against conventional CFAR detectors and 10% against the modified
state-of-the-art deep learning-based approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at IEEE Transaction on Radar Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stable <span class="highlight-title">Diffusion</span> Segmentation for Biomedical Images with Single-step
  Reverse Process <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18361v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18361v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Lin, Zhiguang Chen, Zhonghao Yan, Weijiang Yu, Fudan Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated their effectiveness across various
generative tasks. However, when applied to medical image segmentation, these
models encounter several challenges, including significant resource and time
requirements. They also necessitate a multi-step reverse process and multiple
samples to produce reliable predictions. To address these challenges, we
introduce the first latent diffusion segmentation model, named SDSeg, built
upon stable diffusion (SD). SDSeg incorporates a straightforward latent
estimation strategy to facilitate a single-step reverse process and utilizes
latent fusion concatenation to remove the necessity for multiple samples.
Extensive experiments indicate that SDSeg surpasses existing state-of-the-art
methods on five benchmark datasets featuring diverse imaging modalities.
Remarkably, SDSeg is capable of generating stable predictions with a solitary
reverse step and sample, epitomizing the model's stability as implied by its
name. The code is available at
https://github.com/lin-tianyu/Stable-Diffusion-Seg
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI 2024. Code and citation info see
  https://github.com/lin-tianyu/Stable-Diffusion-Seg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Uncertainty Estimation by Hamiltonian Monte Carlo: Applications
  to Cardiac MRI Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02311v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02311v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yidong Zhao, Joao Tourais, Iain Pierce, Christian Nitsche, Thomas A. Treibel, Sebastian Weingärtner, Artur M. Schweidtmann, Qian Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL)-based methods have achieved state-of-the-art performance
for many medical image segmentation tasks. Nevertheless, recent studies show
that deep neural networks (DNNs) can be miscalibrated and overconfident,
leading to "silent failures" that are risky for clinical applications. Bayesian
DL provides an intuitive approach to DL failure detection, based on posterior
probability estimation. However, the posterior is intractable for large medical
image segmentation DNNs. To tackle this challenge, we propose a Bayesian
learning framework using Hamiltonian Monte Carlo (HMC), tempered by cold
posterior (CP) to accommodate medical data augmentation, named HMC-CP. For HMC
computation, we further propose a cyclical annealing strategy, capturing both
local and global geometries of the posterior distribution, enabling highly
efficient Bayesian DNN training with the same computational budget as training
a single DNN. The resulting Bayesian DNN outputs an ensemble segmentation along
with the segmentation uncertainty. We evaluate the proposed HMC-CP extensively
on cardiac magnetic resonance image (MRI) segmentation, using in-domain
steady-state free precession (SSFP) cine images as well as out-of-domain
datasets of quantitative T1 and T2 mapping. Our results show that the proposed
method improves both segmentation accuracy and uncertainty estimation for in-
and out-of-domain data, compared with well-established baseline methods such as
Monte Carlo Dropout and Deep Ensembles. Additionally, we establish a conceptual
link between HMC and the commonly known stochastic gradient descent (SGD) and
provide general insight into the uncertainty of DL. This uncertainty is
implicitly encoded in the training dynamics but often overlooked. With reliable
uncertainty estimation, our method provides a promising direction toward
trustworthy DL in clinical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2024:011</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards AI Lesion Tracking in PET/CT Imaging: A Siamese-based CNN
  Pipeline applied on PSMA PET/CT Scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09327v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09327v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan P. Hein, Manuel Schultheiss, Andrei Gafita, Raphael Zaum, Farid Yagubbayli, Robert Tauber, Isabel Rauscher, Matthias Eiber, Franz Pfeiffer, Wolfgang A. Weber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing tumor response to systemic therapies is one of the main
applications of PET/CT. Routinely, only a small subset of index lesions out of
multiple lesions is analyzed. However, this operator dependent selection may
bias the results due to possible significant inter-metastatic heterogeneity of
response to therapy. Automated, AI based approaches for lesion tracking hold
promise in enabling the analysis of many more lesions and thus providing a
better assessment of tumor response. This work introduces a Siamese CNN
approach for lesion tracking between PET/CT scans. Our approach is applied on
the laborious task of tracking a high number of bone lesions in full-body
baseline and follow-up [68Ga]Ga- or [18F]F-PSMA PET/CT scans after two cycles
of [177Lu]Lu-PSMA therapy of metastatic castration resistant prostate cancer
patients. Data preparation includes lesion segmentation and affine
registration. Our algorithm extracts suitable lesion patches and forwards them
into a Siamese CNN trained to classify the lesion patch pairs as corresponding
or non-corresponding lesions. Experiments have been performed with different
input patch types and a Siamese network in 2D and 3D. The CNN model
successfully learned to classify lesion assignments, reaching a lesion tracking
accuracy of 83 % in its best configuration with an AUC = 0.91. For remaining
lesions the pipeline accomplished a re-identification rate of 89 %. We proved
that a CNN may facilitate the tracking of multiple lesions in PSMA PET/CT
scans. Future clinical studies are necessary if this improves the prediction of
the outcome of therapies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 9 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LiDAR Depth Map Guided Image Compression Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06517v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06517v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Gnutti, Stefano Della Fiore, Mattia Savardi, Yi-Hsin Chen, Riccardo Leonardi, Wen-Hsiao Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The incorporation of LiDAR technology into some high-end smartphones has
unlocked numerous possibilities across various applications, including
photography, image restoration, augmented reality, and more. In this paper, we
introduce a novel direction that harnesses LiDAR depth maps to enhance the
compression of the corresponding RGB camera images. To the best of our
knowledge, this represents the initial exploration in this particular research
direction. Specifically, we propose a Transformer-based learned image
compression system capable of achieving variable-rate compression using a
single model while utilizing the LiDAR depth map as supplementary information
for both the encoding and decoding processes. Experimental results demonstrate
that integrating LiDAR yields an average PSNR gain of 0.83 dB and an average
bitrate reduction of 16% as compared to its absence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continuous 3D Myocardial Motion Tracking via Echocardiography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02792v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02792v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengkang Shen, Hao Zhu, You Zhou, Yu Liu, Si Yi, Lili Dong, Weipeng Zhao, David J. Brady, Xun Cao, Zhan Ma, Yi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Myocardial motion tracking stands as an essential clinical tool in the
prevention and detection of cardiovascular diseases (CVDs), the foremost cause
of death globally. However, current techniques suffer from incomplete and
inaccurate motion estimation of the myocardium in both spatial and temporal
dimensions, hindering the early identification of myocardial dysfunction. To
address these challenges, this paper introduces the Neural Cardiac Motion Field
(NeuralCMF). NeuralCMF leverages implicit neural representation (INR) to model
the 3D structure and the comprehensive 6D forward/backward motion of the heart.
This method surpasses pixel-wise limitations by offering the capability to
continuously query the precise shape and motion of the myocardium at any
specific point throughout the cardiac cycle, enhancing the detailed analysis of
cardiac dynamics beyond traditional speckle tracking. Notably, NeuralCMF
operates without the need for paired datasets, and its optimization is
self-supervised through the physics knowledge priors in both space and time
dimensions, ensuring compatibility with both 2D and 3D echocardiogram video
inputs. Experimental validations across three representative datasets support
the robustness and innovative nature of the NeuralCMF, marking significant
advantages over existing state-of-the-art methods in cardiac imaging and motion
tracking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PlaNet-S: Automatic Semantic Segmentation of Placenta 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11580v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11580v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shinnosuke Yamamoto, Isso Saito, Eichi Takaya, Ayaka Harigai, Tomomi Sato, Tomoya Kobayashi, Kei Takase, Takuya Ueda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  [Purpose] To develop a fully automated semantic placenta segmentation model
that integrates the U-Net and SegNeXt architectures through ensemble learning.
[Methods] A total of 218 pregnant women with suspected placental anomalies who
underwent magnetic resonance imaging (MRI) were enrolled, yielding 1090
annotated images for developing a deep learning model for placental
segmentation. The images were standardized and divided into training and test
sets. The performance of PlaNet-S, which integrates U-Net and SegNeXt within an
ensemble framework, was assessed using Intersection over Union (IoU) and
counting connected components (CCC) against the U-Net model. [Results] PlaNet-S
had significantly higher IoU (0.73 +/- 0.13) than that of U-Net (0.78 +/-
0.010) (p<0.01). The CCC for PlaNet-S was significantly higher than that for
U-Net (p<0.01), matching the ground truth in 86.0\% and 56.7\% of the cases,
respectively. [Conclusion]PlaNet-S performed better than the traditional U-Net
in placental segmentation tasks. This model addresses the challenges of
time-consuming physician-assisted manual segmentation and offers the potential
for diverse applications in placental imaging analyses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, Shinnosuke Yamamoto and Isso Saito equally
  contributed to this work. In the original submission, there was a
  typographical error in the reported standard deviation for the Intersection
  over Union (IoU) values of the PlaNet-S model. The standard deviation was
  incorrectly listed as 0.01 instead of the correct value of 0.1. This has been
  corrected in the revised version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Twofold Structured Features-Based Siamese Network for Infrared Target
  Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16676v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16676v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Jie Yan, Yun-Kai Xu, Qian Chen, Xiao-Fang Kong, Guo-Hua Gu, A-Jun Shao, Min-Jie Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, infrared target tracking has been a critical technology in the
field of computer vision and has many applications, such as motion analysis,
pedestrian surveillance, intelligent detection, and so forth. Unfortunately,
due to the lack of color, texture and other detailed information, tracking
drift often occurs when the tracker encounters infrared targets that vary in
size or shape. To address this issue, we present a twofold structured
features-based Siamese network for infrared target tracking. First of all, in
order to improve the discriminative capacity for infrared targets, a novel
feature fusion network is proposed to fuse both shallow spatial information and
deep semantic information into the extracted features in a comprehensive
manner. Then, a multi-template update module based on template update mechanism
is designed to effectively deal with interferences from target appearance
changes which are prone to cause early tracking failures. Finally, both
qualitative and quantitative experiments are carried out on VOT-TIR 2016
dataset, which demonstrates that our method achieves the balance of promising
tracking performance and real-time tracking speed against other out-of-the-art
trackers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages,9 figures,references added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FDDM: Unsupervised Medical Image Translation with a Frequency-Decoupled
  <span class="highlight-title">Diffusion</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12070v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12070v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunxiang Li, Hua-Chieh Shao, Xiaoxue Qian, You Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated significant potential in producing
high-quality images in medical image translation to aid disease diagnosis,
localization, and treatment. Nevertheless, current diffusion models have
limited success in achieving faithful image translations that can accurately
preserve the anatomical structures of medical images, especially for unpaired
datasets. The preservation of structural and anatomical details is essential to
reliable medical diagnosis and treatment planning, as structural mismatches can
lead to disease misidentification and treatment errors. In this study, we
introduce the Frequency Decoupled Diffusion Model (FDDM) for MR-to-CT
conversion. FDDM first obtains the anatomical information of the CT image from
the MR image through an initial conversion module. This anatomical information
then guides a subsequent diffusion model to generate high-quality CT images.
Our diffusion model uses a dual-path reverse diffusion process for
low-frequency and high-frequency information, achieving a better balance
between image quality and anatomical accuracy. We extensively evaluated FDDM
using public datasets for brain MR-to-CT and pelvis MR-to-CT translations,
demonstrating its superior performance to other GAN-based, VAE-based, and
diffusion-based models. The evaluation metrics included Frechet Inception
Distance (FID), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity
Index Measure (SSIM). FDDM achieved the best scores on all metrics for both
datasets, particularly excelling in FID, with scores of 25.9 for brain data and
29.2 for pelvis data, significantly outperforming other methods. These results
demonstrate that FDDM can generate high-quality target domain images while
maintaining the accuracy of translated anatomical structures.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-06-26T00:00:00Z">2024-06-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Image and Video Processing <span class="chip" style="font-size: 60%">25</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessment of Clonal Hematopoiesis of Indeterminate Potential from
  Cardiac Magnetic Resonance Imaging using Deep Learning in a Cardio-oncology
  Population 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangeon Ryu, Shawn Ahn, Jeacy Espinoza, Alokkumar Jha, Stephanie Halene, James S. Duncan, Jennifer M Kwan, Nicha C. Dvornek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: We propose a novel method to identify who may likely have clonal
hematopoiesis of indeterminate potential (CHIP), a condition characterized by
the presence of somatic mutations in hematopoietic stem cells without
detectable hematologic malignancy, using deep learning techniques. Methods: We
developed a convolutional neural network (CNN) to predict CHIP status using 4
different views from standard delayed gadolinium-enhanced cardiac magnetic
resonance imaging (CMR). We used 5-fold cross validation on 82 cardio-oncology
patients to assess the performance of our model. Different algorithms were
compared to find the optimal patient-level prediction method using the
image-level CNN predictions. Results: We found that the best model had an area
under the receiver operating characteristic curve of 0.85 and an accuracy of
82%. Conclusions: We conclude that a deep learning-based diagnostic approach
for CHIP using CMR is promising.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IDA-UIE: An Iterative Framework for Deep Network-based Degradation Aware
  Underwater Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranjali Singh, Prithwijit Guha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underwater image quality is affected by fluorescence, low illumination,
absorption, and scattering. Recent works in underwater image enhancement have
proposed different deep network architectures to handle these problems. Most of
these works have proposed a single network to handle all the challenges. We
believe that deep networks trained for specific conditions deliver better
performance than a single network learned from all degradation cases.
Accordingly, the first contribution of this work lies in the proposal of an
iterative framework where a single dominant degradation condition is identified
and resolved. This proposal considers the following eight degradation
conditions -- low illumination, low contrast, haziness, blurred image, presence
of noise and color imbalance in three different channels. A deep network is
designed to identify the dominant degradation condition. Accordingly, an
appropriate deep network is selected for degradation condition-specific
enhancement. The second contribution of this work is the construction of
degradation condition specific datasets from good quality images of two
standard datasets (UIEB and EUVP). This dataset is used to learn the condition
specific enhancement networks. The proposed approach is found to outperform
nine baseline methods on UIEB and EUVP datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Repeat and Concatenate: 2D to 3D Image Translation with 3D to 3D
  <span class="highlight-title">Generative</span> Modeling <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abril Corona-Figueroa, Hubert P. H. Shum, Chris G. Willcocks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates a 2D to 3D image translation method with a
straightforward technique, enabling correlated 2D X-ray to 3D CT-like
reconstruction. We observe that existing approaches, which integrate
information across multiple 2D views in the latent space, lose valuable signal
information during latent encoding. Instead, we simply repeat and concatenate
the 2D views into higher-channel 3D volumes and approach the 3D reconstruction
challenge as a straightforward 3D to 3D generative modeling problem,
sidestepping several complex modeling issues. This method enables the
reconstructed 3D volume to retain valuable information from the 2D inputs,
which are passed between channel states in a Swin UNETR backbone. Our approach
applies neural optimal transport, which is fast and stable to train,
effectively integrating signal information across multiple views without the
requirement for precise alignment; it produces non-collapsed reconstructions
that are highly faithful to the 2D views, even after limited training. We
demonstrate correlated results, both qualitatively and quantitatively, having
trained our model on a single dataset and evaluated its generalization ability
across six datasets, including out-of-distribution samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPRW 2024 - DCA in MI; Best Paper Award</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Reducing Activity with Distillation and Regularization for Energy
  Efficient Spiking Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Louis, Benoit Miramond, Alain Pegatoquet, Adrien Girard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interest in spiking neural networks (SNNs) has been growing steadily,
promising an energy-efficient alternative to formal neural networks (FNNs),
commonly known as artificial neural networks (ANNs). Despite increasing
interest, especially for Edge applications, these event-driven neural networks
suffered from their difficulty to be trained compared to FNNs. To alleviate
this problem, a number of innovative methods have been developed to provide
performance more or less equivalent to that of FNNs. However, the spiking
activity of a network during inference is usually not considered. While SNNs
may usually have performance comparable to that of FNNs, it is often at the
cost of an increase of the network's activity, thus limiting the benefit of
using them as a more energy-efficient solution.
  In this paper, we propose to leverage Knowledge Distillation (KD) for SNNs
training with surrogate gradient descent in order to optimize the trade-off
between performance and spiking activity. Then, after understanding why KD led
to an increase in sparsity, we also explored Activations regularization and
proposed a novel method with Logits Regularization. These approaches, validated
on several datasets, clearly show a reduction in network spiking activity
(-26.73% on GSC and -14.32% on CIFAR-10) while preserving accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-modal Evidential Fusion Network for Trusted PET/CT Tumor
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Qi, Li Lin, Jiajun Wang, Jingya Zhang, Bin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of tumors in PET/CT images is important in
computer-aided diagnosis and treatment of cancer. The key issue of such a
segmentation problem lies in the effective integration of complementary
information from PET and CT images. However, the quality of PET and CT images
varies widely in clinical settings, which leads to uncertainty in the modality
information extracted by networks. To take the uncertainty into account in
multi-modal information fusion, this paper proposes a novel Multi-modal
Evidential Fusion Network (MEFN) comprising a Cross-Modal Feature Learning
(CFL) module and a Multi-modal Trusted Fusion (MTF) module. The CFL module
reduces the domain gap upon modality conversion and highlights common tumor
features, thereby alleviating the needs of the segmentation module to handle
modality specificity. The MTF module utilizes mutual attention mechanisms and
an uncertainty calibrator to fuse modality features based on modality
uncertainty and then fuse the segmentation results under the guidance of
Dempster-Shafer Theory. Besides, a new uncertainty perceptual loss is
introduced to force the model focusing on uncertain features and hence improve
its ability to extract trusted modality information. Extensive comparative
experiments are conducted on two publicly available PET/CT datasets to evaluate
the performance of our proposed method whose results demonstrate that our MEFN
significantly outperforms state-of-the-art methods with improvements of 2.15%
and 3.23% in DSC scores on the AutoPET dataset and the Hecktor dataset,
respectively. More importantly, our model can provide radiologists with
credible uncertainty of the segmentation results for their decision in
accepting or rejecting the automatic segmentation results, which is
particularly important for clinical applications. Our code will be available at
https://github.com/QPaws/MEFN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatial-temporal Hierarchical Reinforcement Learning for Interpretable
  Pathology Image Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenting Chen, Jie Liu, Tommy W. S. Chow, Yixuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pathology image are essential for accurately interpreting lesion cells in
cytopathology screening, but acquiring high-resolution digital slides requires
specialized equipment and long scanning times. Though super-resolution (SR)
techniques can alleviate this problem, existing deep learning models recover
pathology image in a black-box manner, which can lead to untruthful biological
details and misdiagnosis. Additionally, current methods allocate the same
computational resources to recover each pixel of pathology image, leading to
the sub-optimal recovery issue due to the large variation of pathology image.
In this paper, we propose the first hierarchical reinforcement learning
framework named Spatial-Temporal hierARchical Reinforcement Learning (STAR-RL),
mainly for addressing the aforementioned issues in pathology image
super-resolution problem. We reformulate the SR problem as a Markov decision
process of interpretable operations and adopt the hierarchical recovery
mechanism in patch level, to avoid sub-optimal recovery. Specifically, the
higher-level spatial manager is proposed to pick out the most corrupted patch
for the lower-level patch worker. Moreover, the higher-level temporal manager
is advanced to evaluate the selected patch and determine whether the
optimization should be stopped earlier, thereby avoiding the over-processed
problem. Under the guidance of spatial-temporal managers, the lower-level patch
worker processes the selected patch with pixel-wise interpretable actions at
each time step. Experimental results on medical images degraded by different
kernels show the effectiveness of STAR-RL. Furthermore, STAR-RL validates the
promotion in tumor diagnosis with a large margin and shows generalizability
under various degradations. The source code is available at
https://github.com/CUHK-AIM-Group/STAR-RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE TRANSACTIONS ON MEDICAL IMAGING (TMI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Deepfake Attribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sowdagar Mahammad Shahid, Sudev Kumar Padhi, Umesh Kashyap, Sk. Subidh Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The landscape of fake media creation changed with the introduction of
Generative Adversarial Networks (GAN s). Fake media creation has been on the
rise with the rapid advances in generation technology, leading to new
challenges in Detecting fake media. A fundamental characteristic of GAN s is
their sensitivity to parameter initialization, known as seeds. Each distinct
seed utilized during training leads to the creation of unique model instances,
resulting in divergent image outputs despite employing the same architecture.
This means that even if we have one GAN architecture, it can produce countless
variations of GAN models depending on the seed used. Existing methods for
attributing deepfakes work well only if they have seen the specific GAN model
during training. If the GAN architectures are retrained with a different seed,
these methods struggle to attribute the fakes. This seed dependency issue made
it difficult to attribute deepfakes with existing methods. We proposed a
generalized deepfake attribution network (GDA-N et) to attribute fake images to
their respective GAN architectures, even if they are generated from a retrained
version of the GAN architecture with a different seed (cross-seed) or from the
fine-tuned version of the existing GAN model. Extensive experiments on
cross-seed and fine-tuned data of GAN models show that our method is highly
effective compared to existing methods. We have provided the source code to
validate our results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Generative</span> artificial intelligence in ophthalmology: multimodal retinal
  images for the diagnosis of Alzheimer's disease with convolutional neural
  networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        I. R. Slootweg, M. Thach, K. R. Curro-Tafili, F. D. Verbraak, F. H. Bouwman, Y. A. L. Pijnenburg, J. F. Boer, J. H. P. de Kwisthout, L. Bagheriye, P. J. González
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background/Aim. This study aims to predict Amyloid Positron Emission
Tomography (AmyloidPET) status with multimodal retinal imaging and
convolutional neural networks (CNNs) and to improve the performance through
pretraining with synthetic data. Methods. Fundus autofluorescence, optical
coherence tomography (OCT), and OCT angiography images from 328 eyes of 59
AmyloidPET positive subjects and 108 AmyloidPET negative subjects were used for
classification. Denoising Diffusion Probabilistic Models (DDPMs) were trained
to generate synthetic images and unimodal CNNs were pretrained on synthetic
data and finetuned on real data or trained solely on real data. Multimodal
classifiers were developed to combine predictions of the four unimodal CNNs
with patient metadata. Class activation maps of the unimodal classifiers
provided insight into the network's attention to inputs. Results. DDPMs
generated diverse, realistic images without memorization. Pretraining unimodal
CNNs with synthetic data improved AUPR at most from 0.350 to 0.579. Integration
of metadata in multimodal CNNs improved AUPR from 0.486 to 0.634, which was the
best overall best classifier. Class activation maps highlighted relevant
retinal regions which correlated with AD. Conclusion. Our method for generating
and leveraging synthetic data has the potential to improve AmyloidPET
prediction from multimodal retinal imaging. A DDPM can generate realistic and
unique multimodal synthetic retinal images. Our best performing unimodal and
multimodal classifiers were not pretrained on synthetic data, however
pretraining with synthetic data slightly improved classification performance
for two out of the four modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConStyle v2: A Strong Prompter for All-in-One Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqi Fan, Junhao Zhang, Liang Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces ConStyle v2, a strong plug-and-play prompter designed
to output clean visual prompts and assist U-Net Image Restoration models in
handling multiple degradations. The joint training process of IRConStyle, an
Image Restoration framework consisting of ConStyle and a general restoration
network, is divided into two stages: first, pre-training ConStyle alone, and
then freezing its weights to guide the training of the general restoration
network. Three improvements are proposed in the pre-training stage to train
ConStyle: unsupervised pre-training, adding a pretext task (i.e.
classification), and adopting knowledge distillation. Without bells and
whistles, we can get ConStyle v2, a strong prompter for all-in-one Image
Restoration, in less than two GPU days and doesn't require any fine-tuning.
Extensive experiments on Restormer (transformer-based), NAFNet (CNN-based),
MAXIM-1S (MLP-based), and a vanilla CNN network demonstrate that ConStyle v2
can enhance any U-Net style Image Restoration models to all-in-one Image
Restoration models. Furthermore, models guided by the well-trained ConStyle v2
exhibit superior performance in some specific degradation compared to ConStyle.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Stream: Malignant Region Learning for Breast Cancer Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Rehman, Sarfaraz Hussein, Waqas Sultani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early diagnosis of breast cancer (BC) significantly contributes to reducing
the mortality rate worldwide. The detection of different factors and biomarkers
such as Estrogen receptor (ER), Progesterone receptor (PR), Human epidermal
growth factor receptor 2 (HER2) gene, Histological grade (HG), Auxiliary lymph
node (ALN) status, and Molecular subtype (MS) can play a significant role in
improved BC diagnosis. However, the existing methods predict only a single
factor which makes them less suitable to use in diagnosis and designing a
strategy for treatment. In this paper, we propose to classify the six essential
indicating factors (ER, PR, HER2, ALN, HG, MS) for early BC diagnosis using
H\&E stained WSI's. To precisely capture local neighboring relationships, we
use spatial and frequency domain information from the large patch size of WSI's
malignant regions. Furthermore, to cater the variable number of regions of
interest sizes and give due attention to each region, we propose a malignant
region learning attention network. Our experimental results demonstrate that
combining spatial and frequency information using the malignant region learning
module significantly improves multi-factor and single-factor classification
performance on publicly available datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review (Biomedical Signal Processing and Control)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EFCNet: Every Feature Counts for Small Medical Object Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingjie Kong, Qiaoling Wei, Chengming Xu, Han Chen, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the segmentation of very small medical objects with
significant clinical value. While Convolutional Neural Networks (CNNs),
particularly UNet-like models, and recent Transformers have shown substantial
progress in image segmentation, our empirical findings reveal their poor
performance in segmenting the small medical objects and lesions concerned in
this paper. This limitation may be attributed to information loss during their
encoding and decoding process. In response to this challenge, we propose a
novel model named EFCNet for small object segmentation in medical images. Our
model incorporates two modules: the Cross-Stage Axial Attention Module (CSAA)
and the Multi-Precision Supervision Module (MPS). These modules address
information loss during encoding and decoding procedures, respectively.
Specifically, CSAA integrates features from all stages of the encoder to
adaptively learn suitable information needed in different decoding stages,
thereby reducing information loss in the encoder. On the other hand, MPS
introduces a novel multi-precision supervision mechanism to the decoder. This
mechanism prioritizes attention to low-resolution features in the initial
stages of the decoder, mitigating information loss caused by subsequent
convolution and sampling processes and enhancing the model's global perception.
We evaluate our model on two benchmark medical image datasets. The results
demonstrate that EFCNet significantly outperforms previous segmentation methods
designed for both medical and normal images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Lung Nodule <span class="highlight-title">Dataset</span> with Histopathology-based Cancer Type Annotation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muwei Jian, Hongyu Chen, Zaiyong Zhang, Nan Yang, Haorang Zhang, Lifu Ma, Wenjing Xu, Huixiang Zhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Computer-Aided Diagnosis (CAD) systems have emerged as
indispensable tools in clinical diagnostic workflows, significantly alleviating
the burden on radiologists. Nevertheless, despite their integration into
clinical settings, CAD systems encounter limitations. Specifically, while CAD
systems can achieve high performance in the detection of lung nodules, they
face challenges in accurately predicting multiple cancer types. This limitation
can be attributed to the scarcity of publicly available datasets annotated with
expert-level cancer type information. This research aims to bridge this gap by
providing publicly accessible datasets and reliable tools for medical
diagnosis, facilitating a finer categorization of different types of lung
diseases so as to offer precise treatment recommendations. To achieve this
objective, we curated a diverse dataset of lung Computed Tomography (CT)
images, comprising 330 annotated nodules (nodules are labeled as bounding
boxes) from 95 distinct patients. The quality of the dataset was evaluated
using a variety of classical classification and detection models, and these
promising results demonstrate that the dataset has a feasible application and
further facilitate intelligent auxiliary diagnosis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MFDNet: Multi-Frequency Deflare Network for Efficient Nighttime Flare
  Removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiguo Jiang, Xuhang Chen, Chi-Man Pun, Shuqiang Wang, Wei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When light is scattered or reflected accidentally in the lens, flare
artifacts may appear in the captured photos, affecting the photos' visual
quality. The main challenge in flare removal is to eliminate various flare
artifacts while preserving the original content of the image. To address this
challenge, we propose a lightweight Multi-Frequency Deflare Network (MFDNet)
based on the Laplacian Pyramid. Our network decomposes the flare-corrupted
image into low and high-frequency bands, effectively separating the
illumination and content information in the image. The low-frequency part
typically contains illumination information, while the high-frequency part
contains detailed content information. So our MFDNet consists of two main
modules: the Low-Frequency Flare Perception Module (LFFPM) to remove flare in
the low-frequency part and the Hierarchical Fusion Reconstruction Module (HFRM)
to reconstruct the flare-free image. Specifically, to perceive flare from a
global perspective while retaining detailed information for image restoration,
LFFPM utilizes Transformer to extract global information while utilizing a
convolutional neural network to capture detailed local features. Then HFRM
gradually fuses the outputs of LFFPM with the high-frequency component of the
image through feature aggregation. Moreover, our MFDNet can reduce the
computational cost by processing in multiple frequency bands instead of
directly removing the flare on the input image. Experimental results
demonstrate that our approach outperforms state-of-the-art methods in removing
nighttime flare on real-world and synthetic images from the Flare7K dataset.
Furthermore, the computational complexity of our model is remarkably low.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The Visual Computer journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-driven imaging geometric recovery of ultrahigh resolution robotic
  micro-CT for in-vivo and other applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengzhou Li, Guibin Zan, Wenbin Yun, Josef Uher, John Wen, Ge Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce an ultrahigh-resolution (50\mu m\) robotic micro-CT design for
localized imaging of carotid plaques using robotic arms, cutting-edge detector,
and machine learning technologies. To combat geometric error-induced artifacts
in interior CT scans, we propose a data-driven geometry estimation method that
maximizes the consistency between projection data and the reprojection
counterparts of a reconstructed volume. Particularly, we use a normalized cross
correlation metric to overcome the projection truncation effect. Our approach
is validated on a robotic CT scan of a sacrificed mouse and a micro-CT phantom
scan, both producing sharper images with finer details than that prior
correction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4-page paper for 8th International Conference on Computational and
  Mathematical Biomedical Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Pre-trained Models for FF-to-FFPE Histopathological Image
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qilai Zhang, Jiawen Li, Peiran Liao, Jiali Hu, Tian Guan, Anjia Han, Yonghong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The two primary types of Hematoxylin and Eosin (H&E) slides in histopathology
are Formalin-Fixed Paraffin-Embedded (FFPE) and Fresh Frozen (FF). FFPE slides
offer high quality histopathological images but require a labor-intensive
acquisition process. In contrast, FF slides can be prepared quickly, but the
image quality is relatively poor. Our task is to translate FF images into FFPE
style, thereby improving the image quality for diagnostic purposes. In this
paper, we propose Diffusion-FFPE, a method for FF-to-FFPE histopathological
image translation using a pre-trained diffusion model. Specifically, we employ
a one-step diffusion model as the generator and fine-tune it with LoRA adapters
using adversarial learning objectives. To ensure that the model effectively
captures both global structural information and local details, we propose a
multi-scale feature fusion (MFF) module. This module utilizes two VAE encoders
to extract features of varying image sizes and performs feature fusion before
feeding them into the UNet. Furthermore, we utilize a pre-trained
vision-language model for histopathology as the backbone for the discriminator
to further improve performance We conducted FF-to-FFPE translation experiments
on the TCGA-NSCLC datasets, and our method achieved better performance compared
to other methods. The code and models are released at
https://github.com/QilaiZhang/Diffusion-FFPE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Cross Spatio-Temporal Pathology-based Lung Nodule <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muwei Jian, Haoran Zhang, Mingju Shao, Hongyu Chen, Huihui Huang, Yanjie Zhong, Changlei Zhang, Bin Wang, Penghui Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, intelligent analysis of lung nodules with the assistant of computer
aided detection (CAD) techniques can improve the accuracy rate of lung cancer
diagnosis. However, existing CAD systems and pulmonary datasets mainly focus on
Computed Tomography (CT) images from one single period, while ignoring the
cross spatio-temporal features associated with the progression of nodules
contained in imaging data from various captured periods of lung cancer. If the
evolution patterns of nodules across various periods in the patients' CT
sequences can be explored, it will play a crucial role in guiding the precise
screening identification of lung cancer. Therefore, a cross spatio-temporal
lung nodule dataset with pathological information for nodule identification and
diagnosis is constructed, which contains 328 CT sequences and 362 annotated
nodules from 109 patients. This comprehensive database is intended to drive
research in the field of CAD towards more practical and robust methods, and
also contribute to the further exploration of precision medicine related field.
To ensure patient confidentiality, we have removed sensitive information from
the dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ End-to-End Optimization of Metasurfaces for Imaging with Compressed
  Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12348v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12348v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaurav Arya, William F. Li, Charles Roques-Carmes, Marin Soljačić, Steven G. Johnson, Zin Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a framework for the end-to-end optimization of metasurface imaging
systems that reconstruct targets using compressed sensing, a technique for
solving underdetermined imaging problems when the target object exhibits
sparsity (i.e. the object can be described by a small number of non-zero
values, but the positions of these values are unknown). We nest an iterative,
unapproximated compressed sensing reconstruction algorithm into our end-to-end
optimization pipeline, resulting in an interpretable, data-efficient method for
maximally leveraging metaoptics to exploit object sparsity. We apply our
framework to super-resolution imaging and high-resolution depth imaging with a
phase-change material. In both situations, our end-to-end framework
computationally discovers optimal metasurface structures for compressed sensing
recovery, automatically balancing a number of complicated design considerations
to select an imaging measurement matrix from a complex, physically constrained
manifold with millions ofdimensions. The optimized metasurface imaging systems
are robust to noise, significantly improving over random scattering surfaces
and approaching the ideal compressed sensing performance of a Gaussian matrix,
showing how a physical metasurface system can demonstrably approach the
mathematical limits of compressed sensing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diff3Dformer: Leveraging Slice Sequence <span class="highlight-title">Diffusion</span> for Enhanced 3D CT
  Classification with Transformer Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Jin, Yingying Fang, Jiahao Huang, Caiwen Xu, Simon Walsh, Guang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The manifestation of symptoms associated with lung diseases can vary in
different depths for individual patients, highlighting the significance of 3D
information in CT scans for medical image classification. While Vision
Transformer has shown superior performance over convolutional neural networks
in image classification tasks, their effectiveness is often demonstrated on
sufficiently large 2D datasets and they easily encounter overfitting issues on
small medical image datasets. To address this limitation, we propose a
Diffusion-based 3D Vision Transformer (Diff3Dformer), which utilizes the latent
space of the Diffusion model to form the slice sequence for 3D analysis and
incorporates clustering attention into ViT to aggregate repetitive information
within 3D CT scans, thereby harnessing the power of the advanced transformer in
3D classification tasks on small datasets. Our method exhibits improved
performance on two different scales of small datasets of 3D lung CT scans,
surpassing the state of the art 3D methods and other transformer-based
approaches that emerged during the COVID-19 pandemic, demonstrating its robust
and superior performance across different scales of data. Experimental results
underscore the superiority of our proposed method, indicating its potential for
enhancing medical image classification tasks in real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Arbitrary-Scale Histopathology Image Super-resolution: An
  Efficient Dual-branch Framework via Implicit Self-texture Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15613v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15613v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghong Duan, Linhao Qu, Zhiwei Yang, Manning Wang, Chenxi Zhang, Zhijian Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality whole-slide scanners are expensive, complex, and time-consuming,
thus limiting the acquisition and utilization of high-resolution pathology
whole-slide images in daily clinical work. Deep learning-based single-image
super-resolution techniques are an effective way to solve this problem by
synthesizing high-resolution images from low-resolution ones. However, the
existing super-resolution models applied in pathology images can only work in
fixed integer magnifications, significantly decreasing their applicability.
Though methods based on implicit neural representation have shown promising
results in arbitrary-scale super-resolution of natural images, applying them
directly to pathology images is inadequate because they have unique
fine-grained image textures different from natural images. Thus, we propose an
Implicit Self-Texture Enhancement-based dual-branch framework (ISTE) for
arbitrary-scale super-resolution of pathology images to address this challenge.
ISTE contains a pixel learning branch and a texture learning branch, which
first learn pixel features and texture features, respectively. Then, we design
a two-stage texture enhancement strategy to fuse the features from the two
branches to obtain the super-resolution results, where the first stage is
feature-based texture enhancement, and the second stage is spatial-domain-based
texture enhancement. Extensive experiments on three public datasets show that
ISTE outperforms existing fixed-scale and arbitrary-scale algorithms at
multiple magnifications and helps to improve downstream task performance. To
the best of our knowledge, this is the first work to achieve arbitrary-scale
super-resolution in pathology images. Codes will be available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InstantGroup: Instant Template Generation for Scalable Group of Brain
  MRI Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi He, Albert C. S. Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Template generation is a critical step in groupwise image registration, which
involves aligning a group of subjects into a common space. While existing
methods can generate high-quality template images, they often incur substantial
time costs or are limited by fixed group scales. In this paper, we present
InstantGroup, an efficient groupwise template generation framework based on
variational autoencoder (VAE) models that leverage latent representations'
arithmetic properties, enabling scalability to groups of any size. InstantGroup
features a Dual VAEs backbone with shared-weight twin networks to handle pairs
of inputs and incorporates a Displacement Inversion Module (DIM) to maintain
template unbiasedness and a Subject-Template Alignment Module (STAM) to improve
template quality and registration accuracy. Experiments on 3D brain MRI scans
from the OASIS and ADNI datasets reveal that InstantGroup dramatically reduces
runtime, generating templates within seconds for various group sizes while
maintaining superior performance compared to state-of-the-art baselines on
quantitative metrics, including unbiasedness and registration accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Painting Style Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.13459v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.13459v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Galerne, Lara Raad, José Lezama, Jean-Michel Morel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural style transfer (NST) is a deep learning technique that produces an
unprecedentedly rich style transfer from a style image to a content image. It
is particularly impressive when it comes to transferring style from a painting
to an image. NST was originally achieved by solving an optimization problem to
match the global statistics of the style image while preserving the local
geometric features of the content image. The two main drawbacks of this
original approach is that it is computationally expensive and that the
resolution of the output images is limited by high GPU memory requirements.
Many solutions have been proposed to both accelerate NST and produce images
with larger size. However, our investigation shows that these accelerated
methods all compromise the quality of the produced images in the context of
painting style transfer. Indeed, transferring the style of a painting is a
complex task involving features at different scales, from the color palette and
compositional style to the fine brushstrokes and texture of the canvas. This
paper provides a solution to solve the original global optimization for
ultra-high resolution (UHR) images, enabling multiscale NST at unprecedented
image sizes. This is achieved by spatially localizing the computation of each
forward and backward passes through the VGG network. Extensive qualitative and
quantitative comparisons, as well as a \textcolor{coverletter}{perceptual
study}, show that our method produces style transfer of unmatched quality for
such high-resolution painting styles. By a careful comparison, we show that
state-of-the-art fast methods are still prone to artifacts, thus suggesting
that fast painting style transfer remains an open problem. Source code is
available at https://github.com/bgalerne/scaling_painting_style_transfer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, 4 tables, accepted at EGSR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChangeMamba: Remote Sensing Change Detection with Spatio-Temporal State
  Space Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03425v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03425v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongruixuan Chen, Jian Song, Chengxi Han, Junshi Xia, Naoto Yokoya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNN) and Transformers have made impressive
progress in the field of remote sensing change detection (CD). However, both
architectures have inherent shortcomings: CNN are constrained by a limited
receptive field that may hinder their ability to capture broader spatial
contexts, while Transformers are computationally intensive, making them costly
to train and deploy on large datasets. Recently, the Mamba architecture, based
on state space models, has shown remarkable performance in a series of natural
language processing tasks, which can effectively compensate for the
shortcomings of the above two architectures. In this paper, we explore for the
first time the potential of the Mamba architecture for remote sensing CD tasks.
We tailor the corresponding frameworks, called MambaBCD, MambaSCD, and
MambaBDA, for binary change detection (BCD), semantic change detection (SCD),
and building damage assessment (BDA), respectively. All three frameworks adopt
the cutting-edge Visual Mamba architecture as the encoder, which allows full
learning of global spatial contextual information from the input images. For
the change decoder, which is available in all three architectures, we propose
three spatio-temporal relationship modeling mechanisms, which can be naturally
combined with the Mamba architecture and fully utilize its attribute to achieve
spatio-temporal interaction of multi-temporal features, thereby obtaining
accurate change information. On five benchmark datasets, our proposed
frameworks outperform current CNN- and Transformer-based approaches without
using any complex training strategies or tricks, fully demonstrating the
potential of the Mamba architecture in CD tasks. Further experiments show that
our architecture is quite robust to degraded data. The source code will be
available in https://github.com/ChenHongruixuan/MambaCD
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TGRS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neuromorphic Visual Scene Understanding with Resonator Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.12880v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.12880v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alpha Renner, Lazar Supic, Andreea Danielescu, Giacomo Indiveri, Bruno A. Olshausen, Yulia Sandamirskaya, Friedrich T. Sommer, E. Paxon Frady
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing a visual scene by inferring the configuration of a generative model
is widely considered the most flexible and generalizable approach to scene
understanding. Yet, one major problem is the computational challenge of the
inference procedure, involving a combinatorial search across object identities
and poses. Here we propose a neuromorphic solution exploiting three key
concepts: (1) a computational framework based on Vector Symbolic Architectures
(VSA) with complex-valued vectors; (2) the design of Hierarchical Resonator
Networks (HRN) to factorize the non-commutative transforms translation and
rotation in visual scenes; (3) the design of a multi-compartment spiking phasor
neuron model for implementing complex-valued resonator networks on neuromorphic
hardware. The VSA framework uses vector binding operations to form a generative
image model in which binding acts as the equivariant operation for geometric
transformations. A scene can, therefore, be described as a sum of vector
products, which can then be efficiently factorized by a resonator network to
infer objects and their poses. The HRN features a partitioned architecture in
which vector binding is equivariant for horizontal and vertical translation
within one partition and for rotation and scaling within the other partition.
The spiking neuron model allows mapping the resonator network onto efficient
and low-power neuromorphic hardware. Our approach is demonstrated on synthetic
scenes composed of simple 2D shapes undergoing rigid geometric transformations
and color changes. A companion paper demonstrates the same approach in
real-world application scenarios for machine vision and robotics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures, minor revisions and extended supplementary
  material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedMNIST-C: Comprehensive benchmark and improved classifier <span class="highlight-title">robust</span>ness
  by simulating realistic image corruptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17536v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17536v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Di Salvo, Sebastian Doerrich, Christian Ledig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of neural-network-based systems into clinical practice is
limited by challenges related to domain generalization and robustness. The
computer vision community established benchmarks such as ImageNet-C as a
fundamental prerequisite to measure progress towards those challenges. Similar
datasets are largely absent in the medical imaging community which lacks a
comprehensive benchmark that spans across imaging modalities and applications.
To address this gap, we create and open-source MedMNIST-C, a benchmark dataset
based on the MedMNIST+ collection covering 12 datasets and 9 imaging
modalities. We simulate task and modality-specific image corruptions of varying
severity to comprehensively evaluate the robustness of established algorithms
against real-world artifacts and distribution shifts. We further provide
quantitative evidence that our simple-to-use artificial corruptions allow for
highly performant, lightweight data augmentation to enhance model robustness.
Unlike traditional, generic augmentation strategies, our approach leverages
domain knowledge, exhibiting significantly higher robustness when compared to
widely adopted methods. By introducing MedMNIST-C and open-sourcing the
corresponding library allowing for targeted data augmentations, we contribute
to the development of increasingly robust methods tailored to the challenges of
medical imaging. The code is available at
https://github.com/francescodisalvo05/medmnistc-api}{github.com/francescodisalvo05/medmnistc-api .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Low-light Light Field Images with A Deep Compensation
  Unfolding Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.05404v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.05404v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianqiang Lyu, Junhui Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel and interpretable end-to-end learning framework,
called the deep compensation unfolding network (DCUNet), for restoring light
field (LF) images captured under low-light conditions. DCUNet is designed with
a multi-stage architecture that mimics the optimization process of solving an
inverse imaging problem in a data-driven fashion. The framework uses the
intermediate enhanced result to estimate the illumination map, which is then
employed in the unfolding process to produce a new enhanced result.
Additionally, DCUNet includes a content-associated deep compensation module at
each optimization stage to suppress noise and illumination map estimation
errors. To properly mine and leverage the unique characteristics of LF images,
this paper proposes a pseudo-explicit feature interaction module that
comprehensively exploits redundant information in LF images. The experimental
results on both simulated and real datasets demonstrate the superiority of our
DCUNet over state-of-the-art methods, both qualitatively and quantitatively.
Moreover, DCUNet preserves the essential geometric structure of enhanced LF
images much better. The code will be publicly available at
https://github.com/lyuxianqiang/LFLL-DCU.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-06-25T00:00:00Z">2024-06-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Image and Video Processing <span class="chip" style="font-size: 60%">32</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Highly Constrained Coded Aperture Imaging Systems Design Via a Knowledge
  Distillation Approach <span class="chip">ICIP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17970v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17970v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Suarez-Rodriguez, Roman Jacome, Henry Arguello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational optical imaging (COI) systems have enabled the acquisition of
high-dimensional signals through optical coding elements (OCEs). OCEs encode
the high-dimensional signal in one or more snapshots, which are subsequently
decoded using computational algorithms. Currently, COI systems are optimized
through an end-to-end (E2E) approach, where the OCEs are modeled as a layer of
a neural network and the remaining layers perform a specific imaging task.
However, the performance of COI systems optimized through E2E is limited by the
physical constraints imposed by these systems. This paper proposes a knowledge
distillation (KD) framework for the design of highly physically constrained COI
systems. This approach employs the KD methodology, which consists of a
teacher-student relationship, where a high-performance, unconstrained COI
system (the teacher), guides the optimization of a physically constrained
system (the student) characterized by a limited number of snapshots. We
validate the proposed approach, using a binary coded apertures single pixel
camera for monochromatic and multispectral image reconstruction. Simulation
results demonstrate the superiority of the KD scheme over traditional E2E
optimization for the designing of highly physically constrained COI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures. Accepted at ICIP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hot-Distance: Combining One-Hot and Signed Distance Embeddings for
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marwan Zouinkhi, Jeff L. Rhoades, Aubrey V. Weigel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models are only as good as the data to which they are fit.
As such, it is always preferable to use as much data as possible in training
models. What data can be used for fitting a model depends a lot on the
formulation of the task. We introduce Hot-Distance, a novel segmentation target
that incorporates the strength of signed boundary distance prediction with the
flexibility of one-hot encoding, to increase the amount of usable training data
for segmentation of subcellular structures in focused ion beam scanning
electron microscopy (FIB-SEM).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 1 figure, in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Total Variation Regularization for Tomographic Reconstruction of
  Cylindrically Symmetric Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maliha Hossain, Charles A. Bouman, Brendt Wohlberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flash X-ray computed tomography (CT) is an important imaging modality for
characterization of high-speed dynamic events, such as Kolsky bar impact
experiments for the study of mechanical properties of materials subjected to
impulsive forces. Due to experimental constraints, the number of X-ray views
that can be obtained is typically very sparse in both space and time, requiring
strong priors in order to enable a CT reconstruction. In this paper, we propose
an effective method for exploiting the cylindrical symmetry inherent in the
experiment via a variant of total variation (TV) regularization that operates
in cylindrical coordinates, and demonstrate that it outperforms competing
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Adaptation of Echocardiography Segmentation Via Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnaud Judge, Thierry Judge, Nicolas Duchateau, Roman A. Sandler, Joseph Z. Sokol, Olivier Bernard, Pierre-Marc Jodoin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performance of deep learning segmentation models is significantly challenged
in its transferability across different medical imaging domains, particularly
when aiming to adapt these models to a target domain with insufficient
annotated data for effective fine-tuning. While existing domain adaptation (DA)
methods propose strategies to alleviate this problem, these methods do not
explicitly incorporate human-verified segmentation priors, compromising the
potential of a model to produce anatomically plausible segmentations. We
introduce RL4Seg, an innovative reinforcement learning framework that reduces
the need to otherwise incorporate large expertly annotated datasets in the
target domain, and eliminates the need for lengthy manual human review. Using a
target dataset of 10,000 unannotated 2D echocardiographic images, RL4Seg not
only outperforms existing state-of-the-art DA methods in accuracy but also
achieves 99% anatomical validity on a subset of 220 expert-validated subjects
from the target domain. Furthermore, our framework's reward network offers
uncertainty estimates comparable with dedicated state-of-the-art uncertainty
methods, demonstrating the utility and effectiveness of RL4Seg in overcoming
domain adaptation challenges in medical image segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pixel-weighted Multi-pose Fusion for Metal Artifact Reduction in X-ray
  Computed Tomography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diyu Yang, Craig A. J. Kemp, Soumendu Majee, Gregery T. Buzzard, Charles A. Bouman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  X-ray computed tomography (CT) reconstructs the internal morphology of a
three dimensional object from a collection of projection images, most commonly
using a single rotation axis. However, for objects containing dense materials
like metal, the use of a single rotation axis may leave some regions of the
object obscured by the metal, even though projections from other rotation axes
(or poses) might contain complementary information that would better resolve
these obscured regions.
  In this paper, we propose pixel-weighted Multi-pose Fusion to reduce metal
artifacts by fusing the information from complementary measurement poses into a
single reconstruction. Our method uses Multi-Agent Consensus Equilibrium
(MACE), an extension of Plug-and-Play, as a framework for integrating
projection data from different poses. A primary novelty of the proposed method
is that the output of different MACE agents are fused in a pixel-weighted
manner to minimize the effects of metal throughout the reconstruction. Using
real CT data on an object with and without metal inserts, we demonstrate that
the proposed pixel-weighted Multi-pose Fusion method significantly reduces
metal artifacts relative to single-pose reconstructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE MMSP 2024. arXiv admin note: substantial text
  overlap with arXiv:2209.07561</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mask-Guided Attention U-Net for Enhanced Neonatal Brain Extraction and
  Image Preprocessing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bahram Jafrasteh, Simon Pedro Lubian-Lopez, Emiliano Trimarco, Macarena Roman Ruiz, Carmen Rodriguez Barrios, Yolanda Marin Almagro, Isabel Benavente-Fernandez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce MGA-Net, a novel mask-guided attention neural
network, which extends the U-net model for precision neonatal brain imaging.
MGA-Net is designed to extract the brain from other structures and reconstruct
high-quality brain images. The network employs a common encoder and two
decoders: one for brain mask extraction and the other for brain region
reconstruction. A key feature of MGA-Net is its high-level mask-guided
attention module, which leverages features from the brain mask decoder to
enhance image reconstruction. To enable the same encoder and decoder to process
both MRI and ultrasound (US) images, MGA-Net integrates sinusoidal positional
encoding. This encoding assigns distinct positional values to MRI and US
images, allowing the model to effectively learn from both modalities.
Consequently, features learned from a single modality can aid in learning a
modality with less available data, such as US. We extensively validated the
proposed MGA-Net on diverse datasets from varied clinical settings and neonatal
age groups. The metrics used for assessment included the DICE similarity
coefficient, recall, and accuracy for image segmentation; structural similarity
for image reconstruction; and root mean squared error for total brain volume
estimation from 3D ultrasound images. Our results demonstrate that MGA-Net
significantly outperforms traditional methods, offering superior performance in
brain extraction and segmentation while achieving high precision in image
reconstruction and volumetric analysis. Thus, MGA-Net represents a robust and
effective preprocessing tool for MRI and 3D ultrasound images, marking a
significant advance in neuroimaging that enhances both research and clinical
diagnostics in the neonatal period and beyond.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Brain Tumor Classification using Vision Transformer with Selective
  Cross-Attention Mechanism and Feature Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Ali Labbaf Khaniki, Alireza Golkarieh, Mohammad Manthouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain tumor classification is a challenging task in medical image analysis.
In this paper, we propose a novel approach to brain tumor classification using
a vision transformer with a novel cross-attention mechanism. Our approach
leverages the strengths of transformers in modeling long-range dependencies and
multi-scale feature fusion. We introduce two new mechanisms to improve the
performance of the cross-attention fusion module: Feature Calibration Mechanism
(FCM) and Selective Cross-Attention (SCA). FCM calibrates the features from
different branches to make them more compatible, while SCA selectively attends
to the most informative features. Our experiments demonstrate that the proposed
approach outperforms other state-of-the-art methods in brain tumor
classification, achieving improved accuracy and efficiency. The proposed FCM
and SCA mechanisms can be easily integrated into other vision transformer
architectures, making them a promising direction for future research in medical
image analysis. Experimental results confirm that our approach surpasses
existing methods, achieving state-of-the-art performance in brain tumor
classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transformer-based segmentation of adnexal lesions and ovarian implants
  in CT images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aneesh Rangnekar, Kevin M. Boehm, Emily A. Aherne, Ines Nikolovski, Natalie Gangai, Ying Liu, Dimitry Zamarin, Kara L. Roche, Sohrab P. Shah, Yulia Lakhman, Harini Veeraraghavan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two self-supervised pretrained transformer-based segmentation models (SMIT
and Swin UNETR) fine-tuned on a dataset of ovarian cancer CT images provided
reasonably accurate delineations of the tumors in an independent test dataset.
Tumors in the adnexa were segmented more accurately by both transformers (SMIT
and Swin UNETR) than the omental implants. AI-assisted labeling performed on 72
out of 245 omental implants resulted in smaller manual editing effort of 39.55
mm compared to full manual correction of partial labels of 106.49 mm and
resulted in overall improved accuracy performance. Both SMIT and Swin UNETR did
not generate any false detection of omental metastases in the urinary bladder
and relatively few false detections in the small bowel, with 2.16 cc on average
for SMIT and 7.37 cc for Swin UNETR respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embedded event based object detection with spiking neural network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Courtois, Pierre-Emmanuel Novac, Edgar Lemaire, Alain Pegatoquet, Benoit Miramond
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The complexity of event-based object detection (OD) poses considerable
challenges. Spiking Neural Networks (SNNs) show promising results and pave the
way for efficient event-based OD. Despite this success, the path to efficient
SNNs on embedded devices remains a challenge. This is due to the size of the
networks required to accomplish the task and the ability of devices to take
advantage of SNNs benefits. Even when "edge" devices are considered, they
typically use embedded GPUs that consume tens of watts. In response to these
challenges, our research introduces an embedded neuromorphic testbench that
utilizes the SPiking Low-power Event-based ArchiTecture (SPLEAT) accelerator.
Using an extended version of the Qualia framework, we can train, evaluate,
quantize, and deploy spiking neural networks on an FPGA implementation of
SPLEAT. We used this testbench to load a state-of-the-art SNN solution,
estimate the performance loss associated with deploying the network on
dedicated hardware, and run real-world event-based OD on neuromorphic hardware
specifically designed for low-power spiking neural networks. Remarkably, our
embedded spiking solution, which includes a model with 1.08 million parameters,
operates efficiently with 490 mJ per prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Result link: https://youtu.be/TsolUDaMY7Y</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse-view Signal-domain Photoacoustic Tomography Reconstruction Method
  Based on Neural Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowei Yao, Yi Zeng, Haizhao Dai, Qing Wu, Youshen Xiao, Fei Gao, Yuyao Zhang, Jingyi Yu, Xiran Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photoacoustic tomography is a hybrid biomedical technology, which combines
the advantages of acoustic and optical imaging. However, for the conventional
image reconstruction method, the image quality is affected obviously by
artifacts under the condition of sparse sampling. in this paper, a novel
model-based sparse reconstruction method via implicit neural representation was
proposed for improving the image quality reconstructed from sparse data.
Specially, the initial acoustic pressure distribution was modeled as a
continuous function of spatial coordinates, and parameterized by a multi-layer
perceptron. The weights of multi-layer perceptron were determined by training
the network in self-supervised manner. And the total variation regularization
term was used to offer the prior knowledge. We compared our result with some
ablation studies, and the results show that out method outperforms existing
methods on simulation and experimental data. Under the sparse sampling
condition, our method can suppress the artifacts and avoid the ill-posed
problem effectively, which reconstruct images with higher signal-to-noise ratio
and contrast-to-noise ratio than traditional methods. The high-quality results
for sparse data make the proposed method hold the potential for further
decreasing the hardware cost of photoacoustic tomography system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Cell Detection in Anterior Segment Optical Coherence
  Tomography Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyu Chen, Ameenat L. Solebo, Paul Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anterior uveitis, a common form of eye inflammation, can lead to permanent
vision loss if not promptly diagnosed. Monitoring this condition involves
quantifying inflammatory cells in the anterior chamber (AC) of the eye, which
can be captured using Anterior Segment Optical Coherence Tomography (AS-OCT).
However, manually identifying cells in AS-OCT images is time-consuming and
subjective. Moreover, existing automated approaches may have limitations in
both the effectiveness of detecting cells and the reliability of their
detection results. To address these challenges, we propose an automated
framework to detect cells in the AS-OCT images. This framework consists of a
zero-shot chamber segmentation module and a cell detection module. The first
module segments the AC area in the image without requiring human-annotated
training data. Subsequently, the second module identifies individual cells
within the segmented AC region. Through experiments, our framework demonstrates
superior performance compared to current state-of-the-art methods for both AC
segmentation and cell detection tasks. Notably, we find that previous cell
detection approaches could suffer from low recall, potentially overlooking a
significant number of cells. In contrast, our framework offers an improved
solution, which could benefit the diagnosis and study of anterior uveitis. Our
code for cell detection is publicly available at:
https://github.com/joeybyc/cell_detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRIP: Trainable Region-of-Interest Prediction for Hardware-Efficient
  Neuromorphic Processing on Event-based Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cina Arjmand, Yingfu Xu, Kevin Shidqi, Alexandra F. Dobrita, Kanishkan Vadivel, Paul Detterer, Manolis Sifalakis, Amirreza Yousefzadeh, Guangzhi Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuromorphic processors are well-suited for efficiently handling sparse
events from event-based cameras. However, they face significant challenges in
the growth of computing demand and hardware costs as the input resolution
increases. This paper proposes the Trainable Region-of-Interest Prediction
(TRIP), the first hardware-efficient hard attention framework for event-based
vision processing on a neuromorphic processor. Our TRIP framework actively
produces low-resolution Region-of-Interest (ROIs) for efficient and accurate
classification. The framework exploits sparse events' inherent low information
density to reduce the overhead of ROI prediction. We introduced extensive
hardware-aware optimizations for TRIP and implemented the hardware-optimized
algorithm on the SENECA neuromorphic processor. We utilized multiple
event-based classification datasets for evaluation. Our approach achieves
state-of-the-art accuracies in all datasets and produces reasonable ROIs with
varying locations and sizes. On the DvsGesture dataset, our solution requires
46x less computation than the state-of-the-art while achieving higher accuracy.
Furthermore, TRIP enables more than 2x latency and energy improvements on the
SENECA neuromorphic processor compared to the conventional solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICONS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UHD-IQA Benchmark Database: Pushing the Boundaries of Blind Photo
  Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vlad Hosu, Lorenzo Agnolucci, Oliver Wiedemann, Daisuke Iso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel Image Quality Assessment (IQA) dataset comprising 6073
UHD-1 (4K) images, annotated at a fixed width of 3840 pixels. Contrary to
existing No-Reference (NR) IQA datasets, ours focuses on highly aesthetic
photos of high technical quality, filling a gap in the literature. The images,
carefully curated to exclude synthetic content, are sufficiently diverse to
train general NR-IQA models. The dataset is annotated with perceptual quality
ratings obtained through a crowdsourcing study. Ten expert raters, comprising
photographers and graphics artists, assessed each image at least twice in
multiple sessions spanning several days, resulting in highly reliable labels.
Annotators were rigorously selected based on several metrics, including
self-consistency, to ensure their reliability. The dataset includes rich
metadata with user and machine-generated tags from over 5,000 categories and
popularity indicators such as favorites, likes, downloads, and views. With its
unique characteristics, such as its focus on high-quality images, reliable
crowdsourced annotations, and high annotation resolution, our dataset opens up
new opportunities for advancing perceptual image quality assessment research
and developing practical NR-IQA models that apply to modern photos. Our dataset
is available at https://database.mmsp-kn.de/uhd-iqa-benchmark-database.html
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Medical Image Segmentation Using Directional Window Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniya Najiha Abdul Kareem, Mustansar Fiaz, Noa Novershtern, Hisham Cholakkal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of medical images is crucial for diagnostic purposes,
including cell segmentation, tumor identification, and organ localization.
Traditional convolutional neural network (CNN)-based approaches struggled to
achieve precise segmentation results due to their limited receptive fields,
particularly in cases involving multi-organ segmentation with varying shapes
and sizes. The transformer-based approaches address this limitation by
leveraging the global receptive field, but they often face challenges in
capturing local information required for pixel-precise segmentation. In this
work, we introduce DwinFormer, a hierarchical encoder-decoder architecture for
medical image segmentation comprising a directional window (Dwin) attention and
global self-attention (GSA) for feature encoding. The focus of our design is
the introduction of Dwin block within DwinFormer that effectively captures
local and global information along the horizontal, vertical, and depthwise
directions of the input feature map by separately performing attention in each
of these directional volumes. To this end, our Dwin block introduces a nested
Dwin attention (NDA) that progressively increases the receptive field in
horizontal, vertical, and depthwise directions and a convolutional Dwin
attention (CDA) that captures local contextual information for the attention
computation. While the proposed Dwin block captures local and global
dependencies at the first two high-resolution stages of DwinFormer, the GSA
block encodes global dependencies at the last two lower-resolution stages.
Experiments over the challenging 3D Synapse Multi-organ dataset and Cell HMS
dataset demonstrate the benefits of our DwinFormer over the state-of-the-art
approaches. Our source code will be publicly available at
\url{https://github.com/Daniyanaj/DWINFORMER}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep learning-based brain segmentation model performance validation with
  clinical radiotherapy CT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Selena Huisman, Matteo Maspero, Marielle Philippens, Joost Verhoeff, Szabolcs David
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manual segmentation of medical images is labor intensive and especially
challenging for images with poor contrast or resolution. The presence of
disease exacerbates this further, increasing the need for an automated
solution. To this extent, SynthSeg is a robust deep learning model designed for
automatic brain segmentation across various contrasts and resolutions. This
study validates the SynthSeg robust brain segmentation model on computed
tomography (CT), using a multi-center dataset. An open access dataset of 260
paired CT and magnetic resonance imaging (MRI) from radiotherapy patients
treated in 5 centers was collected. Brain segmentations from CT and MRI were
obtained with SynthSeg model, a component of the Freesurfer imaging suite.
These segmentations were compared and evaluated using Dice scores and Hausdorff
95 distance (HD95), treating MRI-based segmentations as the ground truth. Brain
regions that failed to meet performance criteria were excluded based on
automated quality control (QC) scores. Dice scores indicate a median overlap of
0.76 (IQR: 0.65-0.83). The median HD95 is 2.95 mm (IQR: 1.73-5.39). QC score
based thresholding improves median dice by 0.1 and median HD95 by 0.05mm.
Morphological differences related to sex and age, as detected by MRI, were also
replicated with CT, with an approximate 17% difference between the CT and MRI
results for sex and 10% difference between the results for age. SynthSeg can be
utilized for CT-based automatic brain segmentation, but only in applications
where precision is not essential. CT performance is lower than MRI based on the
integrated QC scores, but low-quality segmentations can be excluded with
QC-based thresholding. Additionally, performing CT-based neuroanatomical
studies is encouraged, as the results show correlations in sex- and age-based
analyses similar to those found with MRI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures, 3 supplementary data csv's, 1 supplementary file
  with 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Robust</span>ly Optimized Deep Feature Decoupling Network for Fatty Liver
  Diseases Detection <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Huang, Shu Hu, Bo Peng, Jiashu Zhang, Xi Wu, Xin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current medical image classification efforts mainly aim for higher average
performance, often neglecting the balance between different classes. This can
lead to significant differences in recognition accuracy between classes and
obvious recognition weaknesses. Without the support of massive data, deep
learning faces challenges in fine-grained classification of fatty liver. In
this paper, we propose an innovative deep learning framework that combines
feature decoupling and adaptive adversarial training. Firstly, we employ two
iteratively compressed decouplers to supervised decouple common features and
specific features related to fatty liver in abdominal ultrasound images.
Subsequently, the decoupled features are concatenated with the original image
after transforming the color space and are fed into the classifier. During
adversarial training, we adaptively adjust the perturbation and balance the
adversarial strength by the accuracy of each class. The model will eliminate
recognition weaknesses by correctly classifying adversarial samples, thus
improving recognition robustness. Finally, the accuracy of our method improved
by 4.16%, achieving 82.95%. As demonstrated by extensive experiments, our
method is a generalized learning framework that can be directly used to
eliminate the recognition weaknesses of any classifier while improving its
average performance. Code is available at https://github.com/HP-ML/MICCAI2024.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HD snapshot diffractive spectral imaging and inferencing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Apratim Majumder, Monjurul Meem, Fernando Gonzalez del Cueto, Fernando Guevara-Vasquez, Syed N. Qadri, Freddie Santiago, Rajesh Menon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel high-definition (HD) snapshot diffractive spectral imaging
system utilizing a diffractive filter array (DFA) to capture a single image
that encodes both spatial and spectral information. This single diffractogram
can be computationally reconstructed into a spectral image cube, providing a
high-resolution representation of the scene across 25 spectral channels in the
440-800 nm range at 1304x744 spatial pixels (~1 MP). This unique approach
offers numerous advantages including snapshot capture, a form of optical
compression, flexible offline reconstruction, the ability to select the
spectral basis after capture, and high light throughput due to the absence of
lossy filters. We demonstrate a 30-50 nm spectral resolution and compared our
reconstructed spectra against ground truth obtained by conventional
spectrometers. Proof-of-concept experiments in diverse applications including
biological tissue classification, food quality assessment, and simulated
stellar photometry validate our system's capability to perform robust and
accurate inference. These results establish the DFA-based imaging system as a
versatile and powerful tool for advancing scientific and industrial imaging
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A benchmark for 2D foetal brain ultrasound analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariano Cabezas, Yago Diez, Clara Martinez-Diago, Anna Maroto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain development involves a sequence of structural changes from early stages
of the embryo until several months after birth. Currently, ultrasound is the
established technique for screening due to its ability to acquire dynamic
images in real-time without radiation and to its cost-efficiency. However,
identifying abnormalities remains challenging due to the difficulty in
interpreting foetal brain images. In this work we present a set of 104 2D
foetal brain ultrasound images acquired during the 20th week of gestation that
have been co-registered to a common space from a rough skull segmentation. The
images are provided both on the original space and template space centred on
the ellipses of all the subjects. Furthermore, the images have been annotated
to highlight landmark points from structures of interest to analyse brain
development. Both the final atlas template with probabilistic maps and the
original images can be used to develop new segmentation techniques, test
registration approaches for foetal brain ultrasound, extend our work to
longitudinal datasets and to detect anomalies in new images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expansive Synthesis: Generating Large-Scale <span class="highlight-title">Dataset</span>s from Minimal
  Samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vahid Jebraeeli, Bo Jiang, Hamid Krim, Derya Cansever
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenge of limited availability of data for training in machine
learning arises in many applications and the impact on performance and
generalization is serious. Traditional data augmentation methods aim to enhance
training with a moderately sufficient data set. Generative models like
Generative Adversarial Networks (GANs) often face problematic convergence when
generating significant and diverse data samples. Diffusion models, though
effective, still struggle with high computational cost and long training times.
This paper introduces an innovative Expansive Synthesis model that generates
large-scale, high-fidelity datasets from minimal samples. The proposed approach
exploits expander graph mappings and feature interpolation to synthesize
expanded datasets while preserving the intrinsic data distribution and feature
structural relationships. The rationale of the model is rooted in the
non-linear property of neural networks' latent space and in its capture by a
Koopman operator to yield a linear space of features to facilitate the
construction of larger and enriched consistent datasets starting with a much
smaller dataset. This process is optimized by an autoencoder architecture
enhanced with self-attention layers and further refined for distributional
consistency by optimal transport. We validate our Expansive Synthesis by
training classifiers on the generated datasets and comparing their performance
to classifiers trained on larger, original datasets. Experimental results
demonstrate that classifiers trained on synthesized data achieve performance
metrics on par with those trained on full-scale datasets, showcasing the
model's potential to effectively augment training data. This work represents a
significant advancement in data generation, offering a robust solution to data
scarcity and paving the way for enhanced data availability in machine learning
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages. arXiv admin note: text overlap with arXiv:2405.13866</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Cross-Task Interaction for Survival Analysis in Whole Slide
  Pathological Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songhan Jiang, Zhengyu Gan, Linghan Cai, Yifeng Wang, Yongbing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Survival prediction, utilizing pathological images and genomic profiles, is
increasingly important in cancer analysis and prognosis. Despite significant
progress, precise survival analysis still faces two main challenges: (1) The
massive pixels contained in whole slide images (WSIs) complicate the process of
pathological images, making it difficult to generate an effective
representation of the tumor microenvironment (TME). (2) Existing multimodal
methods often rely on alignment strategies to integrate complementary
information, which may lead to information loss due to the inherent
heterogeneity between pathology and genes. In this paper, we propose a
Multimodal Cross-Task Interaction (MCTI) framework to explore the intrinsic
correlations between subtype classification and survival analysis tasks.
Specifically, to capture TME-related features in WSIs, we leverage the subtype
classification task to mine tumor regions. Simultaneously, multi-head attention
mechanisms are applied in genomic feature extraction, adaptively performing
genes grouping to obtain task-related genomic embedding. With the joint
representation of pathological images and genomic data, we further introduce a
Transport-Guided Attention (TGA) module that uses optimal transport theory to
model the correlation between subtype classification and survival analysis
tasks, effectively transferring potential information. Extensive experiments
demonstrate the superiority of our approaches, with MCTI outperforming
state-of-the-art frameworks on three public benchmarks.
\href{https://github.com/jsh0792/MCTI}{https://github.com/jsh0792/MCTI}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Point Spread Function Invertibility Assessment for Image
  Deconvolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romario Gualdrón-Hurtado, Roman Jacome, Sergio Urrea, Henry Arguello, Luis Gonzalez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep-learning (DL)-based image deconvolution (ID) has exhibited remarkable
recovery performance, surpassing traditional linear methods. However, unlike
traditional ID approaches that rely on analytical properties of the point
spread function (PSF) to achieve high recovery performance - such as specific
spectrum properties or small conditional numbers in the convolution matrix - DL
techniques lack quantifiable metrics for evaluating PSF suitability for
DL-assisted recovery. Aiming to enhance deconvolution quality, we propose a
metric that employs a non-linear approach to learn the invertibility of an
arbitrary PSF using a neural network by mapping it to a unit impulse. A lower
discrepancy between the mapped PSF and a unit impulse indicates a higher
likelihood of successful inversion by a DL network. Our findings reveal that
this metric correlates with high recovery performance in DL and traditional
methods, thereby serving as an effective regularizer in deconvolution tasks.
This approach reduces the computational complexity over conventional condition
number assessments and is a differentiable process. These useful properties
allow its application in designing diffractive optical elements through
end-to-end (E2E) optimization, achieving invertible PSFs, and outperforming the
E2E baseline framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MambaMIR: An Arbitrary-Masked Mamba for Joint Medical Image
  Reconstruction and Uncertainty Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18451v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18451v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Huang, Liutao Yang, Fanwen Wang, Yang Nan, Angelica I. Aviles-Rivero, Carola-Bibiane Schönlieb, Daoqiang Zhang, Guang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent Mamba model has shown remarkable adaptability for visual
representation learning, including in medical imaging tasks. This study
introduces MambaMIR, a Mamba-based model for medical image reconstruction, as
well as its Generative Adversarial Network-based variant, MambaMIR-GAN. Our
proposed MambaMIR inherits several advantages, such as linear complexity,
global receptive fields, and dynamic weights, from the original Mamba model.
The innovated arbitrary-mask mechanism effectively adapt Mamba to our image
reconstruction task, providing randomness for subsequent Monte Carlo-based
uncertainty estimation. Experiments conducted on various medical image
reconstruction tasks, including fast MRI and SVCT, which cover anatomical
regions such as the knee, chest, and abdomen, have demonstrated that MambaMIR
and MambaMIR-GAN achieve comparable or superior reconstruction results relative
to state-of-the-art methods. Additionally, the estimated uncertainty maps offer
further insights into the reliability of the reconstruction quality. The code
is publicly available at https://github.com/ayanglab/MambaMIR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Global Sensitivity and Uncertainty Quantification in Medical
  Image Reconstruction with Monte Carlo Arbitrary-Masked Mamba 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17659v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17659v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Huang, Liutao Yang, Fanwen Wang, Yang Nan, Weiwen Wu, Chengyan Wang, Kuangyu Shi, Angelica I. Aviles-Rivero, Carola-Bibiane Schönlieb, Daoqiang Zhang, Guang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has been extensively applied in medical image reconstruction,
where Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs)
represent the predominant paradigms, each possessing distinct advantages and
inherent limitations: CNNs exhibit linear complexity with local sensitivity,
whereas ViTs demonstrate quadratic complexity with global sensitivity. The
emerging Mamba has shown superiority in learning visual representation, which
combines the advantages of linear scalability and global sensitivity. In this
study, we introduce MambaMIR, an Arbitrary-Masked Mamba-based model with
wavelet decomposition for joint medical image reconstruction and uncertainty
estimation. A novel Arbitrary Scan Masking (ASM) mechanism "masks out"
redundant information to introduce randomness for further uncertainty
estimation. Compared to the commonly used Monte Carlo (MC) dropout, our
proposed MC-ASM provides an uncertainty map without the need for hyperparameter
tuning and mitigates the performance drop typically observed when applying
dropout to low-level tasks. For further texture preservation and better
perceptual quality, we employ the wavelet transformation into MambaMIR and
explore its variant based on the Generative Adversarial Network, namely
MambaMIR-GAN. Comprehensive experiments have been conducted for multiple
representative medical image reconstruction tasks, demonstrating that the
proposed MambaMIR and MambaMIR-GAN outperform other baseline and
state-of-the-art methods in different reconstruction tasks, where MambaMIR
achieves the best reconstruction fidelity and MambaMIR-GAN has the best
perceptual quality. In addition, our MC-ASM provides uncertainty maps as an
additional tool for clinicians, while mitigating the typical performance drop
caused by the commonly used dropout.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instance-level quantitative saliency in multiple sclerosis lesion
  segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Spagnolo, Nataliia Molchanova, Roger Schaer, Meritxell Bach Cuadra, Mario Ocampo Pineda, Lester Melie-Garcia, Cristina Granziera, Vincent Andrearczyk, Adrien Depeursinge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, explainable methods for artificial intelligence (XAI) have
tried to reveal and describe models' decision mechanisms in the case of
classification tasks. However, XAI for semantic segmentation and in particular
for single instances has been little studied to date. Understanding the process
underlying automatic segmentation of single instances is crucial to reveal what
information was used to detect and segment a given object of interest. In this
study, we proposed two instance-level explanation maps for semantic
segmentation based on SmoothGrad and Grad-CAM++ methods. Then, we investigated
their relevance for the detection and segmentation of white matter lesions
(WML), a magnetic resonance imaging (MRI) biomarker in multiple sclerosis (MS).
687 patients diagnosed with MS for a total of 4043 FLAIR and MPRAGE MRI scans
were collected at the University Hospital of Basel, Switzerland. Data were
randomly split into training, validation and test sets to train a 3D U-Net for
MS lesion segmentation. We observed 3050 true positive (TP), 1818 false
positive (FP), and 789 false negative (FN) cases. We generated instance-level
explanation maps for semantic segmentation, by developing two XAI methods based
on SmoothGrad and Grad-CAM++. We investigated: 1) the distribution of gradients
in saliency maps with respect to both input MRI sequences; 2) the model's
response in the case of synthetic lesions; 3) the amount of perilesional tissue
needed by the model to segment a lesion. Saliency maps (based on SmoothGrad) in
FLAIR showed positive values inside a lesion and negative in its neighborhood.
Peak values of saliency maps generated for these four groups of volumes
presented distributions that differ significantly from one another, suggesting
a quantitative nature of the proposed saliency. Contextual information of 7mm
around the lesion border was required for their segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Ninth NTIRE 2024 Efficient Super-Resolution Challenge Report <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Ren, Yawei Li, Nancy Mehta, Radu Timofte, Hongyuan Yu, Cheng Wan, Yuxin Hong, Bingnan Han, Zhuoyuan Wu, Yajun Zou, Yuqing Liu, Jizhe Li, Keji He, Chao Fan, Heng Zhang, Xiaolin Zhang, Xuanwu Yin, Kunlong Zuo, Bohao Liao, Peizhe Xia, Long Peng, Zhibo Du, Xin Di, Wangkai Li, Yang Wang, Wei Zhai, Renjing Pei, Jiaming Guo, Songcen Xu, Yang Cao, Zhengjun Zha, Yan Wang, Yi Liu, Qing Wang, Gang Zhang, Liou Zhang, Shijie Zhao, Long Sun, Jinshan Pan, Jiangxin Dong, Jinhui Tang, Xin Liu, Min Yan, Qian Wang, Menghan Zhou, Yiqiang Yan, Yixuan Liu, Wensong Chan, Dehua Tang, Dong Zhou, Li Wang, Lu Tian, Barsoum Emad, Bohan Jia, Junbo Qiao, Yunshuai Zhou, Yun Zhang, Wei Li, Shaohui Lin, Shenglong Zhou, Binbin Chen, Jincheng Liao, Suiyi Zhao, Zhao Zhang, Bo Wang, Yan Luo, Yanyan Wei, Feng Li, Mingshen Wang, Yawei Li, Jinhan Guan, Dehua Hu, Jiawei Yu, Qisheng Xu, Tao Sun, Long Lan, Kele Xu, Xin Lin, Jingtong Yue, Lehan Yang, Shiyi Du, Lu Qi, Chao Ren, Zeyu Han, Yuhan Wang, Chaolin Chen, Haobo Li, Mingjun Zheng, Zhongbao Yang, Lianhong Song, Xingzhuo Yan, Minghan Fu, Jingyi Zhang, Baiang Li, Qi Zhu, Xiaogang Xu, Dan Guo, Chunle Guo, Jiadi Chen, Huanhuan Long, Chunjiang Duanmu, Xiaoyan Lei, Jie Liu, Weilin Jia, Weifeng Cao, Wenlong Zhang, Yanyu Mao, Ruilong Guo, Nihao Zhang, Qian Wang, Manoj Pandey, Maksym Chernozhukov, Giang Le, Shuli Cheng, Hongyuan Wang, Ziyan Wei, Qingting Tang, Liejun Wang, Yongming Li, Yanhui Guo, Hao Xu, Akram Khatami-Rizi, Ahmad Mahmoudi-Aznaveh, Chih-Chung Hsu, Chia-Ming Lee, Yi-Shiuan Chou, Amogh Joshi, Nikhil Akalwadi, Sampada Malagi, Palani Yashaswini, Chaitra Desai, Ramesh Ashok Tabib, Ujwala Patil, Uma Mudenagudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides a comprehensive review of the NTIRE 2024 challenge,
focusing on efficient single-image super-resolution (ESR) solutions and their
outcomes. The task of this challenge is to super-resolve an input image with a
magnification factor of x4 based on pairs of low and corresponding
high-resolution images. The primary objective is to develop networks that
optimize various aspects such as runtime, parameters, and FLOPs, while still
maintaining a peak signal-to-noise ratio (PSNR) of approximately 26.90 dB on
the DIV2K_LSDIR_valid dataset and 26.99 dB on the DIV2K_LSDIR_test dataset. In
addition, this challenge has 4 tracks including the main track (overall
performance), sub-track 1 (runtime), sub-track 2 (FLOPs), and sub-track 3
(parameters). In the main track, all three metrics (ie runtime, FLOPs, and
parameter count) were considered. The ranking of the main track is calculated
based on a weighted sum-up of the scores of all other sub-tracks. In sub-track
1, the practical runtime performance of the submissions was evaluated, and the
corresponding score was used to determine the ranking. In sub-track 2, the
number of FLOPs was considered. The score calculated based on the corresponding
FLOPs was used to determine the ranking. In sub-track 3, the number of
parameters was considered. The score calculated based on the corresponding
parameters was used to determine the ranking. RLFN is set as the baseline for
efficiency measurement. The challenge had 262 registered participants, and 34
teams made valid submissions. They gauge the state-of-the-art in efficient
single-image super-resolution. To facilitate the reproducibility of the
challenge and enable other researchers to build upon these findings, the code
and the pre-trained model of validated solutions are made publicly available at
https://github.com/Amazingren/NTIRE2024_ESR/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The report paper of NTIRE2024 Efficient Super-resolution, accepted by
  CVPRW2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing dermatological diagnosis: Development of a hyperspectral
  dermatoscope for enhanced skin imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00612v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00612v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin J. Hetz, Carina Nogueira Garcia, Sarah Haggenmüller, Titus J. Brinker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical dermatology necessitates precision and innovation for efficient
diagnosis and treatment of various skin conditions. This paper introduces the
development of a cutting-edge hyperspectral dermatoscope (the Hyperscope)
tailored for human skin analysis. We detail the requirements to such a device
and the design considerations, from optical configurations to sensor selection,
necessary to capture a wide spectral range with high fidelity. Preliminary
results from 15 individuals and 160 recorded skin images demonstrate the
potential of the Hyperscope in identifying and characterizing various skin
conditions, offering a promising avenue for non-invasive skin evaluation and a
platform for future research in dermatology-related hyperspectral imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 11 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sampling Strategies in Bayesian Inversion: A Study of RTO and Langevin
  Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16658v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16658v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Remi Laumont, Yiqiu Dong, Martin Skovgaard Andersen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies two classes of sampling methods for the solution of
inverse problems, namely Randomize-Then-Optimize (RTO), which is rooted in
sensitivity analysis, and Langevin methods, which are rooted in the Bayesian
framework. The two classes of methods correspond to different assumptions and
yield samples from different target distributions. We highlight the main
conceptual and theoretical differences between the two approaches and compare
them from a practical point of view by tackling two classical inverse problems
in imaging: deblurring and inpainting. We show that the choice of the sampling
method has a significant impact on the quality of the reconstruction and that
the RTO method is more robust to the choice of the parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Image Prior for Unsupervised Dynamic Cardiac Cine MRI
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongsen Li, Wenxuan Chen, Shuai Wang, Chuyu Liu, Qing Zou, Rui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The inductive bias of the convolutional neural network (CNN) can be a strong
prior for image restoration, which is known as the Deep Image Prior (DIP).
Recently, DIP is utilized in unsupervised dynamic MRI reconstruction, which
adopts a generative model from the latent space to the image space. However,
existing methods usually use a pyramid-shaped CNN generator shared by all
frames, embedding the temporal modeling within the latent space, which may
hamper the model expression capability. In this work, we propose a novel scheme
for dynamic MRI representation, named ``Graph Image Prior'' (GIP). GIP adopts a
two-stage generative network in a new modeling methodology, which first employs
independent CNNs to recover the image structure for each frame, and then
exploits the spatio-temporal correlations within the feature space
parameterized by a graph model. A graph convolutional network is utilized for
feature fusion and dynamic image generation. In addition, we devise an ADMM
algorithm to alternately optimize the images and the network parameters to
improve the reconstruction performance. Experiments were conducted on cardiac
cine MRI reconstruction, which demonstrate that GIP outperforms compressed
sensing methods and other DIP-based unsupervised methods, significantly
reducing the performance gap with state-of-the-art supervised algorithms.
Moreover, GIP displays superior generalization ability when transferred to a
different reconstruction setting, without the need for any additional data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ X-ray2CTPA: Generating 3D CTPA scans from 2D X-ray conditioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noa Cahan, Eyal Klang, Galit Aviram, Yiftach Barash, Eli Konen, Raja Giryes, Hayit Greenspan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chest X-rays or chest radiography (CXR), commonly used for medical
diagnostics, typically enables limited imaging compared to computed tomography
(CT) scans, which offer more detailed and accurate three-dimensional data,
particularly contrast-enhanced scans like CT Pulmonary Angiography (CTPA).
However, CT scans entail higher costs, greater radiation exposure, and are less
accessible than CXRs. In this work we explore cross-modal translation from a 2D
low contrast-resolution X-ray input to a 3D high contrast and
spatial-resolution CTPA scan. Driven by recent advances in generative AI, we
introduce a novel diffusion-based approach to this task. We evaluate the models
performance using both quantitative metrics and qualitative feedback from
radiologists, ensuring diagnostic relevance of the generated images.
Furthermore, we employ the synthesized 3D images in a classification framework
and show improved AUC in a PE categorization task, using the initial CXR input.
The proposed method is generalizable and capable of performing additional
cross-modality translations in medical imaging. It may pave the way for more
accessible and cost-effective advanced diagnostic tools. The code for this
project is available: https://github.com/NoaCahan/X-ray2CTPA .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint, project code: https://github.com/NoaCahan/X-ray2CTPA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CEST-KAN: Kolmogorov-Arnold Networks for CEST MRI Data Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16026v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16026v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawen Wang, Pei Cai, Ziyan Wang, Huabin Zhang, Jianpan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: This study aims to propose and investigate the feasibility of using
Kolmogorov-Arnold Network (KAN) for CEST MRI data analysis (CEST-KAN). Methods:
CEST MRI data were acquired from twelve healthy volunteers at 3T. Data from ten
subjects were used for training, while the remaining two were reserved for
testing. The performance of multi-layer perceptron (MLP) and KAN models with
the same network settings were evaluated and compared to the conventional
multi-pool Lorentzian fitting (MPLF) method in generating water and multiple
CEST contrasts, including amide, relayed nuclear Overhauser effect (rNOE), and
magnetization transfer (MT). Results: The water and CEST maps generated by both
MLP and KAN were visually comparable to the MPLF results. However, the KAN
model demonstrated higher accuracy in extrapolating the CEST fitting metrics,
as evidenced by the smaller validation loss during training and smaller
absolute error during testing. Voxel-wise correlation analysis showed that all
four CEST fitting metrics generated by KAN consistently exhibited higher
Pearson coefficients than the MLP results, indicating superior performance.
Moreover, the KAN models consistently outperformed the MLP models in varying
hidden layer numbers despite longer training time. Conclusion: In this study,
we demonstrated for the first time the feasibility of utilizing KAN for CEST
MRI data analysis, highlighting its superiority over MLP in this task. The
findings suggest that CEST-KAN has the potential to be a robust and reliable
post-analysis tool for CEST MRI in clinical settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rapid and Accurate Diagnosis of Acute Aortic Syndrome using Non-contrast
  CT: A Large-scale, Retrospective, Multi-center and AI-based Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15222v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15222v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujian Hu, Yilang Xiang, Yan-Jie Zhou, Yangyan He, Shifeng Yang, Xiaolong Du, Chunlan Den, Youyao Xu, Gaofeng Wang, Zhengyao Ding, Jingyong Huang, Wenjun Zhao, Xuejun Wu, Donglin Li, Qianqian Zhu, Zhenjiang Li, Chenyang Qiu, Ziheng Wu, Yunjun He, Chen Tian, Yihui Qiu, Zuodong Lin, Xiaolong Zhang, Yuan He, Zhenpeng Yuan, Xiaoxiang Zhou, Rong Fan, Ruihan Chen, Wenchao Guo, Jianpeng Zhang, Tony C. W. Mok, Zi Li, Le Lu, Dehai Lang, Xiaoqiang Li, Guofu Wang, Wei Lu, Zhengxing Huang, Minfeng Xu, Hongkun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chest pain symptoms are highly prevalent in emergency departments (EDs),
where acute aortic syndrome (AAS) is a catastrophic cardiovascular emergency
with a high fatality rate, especially when timely and accurate treatment is not
administered. However, current triage practices in the ED can cause up to
approximately half of patients with AAS to have an initially missed diagnosis
or be misdiagnosed as having other acute chest pain conditions. Subsequently,
these AAS patients will undergo clinically inaccurate or suboptimal
differential diagnosis. Fortunately, even under these suboptimal protocols,
nearly all these patients underwent non-contrast CT covering the aorta anatomy
at the early stage of differential diagnosis. In this study, we developed an
artificial intelligence model (DeepAAS) using non-contrast CT, which is highly
accurate for identifying AAS and provides interpretable results to assist in
clinical decision-making. Performance was assessed in two major phases: a
multi-center retrospective study (n = 20,750) and an exploration in real-world
emergency scenarios (n = 137,525). In the multi-center cohort, DeepAAS achieved
a mean area under the receiver operating characteristic curve of 0.958 (95% CI
0.950-0.967). In the real-world cohort, DeepAAS detected 109 AAS patients with
misguided initial suspicion, achieving 92.6% (95% CI 76.2%-97.5%) in mean
sensitivity and 99.2% (95% CI 99.1%-99.3%) in mean specificity. Our AI model
performed well on non-contrast CT at all applicable early stages of
differential diagnosis workflows, effectively reduced the overall missed
diagnosis and misdiagnosis rate from 48.8% to 4.8% and shortened the diagnosis
time for patients with misguided initial suspicion from an average of 681.8
(74-11,820) mins to 68.5 (23-195) mins. DeepAAS could effectively fill the gap
in the current clinical workflow without requiring additional tests.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under peer review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ECLIPSE: Expunging Clean-label Indiscriminate Poisons via Sparse
  <span class="highlight-title">Diffusion</span> Purification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianlong Wang, Shengshan Hu, Yechao Zhang, Ziqi Zhou, Leo Yu Zhang, Peng Xu, Wei Wan, Hai Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clean-label indiscriminate poisoning attacks add invisible perturbations to
correctly labeled training images, thus dramatically reducing the
generalization capability of the victim models. Recently, some defense
mechanisms have been proposed such as adversarial training, image
transformation techniques, and image purification. However, these schemes are
either susceptible to adaptive attacks, built on unrealistic assumptions, or
only effective against specific poison types, limiting their universal
applicability. In this research, we propose a more universally effective,
practical, and robust defense scheme called ECLIPSE. We first investigate the
impact of Gaussian noise on the poisons and theoretically prove that any kind
of poison will be largely assimilated when imposing sufficient random noise. In
light of this, we assume the victim has access to an extremely limited number
of clean images (a more practical scene) and subsequently enlarge this sparse
set for training a denoising probabilistic model (a universal denoising tool).
We then begin by introducing Gaussian noise to absorb the poisons and then
apply the model for denoising, resulting in a roughly purified dataset.
Finally, to address the trade-off of the inconsistency in the assimilation
sensitivity of different poisons by Gaussian noise, we propose a lightweight
corruption compensation module to effectively eliminate residual poisons,
providing a more universal defense approach. Extensive experiments demonstrate
that our defense approach outperforms 10 state-of-the-art defenses. We also
propose an adaptive attack against ECLIPSE and verify the robustness of our
defense scheme. Our code is available at https://github.com/CGCL-codes/ECLIPSE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ESORICS 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-06-24T00:00:00Z">2024-06-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Image and Video Processing <span class="chip" style="font-size: 60%">19</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Aperture Fusion of Transformer-Convolutional Network (MFTC-Net)
  for 3D Medical Image Segmentation and Visualization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyavash Shabani, Muhammad Sohaib, Sahar A. Mohammed, Bahram Parvin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers have shown superior performance to the traditional
convolutional-based frameworks in many vision applications, including but not
limited to the segmentation of 3D medical images. To further advance this area,
this study introduces the Multi-Aperture Fusion of Transformer-Convolutional
Network (MFTC-Net), which integrates the output of Swin Transformers and their
corresponding convolutional blocks using 3D fusion blocks. The Multi-Aperture
incorporates each image patch at its original resolutions with its pyramid
representation to better preserve minute details. The proposed architecture has
demonstrated a score of 89.73 and 7.31 for Dice and HD95, respectively, on the
Synapse multi-organs dataset an improvement over the published results. The
improved performance also comes with the added benefits of the reduced
complexity of approximately 40 million parameters. Our code is available at
https://github.com/Siyavashshabani/MFTC-Net
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Domain Adaptation for Pediatric Brain Tumor Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingru Fu, Simone Bendazzoli, Örjan Smedby, Rodrigo Moreno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant advances have been made toward building accurate automatic
segmentation models for adult gliomas. However, the performance of these models
often degrades when applied to pediatric glioma due to their imaging and
clinical differences (domain shift). Obtaining sufficient annotated data for
pediatric glioma is typically difficult because of its rare nature. Also,
manual annotations are scarce and expensive. In this work, we propose
Domain-Adapted nnU-Net (DA-nnUNet) to perform unsupervised domain adaptation
from adult glioma (source domain) to pediatric glioma (target domain).
Specifically, we add a domain classifier connected with a gradient reversal
layer (GRL) to a backbone nnU-Net. Once the classifier reaches a very high
accuracy, the GRL is activated with the goal of transferring domain-invariant
features from the classifier to the segmentation model while preserving
segmentation accuracy on the source domain. The accuracy of the classifier
slowly degrades to chance levels. No annotations are used in the target domain.
The method is compared to 8 different supervised models using BraTS-Adult
glioma (N=1251) and BraTS-PED glioma data (N=99). The proposed method shows
notable performance enhancements in the tumor core (TC) region compared to the
model that only uses adult data: ~32% better Dice scores and ~20 better 95th
percentile Hausdorff distances. Moreover, our unsupervised approach shows no
statistically significant difference compared to the practical upper bound
model using manual annotations from both datasets in TC region. The code is
shared at https://github.com/Fjr9516/DA_nnUNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The MRI Scanner as a Diagnostic: Image-less Active Sampling <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuning Du, Rohan Dharmakumar, Sotirios A. Tsaftaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the high diagnostic accuracy of Magnetic Resonance Imaging (MRI),
using MRI as a Point-of-Care (POC) disease identification tool poses
significant accessibility challenges due to the use of high magnetic field
strength and lengthy acquisition times. We ask a simple question: Can we
dynamically optimise acquired samples, at the patient level, according to an
(automated) downstream decision task, while discounting image reconstruction?
We propose an ML-based framework that learns an active sampling strategy, via
reinforcement learning, at a patient-level to directly infer disease from
undersampled k-space. We validate our approach by inferring Meniscus Tear in
undersampled knee MRI data, where we achieve diagnostic performance comparable
with ML-based diagnosis, using fully sampled k-space data. We analyse
task-specific sampling policies, showcasing the adaptability of our active
sampling approach. The introduced frugal sampling strategies have the potential
to reduce high field strength requirements that in turn strengthen the
viability of MRI-based POC disease identification and associated preliminary
screening tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ μ-Net: A Deep Learning-Based Architecture for μ-CT Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierangela Bruno, Edoardo De Rose, Carlo Adornetto, Francesco Calimeri, Sandro Donato, Raffaele Giuseppe Agostino, Daniela Amelio, Riccardo Barberi, Maria Carmela Cerra, Maria Caterina Crocco, Mariacristina Filice, Raffaele Filosa, Gianluigi Greco, Sandra Imbrogno, Vincenzo Formoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  X-ray computed microtomography ({\mu}-CT) is a non-destructive technique that
can generate high-resolution 3D images of the internal anatomy of medical and
biological samples. These images enable clinicians to examine internal anatomy
and gain insights into the disease or anatomical morphology. However,
extracting relevant information from 3D images requires semantic segmentation
of the regions of interest, which is usually done manually and results
time-consuming and tedious. In this work, we propose a novel framework that
uses a convolutional neural network (CNN) to automatically segment the full
morphology of the heart of Carassius auratus. The framework employs an
optimized 2D CNN architecture that can infer a 3D segmentation of the sample,
avoiding the high computational cost of a 3D CNN architecture. We tackle the
challenges of handling large and high-resoluted image data (over a thousand
pixels in each dimension) and a small training database (only three samples) by
proposing a standard protocol for data normalization and processing. Moreover,
we investigate how the noise, contrast, and spatial resolution of the sample
and the training of the architecture are affected by the reconstruction
technique, which depends on the number of input images. Experiments show that
our framework significantly reduces the time required to segment new samples,
allowing a faster microtomography analysis of the Carassius auratus heart
shape. Furthermore, our framework can work with any bio-image (biological and
medical) from {\mu}-CT with high-resolution and small dataset size
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demystifying the Effect of Receptive Field Size in U-Net Models for
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Loos, Rohit Pardasani, Navchetan Awasthi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation is a critical task in healthcare applications, and
U-Nets have demonstrated promising results. This work delves into the
understudied aspect of receptive field (RF) size and its impact on the U-Net
and Attention U-Net architectures. This work explores several critical elements
including the relationship between RF size, characteristics of the region of
interest, and model performance, as well as the balance between RF size and
computational costs for U-Net and Attention U-Net methods for different
datasets. This work also proposes a mathematical notation for representing the
theoretical receptive field (TRF) of a given layer in a network and proposes
two new metrics - effective receptive field (ERF) rate and the Object rate to
quantify the fraction of significantly contributing pixels within the ERF
against the TRF area and assessing the relative size of the segmentation object
compared to the TRF size respectively. The results demonstrate that there
exists an optimal TRF size that successfully strikes a balance between
capturing a wider global context and maintaining computational efficiency,
thereby optimizing model performance. Interestingly, a distinct correlation is
observed between the data complexity and the required TRF size; segmentation
based solely on contrast achieved peak performance even with smaller TRF sizes,
whereas more complex segmentation tasks necessitated larger TRFs. Attention
U-Net models consistently outperformed their U-Net counterparts, highlighting
the value of attention mechanisms regardless of TRF size. These novel insights
present an invaluable resource for developing more efficient U-Net-based
architectures for medical imaging and pave the way for future exploration. A
tool is also developed that calculates the TRF for a U-Net (and Attention
U-Net) model, and also suggest an appropriate TRF size for a given model and
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLOctolyzer: Fully automatic analysis toolkit for segmentation and
  feature extracting in scanning laser ophthalmoscopy images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamie Burke, Samuel Gibbon, Justin Engelmann, Adam Threlfall, Ylenia Giarratano, Charlene Hamid, Stuart King, Ian J. C. MacCormick, Tom MacGillivray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: To describe SLOctolyzer: an open-source analysis toolkit for en face
retinal vessels appearing in infrared reflectance scanning laser ophthalmoscopy
(SLO) images.
  Methods: SLOctolyzer includes two main modules: segmentation and measurement.
The segmentation module use deep learning methods to delineate retinal anatomy,
while the measurement module quantifies key retinal vascular features such as
vessel complexity, density, tortuosity, and calibre. We evaluate the
segmentation module using unseen data and measure its reproducibility.
  Results: SLOctolyzer's segmentation module performed well against unseen
internal test data (Dice for all-vessels, 0.9097; arteries, 0.8376; veins,
0.8525; optic disc, 0.9430; fovea, 0.8837). External validation against severe
retinal pathology showed decreased performance (Dice for arteries, 0.7180;
veins, 0.7470; optic disc, 0.9032). SLOctolyzer had good reproducibility (mean
difference for fractal dimension, -0.0007; vessel density, -0.0003; vessel
calibre, -0.3154 $\mu$m; tortuosity density, 0.0013). SLOctolyzer can process a
macula-centred SLO image in under 20 seconds and a disc-centred SLO image in
under 30 seconds using a standard laptop CPU.
  Conclusions: To our knowledge, SLOctolyzer is the first open-source tool to
convert raw SLO images into reproducible and clinically meaningful retinal
vascular parameters. SLO images are captured simultaneous to optical coherence
tomography (OCT), and we believe our software will be useful for extracting
retinal vascular measurements from large OCT image sets and linking them to
ocular or systemic diseases. It requires no specialist knowledge or proprietary
software, and allows manual correction of segmentations and re-computing of
vascular metrics. SLOctolyzer is freely available at
https://github.com/jaburke166/SLOctolyzer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 6 tables + Supplementary (7 pages, 10 figures, 4
  tables). Submitted for peer review at Translational Vision Science and
  Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Vision xLSTM Embedded UNet More Reliable in Medical 3D Image
  Segmentation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pallabi Dutta, Soham Bose, Swalpa Kumar Roy, Sushmita Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of developing efficient medical image segmentation has
evolved from initial dependence on Convolutional Neural Networks (CNNs) to the
present investigation of hybrid models that combine CNNs with Vision
Transformers. Furthermore, there is an increasing focus on creating
architectures that are both high-performing in medical image segmentation tasks
and computationally efficient to be deployed on systems with limited resources.
Although transformers have several advantages like capturing global
dependencies in the input data, they face challenges such as high computational
and memory complexity. This paper investigates the integration of CNNs and
Vision Extended Long Short-Term Memory (Vision-xLSTM) models by introducing a
novel approach called UVixLSTM. The Vision-xLSTM blocks captures temporal and
global relationships within the patches extracted from the CNN feature maps.
The convolutional feature reconstruction path upsamples the output volume from
the Vision-xLSTM blocks to produce the segmentation output. Our primary
objective is to propose that Vision-xLSTM forms a reliable backbone for medical
image segmentation tasks, offering excellent segmentation performance and
reduced computational complexity. UVixLSTM exhibits superior performance
compared to state-of-the-art networks on the publicly-available Synapse
dataset. Code is available at: https://github.com/duttapallabi2907/UVixLSTM
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving <span class="highlight-title">Generative</span> Adversarial Networks for Video Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this research, we explore different ways to improve generative adversarial
networks for video super-resolution tasks from a base single image
super-resolution GAN model. Our primary objective is to identify potential
techniques that enhance these models and to analyze which of these techniques
yield the most significant improvements. We evaluate our results using Peak
Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). Our
findings indicate that the most effective techniques include temporal
smoothing, long short-term memory (LSTM) layers, and a temporal loss function.
The integration of these methods results in an 11.97% improvement in PSNR and
an 8% improvement in SSIM compared to the baseline video super-resolution
generative adversarial network (GAN) model. This substantial improvement
suggests potential further applications to enhance current state-of-the-art
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximate DCT and Quantization Techniques for Energy-Constrained Image
  Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming-Che Li, Archisman Ghosh, Shreyas Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent expansions in multimedia devices gather enormous amounts of real-time
images for processing and inference. The images are first compressed using
compression schemes, like JPEG, to reduce storage costs and power for
transmitting the captured data. Due to inherent error resilience and
imperceptibility in images, JPEG can be approximated to reduce the required
computation power and area. This work demonstrates the first end-to-end
approximation computing-based optimization of JPEG hardware using i) an
approximate division realized using bit-shift operators to reduce the
complexity of the quantization block, ii) loop perforation, and iii) precision
scaling on top of a multiplier-less fast DCT architecture to achieve an
extremely energy-efficient JPEG compression unit which will be a perfect fit
for power/bandwidth-limited scenario. Furthermore, a gradient descent-based
heuristic composed of two conventional approximation strategies, i.e.,
Precision Scaling and Loop Perforation, is implemented for tuning the degree of
approximation to trade off energy consumption with the quality degradation of
the decoded image. The entire RTL design is coded in Verilog HDL, synthesized,
mapped to TSMC 65nm CMOS technology, and simulated using Cadence Spectre
Simulator under 25$^{\circ}$\textbf{C}, TT corner. The approximate division
approach achieved around $\textbf{28\%}$ reduction in the active design area.
The heuristic-based approximation technique combined with accelerator
optimization achieves a significant energy reduction of $\textbf{36\%}$ for a
minimal image quality degradation of $\textbf{2\%}$ SAD. Simulation results
also show that the proposed architecture consumes 15uW at the DCT and
quantization stages to compress a colored 480p image at 6fps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lesion-Aware Cross-Phase Attention Network for Renal Tumor Subtype
  Classification on Multi-Phase CT Scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kwang-Hyun Uhm, Seung-Won Jung, Sung-Hoo Hong, Sung-Jea Ko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-phase computed tomography (CT) has been widely used for the
preoperative diagnosis of kidney cancer due to its non-invasive nature and
ability to characterize renal lesions. However, since enhancement patterns of
renal lesions across CT phases are different even for the same lesion type, the
visual assessment by radiologists suffers from inter-observer variability in
clinical practice. Although deep learning-based approaches have been recently
explored for differential diagnosis of kidney cancer, they do not explicitly
model the relationships between CT phases in the network design, limiting the
diagnostic performance. In this paper, we propose a novel lesion-aware
cross-phase attention network (LACPANet) that can effectively capture temporal
dependencies of renal lesions across CT phases to accurately classify the
lesions into five major pathological subtypes from time-series multi-phase CT
images. We introduce a 3D inter-phase lesion-aware attention mechanism to learn
effective 3D lesion features that are used to estimate attention weights
describing the inter-phase relations of the enhancement patterns. We also
present a multi-scale attention scheme to capture and aggregate temporal
patterns of lesion features at different spatial scales for further
improvement. Extensive experiments on multi-phase CT scans of kidney cancer
patients from the collected dataset demonstrate that our LACPANet outperforms
state-of-the-art approaches in diagnostic accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article has been accepted for publication in Computers in
  Biology and Medicine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Priorformer: A UGC-VQA Method with content and distortion priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yajing Pei, Shiyu Huang, Yiting Lu, Xin Li, Zhibo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User Generated Content (UGC) videos are susceptible to complicated and
variant degradations and contents, which prevents the existing blind video
quality assessment (BVQA) models from good performance since the lack of the
adapability of distortions and contents. To mitigate this, we propose a novel
prior-augmented perceptual vision transformer (PriorFormer) for the BVQA of
UGC, which boots its adaptability and representation capability for divergent
contents and distortions. Concretely, we introduce two powerful priors, i.e.,
the content and distortion priors, by extracting the content and distortion
embeddings from two pre-trained feature extractors. Then we adopt these two
powerful embeddings as the adaptive prior tokens, which are transferred to the
vision transformer backbone jointly with implicit quality features. Based on
the above strategy, the proposed PriorFormer achieves state-of-the-art
performance on three public UGC VQA datasets including KoNViD-1K, LIVE-VQC and
YouTube-UGC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coordinate-based neural representations for computational adaptive
  optics in widefield microscopy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03812v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03812v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iksung Kang, Qinrong Zhang, Stella X. Yu, Na Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Widefield microscopy is widely used for non-invasive imaging of biological
structures at subcellular resolution. When applied to complex specimen, its
image quality is degraded by sample-induced optical aberration. Adaptive optics
can correct wavefront distortion and restore diffraction-limited resolution but
require wavefront sensing and corrective devices, increasing system complexity
and cost. Here, we describe a self-supervised machine learning algorithm,
CoCoA, that performs joint wavefront estimation and three-dimensional
structural information extraction from a single input 3D image stack without
the need for external training dataset. We implemented CoCoA for widefield
imaging of mouse brain tissues and validated its performance with
direct-wavefront-sensing-based adaptive optics. Importantly, we systematically
explored and quantitatively characterized the limiting factors of CoCoA's
performance. Using CoCoA, we demonstrated the first in vivo widefield mouse
brain imaging using machine-learning-based adaptive optics. Incorporating
coordinate-based neural representations and a forward physics model, the
self-supervised scheme of CoCoA should be applicable to microscopy modalities
in general.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>60 pages, 20 figures, 2 tables. Nat Mach Intell (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UCM-Net: A Lightweight and Efficient Solution for Skin Lesion
  Segmentation using MLP and CNN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09457v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09457v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunyu Yuan, Dongfang Zhao, Sos S. Agaian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skin cancer poses a significant public health challenge, necessitating
efficient diagnostic tools. We introduce UCM-Net, a novel skin lesion
segmentation model combining Multi-Layer Perceptrons (MLP) and Convolutional
Neural Networks (CNN). This lightweight, efficient architecture, deviating from
traditional UNet designs, dramatically reduces computational demands, making it
ideal for mobile health applications. Evaluated on PH2, ISIC 2017, and ISIC
2018 datasets, UCM-Net demonstrates robust performance with fewer than 50KB
parameters and requires less than 0.05 Giga Operations Per Second (GLOPs).
Moreover, its minimal memory requirement is just 1.19MB in CPU environment
positions. It is a potential benchmark for efficiency in skin lesion
segmentation, suitable for deployment in resource-constrained settings. In
order to facilitate accessibility and further research in the field, the
UCM-Net source code is https://github.com/chunyuyuan/UCM-Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, accepted by Journal of Biomedical Signal Processing and
  Control</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MRISegmentator-Abdomen: A Fully Automated Multi-Organ and Structure
  Segmentation Tool for T1-weighted Abdominal MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05944v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05944v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Zhuang, Tejas Sudharshan Mathai, Pritam Mukherjee, Brandon Khoury, Boah Kim, Benjamin Hou, Nusrat Rabbee, Abhinav Suri, Ronald M. Summers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Segmentation of organs and structures in abdominal MRI is useful
for many clinical applications, such as disease diagnosis and radiotherapy.
Current approaches have focused on delineating a limited set of abdominal
structures (13 types). To date, there is no publicly available abdominal MRI
dataset with voxel-level annotations of multiple organs and structures.
Consequently, a segmentation tool for multi-structure segmentation is also
unavailable. Methods: We curated a T1-weighted abdominal MRI dataset consisting
of 195 patients who underwent imaging at National Institutes of Health (NIH)
Clinical Center. The dataset comprises of axial pre-contrast T1, arterial,
venous, and delayed phases for each patient, thereby amounting to a total of
780 series (69,248 2D slices). Each series contains voxel-level annotations of
62 abdominal organs and structures. A 3D nnUNet model, dubbed as
MRISegmentator-Abdomen (MRISegmentator in short), was trained on this dataset,
and evaluation was conducted on an internal test set and two large external
datasets: AMOS22 and Duke Liver. The predicted segmentations were compared
against the ground-truth using the Dice Similarity Coefficient (DSC) and
Normalized Surface Distance (NSD). Findings: MRISegmentator achieved an average
DSC of 0.861$\pm$0.170 and a NSD of 0.924$\pm$0.163 in the internal test set.
On the AMOS22 dataset, MRISegmentator attained an average DSC of
0.829$\pm$0.133 and a NSD of 0.908$\pm$0.067. For the Duke Liver dataset, an
average DSC of 0.933$\pm$0.015 and a NSD of 0.929$\pm$0.021 was obtained.
Interpretation: The proposed MRISegmentator provides automatic, accurate, and
robust segmentations of 62 organs and structures in T1-weighted abdominal MRI
sequences. The tool has the potential to accelerate research on various
clinical topics, such as abnormality detection, radiotherapy, disease
classification among others.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We made the segmentation model publicly available</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QUBIQ: Uncertainty Quantification for Biomedical Image Segmentation
  Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18435v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18435v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongwei Bran Li, Fernando Navarro, Ivan Ezhov, Amirhossein Bayat, Dhritiman Das, Florian Kofler, Suprosanna Shit, Diana Waldmannstetter, Johannes C. Paetzold, Xiaobin Hu, Benedikt Wiestler, Lucas Zimmer, Tamaz Amiranashvili, Chinmay Prabhakar, Christoph Berger, Jonas Weidner, Michelle Alonso-Basant, Arif Rashid, Ujjwal Baid, Wesam Adel, Deniz Ali, Bhakti Baheti, Yingbin Bai, Ishaan Bhatt, Sabri Can Cetindag, Wenting Chen, Li Cheng, Prasad Dutand, Lara Dular, Mustafa A. Elattar, Ming Feng, Shengbo Gao, Henkjan Huisman, Weifeng Hu, Shubham Innani, Wei Jiat, Davood Karimi, Hugo J. Kuijf, Jin Tae Kwak, Hoang Long Le, Xiang Lia, Huiyan Lin, Tongliang Liu, Jun Ma, Kai Ma, Ting Ma, Ilkay Oksuz, Robbie Holland, Arlindo L. Oliveira, Jimut Bahan Pal, Xuan Pei, Maoying Qiao, Anindo Saha, Raghavendra Selvan, Linlin Shen, Joao Lourenco Silva, Ziga Spiclin, Sanjay Talbar, Dadong Wang, Wei Wang, Xiong Wang, Yin Wang, Ruiling Xia, Kele Xu, Yanwu Yan, Mert Yergin, Shuang Yu, Lingxi Zeng, YingLin Zhang, Jiachen Zhao, Yefeng Zheng, Martin Zukovec, Richard Do, Anton Becker, Amber Simpson, Ender Konukoglu, Andras Jakab, Spyridon Bakas, Leo Joskowicz, Bjoern Menze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty in medical image segmentation tasks, especially inter-rater
variability, arising from differences in interpretations and annotations by
various experts, presents a significant challenge in achieving consistent and
reliable image segmentation. This variability not only reflects the inherent
complexity and subjective nature of medical image interpretation but also
directly impacts the development and evaluation of automated segmentation
algorithms. Accurately modeling and quantifying this variability is essential
for enhancing the robustness and clinical applicability of these algorithms. We
report the set-up and summarize the benchmark results of the Quantification of
Uncertainties in Biomedical Image Quantification Challenge (QUBIQ), which was
organized in conjunction with International Conferences on Medical Image
Computing and Computer-Assisted Intervention (MICCAI) 2020 and 2021. The
challenge focuses on the uncertainty quantification of medical image
segmentation which considers the omnipresence of inter-rater variability in
imaging datasets. The large collection of images with multi-rater annotations
features various modalities such as MRI and CT; various organs such as the
brain, prostate, kidney, and pancreas; and different image dimensions 2D-vs-3D.
A total of 24 teams submitted different solutions to the problem, combining
various baseline models, Bayesian neural networks, and ensemble model
techniques. The obtained results indicate the importance of the ensemble
models, as well as the need for further research to develop efficient 3D
methods for uncertainty quantification methods in 3D segmentation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>initial technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-Informed Deep Learning for Motion-Corrected Reconstruction of
  Quantitative Brain MRI <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08298v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08298v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Eichhorn, Veronika Spieker, Kerstin Hammernik, Elisa Saks, Kilian Weiss, Christine Preibisch, Julia A. Schnabel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose PHIMO, a physics-informed learning-based motion correction method
tailored to quantitative MRI. PHIMO leverages information from the signal
evolution to exclude motion-corrupted k-space lines from a data-consistent
reconstruction. We demonstrate the potential of PHIMO for the application of
T2* quantification from gradient echo MRI, which is particularly sensitive to
motion due to its sensitivity to magnetic field inhomogeneities. A
state-of-the-art technique for motion correction requires redundant acquisition
of the k-space center, prolonging the acquisition. We show that PHIMO can
detect and exclude intra-scan motion events and, thus, correct for severe
motion artifacts. PHIMO approaches the performance of the state-of-the-art
motion correction method, while substantially reducing the acquisition time by
over 40%, facilitating clinical applicability. Our code is available at
https://github.com/HannahEichhorn/PHIMO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep-Learning Approach for Tissue Classification using Acoustic Waves
  during Ablation with an Er:YAG Laser (Updated) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14570v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14570v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlo Seppi, Philippe C. Cattin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's mechanical tools for bone cutting (osteotomy) cause mechanical trauma
that prolongs the healing process. Medical device manufacturers aim to minimize
this trauma, with minimally invasive surgery using laser cutting as one
innovation. This method ablates tissue using laser light instead of mechanical
tools, reducing post-surgery healing time. A reliable feedback system is
crucial during laser surgery to prevent damage to surrounding tissues. We
propose a tissue classification method analyzing acoustic waves generated
during laser ablation, demonstrating its applicability in an ex-vivo
experiment. The ablation process with a microsecond pulsed Er:YAG laser
produces acoustic waves, acquired with an air-coupled transducer. These waves
were used to classify five porcine tissue types: hard bone, soft bone, muscle,
fat, and skin. For automated tissue classification, we compared five Neural
Network (NN) approaches: a one-dimensional Convolutional Neural Network (CNN)
with time-dependent input, a Fully-connected Neural Network (FcNN) with either
the frequency spectrum or principal components of the frequency spectrum as
input, and a combination of a CNN and an FcNN with time-dependent data and its
frequency spectrum as input. Consecutive acoustic waves were used to improve
classification accuracy. Grad-Cam identified the activation map of the
frequencies, showing low frequencies as the most important for this task. Our
results indicated that combining time-dependent data with its frequency
spectrum achieved the highest classification accuracy (65.5%-75.5%). We also
found that using the frequency spectrum alone was sufficient, with no
additional benefit from applying Principal Components Analysis (PCA).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is an updated version of Deep-Learning Approach for Tissue
  Classification using Acoustic Waves during Ablation with an Er:YAG Laser
  originally published in DOI:10.1109/ACCESS.2021.3113055. This update
  addresses several issues and incorporates corrections as outlined in
  DOI:10.1109/ACCESS.2024.3395071. We provide here a detailed description of
  our experiments and the new models we used</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion-<span class="highlight-title">robust</span> free-running volumetric cardiovascular MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02088v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02088v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Syed M. Arshad, Lee C. Potter, Chong Chen, Yingmin Liu, Preethi Chandrasekaran, Christopher Crabtree, Matthew S. Tong, Orlando P. Simonetti, Yuchi Han, Rizwan Ahmad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PURPOSE: To present and assess an outlier mitigation method that makes
free-running volumetric cardiovascular MRI (CMR) more robust to motion.
  METHODS: The proposed method, called compressive recovery with outlier
rejection (CORe), models outliers in the measured data as an additive auxiliary
variable. We enforce MR physics-guided group sparsity on the auxiliary
variable, and jointly estimate it along with the image using an iterative
algorithm. For evaluation, CORe is first compared to traditional compressed
sensing (CS), robust regression (RR), and an existing outlier rejection method
using two simulation studies. Then, CORe is compared to CS using seven
three-dimensional (3D) cine, 12 rest four-dimensional (4D) flow, and eight
stress 4D flow imaging datasets.
  RESULTS: Our simulation studies show that CORe outperforms CS, RR, and the
existing outlier rejection method in terms of normalized mean square error and
structural similarity index across 55 different realizations. The expert reader
evaluation of 3D cine images demonstrates that CORe is more effective in
suppressing artifacts while maintaining or improving image sharpness. Finally,
4D flow images show that CORe yields more reliable and consistent flow
measurements, especially in the presence of involuntary subject motion or
exercise stress.
  CONCLUSION: An outlier rejection method is presented and tested using
simulated and measured data. This method can help suppress motion artifacts in
a wide range of free-running CMR applications.
  CODE & DATA: Implementation code and datasets are available on GitHub at
http://github.com/OSU-MR/motion-robust-CMR
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do High-Performance Image-to-Image Translation Networks Enable the
  Discovery of Radiomic Features? Application to MRI Synthesis from Ultrasound
  in Prostate Cancer <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18651v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18651v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad R. Salmanpour, Amin Mousavi, Yixi Xu, William B Weeks, Ilker Hacihaliloglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the foundational characteristics of image-to-image
translation networks, specifically examining their suitability and
transferability within the context of routine clinical environments, despite
achieving high levels of performance, as indicated by a Structural Similarity
Index (SSIM) exceeding 0.95. The evaluation study was conducted using data from
794 patients diagnosed with Prostate cancer. To synthesize MRI from Ultrasound
images, we employed five widely recognized image to image translation networks
in medical imaging: 2DPix2Pix, 2DCycleGAN, 3DCycleGAN, 3DUNET, and
3DAutoEncoder. For quantitative assessment, we report four prevalent evaluation
metrics Mean Absolute Error, Mean Square Error, Structural Similarity Index
(SSIM), and Peak Signal to Noise Ratio. Moreover, a complementary analysis
employing Radiomic features (RF) via Spearman correlation coefficient was
conducted to investigate, for the first time, whether networks achieving high
performance, SSIM greater than 0.85, could identify low-level RFs. The RF
analysis showed 75 features out of 186 RFs were discovered via just 2DPix2Pix
algorithm while half of RFs were lost in the translation process. Finally, a
detailed qualitative assessment by five medical doctors indicated a lack of low
level feature discovery in image to image translation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-06-23T00:00:00Z">2024-06-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Image and Video Processing <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reducing the Sampling Burden of Fourier Sensing with a Non-rectangular
  Field-of-View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Dwork, Erin K. Englund, Alex J. Barker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With Fourier sensing, it is commonly the case that the field-of-view (FOV),
the area of space to be imaged, is known prior to reconstruction. To date,
reconstruction algorithms have focused on FOVs with simple geometries: a
rectangle or a hexagon. This yields sampling patterns that are more burdensome
than necessary. Due to the reduced area of imaging possible with an arbitrary
(e.g., non-rectangular) FOV, the number of samples required for a high-quality
images is reduced. However, when an arbitrary FOV has been considered, the
reconstruction algorithm is computationally expensive. In this manuscript, we
present a method to reduce the sampling pattern for an arbitrary FOV with an
accompanying direct (non-iterative) reconstruction algorithm. We also present a
method to decrease the computational cost of the (iterative) model-based
reconstruction (MBR) algorithm. We present results using MRI data of an ankle,
a pineapple, and a brain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Instabilities of Unsupervised Denoising <span class="highlight-title">Diffusion</span> Models in Magnetic
  Resonance Imaging Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Han, Sven Nebelung, Firas Khader, Jakob Nikolas Kather, Daniel Truhn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising diffusion models offer a promising approach to accelerating
magnetic resonance imaging (MRI) and producing diagnostic-level images in an
unsupervised manner. However, our study demonstrates that even tiny worst-case
potential perturbations transferred from a surrogate model can cause these
models to generate fake tissue structures that may mislead clinicians. The
transferability of such worst-case perturbations indicates that the robustness
of image reconstruction may be compromised due to MR system imperfections or
other sources of noise. Moreover, at larger perturbation strengths, diffusion
models exhibit Gaussian noise-like artifacts that are distinct from those
observed in supervised models and are more challenging to detect. Our results
highlight the vulnerability of current state-of-the-art diffusion-based
reconstruction models to possible worst-case perturbations and underscore the
need for further research to improve their robustness and reliability in
clinical settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fuzzy Attention-based Border Rendering Network for Lung Organ
  Segmentation <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Zhang, Yang Nan, Yingying Fang, Shiyi Wang, Xiaodan Xing, Zhifan Gao, Guang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic lung organ segmentation on CT images is crucial for lung disease
diagnosis. However, the unlimited voxel values and class imbalance of lung
organs can lead to false-negative/positive and leakage issues in advanced
methods. Additionally, some slender lung organs are easily lost during the
recycled down/up-sample procedure, e.g., bronchioles & arterioles, causing
severe discontinuity issue. Inspired by these, this paper introduces an
effective lung organ segmentation method called Fuzzy Attention-based Border
Rendering (FABR) network. Since fuzzy logic can handle the uncertainty in
feature extraction, hence the fusion of deep networks and fuzzy sets should be
a viable solution for better performance. Meanwhile, unlike prior top-tier
methods that operate on all regular dense points, our FABR depicts lung organ
regions as cube-trees, focusing only on recycle-sampled border vulnerable
points, rendering the severely discontinuous, false-negative/positive organ
regions with a novel Global-Local Cube-tree Fusion (GLCF) module. All
experimental results, on four challenging datasets of airway & artery,
demonstrate that our method can achieve the favorable performance
significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Research on Feature Extraction Data Processing System For MRI of Brain
  Diseases Based on Computer Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingxi Xiao, Jinxin Hu, Yutian Yang, Yinqiu Feng, Zichao Li, Zexi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of the existing wavelet image processing techniques are carried out in
the form of single-scale reconstruction and multiple iterations. However,
processing high-quality fMRI data presents problems such as mixed noise and
excessive computation time. This project proposes the use of matrix operations
by combining mixed noise elimination methods with wavelet analysis to replace
traditional iterative algorithms. Functional magnetic resonance imaging (fMRI)
of the auditory cortex of a single subject is analyzed and compared to the
wavelet domain signal processing technology based on repeated times and the
world's most influential SPM8. Experiments show that this algorithm is the
fastest in computing time, and its detection effect is comparable to the
traditional iterative algorithm. However, this has a higher practical value for
the processing of FMRI data. In addition, the wavelet analysis method proposed
signal processing to speed up the calculation rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intensity Confusion Matters: An Intensity-Distance Guided Loss for
  Bronchus Segmentation <span class="chip">ICME</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haifan Gong, Wenhao Huang, Huan Zhang, Yu Wang, Xiang Wan, Hong Shen, Guanbin Li, Haofeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic segmentation of the bronchial tree from CT imaging is important, as
it provides structural information for disease diagnosis. Despite the merits of
previous automatic bronchus segmentation methods, they have paied less
attention to the issue we term as \textit{Intensity Confusion}, wherein the
intensity values of certain background voxels approach those of the foreground
voxels within bronchi. Conversely, the intensity values of some foreground
voxels are nearly identical to those of background voxels. This proximity in
intensity values introduces significant challenges to neural network
methodologies. To address the issue, we introduce a novel Intensity-Distance
Guided loss function, which assigns adaptive weights to different image voxels
for mining hard samples that cause the intensity confusion. The proposed loss
estimates the voxel-level hardness of samples, on the basis of the following
intensity and distance priors. We regard a voxel as a hard sample if it is in:
(1) the background and has an intensity value close to the bronchus region; (2)
the bronchus region and is of higher intensity than most voxels inside the
bronchus; (3) the background region and at a short distance from the bronchus.
Extensive experiments not only show the superiority of our method compared with
the state-of-the-art methods, but also verify that tackling the intensity
confusion issue helps to significantly improve bronchus segmentation. Project
page: https://github.com/lhaof/ICM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE International Conference on Multimedia & Expo (ICME) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mamba-based Light Field Super-Resolution with Efficient Subspace
  Scanning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruisheng Gao, Zeyu Xiao, Zhiwei Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based methods have demonstrated impressive performance in 4D
light field (LF) super-resolution by effectively modeling long-range
spatial-angular correlations, but their quadratic complexity hinders the
efficient processing of high resolution 4D inputs, resulting in slow inference
speed and high memory cost. As a compromise, most prior work adopts a
patch-based strategy, which fails to leverage the full information from the
entire input LFs. The recently proposed selective state-space model, Mamba, has
gained popularity for its efficient long-range sequence modeling. In this
paper, we propose a Mamba-based Light Field Super-Resolution method, named
MLFSR, by designing an efficient subspace scanning strategy. Specifically, we
tokenize 4D LFs into subspace sequences and conduct bi-directional scanning on
each subspace. Based on our scanning strategy, we then design the Mamba-based
Global Interaction (MGI) module to capture global information and the local
Spatial- Angular Modulator (SAM) to complement local details. Additionally, we
introduce a Transformer-to-Mamba (T2M) loss to further enhance overall
performance. Extensive experiments on public benchmarks demonstrate that MLFSR
surpasses CNN-based models and rivals Transformer-based methods in performance
while maintaining higher efficiency. With quicker inference speed and reduced
memory demand, MLFSR facilitates full-image processing of high-resolution 4D
LFs with enhanced performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages,7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAVM: Conditional Autoregressive Vision Model for Contrast-Enhanced
  Brain Tumor MRI Synthesis <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lujun Gui, Chuyang Ye, Tianyi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrast-enhanced magnetic resonance imaging (MRI) is pivotal in the pipeline
of brain tumor segmentation and analysis. Gadolinium-based contrast agents, as
the most commonly used contrast agents, are expensive and may have potential
side effects, and it is desired to obtain contrast-enhanced brain tumor MRI
scans without the actual use of contrast agents. Deep learning methods have
been applied to synthesize virtual contrast-enhanced MRI scans from
non-contrast images. However, as this synthesis problem is inherently
ill-posed, these methods fall short in producing high-quality results. In this
work, we propose Conditional Autoregressive Vision Model (CAVM) for improving
the synthesis of contrast-enhanced brain tumor MRI. As the enhancement of image
intensity grows with a higher dose of contrast agents, we assume that it is
less challenging to synthesize a virtual image with a lower dose, where the
difference between the contrast-enhanced and non-contrast images is smaller.
Thus, CAVM gradually increases the contrast agent dosage and produces
higher-dose images based on previous lower-dose ones until the final desired
dose is achieved. Inspired by the resemblance between the gradual dose increase
and the Chain-of-Thought approach in natural language processing, CAVM uses an
autoregressive strategy with a decomposition tokenizer and a decoder.
Specifically, the tokenizer is applied to obtain a more compact image
representation for computational efficiency, and it decomposes the image into
dose-variant and dose-invariant tokens. Then, a masked self-attention mechanism
is developed for autoregression that gradually increases the dose of the
virtual image based on the dose-variant tokens. Finally, the updated
dose-variant tokens corresponding to the desired dose are decoded together with
dose-invariant tokens to produce the final contrast-enhanced MRI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The work has been accepted by MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wound Tissue Segmentation in Diabetic Foot Ulcer Images Using Deep
  Learning: A Pilot Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mrinal Kanti Dhar, Chuanbo Wang, Yash Patel, Taiyu Zhang, Jeffrey Niezgoda, Sandeep Gopalakrishnan, Keke Chen, Zeyun Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying individual tissues, so-called tissue segmentation, in diabetic
foot ulcer (DFU) images is a challenging task and little work has been
published, largely due to the limited availability of a clinical image dataset.
To address this gap, we have created a DFUTissue dataset for the research
community to evaluate wound tissue segmentation algorithms. The dataset
contains 110 images with tissues labeled by wound experts and 600 unlabeled
images. Additionally, we conducted a pilot study on segmenting wound
characteristics including fibrin, granulation, and callus using deep learning.
Due to the limited amount of annotated data, our framework consists of both
supervised learning (SL) and semi-supervised learning (SSL) phases. In the SL
phase, we propose a hybrid model featuring a Mix Transformer (MiT-b3) in the
encoder and a CNN in the decoder, enhanced by the integration of a parallel
spatial and channel squeeze-and-excitation (P-scSE) module known for its
efficacy in improving boundary accuracy. The SSL phase employs a
pseudo-labeling-based approach, iteratively identifying and incorporating
valuable unlabeled images to enhance overall segmentation performance.
Comparative evaluations with state-of-the-art methods are conducted for both SL
and SSL phases. The SL achieves a Dice Similarity Coefficient (DSC) of 84.89%,
which has been improved to 87.64% in the SSL phase. Furthermore, the results
are benchmarked against two widely used SSL approaches: Generative Adversarial
Networks and Cross-Consistency Training. Additionally, our hybrid model
outperforms the state-of-the-art methods with a 92.99% DSC in performing binary
segmentation of DFU wound areas when tested on the Chronic Wound dataset. Codes
and data are available at https://github.com/uwm-bigdata/DFUTissueSegNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning Segmentation of Ascites on Abdominal CT Scans for
  Automatic Volume Quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Hou, Sung-Won Lee, Jung-Min Lee, Christopher Koh, Jing Xiao, Perry J. Pickhardt, Ronald M. Summers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: To evaluate the performance of an automated deep learning method in
detecting ascites and subsequently quantifying its volume in patients with
liver cirrhosis and ovarian cancer.
  Materials and Methods: This retrospective study included contrast-enhanced
and non-contrast abdominal-pelvic CT scans of patients with cirrhotic ascites
and patients with ovarian cancer from two institutions, National Institutes of
Health (NIH) and University of Wisconsin (UofW). The model, trained on The
Cancer Genome Atlas Ovarian Cancer dataset (mean age, 60 years +/- 11 [s.d.];
143 female), was tested on two internal (NIH-LC and NIH-OV) and one external
dataset (UofW-LC). Its performance was measured by the Dice coefficient,
standard deviations, and 95% confidence intervals, focusing on ascites volume
in the peritoneal cavity.
  Results: On NIH-LC (25 patients; mean age, 59 years +/- 14 [s.d.]; 14 male)
and NIH-OV (166 patients; mean age, 65 years +/- 9 [s.d.]; all female), the
model achieved Dice scores of 0.855 +/- 0.061 (CI: 0.831-0.878) and 0.826 +/-
0.153 (CI: 0.764-0.887), with median volume estimation errors of 19.6% (IQR:
13.2-29.0) and 5.3% (IQR: 2.4-9.7) respectively. On UofW-LC (124 patients; mean
age, 46 years +/- 12 [s.d.]; 73 female), the model had a Dice score of 0.830
+/- 0.107 (CI: 0.798-0.863) and median volume estimation error of 9.7% (IQR:
4.5-15.1). The model showed strong agreement with expert assessments, with r^2
values of 0.79, 0.98, and 0.97 across the test sets.
  Conclusion: The proposed deep learning method performed well in segmenting
and quantifying the volume of ascites in concordance with expert radiologist
assessments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Latents for Efficient Thermography Classification and
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06589v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06589v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tamir Shor, Chaim Baskin, Alex Bronstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer is a prominent health concern worldwide, currently being the
secondmost common and second-deadliest type of cancer in women. While current
breast cancer diagnosis mainly relies on mammography imaging, in recent years
the use of thermography for breast cancer imaging has been garnering growing
popularity. Thermographic imaging relies on infrared cameras to capture
body-emitted heat distributions. While these heat signatures have proven useful
for computer-vision systems for accurate breast cancer segmentation and
classification, prior work often relies on handcrafted feature engineering or
complex architectures, potentially limiting the comparability and applicability
of these methods. In this work, we present a novel algorithm for both breast
cancer classification and segmentation. Rather than focusing efforts on manual
feature and architecture engineering, our algorithm focuses on leveraging an
informative, learned feature space, thus making our solution simpler to use and
extend to other frameworks and downstream tasks, as well as more applicable to
data-scarce settings. Our classification produces SOTA results, while we are
the first work to produce segmentation regions studied in this paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiview Contrastive Learning for Completely Blind Video Quality
  Assessment of User Generated Content 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06148v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06148v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shankhanil Mitra, Rajiv Soundararajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Completely blind video quality assessment (VQA) refers to a class of quality
assessment methods that do not use any reference videos, human opinion scores
or training videos from the target database to learn a quality model. The
design of this class of methods is particularly important since it can allow
for superior generalization in performance across various datasets. We consider
the design of completely blind VQA for user generated content. While several
deep feature extraction methods have been considered in supervised and weakly
supervised settings, such approaches have not been studied in the context of
completely blind VQA. We bridge this gap by presenting a self-supervised
multiview contrastive learning framework to learn spatio-temporal quality
representations. In particular, we capture the common information between frame
differences and frames by treating them as a pair of views and similarly obtain
the shared representations between frame differences and optical flow. The
resulting features are then compared with a corpus of pristine natural video
patches to predict the quality of the distorted video. Detailed experiments on
multiple camera captured VQA datasets reveal the superior performance of our
method over other features when evaluated without training on human scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rate Splitting Multiple Access-Enabled Adaptive Panoramic Video Semantic
  Transmission 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haixiao Gao, Mengying Sun, Xiaodong Xu, Shujun Han, Bizhu Wang, Jingxuan Zhang, Ping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose an adaptive panoramic video semantic transmission
(APVST) framework enabled by rate splitting multiple access (RSMA). The APVST
framework consists of a semantic transmitter and receiver, utilizing a deep
joint source-channel coding structure to adaptively extract and encode semantic
features from panoramic frames. To achieve higher spectral efficiency and
conserve bandwidth, APVST employs an entropy model and a dimension-adaptive
module to control the transmission rate. Additionally, we take
weighted-to-spherically-uniform peak signal-to-noise ratio (WS-PSNR) and
weighted-to-spherically-uniform structural similarity (WS-SSIM) as distortion
evaluation metrics for panoramic videos and design a weighted self-attention
module for APVST. This module integrates weights and feature maps to enhance
the quality of the immersive experience. Considering the overlap in the field
of view when users watch panoramic videos, we further utilize RSMA to split the
required panoramic video semantic streams into common and private messages for
transmission. We propose an RSMA-enabled semantic stream transmission scheme
and formulate a joint problem of latency and immersive experience quality by
optimizing the allocation ratios of power, common rate, and channel bandwidth,
aiming to maximize the quality of service (QoS) scores for users. To address
the above problem, we propose a deep reinforcement learning algorithm based on
proximal policy optimization (PPO) with high efficiency to handle dynamically
changing environments. Simulation results demonstrate that our proposed APVST
framework saves up to 20% and 50% of channel bandwidth compared to other
semantic and traditional video transmission schemes, respectively. Moreover,
our study confirms the efficiency of RSMA in panoramic video transmission,
achieving performance gains of 13% and 20% compared to NOMA and OFDMA.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-07-01T05:28:16.867817229Z">
            2024-07-01 05:28:16 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
